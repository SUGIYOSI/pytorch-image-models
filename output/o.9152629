--Start--
Mon May 24 23:59:55 JST 2021
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 3
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
wandb: Currently logged in as: gyama_x (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.10.30 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.10.27
wandb: Resuming run PreTraining_vit_deit_tiny_patch16_224_fake_1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gyama_x/pytorch-image-models
wandb: üöÄ View run at https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run data is saved locally in /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210525_000123-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run `wandb offline` to turn off syncing.

Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar' (epoch 21)
Using native Torch DistributedDataParallel.
Scheduled epochs: 44
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Train: 22 [   0/1171 (  0%)]  Loss:  3.543956 (3.5440)  Time: 9.099s,  112.55/s  (9.099s,  112.55/s)  LR: 5.050e-04  Data: 8.044 (8.044)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 22 [  50/1171 (  4%)]  Loss:  3.755325 (3.6496)  Time: 1.180s,  867.78/s  (2.582s,  396.63/s)  LR: 5.050e-04  Data: 0.611 (1.976)
Train: 22 [ 100/1171 (  9%)]  Loss:  3.793118 (3.6975)  Time: 0.584s, 1753.16/s  (2.582s,  396.64/s)  LR: 5.050e-04  Data: 0.019 (1.983)
Train: 22 [ 150/1171 ( 13%)]  Loss:  3.236333 (3.5822)  Time: 0.589s, 1738.12/s  (2.502s,  409.23/s)  LR: 5.050e-04  Data: 0.016 (1.909)
Train: 22 [ 200/1171 ( 17%)]  Loss:  3.331998 (3.5321)  Time: 0.584s, 1753.17/s  (2.463s,  415.75/s)  LR: 5.050e-04  Data: 0.018 (1.872)
Train: 22 [ 250/1171 ( 21%)]  Loss:  3.045639 (3.4511)  Time: 0.587s, 1744.29/s  (2.409s,  425.00/s)  LR: 5.050e-04  Data: 0.017 (1.820)
Train: 22 [ 300/1171 ( 26%)]  Loss:  3.855144 (3.5088)  Time: 0.584s, 1754.03/s  (2.393s,  427.89/s)  LR: 5.050e-04  Data: 0.020 (1.802)
Train: 22 [ 350/1171 ( 30%)]  Loss:  3.221296 (3.4729)  Time: 0.582s, 1759.47/s  (2.411s,  424.77/s)  LR: 5.050e-04  Data: 0.017 (1.816)
Train: 22 [ 400/1171 ( 34%)]  Loss:  3.860159 (3.5159)  Time: 0.586s, 1748.49/s  (2.420s,  423.17/s)  LR: 5.050e-04  Data: 0.021 (1.824)
Train: 22 [ 450/1171 ( 38%)]  Loss:  3.260181 (3.4903)  Time: 0.585s, 1751.64/s  (2.409s,  425.11/s)  LR: 5.050e-04  Data: 0.020 (1.815)
Train: 22 [ 500/1171 ( 43%)]  Loss:  3.315134 (3.4744)  Time: 0.584s, 1754.91/s  (2.404s,  425.88/s)  LR: 5.050e-04  Data: 0.018 (1.811)
Train: 22 [ 550/1171 ( 47%)]  Loss:  3.404504 (3.4686)  Time: 0.584s, 1754.00/s  (2.397s,  427.22/s)  LR: 5.050e-04  Data: 0.019 (1.803)
Train: 22 [ 600/1171 ( 51%)]  Loss:  4.289023 (3.5317)  Time: 0.585s, 1749.71/s  (2.397s,  427.18/s)  LR: 5.050e-04  Data: 0.018 (1.803)
Train: 22 [ 650/1171 ( 56%)]  Loss:  3.330777 (3.5173)  Time: 0.582s, 1759.83/s  (2.383s,  429.79/s)  LR: 5.050e-04  Data: 0.019 (1.789)
Train: 22 [ 700/1171 ( 60%)]  Loss:  3.592668 (3.5224)  Time: 0.583s, 1757.28/s  (2.409s,  425.13/s)  LR: 5.050e-04  Data: 0.016 (1.815)
Train: 22 [ 750/1171 ( 64%)]  Loss:  3.937314 (3.5483)  Time: 0.587s, 1743.05/s  (2.425s,  422.29/s)  LR: 5.050e-04  Data: 0.021 (1.831)
Train: 22 [ 800/1171 ( 68%)]  Loss:  3.395123 (3.5393)  Time: 0.582s, 1760.64/s  (2.453s,  417.43/s)  LR: 5.050e-04  Data: 0.020 (1.859)
Train: 22 [ 850/1171 ( 73%)]  Loss:  3.542359 (3.5394)  Time: 0.585s, 1749.63/s  (2.461s,  416.02/s)  LR: 5.050e-04  Data: 0.018 (1.868)
Train: 22 [ 900/1171 ( 77%)]  Loss:  3.909080 (3.5589)  Time: 0.582s, 1760.59/s  (2.473s,  414.15/s)  LR: 5.050e-04  Data: 0.018 (1.880)
Train: 22 [ 950/1171 ( 81%)]  Loss:  3.320265 (3.5470)  Time: 0.587s, 1743.42/s  (2.474s,  413.95/s)  LR: 5.050e-04  Data: 0.024 (1.881)
Train: 22 [1000/1171 ( 85%)]  Loss:  2.975311 (3.5197)  Time: 0.588s, 1741.64/s  (2.479s,  413.01/s)  LR: 5.050e-04  Data: 0.018 (1.886)
Train: 22 [1050/1171 ( 90%)]  Loss:  3.615705 (3.5241)  Time: 9.274s,  110.41/s  (2.497s,  410.06/s)  LR: 5.050e-04  Data: 8.701 (1.903)
Train: 22 [1100/1171 ( 94%)]  Loss:  3.575641 (3.5264)  Time: 0.582s, 1758.94/s  (2.506s,  408.58/s)  LR: 5.050e-04  Data: 0.017 (1.912)
Train: 22 [1150/1171 ( 98%)]  Loss:  3.070035 (3.5073)  Time: 8.222s,  124.55/s  (2.514s,  407.30/s)  LR: 5.050e-04  Data: 7.563 (1.920)
Train: 22 [1170/1171 (100%)]  Loss:  3.564920 (3.5096)  Time: 0.564s, 1816.04/s  (2.509s,  408.16/s)  LR: 5.050e-04  Data: 0.000 (1.915)
Test: [   0/97]  Time: 16.076 (16.076)  Loss:  0.4755 (0.4755)  Acc@1: 91.4062 (91.4062)  Acc@5: 99.3164 (99.3164)
Test: [  50/97]  Time: 0.194 (3.439)  Loss:  0.8178 (0.6262)  Acc@1: 84.3750 (89.0644)  Acc@5: 96.4844 (97.9243)
Test: [  97/97]  Time: 0.466 (3.301)  Loss:  0.5450 (0.6352)  Acc@1: 90.9226 (88.8830)  Acc@5: 97.9167 (97.4100)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-22.pth.tar', 88.88300002441406)

Train: 23 [   0/1171 (  0%)]  Loss:  3.550431 (3.5504)  Time: 6.460s,  158.52/s  (6.460s,  158.52/s)  LR: 4.697e-04  Data: 5.384 (5.384)
Train: 23 [  50/1171 (  4%)]  Loss:  3.959191 (3.7548)  Time: 0.588s, 1742.72/s  (2.276s,  449.89/s)  LR: 4.697e-04  Data: 0.022 (1.677)
Train: 23 [ 100/1171 (  9%)]  Loss:  3.700500 (3.7367)  Time: 5.030s,  203.59/s  (2.500s,  409.62/s)  LR: 4.697e-04  Data: 4.466 (1.901)
Train: 23 [ 150/1171 ( 13%)]  Loss:  3.455673 (3.6664)  Time: 0.583s, 1757.91/s  (2.533s,  404.30/s)  LR: 4.697e-04  Data: 0.019 (1.932)
Train: 23 [ 200/1171 ( 17%)]  Loss:  3.336636 (3.6005)  Time: 7.667s,  133.56/s  (2.561s,  399.85/s)  LR: 4.697e-04  Data: 7.090 (1.964)
Train: 23 [ 250/1171 ( 21%)]  Loss:  3.159802 (3.5270)  Time: 0.583s, 1755.02/s  (2.544s,  402.49/s)  LR: 4.697e-04  Data: 0.020 (1.946)
Train: 23 [ 300/1171 ( 26%)]  Loss:  3.366245 (3.5041)  Time: 5.165s,  198.26/s  (2.544s,  402.50/s)  LR: 4.697e-04  Data: 4.578 (1.942)
Train: 23 [ 350/1171 ( 30%)]  Loss:  3.402457 (3.4914)  Time: 0.583s, 1757.09/s  (2.515s,  407.11/s)  LR: 4.697e-04  Data: 0.018 (1.908)
Train: 23 [ 400/1171 ( 34%)]  Loss:  3.964601 (3.5439)  Time: 2.935s,  348.85/s  (2.505s,  408.86/s)  LR: 4.697e-04  Data: 2.373 (1.898)
Train: 23 [ 450/1171 ( 38%)]  Loss:  3.264648 (3.5160)  Time: 0.582s, 1758.31/s  (2.529s,  404.97/s)  LR: 4.697e-04  Data: 0.020 (1.921)
Train: 23 [ 500/1171 ( 43%)]  Loss:  3.613472 (3.5249)  Time: 5.349s,  191.42/s  (2.551s,  401.38/s)  LR: 4.697e-04  Data: 4.688 (1.943)
Train: 23 [ 550/1171 ( 47%)]  Loss:  3.422512 (3.5163)  Time: 0.583s, 1757.91/s  (2.567s,  398.86/s)  LR: 4.697e-04  Data: 0.020 (1.960)
Train: 23 [ 600/1171 ( 51%)]  Loss:  3.797003 (3.5379)  Time: 5.649s,  181.28/s  (2.579s,  397.10/s)  LR: 4.697e-04  Data: 5.074 (1.972)
Train: 23 [ 650/1171 ( 56%)]  Loss:  3.686024 (3.5485)  Time: 0.586s, 1748.11/s  (2.577s,  397.32/s)  LR: 4.697e-04  Data: 0.021 (1.971)
Train: 23 [ 700/1171 ( 60%)]  Loss:  3.408596 (3.5392)  Time: 4.782s,  214.12/s  (2.572s,  398.21/s)  LR: 4.697e-04  Data: 4.177 (1.967)
Train: 23 [ 750/1171 ( 64%)]  Loss:  3.398891 (3.5304)  Time: 0.581s, 1763.44/s  (2.563s,  399.53/s)  LR: 4.697e-04  Data: 0.018 (1.957)
Train: 23 [ 800/1171 ( 68%)]  Loss:  3.131100 (3.5069)  Time: 0.587s, 1745.24/s  (2.572s,  398.17/s)  LR: 4.697e-04  Data: 0.020 (1.968)
Train: 23 [ 850/1171 ( 73%)]  Loss:  3.585418 (3.5113)  Time: 0.582s, 1760.32/s  (2.581s,  396.78/s)  LR: 4.697e-04  Data: 0.018 (1.978)
Train: 23 [ 900/1171 ( 77%)]  Loss:  3.641371 (3.5181)  Time: 0.583s, 1756.19/s  (2.577s,  397.32/s)  LR: 4.697e-04  Data: 0.021 (1.974)
Train: 23 [ 950/1171 ( 81%)]  Loss:  3.690991 (3.5268)  Time: 0.582s, 1760.01/s  (2.584s,  396.26/s)  LR: 4.697e-04  Data: 0.019 (1.982)
Train: 23 [1000/1171 ( 85%)]  Loss:  4.057557 (3.5521)  Time: 0.588s, 1740.35/s  (2.573s,  397.91/s)  LR: 4.697e-04  Data: 0.018 (1.972)
Train: 23 [1050/1171 ( 90%)]  Loss:  3.915714 (3.5686)  Time: 0.580s, 1765.01/s  (2.571s,  398.27/s)  LR: 4.697e-04  Data: 0.018 (1.970)
Train: 23 [1100/1171 ( 94%)]  Loss:  3.590771 (3.5695)  Time: 0.582s, 1758.16/s  (2.558s,  400.38/s)  LR: 4.697e-04  Data: 0.018 (1.958)
Train: 23 [1150/1171 ( 98%)]  Loss:  3.449994 (3.5646)  Time: 0.586s, 1747.02/s  (2.567s,  398.96/s)  LR: 4.697e-04  Data: 0.017 (1.965)
Train: 23 [1170/1171 (100%)]  Loss:  3.496288 (3.5618)  Time: 0.563s, 1820.12/s  (2.566s,  398.99/s)  LR: 4.697e-04  Data: 0.000 (1.965)
Test: [   0/97]  Time: 15.260 (15.260)  Loss:  0.4350 (0.4350)  Acc@1: 93.0664 (93.0664)  Acc@5: 99.3164 (99.3164)
Test: [  50/97]  Time: 0.197 (3.489)  Loss:  0.8284 (0.6010)  Acc@1: 83.7891 (89.1850)  Acc@5: 95.6055 (97.9052)
Test: [  97/97]  Time: 0.120 (3.300)  Loss:  0.4478 (0.6185)  Acc@1: 93.6012 (88.8670)  Acc@5: 98.6607 (97.3700)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-22.pth.tar', 88.88300002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-23.pth.tar', 88.8670000390625)

Train: 24 [   0/1171 (  0%)]  Loss:  3.428487 (3.4285)  Time: 11.731s,   87.29/s  (11.731s,   87.29/s)  LR: 4.346e-04  Data: 11.123 (11.123)
Train: 24 [  50/1171 (  4%)]  Loss:  3.366679 (3.3976)  Time: 0.581s, 1762.05/s  (2.546s,  402.24/s)  LR: 4.346e-04  Data: 0.018 (1.956)
Train: 24 [ 100/1171 (  9%)]  Loss:  4.015404 (3.6035)  Time: 3.599s,  284.52/s  (2.486s,  411.93/s)  LR: 4.346e-04  Data: 3.009 (1.890)
Train: 24 [ 150/1171 ( 13%)]  Loss:  2.853870 (3.4161)  Time: 0.582s, 1758.63/s  (2.421s,  422.92/s)  LR: 4.346e-04  Data: 0.019 (1.832)
Train: 24 [ 200/1171 ( 17%)]  Loss:  3.231912 (3.3793)  Time: 2.849s,  359.46/s  (2.483s,  412.36/s)  LR: 4.346e-04  Data: 2.193 (1.888)
Train: 24 [ 250/1171 ( 21%)]  Loss:  3.394581 (3.3818)  Time: 0.582s, 1760.63/s  (2.505s,  408.82/s)  LR: 4.346e-04  Data: 0.017 (1.908)
Train: 24 [ 300/1171 ( 26%)]  Loss:  3.062320 (3.3362)  Time: 3.557s,  287.91/s  (2.526s,  405.42/s)  LR: 4.346e-04  Data: 2.978 (1.924)
Train: 24 [ 350/1171 ( 30%)]  Loss:  3.380798 (3.3418)  Time: 0.921s, 1112.22/s  (2.510s,  407.90/s)  LR: 4.346e-04  Data: 0.019 (1.906)
Train: 24 [ 400/1171 ( 34%)]  Loss:  3.588062 (3.3691)  Time: 3.308s,  309.52/s  (2.520s,  406.41/s)  LR: 4.346e-04  Data: 2.746 (1.916)
Train: 24 [ 450/1171 ( 38%)]  Loss:  3.464960 (3.3787)  Time: 0.583s, 1755.09/s  (2.509s,  408.14/s)  LR: 4.346e-04  Data: 0.021 (1.903)
Train: 24 [ 500/1171 ( 43%)]  Loss:  3.563403 (3.3955)  Time: 5.680s,  180.29/s  (2.511s,  407.80/s)  LR: 4.346e-04  Data: 4.971 (1.905)
Train: 24 [ 550/1171 ( 47%)]  Loss:  3.446014 (3.3997)  Time: 0.582s, 1760.16/s  (2.530s,  404.79/s)  LR: 4.346e-04  Data: 0.018 (1.924)
Train: 24 [ 600/1171 ( 51%)]  Loss:  3.233527 (3.3869)  Time: 5.024s,  203.81/s  (2.565s,  399.23/s)  LR: 4.346e-04  Data: 4.345 (1.958)
Train: 24 [ 650/1171 ( 56%)]  Loss:  3.647582 (3.4055)  Time: 0.582s, 1759.36/s  (2.573s,  397.96/s)  LR: 4.346e-04  Data: 0.018 (1.967)
Train: 24 [ 700/1171 ( 60%)]  Loss:  3.738385 (3.4277)  Time: 5.302s,  193.13/s  (2.575s,  397.65/s)  LR: 4.346e-04  Data: 4.642 (1.968)
Train: 24 [ 750/1171 ( 64%)]  Loss:  3.526531 (3.4339)  Time: 0.584s, 1752.76/s  (2.568s,  398.69/s)  LR: 4.346e-04  Data: 0.020 (1.961)
Train: 24 [ 800/1171 ( 68%)]  Loss:  3.600846 (3.4437)  Time: 6.136s,  166.87/s  (2.559s,  400.16/s)  LR: 4.346e-04  Data: 5.574 (1.952)
Train: 24 [ 850/1171 ( 73%)]  Loss:  2.930809 (3.4152)  Time: 0.585s, 1750.04/s  (2.545s,  402.43/s)  LR: 4.346e-04  Data: 0.021 (1.938)
Train: 24 [ 900/1171 ( 77%)]  Loss:  3.234383 (3.4057)  Time: 8.177s,  125.23/s  (2.557s,  400.44/s)  LR: 4.346e-04  Data: 7.474 (1.951)
Train: 24 [ 950/1171 ( 81%)]  Loss:  4.004204 (3.4356)  Time: 0.590s, 1736.11/s  (2.561s,  399.91/s)  LR: 4.346e-04  Data: 0.018 (1.955)
Train: 24 [1000/1171 ( 85%)]  Loss:  3.251870 (3.4269)  Time: 4.287s,  238.85/s  (2.566s,  399.13/s)  LR: 4.346e-04  Data: 3.671 (1.959)
Train: 24 [1050/1171 ( 90%)]  Loss:  3.653651 (3.4372)  Time: 0.585s, 1749.76/s  (2.562s,  399.74/s)  LR: 4.346e-04  Data: 0.018 (1.956)
Train: 24 [1100/1171 ( 94%)]  Loss:  4.005954 (3.4619)  Time: 0.635s, 1613.02/s  (2.563s,  399.46/s)  LR: 4.346e-04  Data: 0.020 (1.958)
Train: 24 [1150/1171 ( 98%)]  Loss:  3.426337 (3.4604)  Time: 0.583s, 1757.74/s  (2.557s,  400.47/s)  LR: 4.346e-04  Data: 0.019 (1.952)
Train: 24 [1170/1171 (100%)]  Loss:  3.751873 (3.4721)  Time: 0.565s, 1813.05/s  (2.555s,  400.82/s)  LR: 4.346e-04  Data: 0.000 (1.949)
Test: [   0/97]  Time: 13.891 (13.891)  Loss:  0.4507 (0.4507)  Acc@1: 93.9453 (93.9453)  Acc@5: 99.1211 (99.1211)
Test: [  50/97]  Time: 0.197 (3.218)  Loss:  0.8464 (0.6324)  Acc@1: 85.0586 (89.6599)  Acc@5: 97.0703 (98.0430)
Test: [  97/97]  Time: 0.119 (3.348)  Loss:  0.5084 (0.6456)  Acc@1: 92.7083 (89.3490)  Acc@5: 98.6607 (97.5140)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-24.pth.tar', 89.34900001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-22.pth.tar', 88.88300002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-23.pth.tar', 88.8670000390625)

Train: 25 [   0/1171 (  0%)]  Loss:  3.907641 (3.9076)  Time: 11.837s,   86.51/s  (11.837s,   86.51/s)  LR: 3.998e-04  Data: 10.815 (10.815)
Train: 25 [  50/1171 (  4%)]  Loss:  3.666634 (3.7871)  Time: 0.587s, 1744.15/s  (2.720s,  376.44/s)  LR: 3.998e-04  Data: 0.022 (2.122)
Train: 25 [ 100/1171 (  9%)]  Loss:  3.312662 (3.6290)  Time: 0.583s, 1757.55/s  (2.682s,  381.74/s)  LR: 3.998e-04  Data: 0.017 (2.076)
Train: 25 [ 150/1171 ( 13%)]  Loss:  3.532643 (3.6049)  Time: 0.586s, 1747.07/s  (2.594s,  394.74/s)  LR: 3.998e-04  Data: 0.019 (1.996)
Train: 25 [ 200/1171 ( 17%)]  Loss:  2.971697 (3.4783)  Time: 0.587s, 1744.56/s  (2.582s,  396.61/s)  LR: 3.998e-04  Data: 0.019 (1.988)
Train: 25 [ 250/1171 ( 21%)]  Loss:  3.434998 (3.4710)  Time: 0.591s, 1732.73/s  (2.521s,  406.18/s)  LR: 3.998e-04  Data: 0.018 (1.930)
Train: 25 [ 300/1171 ( 26%)]  Loss:  3.809045 (3.5193)  Time: 0.584s, 1752.95/s  (2.560s,  400.07/s)  LR: 3.998e-04  Data: 0.021 (1.972)
Train: 25 [ 350/1171 ( 30%)]  Loss:  3.985150 (3.5776)  Time: 0.587s, 1743.15/s  (2.576s,  397.53/s)  LR: 3.998e-04  Data: 0.023 (1.987)
Train: 25 [ 400/1171 ( 34%)]  Loss:  3.347430 (3.5520)  Time: 0.583s, 1757.25/s  (2.602s,  393.49/s)  LR: 3.998e-04  Data: 0.018 (2.015)
Train: 25 [ 450/1171 ( 38%)]  Loss:  3.733271 (3.5701)  Time: 0.586s, 1746.77/s  (2.594s,  394.79/s)  LR: 3.998e-04  Data: 0.022 (2.007)
Train: 25 [ 500/1171 ( 43%)]  Loss:  3.355441 (3.5506)  Time: 0.587s, 1743.35/s  (2.607s,  392.82/s)  LR: 3.998e-04  Data: 0.019 (2.018)
Train: 25 [ 550/1171 ( 47%)]  Loss:  3.054814 (3.5093)  Time: 0.583s, 1756.79/s  (2.605s,  393.11/s)  LR: 3.998e-04  Data: 0.018 (2.016)
Train: 25 [ 600/1171 ( 51%)]  Loss:  2.904560 (3.4628)  Time: 0.591s, 1734.03/s  (2.607s,  392.73/s)  LR: 3.998e-04  Data: 0.018 (2.019)
Train: 25 [ 650/1171 ( 56%)]  Loss:  3.768401 (3.4846)  Time: 0.584s, 1754.83/s  (2.621s,  390.68/s)  LR: 3.998e-04  Data: 0.017 (2.034)
Train: 25 [ 700/1171 ( 60%)]  Loss:  3.265836 (3.4700)  Time: 2.908s,  352.09/s  (2.632s,  389.04/s)  LR: 3.998e-04  Data: 2.243 (2.045)
Train: 25 [ 750/1171 ( 64%)]  Loss:  3.601864 (3.4783)  Time: 0.582s, 1758.74/s  (2.627s,  389.80/s)  LR: 3.998e-04  Data: 0.020 (2.041)
Train: 25 [ 800/1171 ( 68%)]  Loss:  3.812603 (3.4979)  Time: 6.300s,  162.55/s  (2.631s,  389.20/s)  LR: 3.998e-04  Data: 5.636 (2.042)
Train: 25 [ 850/1171 ( 73%)]  Loss:  3.724176 (3.5105)  Time: 0.584s, 1752.63/s  (2.622s,  390.59/s)  LR: 3.998e-04  Data: 0.021 (2.032)
Train: 25 [ 900/1171 ( 77%)]  Loss:  3.443111 (3.5069)  Time: 6.751s,  151.68/s  (2.617s,  391.32/s)  LR: 3.998e-04  Data: 6.069 (2.026)
Train: 25 [ 950/1171 ( 81%)]  Loss:  3.266371 (3.4949)  Time: 0.585s, 1749.79/s  (2.605s,  393.13/s)  LR: 3.998e-04  Data: 0.019 (2.014)
Train: 25 [1000/1171 ( 85%)]  Loss:  3.286304 (3.4850)  Time: 7.459s,  137.29/s  (2.624s,  390.32/s)  LR: 3.998e-04  Data: 6.887 (2.033)
Train: 25 [1050/1171 ( 90%)]  Loss:  3.563991 (3.4886)  Time: 0.584s, 1753.52/s  (2.624s,  390.22/s)  LR: 3.998e-04  Data: 0.019 (2.034)
Train: 25 [1100/1171 ( 94%)]  Loss:  3.526957 (3.4902)  Time: 9.018s,  113.55/s  (2.630s,  389.37/s)  LR: 3.998e-04  Data: 8.396 (2.040)
Train: 25 [1150/1171 ( 98%)]  Loss:  3.661375 (3.4974)  Time: 0.588s, 1740.71/s  (2.626s,  390.01/s)  LR: 3.998e-04  Data: 0.021 (2.036)
Train: 25 [1170/1171 (100%)]  Loss:  3.696486 (3.5053)  Time: 0.563s, 1818.90/s  (2.624s,  390.29/s)  LR: 3.998e-04  Data: 0.000 (2.034)
Test: [   0/97]  Time: 14.212 (14.212)  Loss:  0.4627 (0.4627)  Acc@1: 93.2617 (93.2617)  Acc@5: 99.2188 (99.2188)
Test: [  50/97]  Time: 0.201 (3.270)  Loss:  0.8379 (0.5832)  Acc@1: 84.0820 (90.1214)  Acc@5: 96.3867 (98.1024)
Test: [  97/97]  Time: 0.119 (3.153)  Loss:  0.4860 (0.5962)  Acc@1: 92.4107 (89.7180)  Acc@5: 98.3631 (97.5960)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-25.pth.tar', 89.71800004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-24.pth.tar', 89.34900001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-22.pth.tar', 88.88300002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-23.pth.tar', 88.8670000390625)

Train: 26 [   0/1171 (  0%)]  Loss:  3.334487 (3.3345)  Time: 11.713s,   87.43/s  (11.713s,   87.43/s)  LR: 3.655e-04  Data: 10.582 (10.582)
Train: 26 [  50/1171 (  4%)]  Loss:  3.325017 (3.3298)  Time: 0.585s, 1750.99/s  (2.952s,  346.89/s)  LR: 3.655e-04  Data: 0.022 (2.339)
Train: 26 [ 100/1171 (  9%)]  Loss:  3.059684 (3.2397)  Time: 5.610s,  182.54/s  (2.865s,  357.38/s)  LR: 3.655e-04  Data: 5.045 (2.259)
Train: 26 [ 150/1171 ( 13%)]  Loss:  3.266262 (3.2464)  Time: 0.585s, 1750.72/s  (2.747s,  372.82/s)  LR: 3.655e-04  Data: 0.020 (2.147)
Train: 26 [ 200/1171 ( 17%)]  Loss:  4.018330 (3.4008)  Time: 1.249s,  819.71/s  (2.688s,  380.90/s)  LR: 3.655e-04  Data: 0.687 (2.083)
Train: 26 [ 250/1171 ( 21%)]  Loss:  3.312483 (3.3860)  Time: 0.585s, 1750.45/s  (2.662s,  384.74/s)  LR: 3.655e-04  Data: 0.018 (2.056)
Train: 26 [ 300/1171 ( 26%)]  Loss:  3.659037 (3.4250)  Time: 0.583s, 1755.38/s  (2.618s,  391.11/s)  LR: 3.655e-04  Data: 0.019 (2.015)
Train: 26 [ 350/1171 ( 30%)]  Loss:  3.099112 (3.3843)  Time: 0.584s, 1753.30/s  (2.591s,  395.20/s)  LR: 3.655e-04  Data: 0.021 (1.988)
Train: 26 [ 400/1171 ( 34%)]  Loss:  3.912092 (3.4429)  Time: 0.585s, 1751.52/s  (2.634s,  388.78/s)  LR: 3.655e-04  Data: 0.022 (2.031)
Train: 26 [ 450/1171 ( 38%)]  Loss:  3.331704 (3.4318)  Time: 4.669s,  219.32/s  (2.634s,  388.76/s)  LR: 3.655e-04  Data: 3.997 (2.032)
Train: 26 [ 500/1171 ( 43%)]  Loss:  3.436718 (3.4323)  Time: 0.582s, 1759.48/s  (2.639s,  388.03/s)  LR: 3.655e-04  Data: 0.018 (2.035)
Train: 26 [ 550/1171 ( 47%)]  Loss:  3.659884 (3.4512)  Time: 1.701s,  602.13/s  (2.648s,  386.67/s)  LR: 3.655e-04  Data: 1.034 (2.042)
Train: 26 [ 600/1171 ( 51%)]  Loss:  3.861625 (3.4828)  Time: 0.586s, 1748.22/s  (2.649s,  386.62/s)  LR: 3.655e-04  Data: 0.019 (2.043)
Train: 26 [ 650/1171 ( 56%)]  Loss:  3.772375 (3.5035)  Time: 4.269s,  239.85/s  (2.641s,  387.77/s)  LR: 3.655e-04  Data: 3.707 (2.036)
Train: 26 [ 700/1171 ( 60%)]  Loss:  3.640007 (3.5126)  Time: 1.240s,  825.83/s  (2.623s,  390.37/s)  LR: 3.655e-04  Data: 0.230 (2.017)
Train: 26 [ 750/1171 ( 64%)]  Loss:  3.146790 (3.4897)  Time: 8.180s,  125.19/s  (2.649s,  386.50/s)  LR: 3.655e-04  Data: 7.527 (2.043)
Train: 26 [ 800/1171 ( 68%)]  Loss:  3.774411 (3.5065)  Time: 0.586s, 1746.86/s  (2.659s,  385.13/s)  LR: 3.655e-04  Data: 0.021 (2.052)
Train: 26 [ 850/1171 ( 73%)]  Loss:  3.401709 (3.5007)  Time: 8.525s,  120.12/s  (2.668s,  383.82/s)  LR: 3.655e-04  Data: 7.682 (2.061)
Train: 26 [ 900/1171 ( 77%)]  Loss:  3.455296 (3.4983)  Time: 0.587s, 1744.12/s  (2.656s,  385.48/s)  LR: 3.655e-04  Data: 0.020 (2.050)
Train: 26 [ 950/1171 ( 81%)]  Loss:  3.851172 (3.5159)  Time: 8.125s,  126.03/s  (2.653s,  385.95/s)  LR: 3.655e-04  Data: 7.458 (2.047)
Train: 26 [1000/1171 ( 85%)]  Loss:  3.066127 (3.4945)  Time: 0.586s, 1747.24/s  (2.644s,  387.35/s)  LR: 3.655e-04  Data: 0.020 (2.039)
Train: 26 [1050/1171 ( 90%)]  Loss:  2.608840 (3.4542)  Time: 11.475s,   89.24/s  (2.646s,  387.04/s)  LR: 3.655e-04  Data: 10.896 (2.042)
Train: 26 [1100/1171 ( 94%)]  Loss:  3.712456 (3.4655)  Time: 0.583s, 1757.62/s  (2.645s,  387.11/s)  LR: 3.655e-04  Data: 0.020 (2.042)
Train: 26 [1150/1171 ( 98%)]  Loss:  3.778986 (3.4785)  Time: 9.093s,  112.62/s  (2.651s,  386.32/s)  LR: 3.655e-04  Data: 8.515 (2.048)
Train: 26 [1170/1171 (100%)]  Loss:  3.187186 (3.4669)  Time: 0.565s, 1813.88/s  (2.643s,  387.43/s)  LR: 3.655e-04  Data: 0.000 (2.040)
Test: [   0/97]  Time: 15.649 (15.649)  Loss:  0.4468 (0.4468)  Acc@1: 94.3359 (94.3359)  Acc@5: 99.0234 (99.0234)
Test: [  50/97]  Time: 0.197 (3.389)  Loss:  0.7698 (0.5669)  Acc@1: 85.3516 (90.7667)  Acc@5: 96.2891 (98.2116)
Test: [  97/97]  Time: 0.198 (3.219)  Loss:  0.3613 (0.5819)  Acc@1: 95.8333 (90.2830)  Acc@5: 98.9583 (97.7230)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-26.pth.tar', 90.28300001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-25.pth.tar', 89.71800004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-24.pth.tar', 89.34900001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-22.pth.tar', 88.88300002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-23.pth.tar', 88.8670000390625)

Train: 27 [   0/1171 (  0%)]  Loss:  3.323344 (3.3233)  Time: 11.698s,   87.53/s  (11.698s,   87.53/s)  LR: 3.320e-04  Data: 10.788 (10.788)
Train: 27 [  50/1171 (  4%)]  Loss:  3.007288 (3.1653)  Time: 0.581s, 1761.43/s  (2.460s,  416.18/s)  LR: 3.320e-04  Data: 0.018 (1.865)
Train: 27 [ 100/1171 (  9%)]  Loss:  3.576935 (3.3025)  Time: 1.024s, 1000.35/s  (2.462s,  415.89/s)  LR: 3.320e-04  Data: 0.343 (1.858)
Train: 27 [ 150/1171 ( 13%)]  Loss:  3.627325 (3.3837)  Time: 0.589s, 1739.45/s  (2.580s,  396.95/s)  LR: 3.320e-04  Data: 0.025 (1.972)
Train: 27 [ 200/1171 ( 17%)]  Loss:  2.898039 (3.2866)  Time: 2.078s,  492.68/s  (2.591s,  395.25/s)  LR: 3.320e-04  Data: 1.508 (1.986)
Train: 27 [ 250/1171 ( 21%)]  Loss:  3.191700 (3.2708)  Time: 0.586s, 1746.40/s  (2.587s,  395.88/s)  LR: 3.320e-04  Data: 0.018 (1.983)
Train: 27 [ 300/1171 ( 26%)]  Loss:  3.968593 (3.3705)  Time: 0.583s, 1757.43/s  (2.575s,  397.70/s)  LR: 3.320e-04  Data: 0.019 (1.968)
Train: 27 [ 350/1171 ( 30%)]  Loss:  4.082262 (3.4594)  Time: 0.586s, 1748.75/s  (2.555s,  400.81/s)  LR: 3.320e-04  Data: 0.020 (1.948)
Train: 27 [ 400/1171 ( 34%)]  Loss:  3.513583 (3.4655)  Time: 3.360s,  304.74/s  (2.555s,  400.77/s)  LR: 3.320e-04  Data: 2.305 (1.946)
Train: 27 [ 450/1171 ( 38%)]  Loss:  3.726800 (3.4916)  Time: 0.585s, 1750.22/s  (2.538s,  403.50/s)  LR: 3.320e-04  Data: 0.019 (1.930)
Train: 27 [ 500/1171 ( 43%)]  Loss:  3.194591 (3.4646)  Time: 5.299s,  193.23/s  (2.605s,  393.02/s)  LR: 3.320e-04  Data: 4.657 (1.998)
Train: 27 [ 550/1171 ( 47%)]  Loss:  3.305426 (3.4513)  Time: 0.581s, 1761.78/s  (2.622s,  390.48/s)  LR: 3.320e-04  Data: 0.018 (2.016)
Train: 27 [ 600/1171 ( 51%)]  Loss:  3.552505 (3.4591)  Time: 7.414s,  138.12/s  (2.638s,  388.21/s)  LR: 3.320e-04  Data: 6.757 (2.031)
Train: 27 [ 650/1171 ( 56%)]  Loss:  3.490664 (3.4614)  Time: 0.583s, 1755.41/s  (2.631s,  389.17/s)  LR: 3.320e-04  Data: 0.019 (2.025)
Train: 27 [ 700/1171 ( 60%)]  Loss:  3.107650 (3.4378)  Time: 8.930s,  114.67/s  (2.634s,  388.73/s)  LR: 3.320e-04  Data: 8.244 (2.028)
Train: 27 [ 750/1171 ( 64%)]  Loss:  3.271974 (3.4274)  Time: 0.583s, 1755.78/s  (2.615s,  391.58/s)  LR: 3.320e-04  Data: 0.019 (2.011)
Train: 27 [ 800/1171 ( 68%)]  Loss:  3.618765 (3.4387)  Time: 8.643s,  118.48/s  (2.634s,  388.79/s)  LR: 3.320e-04  Data: 8.077 (2.031)
Train: 27 [ 850/1171 ( 73%)]  Loss:  3.786052 (3.4580)  Time: 0.583s, 1754.93/s  (2.637s,  388.26/s)  LR: 3.320e-04  Data: 0.019 (2.034)
Train: 27 [ 900/1171 ( 77%)]  Loss:  3.144420 (3.4415)  Time: 8.150s,  125.65/s  (2.641s,  387.78/s)  LR: 3.320e-04  Data: 7.451 (2.038)
Train: 27 [ 950/1171 ( 81%)]  Loss:  3.694331 (3.4541)  Time: 0.584s, 1752.73/s  (2.635s,  388.55/s)  LR: 3.320e-04  Data: 0.021 (2.032)
Train: 27 [1000/1171 ( 85%)]  Loss:  3.844780 (3.4727)  Time: 9.639s,  106.23/s  (2.635s,  388.65/s)  LR: 3.320e-04  Data: 9.010 (2.033)
Train: 27 [1050/1171 ( 90%)]  Loss:  3.743567 (3.4850)  Time: 0.589s, 1738.30/s  (2.624s,  390.29/s)  LR: 3.320e-04  Data: 0.023 (2.023)
Train: 27 [1100/1171 ( 94%)]  Loss:  3.712726 (3.4949)  Time: 7.227s,  141.68/s  (2.618s,  391.12/s)  LR: 3.320e-04  Data: 6.566 (2.018)
Train: 27 [1150/1171 ( 98%)]  Loss:  3.737699 (3.5050)  Time: 0.587s, 1744.66/s  (2.626s,  389.99/s)  LR: 3.320e-04  Data: 0.018 (2.025)
Train: 27 [1170/1171 (100%)]  Loss:  4.052087 (3.5269)  Time: 0.565s, 1811.38/s  (2.626s,  389.87/s)  LR: 3.320e-04  Data: 0.000 (2.026)
Test: [   0/97]  Time: 15.288 (15.288)  Loss:  0.4284 (0.4284)  Acc@1: 94.9219 (94.9219)  Acc@5: 99.2188 (99.2188)
Test: [  50/97]  Time: 4.538 (3.474)  Loss:  0.7752 (0.5766)  Acc@1: 85.8398 (91.1784)  Acc@5: 97.1680 (98.2671)
Test: [  97/97]  Time: 0.119 (3.354)  Loss:  0.4998 (0.5887)  Acc@1: 92.8571 (90.6490)  Acc@5: 98.9583 (97.8280)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-27.pth.tar', 90.64900002929687)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-26.pth.tar', 90.28300001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-25.pth.tar', 89.71800004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-24.pth.tar', 89.34900001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-22.pth.tar', 88.88300002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-23.pth.tar', 88.8670000390625)

Train: 28 [   0/1171 (  0%)]  Loss:  3.920307 (3.9203)  Time: 11.749s,   87.15/s  (11.749s,   87.15/s)  LR: 2.994e-04  Data: 10.926 (10.926)
Train: 28 [  50/1171 (  4%)]  Loss:  3.297525 (3.6089)  Time: 1.088s,  941.03/s  (2.517s,  406.77/s)  LR: 2.994e-04  Data: 0.526 (1.911)
Train: 28 [ 100/1171 (  9%)]  Loss:  3.638196 (3.6187)  Time: 0.582s, 1758.05/s  (2.472s,  414.16/s)  LR: 2.994e-04  Data: 0.019 (1.876)
Train: 28 [ 150/1171 ( 13%)]  Loss:  3.855558 (3.6779)  Time: 0.583s, 1756.70/s  (2.414s,  424.21/s)  LR: 2.994e-04  Data: 0.019 (1.815)
Train: 28 [ 200/1171 ( 17%)]  Loss:  3.541250 (3.6506)  Time: 4.866s,  210.43/s  (2.486s,  411.98/s)  LR: 2.994e-04  Data: 3.903 (1.879)
Train: 28 [ 250/1171 ( 21%)]  Loss:  3.497226 (3.6250)  Time: 0.585s, 1749.72/s  (2.492s,  410.92/s)  LR: 2.994e-04  Data: 0.021 (1.883)
Train: 28 [ 300/1171 ( 26%)]  Loss:  2.884353 (3.5192)  Time: 7.911s,  129.43/s  (2.497s,  410.16/s)  LR: 2.994e-04  Data: 7.327 (1.890)
Train: 28 [ 350/1171 ( 30%)]  Loss:  3.017856 (3.4565)  Time: 0.587s, 1744.21/s  (2.473s,  414.15/s)  LR: 2.994e-04  Data: 0.021 (1.868)
Train: 28 [ 400/1171 ( 34%)]  Loss:  3.229012 (3.4313)  Time: 1.561s,  656.16/s  (2.463s,  415.82/s)  LR: 2.994e-04  Data: 0.983 (1.856)
Train: 28 [ 450/1171 ( 38%)]  Loss:  3.657767 (3.4539)  Time: 0.582s, 1760.51/s  (2.432s,  421.06/s)  LR: 2.994e-04  Data: 0.019 (1.827)
Train: 28 [ 500/1171 ( 43%)]  Loss:  3.489552 (3.4571)  Time: 0.581s, 1763.67/s  (2.430s,  421.35/s)  LR: 2.994e-04  Data: 0.018 (1.828)
Train: 28 [ 550/1171 ( 47%)]  Loss:  3.389230 (3.4515)  Time: 0.582s, 1760.68/s  (2.417s,  423.64/s)  LR: 2.994e-04  Data: 0.019 (1.817)
Train: 28 [ 600/1171 ( 51%)]  Loss:  3.776844 (3.4765)  Time: 2.257s,  453.75/s  (2.464s,  415.66/s)  LR: 2.994e-04  Data: 1.671 (1.861)
Train: 28 [ 650/1171 ( 56%)]  Loss:  3.353684 (3.4677)  Time: 0.583s, 1755.59/s  (2.467s,  415.13/s)  LR: 2.994e-04  Data: 0.020 (1.865)
Train: 28 [ 700/1171 ( 60%)]  Loss:  3.231231 (3.4520)  Time: 0.902s, 1134.70/s  (2.474s,  413.95/s)  LR: 2.994e-04  Data: 0.340 (1.873)
Train: 28 [ 750/1171 ( 64%)]  Loss:  3.588633 (3.4605)  Time: 0.586s, 1748.50/s  (2.465s,  415.47/s)  LR: 2.994e-04  Data: 0.019 (1.866)
Train: 28 [ 800/1171 ( 68%)]  Loss:  3.441743 (3.4594)  Time: 0.970s, 1055.91/s  (2.458s,  416.66/s)  LR: 2.994e-04  Data: 0.307 (1.859)
Train: 28 [ 850/1171 ( 73%)]  Loss:  3.318019 (3.4516)  Time: 2.882s,  355.27/s  (2.444s,  418.93/s)  LR: 2.994e-04  Data: 2.201 (1.844)
Train: 28 [ 900/1171 ( 77%)]  Loss:  3.627411 (3.4608)  Time: 3.090s,  331.38/s  (2.436s,  420.43/s)  LR: 2.994e-04  Data: 2.508 (1.834)
Train: 28 [ 950/1171 ( 81%)]  Loss:  3.718759 (3.4737)  Time: 2.414s,  424.20/s  (2.454s,  417.30/s)  LR: 2.994e-04  Data: 1.794 (1.850)
Train: 28 [1000/1171 ( 85%)]  Loss:  2.906817 (3.4467)  Time: 2.898s,  353.35/s  (2.457s,  416.77/s)  LR: 2.994e-04  Data: 2.309 (1.853)
Train: 28 [1050/1171 ( 90%)]  Loss:  3.503938 (3.4493)  Time: 0.585s, 1751.89/s  (2.455s,  417.13/s)  LR: 2.994e-04  Data: 0.021 (1.851)
Train: 28 [1100/1171 ( 94%)]  Loss:  3.294835 (3.4426)  Time: 3.351s,  305.55/s  (2.456s,  416.97/s)  LR: 2.994e-04  Data: 2.767 (1.851)
Train: 28 [1150/1171 ( 98%)]  Loss:  3.300107 (3.4367)  Time: 0.584s, 1752.69/s  (2.449s,  418.09/s)  LR: 2.994e-04  Data: 0.021 (1.845)
Train: 28 [1170/1171 (100%)]  Loss:  3.797802 (3.4511)  Time: 0.563s, 1817.39/s  (2.450s,  418.03/s)  LR: 2.994e-04  Data: 0.000 (1.846)
Test: [   0/97]  Time: 13.078 (13.078)  Loss:  0.4242 (0.4242)  Acc@1: 94.7266 (94.7266)  Acc@5: 99.3164 (99.3164)
Test: [  50/97]  Time: 0.195 (3.102)  Loss:  0.6785 (0.5522)  Acc@1: 87.2070 (91.3775)  Acc@5: 97.6562 (98.3284)
Test: [  97/97]  Time: 0.119 (3.150)  Loss:  0.4477 (0.5686)  Acc@1: 93.6012 (90.6840)  Acc@5: 98.9583 (97.8590)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-28.pth.tar', 90.6840000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-27.pth.tar', 90.64900002929687)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-26.pth.tar', 90.28300001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-25.pth.tar', 89.71800004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-24.pth.tar', 89.34900001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-22.pth.tar', 88.88300002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-23.pth.tar', 88.8670000390625)

Train: 29 [   0/1171 (  0%)]  Loss:  3.808892 (3.8089)  Time: 12.691s,   80.68/s  (12.691s,   80.68/s)  LR: 2.678e-04  Data: 11.715 (11.715)
Train: 29 [  50/1171 (  4%)]  Loss:  3.308001 (3.5584)  Time: 0.583s, 1756.20/s  (2.546s,  402.22/s)  LR: 2.678e-04  Data: 0.019 (1.953)
Train: 29 [ 100/1171 (  9%)]  Loss:  3.132277 (3.4164)  Time: 3.771s,  271.56/s  (2.518s,  406.69/s)  LR: 2.678e-04  Data: 3.193 (1.918)
Train: 29 [ 150/1171 ( 13%)]  Loss:  2.883445 (3.2832)  Time: 0.902s, 1135.14/s  (2.484s,  412.22/s)  LR: 2.678e-04  Data: 0.021 (1.875)
Train: 29 [ 200/1171 ( 17%)]  Loss:  3.890437 (3.4046)  Time: 5.922s,  172.92/s  (2.490s,  411.27/s)  LR: 2.678e-04  Data: 5.329 (1.883)
Train: 29 [ 250/1171 ( 21%)]  Loss:  3.254757 (3.3796)  Time: 0.583s, 1756.96/s  (2.432s,  421.02/s)  LR: 2.678e-04  Data: 0.019 (1.826)
Train: 29 [ 300/1171 ( 26%)]  Loss:  3.862222 (3.4486)  Time: 5.807s,  176.33/s  (2.411s,  424.77/s)  LR: 2.678e-04  Data: 5.232 (1.801)
Train: 29 [ 350/1171 ( 30%)]  Loss:  4.024495 (3.5206)  Time: 0.583s, 1757.41/s  (2.391s,  428.20/s)  LR: 2.678e-04  Data: 0.019 (1.784)
Train: 29 [ 400/1171 ( 34%)]  Loss:  3.083306 (3.4720)  Time: 7.052s,  145.22/s  (2.447s,  418.54/s)  LR: 2.678e-04  Data: 6.363 (1.842)
Train: 29 [ 450/1171 ( 38%)]  Loss:  3.619167 (3.4867)  Time: 0.585s, 1749.71/s  (2.447s,  418.43/s)  LR: 2.678e-04  Data: 0.020 (1.845)
Train: 29 [ 500/1171 ( 43%)]  Loss:  3.534964 (3.4911)  Time: 9.144s,  111.99/s  (2.455s,  417.11/s)  LR: 2.678e-04  Data: 8.172 (1.855)
Train: 29 [ 550/1171 ( 47%)]  Loss:  3.502806 (3.4921)  Time: 0.589s, 1738.94/s  (2.453s,  417.49/s)  LR: 2.678e-04  Data: 0.017 (1.853)
Train: 29 [ 600/1171 ( 51%)]  Loss:  3.566585 (3.4978)  Time: 7.049s,  145.27/s  (2.455s,  417.17/s)  LR: 2.678e-04  Data: 6.421 (1.853)
Train: 29 [ 650/1171 ( 56%)]  Loss:  3.579122 (3.5036)  Time: 0.586s, 1746.64/s  (2.442s,  419.37/s)  LR: 2.678e-04  Data: 0.020 (1.840)
Train: 29 [ 700/1171 ( 60%)]  Loss:  3.281996 (3.4888)  Time: 3.440s,  297.70/s  (2.433s,  420.91/s)  LR: 2.678e-04  Data: 2.826 (1.832)
Train: 29 [ 750/1171 ( 64%)]  Loss:  3.337173 (3.4794)  Time: 0.585s, 1750.86/s  (2.451s,  417.80/s)  LR: 2.678e-04  Data: 0.018 (1.850)
Train: 29 [ 800/1171 ( 68%)]  Loss:  3.426381 (3.4762)  Time: 6.897s,  148.47/s  (2.458s,  416.56/s)  LR: 2.678e-04  Data: 6.284 (1.859)
Train: 29 [ 850/1171 ( 73%)]  Loss:  3.509572 (3.4781)  Time: 0.584s, 1753.80/s  (2.454s,  417.27/s)  LR: 2.678e-04  Data: 0.019 (1.855)
Train: 29 [ 900/1171 ( 77%)]  Loss:  3.161475 (3.4614)  Time: 8.341s,  122.77/s  (2.460s,  416.30/s)  LR: 2.678e-04  Data: 7.610 (1.860)
Train: 29 [ 950/1171 ( 81%)]  Loss:  3.643690 (3.4705)  Time: 0.584s, 1754.42/s  (2.451s,  417.75/s)  LR: 2.678e-04  Data: 0.019 (1.852)
Train: 29 [1000/1171 ( 85%)]  Loss:  3.463342 (3.4702)  Time: 6.968s,  146.96/s  (2.443s,  419.12/s)  LR: 2.678e-04  Data: 6.389 (1.845)
Train: 29 [1050/1171 ( 90%)]  Loss:  3.336905 (3.4641)  Time: 0.586s, 1748.24/s  (2.434s,  420.78/s)  LR: 2.678e-04  Data: 0.018 (1.836)
Train: 29 [1100/1171 ( 94%)]  Loss:  3.629251 (3.4713)  Time: 11.888s,   86.14/s  (2.444s,  419.07/s)  LR: 2.678e-04  Data: 11.239 (1.846)
Train: 29 [1150/1171 ( 98%)]  Loss:  3.046000 (3.4536)  Time: 0.587s, 1743.71/s  (2.444s,  419.03/s)  LR: 2.678e-04  Data: 0.018 (1.847)
Train: 29 [1170/1171 (100%)]  Loss:  3.662766 (3.4620)  Time: 0.565s, 1812.84/s  (2.449s,  418.18/s)  LR: 2.678e-04  Data: 0.000 (1.852)
Test: [   0/97]  Time: 15.294 (15.294)  Loss:  0.4092 (0.4092)  Acc@1: 94.6289 (94.6289)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.197 (3.149)  Loss:  0.7162 (0.5211)  Acc@1: 85.9375 (91.6188)  Acc@5: 97.2656 (98.4471)
Test: [  97/97]  Time: 0.119 (3.067)  Loss:  0.4539 (0.5351)  Acc@1: 93.0060 (91.1590)  Acc@5: 98.9583 (98.0090)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-29.pth.tar', 91.15899999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-28.pth.tar', 90.6840000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-27.pth.tar', 90.64900002929687)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-26.pth.tar', 90.28300001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-25.pth.tar', 89.71800004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-24.pth.tar', 89.34900001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-22.pth.tar', 88.88300002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-23.pth.tar', 88.8670000390625)

Train: 30 [   0/1171 (  0%)]  Loss:  3.093337 (3.0933)  Time: 10.676s,   95.91/s  (10.676s,   95.91/s)  LR: 2.374e-04  Data: 9.942 (9.942)
Train: 30 [  50/1171 (  4%)]  Loss:  3.199247 (3.1463)  Time: 0.584s, 1752.24/s  (2.367s,  432.63/s)  LR: 2.374e-04  Data: 0.021 (1.787)
Train: 30 [ 100/1171 (  9%)]  Loss:  3.085133 (3.1259)  Time: 0.588s, 1740.76/s  (2.300s,  445.16/s)  LR: 2.374e-04  Data: 0.020 (1.719)
Train: 30 [ 150/1171 ( 13%)]  Loss:  3.265912 (3.1609)  Time: 0.588s, 1740.99/s  (2.242s,  456.81/s)  LR: 2.374e-04  Data: 0.021 (1.662)
Train: 30 [ 200/1171 ( 17%)]  Loss:  3.328613 (3.1944)  Time: 0.892s, 1148.28/s  (2.359s,  434.04/s)  LR: 2.374e-04  Data: 0.257 (1.775)
Train: 30 [ 250/1171 ( 21%)]  Loss:  3.585815 (3.2597)  Time: 0.583s, 1755.10/s  (2.355s,  434.81/s)  LR: 2.374e-04  Data: 0.020 (1.760)
Train: 30 [ 300/1171 ( 26%)]  Loss:  3.154195 (3.2446)  Time: 0.585s, 1749.77/s  (2.368s,  432.41/s)  LR: 2.374e-04  Data: 0.018 (1.772)
Train: 30 [ 350/1171 ( 30%)]  Loss:  3.172345 (3.2356)  Time: 0.583s, 1757.34/s  (2.371s,  431.86/s)  LR: 2.374e-04  Data: 0.019 (1.775)
Train: 30 [ 400/1171 ( 34%)]  Loss:  2.825251 (3.1900)  Time: 5.070s,  201.96/s  (2.375s,  431.22/s)  LR: 2.374e-04  Data: 4.344 (1.777)
Train: 30 [ 450/1171 ( 38%)]  Loss:  3.303664 (3.2014)  Time: 0.958s, 1068.74/s  (2.356s,  434.66/s)  LR: 2.374e-04  Data: 0.299 (1.756)
Train: 30 [ 500/1171 ( 43%)]  Loss:  3.644476 (3.2416)  Time: 0.588s, 1741.58/s  (2.357s,  434.42/s)  LR: 2.374e-04  Data: 0.022 (1.758)
Train: 30 [ 550/1171 ( 47%)]  Loss:  3.535230 (3.2661)  Time: 0.583s, 1755.02/s  (2.382s,  429.80/s)  LR: 2.374e-04  Data: 0.021 (1.784)
Train: 30 [ 600/1171 ( 51%)]  Loss:  3.310657 (3.2695)  Time: 1.702s,  601.71/s  (2.448s,  418.38/s)  LR: 2.374e-04  Data: 1.043 (1.849)
Train: 30 [ 650/1171 ( 56%)]  Loss:  3.649301 (3.2967)  Time: 0.588s, 1741.74/s  (2.470s,  414.61/s)  LR: 2.374e-04  Data: 0.020 (1.872)
Train: 30 [ 700/1171 ( 60%)]  Loss:  3.516162 (3.3113)  Time: 5.804s,  176.42/s  (2.489s,  411.42/s)  LR: 2.374e-04  Data: 5.140 (1.891)
Train: 30 [ 750/1171 ( 64%)]  Loss:  3.228636 (3.3061)  Time: 0.587s, 1744.88/s  (2.485s,  412.06/s)  LR: 2.374e-04  Data: 0.022 (1.887)
Train: 30 [ 800/1171 ( 68%)]  Loss:  3.308203 (3.3062)  Time: 3.902s,  262.46/s  (2.492s,  410.94/s)  LR: 2.374e-04  Data: 3.054 (1.893)
Train: 30 [ 850/1171 ( 73%)]  Loss:  3.742059 (3.3305)  Time: 0.582s, 1758.17/s  (2.485s,  412.09/s)  LR: 2.374e-04  Data: 0.019 (1.887)
Train: 30 [ 900/1171 ( 77%)]  Loss:  3.174747 (3.3223)  Time: 0.585s, 1749.44/s  (2.512s,  407.69/s)  LR: 2.374e-04  Data: 0.019 (1.913)
Train: 30 [ 950/1171 ( 81%)]  Loss:  3.608711 (3.3366)  Time: 0.690s, 1483.02/s  (2.526s,  405.34/s)  LR: 2.374e-04  Data: 0.104 (1.927)
Train: 30 [1000/1171 ( 85%)]  Loss:  3.906230 (3.3637)  Time: 0.584s, 1752.87/s  (2.542s,  402.84/s)  LR: 2.374e-04  Data: 0.020 (1.943)
Train: 30 [1050/1171 ( 90%)]  Loss:  2.891792 (3.3423)  Time: 0.583s, 1756.34/s  (2.543s,  402.60/s)  LR: 2.374e-04  Data: 0.017 (1.946)
Train: 30 [1100/1171 ( 94%)]  Loss:  3.373537 (3.3436)  Time: 0.584s, 1752.42/s  (2.549s,  401.74/s)  LR: 2.374e-04  Data: 0.020 (1.951)
Train: 30 [1150/1171 ( 98%)]  Loss:  3.497097 (3.3500)  Time: 1.971s,  519.62/s  (2.542s,  402.80/s)  LR: 2.374e-04  Data: 1.386 (1.943)
Train: 30 [1170/1171 (100%)]  Loss:  3.131538 (3.3413)  Time: 0.563s, 1820.29/s  (2.541s,  403.01/s)  LR: 2.374e-04  Data: 0.000 (1.942)
Test: [   0/97]  Time: 14.292 (14.292)  Loss:  0.3790 (0.3790)  Acc@1: 94.3359 (94.3359)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 1.246 (3.507)  Loss:  0.6325 (0.4800)  Acc@1: 86.8164 (92.1894)  Acc@5: 97.8516 (98.5198)
Test: [  97/97]  Time: 0.120 (3.427)  Loss:  0.4242 (0.4995)  Acc@1: 93.4524 (91.5930)  Acc@5: 98.9583 (98.1190)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-30.pth.tar', 91.59299997558594)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-29.pth.tar', 91.15899999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-28.pth.tar', 90.6840000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-27.pth.tar', 90.64900002929687)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-26.pth.tar', 90.28300001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-25.pth.tar', 89.71800004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-24.pth.tar', 89.34900001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-22.pth.tar', 88.88300002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-23.pth.tar', 88.8670000390625)

Train: 31 [   0/1171 (  0%)]  Loss:  3.153888 (3.1539)  Time: 12.721s,   80.50/s  (12.721s,   80.50/s)  LR: 2.084e-04  Data: 12.001 (12.001)
Train: 31 [  50/1171 (  4%)]  Loss:  3.095572 (3.1247)  Time: 0.583s, 1755.39/s  (2.706s,  378.44/s)  LR: 2.084e-04  Data: 0.019 (2.072)
Train: 31 [ 100/1171 (  9%)]  Loss:  3.496418 (3.2486)  Time: 3.766s,  271.88/s  (2.658s,  385.21/s)  LR: 2.084e-04  Data: 3.191 (2.036)
Train: 31 [ 150/1171 ( 13%)]  Loss:  3.311899 (3.2644)  Time: 0.583s, 1756.92/s  (2.559s,  400.12/s)  LR: 2.084e-04  Data: 0.019 (1.941)
Train: 31 [ 200/1171 ( 17%)]  Loss:  3.141006 (3.2398)  Time: 3.280s,  312.22/s  (2.542s,  402.91/s)  LR: 2.084e-04  Data: 2.615 (1.926)
Train: 31 [ 250/1171 ( 21%)]  Loss:  3.081402 (3.2134)  Time: 0.584s, 1752.41/s  (2.498s,  409.90/s)  LR: 2.084e-04  Data: 0.019 (1.884)
Train: 31 [ 300/1171 ( 26%)]  Loss:  3.182467 (3.2090)  Time: 7.265s,  140.95/s  (2.575s,  397.62/s)  LR: 2.084e-04  Data: 6.610 (1.961)
Train: 31 [ 350/1171 ( 30%)]  Loss:  3.272255 (3.2169)  Time: 0.584s, 1754.12/s  (2.590s,  395.33/s)  LR: 2.084e-04  Data: 0.019 (1.978)
Train: 31 [ 400/1171 ( 34%)]  Loss:  3.199202 (3.2149)  Time: 8.393s,  122.01/s  (2.616s,  391.36/s)  LR: 2.084e-04  Data: 7.690 (2.005)
Train: 31 [ 450/1171 ( 38%)]  Loss:  3.569092 (3.2503)  Time: 0.584s, 1752.31/s  (2.606s,  392.88/s)  LR: 2.084e-04  Data: 0.019 (1.992)
Train: 31 [ 500/1171 ( 43%)]  Loss:  3.004944 (3.2280)  Time: 9.128s,  112.19/s  (2.615s,  391.61/s)  LR: 2.084e-04  Data: 8.489 (2.001)
Train: 31 [ 550/1171 ( 47%)]  Loss:  3.430317 (3.2449)  Time: 0.585s, 1750.00/s  (2.609s,  392.54/s)  LR: 2.084e-04  Data: 0.020 (1.997)
Train: 31 [ 600/1171 ( 51%)]  Loss:  3.744957 (3.2833)  Time: 8.488s,  120.64/s  (2.608s,  392.56/s)  LR: 2.084e-04  Data: 7.925 (1.999)
Train: 31 [ 650/1171 ( 56%)]  Loss:  3.510159 (3.2995)  Time: 0.585s, 1750.19/s  (2.645s,  387.09/s)  LR: 2.084e-04  Data: 0.022 (2.037)
Train: 31 [ 700/1171 ( 60%)]  Loss:  3.360988 (3.3036)  Time: 9.020s,  113.52/s  (2.658s,  385.25/s)  LR: 2.084e-04  Data: 8.390 (2.052)
Train: 31 [ 750/1171 ( 64%)]  Loss:  3.936813 (3.3432)  Time: 0.585s, 1749.05/s  (2.646s,  386.96/s)  LR: 2.084e-04  Data: 0.021 (2.041)
Train: 31 [ 800/1171 ( 68%)]  Loss:  3.336540 (3.3428)  Time: 6.897s,  148.46/s  (2.645s,  387.21/s)  LR: 2.084e-04  Data: 6.247 (2.039)
Train: 31 [ 850/1171 ( 73%)]  Loss:  3.391923 (3.3455)  Time: 0.587s, 1745.63/s  (2.633s,  388.89/s)  LR: 2.084e-04  Data: 0.017 (2.028)
Train: 31 [ 900/1171 ( 77%)]  Loss:  3.270413 (3.3416)  Time: 7.857s,  130.33/s  (2.631s,  389.22/s)  LR: 2.084e-04  Data: 7.232 (2.027)
Train: 31 [ 950/1171 ( 81%)]  Loss:  3.513289 (3.3502)  Time: 0.581s, 1761.82/s  (2.636s,  388.42/s)  LR: 2.084e-04  Data: 0.018 (2.033)
Train: 31 [1000/1171 ( 85%)]  Loss:  3.676597 (3.3657)  Time: 9.633s,  106.30/s  (2.651s,  386.30/s)  LR: 2.084e-04  Data: 9.044 (2.048)
Train: 31 [1050/1171 ( 90%)]  Loss:  3.250304 (3.3605)  Time: 0.736s, 1391.47/s  (2.648s,  386.68/s)  LR: 2.084e-04  Data: 0.171 (2.046)
Train: 31 [1100/1171 ( 94%)]  Loss:  3.882832 (3.3832)  Time: 8.879s,  115.33/s  (2.651s,  386.27/s)  LR: 2.084e-04  Data: 8.317 (2.050)
Train: 31 [1150/1171 ( 98%)]  Loss:  3.403548 (3.3840)  Time: 0.584s, 1754.24/s  (2.642s,  387.53/s)  LR: 2.084e-04  Data: 0.019 (2.042)
Train: 31 [1170/1171 (100%)]  Loss:  3.101897 (3.3727)  Time: 0.563s, 1818.84/s  (2.639s,  387.99/s)  LR: 2.084e-04  Data: 0.000 (2.039)
Test: [   0/97]  Time: 14.045 (14.045)  Loss:  0.4022 (0.4022)  Acc@1: 95.3125 (95.3125)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.198 (3.197)  Loss:  0.6939 (0.5155)  Acc@1: 87.8906 (92.2507)  Acc@5: 96.8750 (98.5007)
Test: [  97/97]  Time: 0.119 (3.284)  Loss:  0.4885 (0.5333)  Acc@1: 92.7083 (91.6220)  Acc@5: 98.6607 (98.0770)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-31.pth.tar', 91.62200006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-30.pth.tar', 91.59299997558594)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-29.pth.tar', 91.15899999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-28.pth.tar', 90.6840000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-27.pth.tar', 90.64900002929687)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-26.pth.tar', 90.28300001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-25.pth.tar', 89.71800004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-24.pth.tar', 89.34900001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-22.pth.tar', 88.88300002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-23.pth.tar', 88.8670000390625)

Train: 32 [   0/1171 (  0%)]  Loss:  3.037098 (3.0371)  Time: 13.212s,   77.50/s  (13.212s,   77.50/s)  LR: 1.808e-04  Data: 11.845 (11.845)
Train: 32 [  50/1171 (  4%)]  Loss:  2.714869 (2.8760)  Time: 0.585s, 1749.25/s  (2.798s,  366.02/s)  LR: 1.808e-04  Data: 0.023 (2.192)
Train: 32 [ 100/1171 (  9%)]  Loss:  2.671826 (2.8079)  Time: 0.583s, 1756.10/s  (2.769s,  369.87/s)  LR: 1.808e-04  Data: 0.018 (2.169)
Train: 32 [ 150/1171 ( 13%)]  Loss:  3.211714 (2.9089)  Time: 0.585s, 1751.23/s  (2.697s,  379.65/s)  LR: 1.808e-04  Data: 0.021 (2.100)
Train: 32 [ 200/1171 ( 17%)]  Loss:  3.401269 (3.0074)  Time: 4.853s,  210.99/s  (2.690s,  380.65/s)  LR: 1.808e-04  Data: 4.215 (2.093)
Train: 32 [ 250/1171 ( 21%)]  Loss:  3.520303 (3.0928)  Time: 0.584s, 1754.92/s  (2.628s,  389.70/s)  LR: 1.808e-04  Data: 0.019 (2.031)
Train: 32 [ 300/1171 ( 26%)]  Loss:  3.057607 (3.0878)  Time: 8.063s,  127.00/s  (2.618s,  391.15/s)  LR: 1.808e-04  Data: 7.483 (2.017)
Train: 32 [ 350/1171 ( 30%)]  Loss:  3.541512 (3.1445)  Time: 0.588s, 1742.86/s  (2.643s,  387.38/s)  LR: 1.808e-04  Data: 0.022 (2.043)
Train: 32 [ 400/1171 ( 34%)]  Loss:  3.369232 (3.1695)  Time: 9.136s,  112.09/s  (2.668s,  383.80/s)  LR: 1.808e-04  Data: 8.570 (2.070)
Train: 32 [ 450/1171 ( 38%)]  Loss:  3.054070 (3.1579)  Time: 0.585s, 1749.55/s  (2.656s,  385.52/s)  LR: 1.808e-04  Data: 0.022 (2.058)
Train: 32 [ 500/1171 ( 43%)]  Loss:  3.224323 (3.1640)  Time: 8.928s,  114.69/s  (2.677s,  382.55/s)  LR: 1.808e-04  Data: 8.253 (2.080)
Train: 32 [ 550/1171 ( 47%)]  Loss:  2.822847 (3.1356)  Time: 0.585s, 1749.92/s  (2.664s,  384.33/s)  LR: 1.808e-04  Data: 0.021 (2.069)
Train: 32 [ 600/1171 ( 51%)]  Loss:  3.333614 (3.1508)  Time: 9.085s,  112.71/s  (2.668s,  383.82/s)  LR: 1.808e-04  Data: 7.878 (2.071)
Train: 32 [ 650/1171 ( 56%)]  Loss:  3.126658 (3.1491)  Time: 0.584s, 1754.32/s  (2.651s,  386.23/s)  LR: 1.808e-04  Data: 0.019 (2.055)
Train: 32 [ 700/1171 ( 60%)]  Loss:  3.450123 (3.1691)  Time: 7.219s,  141.85/s  (2.670s,  383.59/s)  LR: 1.808e-04  Data: 6.506 (2.074)
Train: 32 [ 750/1171 ( 64%)]  Loss:  3.296458 (3.1771)  Time: 0.583s, 1755.90/s  (2.664s,  384.37/s)  LR: 1.808e-04  Data: 0.020 (2.066)
Train: 32 [ 800/1171 ( 68%)]  Loss:  3.380878 (3.1891)  Time: 2.500s,  409.64/s  (2.676s,  382.73/s)  LR: 1.808e-04  Data: 1.927 (2.077)
Train: 32 [ 850/1171 ( 73%)]  Loss:  3.245525 (3.1922)  Time: 0.587s, 1745.33/s  (2.669s,  383.72/s)  LR: 1.808e-04  Data: 0.022 (2.070)
Train: 32 [ 900/1171 ( 77%)]  Loss:  3.355404 (3.2008)  Time: 0.584s, 1753.19/s  (2.664s,  384.45/s)  LR: 1.808e-04  Data: 0.019 (2.066)
Train: 32 [ 950/1171 ( 81%)]  Loss:  2.669862 (3.1743)  Time: 0.588s, 1742.53/s  (2.654s,  385.87/s)  LR: 1.808e-04  Data: 0.019 (2.055)
Train: 32 [1000/1171 ( 85%)]  Loss:  3.530475 (3.1912)  Time: 2.808s,  364.63/s  (2.648s,  386.77/s)  LR: 1.808e-04  Data: 2.219 (2.048)
Train: 32 [1050/1171 ( 90%)]  Loss:  3.351470 (3.1985)  Time: 0.581s, 1761.87/s  (2.662s,  384.61/s)  LR: 1.808e-04  Data: 0.018 (2.063)
Train: 32 [1100/1171 ( 94%)]  Loss:  3.731475 (3.2217)  Time: 5.543s,  184.75/s  (2.670s,  383.56/s)  LR: 1.808e-04  Data: 4.846 (2.070)
Train: 32 [1150/1171 ( 98%)]  Loss:  2.705846 (3.2002)  Time: 0.584s, 1752.42/s  (2.662s,  384.71/s)  LR: 1.808e-04  Data: 0.020 (2.062)
Train: 32 [1170/1171 (100%)]  Loss:  3.144404 (3.1980)  Time: 0.563s, 1817.30/s  (2.662s,  384.66/s)  LR: 1.808e-04  Data: 0.000 (2.062)
Test: [   0/97]  Time: 14.122 (14.122)  Loss:  0.3522 (0.3522)  Acc@1: 95.5078 (95.5078)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.197 (3.275)  Loss:  0.6117 (0.4904)  Acc@1: 88.9648 (92.3292)  Acc@5: 97.7539 (98.5639)
Test: [  97/97]  Time: 0.120 (3.190)  Loss:  0.4178 (0.5088)  Acc@1: 93.4524 (91.8390)  Acc@5: 98.5119 (98.1530)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-32.pth.tar', 91.83900002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-31.pth.tar', 91.62200006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-30.pth.tar', 91.59299997558594)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-29.pth.tar', 91.15899999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-28.pth.tar', 90.6840000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-27.pth.tar', 90.64900002929687)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-26.pth.tar', 90.28300001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-25.pth.tar', 89.71800004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-24.pth.tar', 89.34900001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-22.pth.tar', 88.88300002441406)

Train: 33 [   0/1171 (  0%)]  Loss:  3.180765 (3.1808)  Time: 10.180s,  100.59/s  (10.180s,  100.59/s)  LR: 1.550e-04  Data: 9.220 (9.220)
Train: 33 [  50/1171 (  4%)]  Loss:  2.470236 (2.8255)  Time: 0.584s, 1752.04/s  (2.367s,  432.60/s)  LR: 1.550e-04  Data: 0.022 (1.757)
Train: 33 [ 100/1171 (  9%)]  Loss:  3.203696 (2.9516)  Time: 4.523s,  226.42/s  (2.709s,  377.93/s)  LR: 1.550e-04  Data: 3.887 (2.106)
Train: 33 [ 150/1171 ( 13%)]  Loss:  3.083980 (2.9847)  Time: 0.584s, 1751.95/s  (2.654s,  385.90/s)  LR: 1.550e-04  Data: 0.020 (2.050)
Train: 33 [ 200/1171 ( 17%)]  Loss:  3.225000 (3.0327)  Time: 0.583s, 1757.67/s  (2.635s,  388.62/s)  LR: 1.550e-04  Data: 0.020 (2.031)
Train: 33 [ 250/1171 ( 21%)]  Loss:  2.805163 (2.9948)  Time: 1.705s,  600.71/s  (2.600s,  393.86/s)  LR: 1.550e-04  Data: 1.000 (1.992)
Train: 33 [ 300/1171 ( 26%)]  Loss:  3.408696 (3.0539)  Time: 3.059s,  334.74/s  (2.593s,  394.86/s)  LR: 1.550e-04  Data: 2.185 (1.984)
Train: 33 [ 350/1171 ( 30%)]  Loss:  3.422262 (3.1000)  Time: 6.552s,  156.28/s  (2.569s,  398.57/s)  LR: 1.550e-04  Data: 5.858 (1.957)
Train: 33 [ 400/1171 ( 34%)]  Loss:  2.561928 (3.0402)  Time: 0.589s, 1739.16/s  (2.535s,  403.94/s)  LR: 1.550e-04  Data: 0.021 (1.924)
Train: 33 [ 450/1171 ( 38%)]  Loss:  3.668130 (3.1030)  Time: 7.307s,  140.14/s  (2.582s,  396.54/s)  LR: 1.550e-04  Data: 6.642 (1.972)
Train: 33 [ 500/1171 ( 43%)]  Loss:  3.990299 (3.1837)  Time: 0.585s, 1751.17/s  (2.582s,  396.60/s)  LR: 1.550e-04  Data: 0.020 (1.973)
Train: 33 [ 550/1171 ( 47%)]  Loss:  2.796053 (3.1514)  Time: 7.955s,  128.73/s  (2.601s,  393.68/s)  LR: 1.550e-04  Data: 7.349 (1.994)
Train: 33 [ 600/1171 ( 51%)]  Loss:  3.685953 (3.1925)  Time: 0.584s, 1752.39/s  (2.600s,  393.81/s)  LR: 1.550e-04  Data: 0.022 (1.994)
Train: 33 [ 650/1171 ( 56%)]  Loss:  2.961512 (3.1760)  Time: 4.668s,  219.39/s  (2.596s,  394.46/s)  LR: 1.550e-04  Data: 3.997 (1.989)
Train: 33 [ 700/1171 ( 60%)]  Loss:  3.738695 (3.2135)  Time: 0.584s, 1752.52/s  (2.584s,  396.21/s)  LR: 1.550e-04  Data: 0.021 (1.979)
Train: 33 [ 750/1171 ( 64%)]  Loss:  3.341875 (3.2215)  Time: 3.918s,  261.34/s  (2.586s,  395.95/s)  LR: 1.550e-04  Data: 3.231 (1.980)
Train: 33 [ 800/1171 ( 68%)]  Loss:  2.720767 (3.1921)  Time: 0.581s, 1763.24/s  (2.601s,  393.73/s)  LR: 1.550e-04  Data: 0.017 (1.994)
Train: 33 [ 850/1171 ( 73%)]  Loss:  3.210082 (3.1931)  Time: 3.407s,  300.59/s  (2.603s,  393.45/s)  LR: 1.550e-04  Data: 2.801 (1.995)
Train: 33 [ 900/1171 ( 77%)]  Loss:  3.135188 (3.1900)  Time: 0.585s, 1749.48/s  (2.609s,  392.43/s)  LR: 1.550e-04  Data: 0.021 (2.002)
Train: 33 [ 950/1171 ( 81%)]  Loss:  3.441780 (3.2026)  Time: 2.397s,  427.20/s  (2.606s,  392.87/s)  LR: 1.550e-04  Data: 1.646 (2.000)
Train: 33 [1000/1171 ( 85%)]  Loss:  3.437481 (3.2138)  Time: 0.583s, 1756.47/s  (2.605s,  393.11/s)  LR: 1.550e-04  Data: 0.020 (1.999)
Train: 33 [1050/1171 ( 90%)]  Loss:  3.081091 (3.2078)  Time: 0.587s, 1742.99/s  (2.596s,  394.52/s)  LR: 1.550e-04  Data: 0.018 (1.991)
Train: 33 [1100/1171 ( 94%)]  Loss:  3.838437 (3.2352)  Time: 0.583s, 1756.36/s  (2.609s,  392.44/s)  LR: 1.550e-04  Data: 0.021 (2.005)
Train: 33 [1150/1171 ( 98%)]  Loss:  3.592385 (3.2501)  Time: 0.586s, 1747.71/s  (2.611s,  392.16/s)  LR: 1.550e-04  Data: 0.020 (2.009)
Train: 33 [1170/1171 (100%)]  Loss:  3.325845 (3.2531)  Time: 0.563s, 1818.60/s  (2.615s,  391.52/s)  LR: 1.550e-04  Data: 0.000 (2.012)
Test: [   0/97]  Time: 17.270 (17.270)  Loss:  0.3774 (0.3774)  Acc@1: 95.6055 (95.6055)  Acc@5: 99.3164 (99.3164)
Test: [  50/97]  Time: 0.196 (3.898)  Loss:  0.6202 (0.4808)  Acc@1: 89.0625 (92.5513)  Acc@5: 97.6562 (98.5466)
Test: [  97/97]  Time: 0.119 (3.675)  Loss:  0.4371 (0.4964)  Acc@1: 93.4524 (92.1120)  Acc@5: 98.8095 (98.1790)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-33.pth.tar', 92.11200002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-32.pth.tar', 91.83900002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-31.pth.tar', 91.62200006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-30.pth.tar', 91.59299997558594)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-29.pth.tar', 91.15899999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-28.pth.tar', 90.6840000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-27.pth.tar', 90.64900002929687)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-26.pth.tar', 90.28300001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-25.pth.tar', 89.71800004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-24.pth.tar', 89.34900001708985)

Train: 34 [   0/1171 (  0%)]  Loss:  2.787255 (2.7873)  Time: 12.608s,   81.22/s  (12.608s,   81.22/s)  LR: 1.309e-04  Data: 11.816 (11.816)
Train: 34 [  50/1171 (  4%)]  Loss:  3.495780 (3.1415)  Time: 0.586s, 1746.28/s  (2.802s,  365.41/s)  LR: 1.309e-04  Data: 0.020 (2.207)
Train: 34 [ 100/1171 (  9%)]  Loss:  3.478719 (3.2539)  Time: 0.587s, 1743.53/s  (2.710s,  377.86/s)  LR: 1.309e-04  Data: 0.022 (2.123)
Train: 34 [ 150/1171 ( 13%)]  Loss:  2.920984 (3.1707)  Time: 0.583s, 1755.53/s  (2.810s,  364.38/s)  LR: 1.309e-04  Data: 0.018 (2.225)
Train: 34 [ 200/1171 ( 17%)]  Loss:  3.499376 (3.2364)  Time: 0.584s, 1752.18/s  (2.820s,  363.07/s)  LR: 1.309e-04  Data: 0.020 (2.233)
Train: 34 [ 250/1171 ( 21%)]  Loss:  3.536697 (3.2865)  Time: 0.589s, 1739.21/s  (2.764s,  370.46/s)  LR: 1.309e-04  Data: 0.018 (2.173)
Train: 34 [ 300/1171 ( 26%)]  Loss:  3.614347 (3.3333)  Time: 0.586s, 1748.00/s  (2.734s,  374.59/s)  LR: 1.309e-04  Data: 0.020 (2.143)
Train: 34 [ 350/1171 ( 30%)]  Loss:  3.089648 (3.3029)  Time: 0.584s, 1754.67/s  (2.690s,  380.67/s)  LR: 1.309e-04  Data: 0.020 (2.097)
Train: 34 [ 400/1171 ( 34%)]  Loss:  3.139110 (3.2847)  Time: 0.587s, 1744.18/s  (2.673s,  383.14/s)  LR: 1.309e-04  Data: 0.017 (2.076)
Train: 34 [ 450/1171 ( 38%)]  Loss:  3.173482 (3.2735)  Time: 0.587s, 1744.74/s  (2.646s,  386.96/s)  LR: 1.309e-04  Data: 0.021 (2.049)
Train: 34 [ 500/1171 ( 43%)]  Loss:  3.575587 (3.3010)  Time: 0.589s, 1739.32/s  (2.700s,  379.27/s)  LR: 1.309e-04  Data: 0.017 (2.103)
Train: 34 [ 550/1171 ( 47%)]  Loss:  3.209551 (3.2934)  Time: 0.587s, 1745.34/s  (2.707s,  378.34/s)  LR: 1.309e-04  Data: 0.020 (2.112)
Train: 34 [ 600/1171 ( 51%)]  Loss:  3.165370 (3.2835)  Time: 0.582s, 1760.88/s  (2.725s,  375.71/s)  LR: 1.309e-04  Data: 0.018 (2.131)
Train: 34 [ 650/1171 ( 56%)]  Loss:  3.349238 (3.2882)  Time: 0.585s, 1751.42/s  (2.720s,  376.51/s)  LR: 1.309e-04  Data: 0.022 (2.126)
Train: 34 [ 700/1171 ( 60%)]  Loss:  3.160912 (3.2797)  Time: 0.585s, 1751.30/s  (2.710s,  377.81/s)  LR: 1.309e-04  Data: 0.018 (2.117)
Train: 34 [ 750/1171 ( 64%)]  Loss:  3.547469 (3.2965)  Time: 0.584s, 1753.86/s  (2.688s,  381.01/s)  LR: 1.309e-04  Data: 0.018 (2.095)
Train: 34 [ 800/1171 ( 68%)]  Loss:  2.988969 (3.2784)  Time: 0.581s, 1761.80/s  (2.703s,  378.79/s)  LR: 1.309e-04  Data: 0.018 (2.111)
Train: 34 [ 850/1171 ( 73%)]  Loss:  3.433230 (3.2870)  Time: 0.585s, 1749.24/s  (2.700s,  379.21/s)  LR: 1.309e-04  Data: 0.021 (2.108)
Train: 34 [ 900/1171 ( 77%)]  Loss:  3.693895 (3.3084)  Time: 0.585s, 1750.83/s  (2.703s,  378.78/s)  LR: 1.309e-04  Data: 0.019 (2.111)
Train: 34 [ 950/1171 ( 81%)]  Loss:  3.630836 (3.3245)  Time: 0.585s, 1750.24/s  (2.697s,  379.74/s)  LR: 1.309e-04  Data: 0.019 (2.105)
Train: 34 [1000/1171 ( 85%)]  Loss:  3.268039 (3.3218)  Time: 0.586s, 1746.97/s  (2.693s,  380.29/s)  LR: 1.309e-04  Data: 0.020 (2.102)
Train: 34 [1050/1171 ( 90%)]  Loss:  3.023729 (3.3083)  Time: 0.582s, 1759.66/s  (2.680s,  382.05/s)  LR: 1.309e-04  Data: 0.020 (2.089)
Train: 34 [1100/1171 ( 94%)]  Loss:  2.813863 (3.2868)  Time: 0.586s, 1748.85/s  (2.675s,  382.75/s)  LR: 1.309e-04  Data: 0.018 (2.084)
Train: 34 [1150/1171 ( 98%)]  Loss:  3.311583 (3.2878)  Time: 2.713s,  377.47/s  (2.680s,  382.08/s)  LR: 1.309e-04  Data: 2.150 (2.088)
Train: 34 [1170/1171 (100%)]  Loss:  2.623897 (3.2613)  Time: 0.562s, 1820.78/s  (2.676s,  382.67/s)  LR: 1.309e-04  Data: 0.000 (2.084)
Test: [   0/97]  Time: 14.662 (14.662)  Loss:  0.3632 (0.3632)  Acc@1: 95.7031 (95.7031)  Acc@5: 99.3164 (99.3164)
Test: [  50/97]  Time: 0.196 (3.533)  Loss:  0.5898 (0.4656)  Acc@1: 88.5742 (92.7888)  Acc@5: 97.8516 (98.6252)
Test: [  97/97]  Time: 0.119 (3.418)  Loss:  0.4169 (0.4819)  Acc@1: 94.3452 (92.2190)  Acc@5: 98.8095 (98.2370)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-34.pth.tar', 92.21900004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-33.pth.tar', 92.11200002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-32.pth.tar', 91.83900002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-31.pth.tar', 91.62200006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-30.pth.tar', 91.59299997558594)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-29.pth.tar', 91.15899999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-28.pth.tar', 90.6840000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-27.pth.tar', 90.64900002929687)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-26.pth.tar', 90.28300001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-25.pth.tar', 89.71800004394531)

Train: 35 [   0/1171 (  0%)]  Loss:  2.611861 (2.6119)  Time: 12.079s,   84.77/s  (12.079s,   84.77/s)  LR: 1.087e-04  Data: 11.391 (11.391)
Train: 35 [  50/1171 (  4%)]  Loss:  3.385889 (2.9989)  Time: 0.586s, 1746.20/s  (2.574s,  397.78/s)  LR: 1.087e-04  Data: 0.018 (1.977)
Train: 35 [ 100/1171 (  9%)]  Loss:  3.036242 (3.0113)  Time: 6.143s,  166.68/s  (2.599s,  393.93/s)  LR: 1.087e-04  Data: 5.578 (1.990)
Train: 35 [ 150/1171 ( 13%)]  Loss:  2.675456 (2.9274)  Time: 0.583s, 1756.72/s  (2.521s,  406.22/s)  LR: 1.087e-04  Data: 0.019 (1.916)
Train: 35 [ 200/1171 ( 17%)]  Loss:  2.911890 (2.9243)  Time: 7.454s,  137.38/s  (2.639s,  387.99/s)  LR: 1.087e-04  Data: 6.794 (2.034)
Train: 35 [ 250/1171 ( 21%)]  Loss:  3.129572 (2.9585)  Time: 0.583s, 1756.05/s  (2.642s,  387.59/s)  LR: 1.087e-04  Data: 0.020 (2.035)
Train: 35 [ 300/1171 ( 26%)]  Loss:  3.371851 (3.0175)  Time: 7.891s,  129.77/s  (2.666s,  384.07/s)  LR: 1.087e-04  Data: 7.328 (2.058)
Train: 35 [ 350/1171 ( 30%)]  Loss:  3.572035 (3.0868)  Time: 0.586s, 1746.07/s  (2.648s,  386.75/s)  LR: 1.087e-04  Data: 0.018 (2.042)
Train: 35 [ 400/1171 ( 34%)]  Loss:  3.587199 (3.1424)  Time: 8.003s,  127.95/s  (2.655s,  385.69/s)  LR: 1.087e-04  Data: 7.323 (2.051)
Train: 35 [ 450/1171 ( 38%)]  Loss:  3.239103 (3.1521)  Time: 0.583s, 1755.40/s  (2.636s,  388.41/s)  LR: 1.087e-04  Data: 0.020 (2.035)
Train: 35 [ 500/1171 ( 43%)]  Loss:  2.781925 (3.1185)  Time: 8.535s,  119.97/s  (2.638s,  388.16/s)  LR: 1.087e-04  Data: 7.968 (2.039)
Train: 35 [ 550/1171 ( 47%)]  Loss:  3.369502 (3.1394)  Time: 0.584s, 1754.22/s  (2.682s,  381.82/s)  LR: 1.087e-04  Data: 0.019 (2.084)
Train: 35 [ 600/1171 ( 51%)]  Loss:  2.279276 (3.0732)  Time: 8.536s,  119.96/s  (2.701s,  379.14/s)  LR: 1.087e-04  Data: 7.947 (2.104)
Train: 35 [ 650/1171 ( 56%)]  Loss:  2.945615 (3.0641)  Time: 0.587s, 1743.54/s  (2.686s,  381.30/s)  LR: 1.087e-04  Data: 0.022 (2.089)

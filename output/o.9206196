--Start--
Sun Jun 6 13:59:02 JST 2021
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 3
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
wandb: Currently logged in as: gyama_x (use `wandb login --relogin` to force relogin)
wandb: WARNING Tried to auto resume run with id PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1 but id PreTraining_vit_deit_tiny_patch16_224_1k is set.
wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
wandb: Tracking run with wandb version 0.10.27
wandb: Resuming run PreTraining_vit_deit_tiny_patch16_224_1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gyama_x/pytorch-image-models
wandb: üöÄ View run at https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_1k
wandb: Run data is saved locally in /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210606_135942-PreTraining_vit_deit_tiny_patch16_224_1k
wandb: Run `wandb offline` to turn off syncing.

Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar' (epoch 187)
Using native Torch DistributedDataParallel.
Scheduled epochs: 210
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Train: 188 [   0/1251 (  0%)]  Loss:  4.586035 (4.5860)  Time: 17.578s,   58.26/s  (17.578s,   58.26/s)  LR: 3.657e-05  Data: 16.245 (16.245)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 188 [  50/1251 (  4%)]  Loss:  4.754755 (4.6704)  Time: 0.584s, 1752.42/s  (2.545s,  402.43/s)  LR: 3.657e-05  Data: 0.021 (1.933)
Train: 188 [ 100/1251 (  8%)]  Loss:  4.629257 (4.6567)  Time: 0.589s, 1738.17/s  (2.420s,  423.05/s)  LR: 3.657e-05  Data: 0.021 (1.819)
Train: 188 [ 150/1251 ( 12%)]  Loss:  4.147996 (4.5295)  Time: 0.583s, 1756.23/s  (2.292s,  446.70/s)  LR: 3.657e-05  Data: 0.020 (1.698)
Train: 188 [ 200/1251 ( 16%)]  Loss:  4.216698 (4.4669)  Time: 0.586s, 1748.55/s  (2.264s,  452.34/s)  LR: 3.657e-05  Data: 0.021 (1.670)
Train: 188 [ 250/1251 ( 20%)]  Loss:  3.972712 (4.3846)  Time: 0.586s, 1746.44/s  (2.224s,  460.50/s)  LR: 3.657e-05  Data: 0.022 (1.630)
Train: 188 [ 300/1251 ( 24%)]  Loss:  4.728921 (4.4338)  Time: 0.584s, 1754.25/s  (2.204s,  464.64/s)  LR: 3.657e-05  Data: 0.021 (1.612)
Train: 188 [ 350/1251 ( 28%)]  Loss:  4.268163 (4.4131)  Time: 0.586s, 1746.11/s  (2.172s,  471.40/s)  LR: 3.657e-05  Data: 0.023 (1.578)
Train: 188 [ 400/1251 ( 32%)]  Loss:  4.672135 (4.4419)  Time: 0.587s, 1743.65/s  (2.172s,  471.42/s)  LR: 3.657e-05  Data: 0.022 (1.580)
Train: 188 [ 450/1251 ( 36%)]  Loss:  4.323913 (4.4301)  Time: 0.814s, 1258.28/s  (2.155s,  475.17/s)  LR: 3.657e-05  Data: 0.246 (1.564)
Train: 188 [ 500/1251 ( 40%)]  Loss:  4.222942 (4.4112)  Time: 0.583s, 1755.51/s  (2.154s,  475.31/s)  LR: 3.657e-05  Data: 0.020 (1.562)
Train: 188 [ 550/1251 ( 44%)]  Loss:  4.443089 (4.4139)  Time: 0.908s, 1127.25/s  (2.133s,  480.11/s)  LR: 3.657e-05  Data: 0.234 (1.539)
Train: 188 [ 600/1251 ( 48%)]  Loss:  5.168708 (4.4719)  Time: 1.964s,  521.32/s  (2.133s,  480.13/s)  LR: 3.657e-05  Data: 1.298 (1.537)
Train: 188 [ 650/1251 ( 52%)]  Loss:  4.352101 (4.4634)  Time: 0.585s, 1750.37/s  (2.120s,  483.04/s)  LR: 3.657e-05  Data: 0.022 (1.525)
Train: 188 [ 700/1251 ( 56%)]  Loss:  4.648986 (4.4758)  Time: 0.583s, 1755.35/s  (2.122s,  482.63/s)  LR: 3.657e-05  Data: 0.020 (1.528)
Train: 188 [ 750/1251 ( 60%)]  Loss:  4.975553 (4.5070)  Time: 0.587s, 1745.53/s  (2.118s,  483.56/s)  LR: 3.657e-05  Data: 0.020 (1.524)
Train: 188 [ 800/1251 ( 64%)]  Loss:  4.306664 (4.4952)  Time: 0.585s, 1751.91/s  (2.126s,  481.66/s)  LR: 3.657e-05  Data: 0.022 (1.533)
Train: 188 [ 850/1251 ( 68%)]  Loss:  4.462167 (4.4934)  Time: 0.589s, 1738.09/s  (2.146s,  477.22/s)  LR: 3.657e-05  Data: 0.019 (1.553)
Train: 188 [ 900/1251 ( 72%)]  Loss:  4.813373 (4.5102)  Time: 0.670s, 1527.51/s  (2.157s,  474.84/s)  LR: 3.657e-05  Data: 0.088 (1.564)
Train: 188 [ 950/1251 ( 76%)]  Loss:  4.250782 (4.4972)  Time: 0.584s, 1752.11/s  (2.162s,  473.60/s)  LR: 3.657e-05  Data: 0.021 (1.570)
Train: 188 [1000/1251 ( 80%)]  Loss:  4.106246 (4.4786)  Time: 1.090s,  939.78/s  (2.175s,  470.70/s)  LR: 3.657e-05  Data: 0.395 (1.583)
Train: 188 [1050/1251 ( 84%)]  Loss:  4.341173 (4.4724)  Time: 0.584s, 1752.93/s  (2.179s,  469.94/s)  LR: 3.657e-05  Data: 0.021 (1.585)
Train: 188 [1100/1251 ( 88%)]  Loss:  4.480535 (4.4727)  Time: 1.451s,  705.80/s  (2.198s,  465.96/s)  LR: 3.657e-05  Data: 0.856 (1.604)
Train: 188 [1150/1251 ( 92%)]  Loss:  4.122288 (4.4581)  Time: 0.585s, 1750.84/s  (2.203s,  464.81/s)  LR: 3.657e-05  Data: 0.023 (1.609)
Train: 188 [1200/1251 ( 96%)]  Loss:  4.220811 (4.4486)  Time: 10.361s,   98.83/s  (2.210s,  463.38/s)  LR: 3.657e-05  Data: 9.693 (1.616)
Train: 188 [1250/1251 (100%)]  Loss:  4.423550 (4.4477)  Time: 0.563s, 1819.61/s  (2.213s,  462.66/s)  LR: 3.657e-05  Data: 0.000 (1.619)
Test: [   0/48]  Time: 14.693 (14.693)  Loss:  1.0179 (1.0179)  Acc@1: 79.0039 (79.0039)  Acc@5: 93.4570 (93.4570)
Test: [  48/48]  Time: 0.569 (3.500)  Loss:  1.0706 (1.8938)  Acc@1: 77.5943 (58.0180)  Acc@5: 90.9198 (81.7720)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-188.pth.tar', 58.018000092773434)

Train: 189 [   0/1251 (  0%)]  Loss:  4.354164 (4.3542)  Time: 6.743s,  151.87/s  (6.743s,  151.87/s)  LR: 3.423e-05  Data: 6.107 (6.107)
Train: 189 [  50/1251 (  4%)]  Loss:  4.395271 (4.3747)  Time: 0.586s, 1746.12/s  (2.390s,  428.41/s)  LR: 3.423e-05  Data: 0.019 (1.799)
Train: 189 [ 100/1251 (  8%)]  Loss:  4.784398 (4.5113)  Time: 0.583s, 1755.12/s  (2.372s,  431.76/s)  LR: 3.423e-05  Data: 0.020 (1.778)
Train: 189 [ 150/1251 ( 12%)]  Loss:  4.127767 (4.4154)  Time: 0.586s, 1746.74/s  (2.306s,  444.13/s)  LR: 3.423e-05  Data: 0.019 (1.713)
Train: 189 [ 200/1251 ( 16%)]  Loss:  4.427083 (4.4177)  Time: 2.072s,  494.18/s  (2.294s,  446.41/s)  LR: 3.423e-05  Data: 1.485 (1.697)
Train: 189 [ 250/1251 ( 20%)]  Loss:  3.863183 (4.3253)  Time: 0.585s, 1750.00/s  (2.258s,  453.58/s)  LR: 3.423e-05  Data: 0.021 (1.658)
Train: 189 [ 300/1251 ( 24%)]  Loss:  5.015461 (4.4239)  Time: 3.657s,  280.03/s  (2.316s,  442.06/s)  LR: 3.423e-05  Data: 3.074 (1.716)
Train: 189 [ 350/1251 ( 28%)]  Loss:  4.167601 (4.3919)  Time: 0.589s, 1737.79/s  (2.310s,  443.33/s)  LR: 3.423e-05  Data: 0.020 (1.711)
Train: 189 [ 400/1251 ( 32%)]  Loss:  4.507111 (4.4047)  Time: 1.673s,  612.09/s  (2.313s,  442.70/s)  LR: 3.423e-05  Data: 1.020 (1.712)
Train: 189 [ 450/1251 ( 36%)]  Loss:  4.843587 (4.4486)  Time: 0.590s, 1735.95/s  (2.300s,  445.18/s)  LR: 3.423e-05  Data: 0.019 (1.700)
Train: 189 [ 500/1251 ( 40%)]  Loss:  4.103628 (4.4172)  Time: 3.069s,  333.62/s  (2.302s,  444.77/s)  LR: 3.423e-05  Data: 2.497 (1.701)
Train: 189 [ 550/1251 ( 44%)]  Loss:  4.763990 (4.4461)  Time: 0.588s, 1742.14/s  (2.290s,  447.20/s)  LR: 3.423e-05  Data: 0.019 (1.689)
Train: 189 [ 600/1251 ( 48%)]  Loss:  4.783237 (4.4720)  Time: 1.584s,  646.59/s  (2.292s,  446.82/s)  LR: 3.423e-05  Data: 1.015 (1.692)
Train: 189 [ 650/1251 ( 52%)]  Loss:  4.780589 (4.4941)  Time: 0.584s, 1753.12/s  (2.285s,  448.20/s)  LR: 3.423e-05  Data: 0.022 (1.686)
Train: 189 [ 700/1251 ( 56%)]  Loss:  4.250509 (4.4778)  Time: 0.587s, 1743.29/s  (2.314s,  442.44/s)  LR: 3.423e-05  Data: 0.019 (1.713)
Train: 189 [ 750/1251 ( 60%)]  Loss:  4.775510 (4.4964)  Time: 0.585s, 1749.20/s  (2.319s,  441.59/s)  LR: 3.423e-05  Data: 0.019 (1.719)
Train: 189 [ 800/1251 ( 64%)]  Loss:  5.007891 (4.5265)  Time: 0.587s, 1744.91/s  (2.328s,  439.87/s)  LR: 3.423e-05  Data: 0.019 (1.728)
Train: 189 [ 850/1251 ( 68%)]  Loss:  4.430472 (4.5212)  Time: 1.172s,  873.86/s  (2.325s,  440.39/s)  LR: 3.423e-05  Data: 0.580 (1.726)
Train: 189 [ 900/1251 ( 72%)]  Loss:  4.606414 (4.5257)  Time: 0.585s, 1750.63/s  (2.329s,  439.59/s)  LR: 3.423e-05  Data: 0.021 (1.731)
Train: 189 [ 950/1251 ( 76%)]  Loss:  4.347428 (4.5168)  Time: 0.587s, 1745.19/s  (2.327s,  439.99/s)  LR: 3.423e-05  Data: 0.020 (1.729)
Train: 189 [1000/1251 ( 80%)]  Loss:  4.507707 (4.5163)  Time: 0.658s, 1556.36/s  (2.327s,  439.97/s)  LR: 3.423e-05  Data: 0.020 (1.729)
Train: 189 [1050/1251 ( 84%)]  Loss:  4.679149 (4.5237)  Time: 0.582s, 1759.18/s  (2.333s,  438.95/s)  LR: 3.423e-05  Data: 0.020 (1.735)
Train: 189 [1100/1251 ( 88%)]  Loss:  4.939844 (4.5418)  Time: 0.587s, 1745.41/s  (2.337s,  438.14/s)  LR: 3.423e-05  Data: 0.021 (1.740)
Train: 189 [1150/1251 ( 92%)]  Loss:  4.981184 (4.5601)  Time: 0.585s, 1750.71/s  (2.336s,  438.36/s)  LR: 3.423e-05  Data: 0.021 (1.740)
Train: 189 [1200/1251 ( 96%)]  Loss:  4.604986 (4.5619)  Time: 0.589s, 1737.22/s  (2.337s,  438.17/s)  LR: 3.423e-05  Data: 0.019 (1.742)
Train: 189 [1250/1251 (100%)]  Loss:  4.661406 (4.5658)  Time: 0.563s, 1819.24/s  (2.334s,  438.73/s)  LR: 3.423e-05  Data: 0.000 (1.738)
Test: [   0/48]  Time: 14.419 (14.419)  Loss:  1.0287 (1.0287)  Acc@1: 78.4180 (78.4180)  Acc@5: 93.6523 (93.6523)
Test: [  48/48]  Time: 0.150 (3.288)  Loss:  1.0596 (1.8794)  Acc@1: 77.7123 (58.1920)  Acc@5: 91.3915 (81.8360)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-189.pth.tar', 58.192000014648436)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-188.pth.tar', 58.018000092773434)

Train: 190 [   0/1251 (  0%)]  Loss:  4.804276 (4.8043)  Time: 10.770s,   95.08/s  (10.770s,   95.08/s)  LR: 3.199e-05  Data: 10.208 (10.208)
Train: 190 [  50/1251 (  4%)]  Loss:  4.106849 (4.4556)  Time: 0.587s, 1744.09/s  (2.323s,  440.75/s)  LR: 3.199e-05  Data: 0.024 (1.725)
Train: 190 [ 100/1251 (  8%)]  Loss:  4.242973 (4.3847)  Time: 0.791s, 1295.34/s  (2.359s,  434.10/s)  LR: 3.199e-05  Data: 0.205 (1.763)
Train: 190 [ 150/1251 ( 12%)]  Loss:  4.777153 (4.4828)  Time: 0.586s, 1748.29/s  (2.404s,  426.01/s)  LR: 3.199e-05  Data: 0.019 (1.812)
Train: 190 [ 200/1251 ( 16%)]  Loss:  4.778717 (4.5420)  Time: 3.115s,  328.73/s  (2.439s,  419.83/s)  LR: 3.199e-05  Data: 2.464 (1.844)
Train: 190 [ 250/1251 ( 20%)]  Loss:  4.781758 (4.5820)  Time: 0.583s, 1757.17/s  (2.412s,  424.48/s)  LR: 3.199e-05  Data: 0.019 (1.817)
Train: 190 [ 300/1251 ( 24%)]  Loss:  4.773544 (4.6093)  Time: 3.216s,  318.36/s  (2.395s,  427.49/s)  LR: 3.199e-05  Data: 2.540 (1.796)
Train: 190 [ 350/1251 ( 28%)]  Loss:  4.500309 (4.5957)  Time: 0.843s, 1214.32/s  (2.364s,  433.16/s)  LR: 3.199e-05  Data: 0.192 (1.765)
Train: 190 [ 400/1251 ( 32%)]  Loss:  4.459534 (4.5806)  Time: 4.590s,  223.08/s  (2.357s,  434.51/s)  LR: 3.199e-05  Data: 3.991 (1.758)
Train: 190 [ 450/1251 ( 36%)]  Loss:  4.536391 (4.5762)  Time: 0.586s, 1746.44/s  (2.330s,  439.42/s)  LR: 3.199e-05  Data: 0.022 (1.733)
Train: 190 [ 500/1251 ( 40%)]  Loss:  4.172258 (4.5394)  Time: 0.989s, 1035.15/s  (2.357s,  434.46/s)  LR: 3.199e-05  Data: 0.318 (1.759)
Train: 190 [ 550/1251 ( 44%)]  Loss:  4.166704 (4.5084)  Time: 0.589s, 1737.92/s  (2.356s,  434.72/s)  LR: 3.199e-05  Data: 0.018 (1.756)
Train: 190 [ 600/1251 ( 48%)]  Loss:  4.125256 (4.4789)  Time: 2.711s,  377.67/s  (2.378s,  430.60/s)  LR: 3.199e-05  Data: 2.030 (1.780)
Train: 190 [ 650/1251 ( 52%)]  Loss:  3.875462 (4.4358)  Time: 0.587s, 1744.55/s  (2.384s,  429.49/s)  LR: 3.199e-05  Data: 0.020 (1.785)
Train: 190 [ 700/1251 ( 56%)]  Loss:  4.214982 (4.4211)  Time: 2.368s,  432.41/s  (2.391s,  428.30/s)  LR: 3.199e-05  Data: 1.805 (1.792)
Train: 190 [ 750/1251 ( 60%)]  Loss:  4.851133 (4.4480)  Time: 0.583s, 1754.93/s  (2.382s,  429.84/s)  LR: 3.199e-05  Data: 0.021 (1.783)
Train: 190 [ 800/1251 ( 64%)]  Loss:  4.375456 (4.4437)  Time: 1.799s,  569.32/s  (2.377s,  430.71/s)  LR: 3.199e-05  Data: 1.234 (1.778)
Train: 190 [ 850/1251 ( 68%)]  Loss:  3.585431 (4.3960)  Time: 0.589s, 1739.46/s  (2.365s,  432.93/s)  LR: 3.199e-05  Data: 0.023 (1.766)
Train: 190 [ 900/1251 ( 72%)]  Loss:  4.617115 (4.4076)  Time: 0.585s, 1750.28/s  (2.389s,  428.65/s)  LR: 3.199e-05  Data: 0.020 (1.791)
Train: 190 [ 950/1251 ( 76%)]  Loss:  4.773388 (4.4259)  Time: 0.585s, 1751.76/s  (2.390s,  428.45/s)  LR: 3.199e-05  Data: 0.023 (1.792)
Train: 190 [1000/1251 ( 80%)]  Loss:  4.393237 (4.4244)  Time: 0.587s, 1743.42/s  (2.396s,  427.40/s)  LR: 3.199e-05  Data: 0.020 (1.799)
Train: 190 [1050/1251 ( 84%)]  Loss:  4.260044 (4.4169)  Time: 0.583s, 1755.44/s  (2.390s,  428.37/s)  LR: 3.199e-05  Data: 0.019 (1.794)
Train: 190 [1100/1251 ( 88%)]  Loss:  4.594455 (4.4246)  Time: 1.917s,  534.14/s  (2.390s,  428.45/s)  LR: 3.199e-05  Data: 1.240 (1.794)
Train: 190 [1150/1251 ( 92%)]  Loss:  4.165186 (4.4138)  Time: 1.724s,  593.94/s  (2.382s,  429.91/s)  LR: 3.199e-05  Data: 1.161 (1.785)
Train: 190 [1200/1251 ( 96%)]  Loss:  4.459810 (4.4157)  Time: 0.589s, 1738.03/s  (2.380s,  430.32/s)  LR: 3.199e-05  Data: 0.021 (1.781)
Train: 190 [1250/1251 (100%)]  Loss:  4.625083 (4.4237)  Time: 0.564s, 1816.20/s  (2.383s,  429.71/s)  LR: 3.199e-05  Data: 0.000 (1.785)
Test: [   0/48]  Time: 13.703 (13.703)  Loss:  1.0239 (1.0239)  Acc@1: 78.6133 (78.6133)  Acc@5: 92.3828 (92.3828)
Test: [  48/48]  Time: 0.149 (3.448)  Loss:  1.0505 (1.8737)  Acc@1: 77.4764 (58.3860)  Acc@5: 91.2736 (81.9420)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-190.pth.tar', 58.38599991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-189.pth.tar', 58.192000014648436)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-188.pth.tar', 58.018000092773434)

Train: 191 [   0/1251 (  0%)]  Loss:  4.531277 (4.5313)  Time: 12.236s,   83.69/s  (12.236s,   83.69/s)  LR: 2.986e-05  Data: 11.526 (11.526)
Train: 191 [  50/1251 (  4%)]  Loss:  4.210066 (4.3707)  Time: 0.585s, 1749.11/s  (2.499s,  409.81/s)  LR: 2.986e-05  Data: 0.023 (1.909)
Train: 191 [ 100/1251 (  8%)]  Loss:  4.594501 (4.4453)  Time: 1.768s,  579.28/s  (2.444s,  418.96/s)  LR: 2.986e-05  Data: 1.201 (1.843)
Train: 191 [ 150/1251 ( 12%)]  Loss:  4.726914 (4.5157)  Time: 0.589s, 1738.85/s  (2.373s,  431.50/s)  LR: 2.986e-05  Data: 0.024 (1.778)
Train: 191 [ 200/1251 ( 16%)]  Loss:  4.229507 (4.4585)  Time: 2.428s,  421.70/s  (2.344s,  436.92/s)  LR: 2.986e-05  Data: 1.785 (1.749)
Train: 191 [ 250/1251 ( 20%)]  Loss:  4.725254 (4.5029)  Time: 0.582s, 1760.50/s  (2.303s,  444.69/s)  LR: 2.986e-05  Data: 0.020 (1.706)
Train: 191 [ 300/1251 ( 24%)]  Loss:  4.703099 (4.5315)  Time: 4.572s,  223.96/s  (2.339s,  437.84/s)  LR: 2.986e-05  Data: 3.907 (1.739)
Train: 191 [ 350/1251 ( 28%)]  Loss:  4.620875 (4.5427)  Time: 0.585s, 1750.86/s  (2.342s,  437.31/s)  LR: 2.986e-05  Data: 0.020 (1.742)
Train: 191 [ 400/1251 ( 32%)]  Loss:  4.395501 (4.5263)  Time: 5.605s,  182.69/s  (2.354s,  435.03/s)  LR: 2.986e-05  Data: 5.018 (1.754)
Train: 191 [ 450/1251 ( 36%)]  Loss:  3.878950 (4.4616)  Time: 0.583s, 1755.12/s  (2.350s,  435.76/s)  LR: 2.986e-05  Data: 0.021 (1.751)
Train: 191 [ 500/1251 ( 40%)]  Loss:  5.124939 (4.5219)  Time: 6.769s,  151.28/s  (2.356s,  434.59/s)  LR: 2.986e-05  Data: 6.085 (1.758)
Train: 191 [ 550/1251 ( 44%)]  Loss:  4.609525 (4.5292)  Time: 0.584s, 1752.01/s  (2.351s,  435.50/s)  LR: 2.986e-05  Data: 0.019 (1.752)
Train: 191 [ 600/1251 ( 48%)]  Loss:  4.670357 (4.5401)  Time: 2.026s,  505.52/s  (2.348s,  436.16/s)  LR: 2.986e-05  Data: 1.425 (1.748)
Train: 191 [ 650/1251 ( 52%)]  Loss:  4.696961 (4.5513)  Time: 0.587s, 1745.91/s  (2.348s,  436.09/s)  LR: 2.986e-05  Data: 0.020 (1.749)
Train: 191 [ 700/1251 ( 56%)]  Loss:  4.726434 (4.5629)  Time: 0.586s, 1746.27/s  (2.364s,  433.17/s)  LR: 2.986e-05  Data: 0.020 (1.763)
Train: 191 [ 750/1251 ( 60%)]  Loss:  4.615806 (4.5662)  Time: 0.584s, 1753.32/s  (2.375s,  431.24/s)  LR: 2.986e-05  Data: 0.020 (1.773)
Train: 191 [ 800/1251 ( 64%)]  Loss:  4.609824 (4.5688)  Time: 0.586s, 1748.59/s  (2.375s,  431.24/s)  LR: 2.986e-05  Data: 0.020 (1.773)
Train: 191 [ 850/1251 ( 68%)]  Loss:  4.004054 (4.5374)  Time: 0.586s, 1746.85/s  (2.376s,  430.98/s)  LR: 2.986e-05  Data: 0.022 (1.774)
Train: 191 [ 900/1251 ( 72%)]  Loss:  4.082178 (4.5135)  Time: 0.589s, 1739.16/s  (2.374s,  431.42/s)  LR: 2.986e-05  Data: 0.020 (1.772)
Train: 191 [ 950/1251 ( 76%)]  Loss:  4.140582 (4.4948)  Time: 0.584s, 1752.50/s  (2.374s,  431.26/s)  LR: 2.986e-05  Data: 0.020 (1.773)
Train: 191 [1000/1251 ( 80%)]  Loss:  4.035374 (4.4730)  Time: 0.592s, 1728.97/s  (2.366s,  432.77/s)  LR: 2.986e-05  Data: 0.019 (1.765)
Train: 191 [1050/1251 ( 84%)]  Loss:  4.731863 (4.4847)  Time: 0.587s, 1743.85/s  (2.372s,  431.70/s)  LR: 2.986e-05  Data: 0.025 (1.771)
Train: 191 [1100/1251 ( 88%)]  Loss:  4.635331 (4.4913)  Time: 0.817s, 1253.34/s  (2.371s,  431.86/s)  LR: 2.986e-05  Data: 0.136 (1.770)
Train: 191 [1150/1251 ( 92%)]  Loss:  4.802713 (4.5042)  Time: 0.585s, 1749.39/s  (2.375s,  431.21/s)  LR: 2.986e-05  Data: 0.020 (1.774)
Train: 191 [1200/1251 ( 96%)]  Loss:  4.157796 (4.4904)  Time: 0.587s, 1745.81/s  (2.372s,  431.65/s)  LR: 2.986e-05  Data: 0.021 (1.772)
Train: 191 [1250/1251 (100%)]  Loss:  4.399914 (4.4869)  Time: 0.562s, 1820.88/s  (2.372s,  431.74/s)  LR: 2.986e-05  Data: 0.000 (1.772)
Test: [   0/48]  Time: 14.161 (14.161)  Loss:  1.0458 (1.0458)  Acc@1: 78.2227 (78.2227)  Acc@5: 92.2852 (92.2852)
Test: [  48/48]  Time: 0.149 (3.325)  Loss:  1.0427 (1.8827)  Acc@1: 77.9481 (58.4480)  Acc@5: 91.6274 (81.9900)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-191.pth.tar', 58.44799998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-190.pth.tar', 58.38599991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-189.pth.tar', 58.192000014648436)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-188.pth.tar', 58.018000092773434)

Train: 192 [   0/1251 (  0%)]  Loss:  4.316085 (4.3161)  Time: 10.864s,   94.25/s  (10.864s,   94.25/s)  LR: 2.784e-05  Data: 9.721 (9.721)
Train: 192 [  50/1251 (  4%)]  Loss:  4.166614 (4.2413)  Time: 0.583s, 1757.07/s  (2.343s,  437.12/s)  LR: 2.784e-05  Data: 0.020 (1.734)
Train: 192 [ 100/1251 (  8%)]  Loss:  4.886542 (4.4564)  Time: 3.762s,  272.22/s  (2.308s,  443.64/s)  LR: 2.784e-05  Data: 2.829 (1.697)
Train: 192 [ 150/1251 ( 12%)]  Loss:  3.710536 (4.2699)  Time: 0.587s, 1743.98/s  (2.380s,  430.19/s)  LR: 2.784e-05  Data: 0.024 (1.770)
Train: 192 [ 200/1251 ( 16%)]  Loss:  4.854259 (4.3868)  Time: 0.587s, 1744.48/s  (2.432s,  421.01/s)  LR: 2.784e-05  Data: 0.022 (1.829)
Train: 192 [ 250/1251 ( 20%)]  Loss:  4.793570 (4.4546)  Time: 0.586s, 1746.11/s  (2.408s,  425.19/s)  LR: 2.784e-05  Data: 0.023 (1.808)
Train: 192 [ 300/1251 ( 24%)]  Loss:  4.531711 (4.4656)  Time: 0.584s, 1752.83/s  (2.414s,  424.11/s)  LR: 2.784e-05  Data: 0.020 (1.816)
Train: 192 [ 350/1251 ( 28%)]  Loss:  4.184174 (4.4304)  Time: 0.585s, 1751.20/s  (2.392s,  428.03/s)  LR: 2.784e-05  Data: 0.022 (1.795)
Train: 192 [ 400/1251 ( 32%)]  Loss:  5.112367 (4.5062)  Time: 0.586s, 1746.28/s  (2.385s,  429.40/s)  LR: 2.784e-05  Data: 0.020 (1.789)
Train: 192 [ 450/1251 ( 36%)]  Loss:  4.130925 (4.4687)  Time: 0.588s, 1741.14/s  (2.357s,  434.42/s)  LR: 2.784e-05  Data: 0.024 (1.762)
Train: 192 [ 500/1251 ( 40%)]  Loss:  4.322913 (4.4554)  Time: 0.584s, 1752.13/s  (2.380s,  430.18/s)  LR: 2.784e-05  Data: 0.021 (1.787)
Train: 192 [ 550/1251 ( 44%)]  Loss:  4.138466 (4.4290)  Time: 0.589s, 1738.71/s  (2.375s,  431.07/s)  LR: 2.784e-05  Data: 0.024 (1.782)
Train: 192 [ 600/1251 ( 48%)]  Loss:  4.788383 (4.4567)  Time: 0.586s, 1746.31/s  (2.390s,  428.36/s)  LR: 2.784e-05  Data: 0.020 (1.797)
Train: 192 [ 650/1251 ( 52%)]  Loss:  4.063184 (4.4286)  Time: 0.584s, 1753.10/s  (2.399s,  426.92/s)  LR: 2.784e-05  Data: 0.020 (1.804)
Train: 192 [ 700/1251 ( 56%)]  Loss:  4.215404 (4.4143)  Time: 0.592s, 1729.37/s  (2.403s,  426.22/s)  LR: 2.784e-05  Data: 0.029 (1.807)
Train: 192 [ 750/1251 ( 60%)]  Loss:  4.536048 (4.4219)  Time: 0.587s, 1744.55/s  (2.403s,  426.18/s)  LR: 2.784e-05  Data: 0.021 (1.806)
Train: 192 [ 800/1251 ( 64%)]  Loss:  4.261592 (4.4125)  Time: 0.590s, 1735.21/s  (2.400s,  426.65/s)  LR: 2.784e-05  Data: 0.024 (1.802)
Train: 192 [ 850/1251 ( 68%)]  Loss:  4.301887 (4.4064)  Time: 0.586s, 1747.47/s  (2.390s,  428.45/s)  LR: 2.784e-05  Data: 0.021 (1.793)
Train: 192 [ 900/1251 ( 72%)]  Loss:  4.194608 (4.3952)  Time: 0.586s, 1746.32/s  (2.410s,  424.86/s)  LR: 2.784e-05  Data: 0.023 (1.813)
Train: 192 [ 950/1251 ( 76%)]  Loss:  4.635395 (4.4072)  Time: 0.585s, 1751.21/s  (2.411s,  424.79/s)  LR: 2.784e-05  Data: 0.020 (1.813)
Train: 192 [1000/1251 ( 80%)]  Loss:  4.713213 (4.4218)  Time: 0.587s, 1745.85/s  (2.414s,  424.22/s)  LR: 2.784e-05  Data: 0.018 (1.816)
Train: 192 [1050/1251 ( 84%)]  Loss:  4.106476 (4.4075)  Time: 0.585s, 1751.47/s  (2.407s,  425.50/s)  LR: 2.784e-05  Data: 0.020 (1.809)
Train: 192 [1100/1251 ( 88%)]  Loss:  4.295608 (4.4026)  Time: 0.586s, 1747.27/s  (2.404s,  426.01/s)  LR: 2.784e-05  Data: 0.019 (1.806)
Train: 192 [1150/1251 ( 92%)]  Loss:  4.931422 (4.4246)  Time: 0.589s, 1737.54/s  (2.397s,  427.25/s)  LR: 2.784e-05  Data: 0.023 (1.799)
Train: 192 [1200/1251 ( 96%)]  Loss:  4.995110 (4.4475)  Time: 0.587s, 1745.73/s  (2.394s,  427.77/s)  LR: 2.784e-05  Data: 0.024 (1.797)
Train: 192 [1250/1251 (100%)]  Loss:  4.554426 (4.4516)  Time: 0.564s, 1815.58/s  (2.398s,  426.96/s)  LR: 2.784e-05  Data: 0.000 (1.801)
Test: [   0/48]  Time: 14.295 (14.295)  Loss:  1.0080 (1.0080)  Acc@1: 78.8086 (78.8086)  Acc@5: 93.4570 (93.4570)
Test: [  48/48]  Time: 0.149 (3.426)  Loss:  1.0460 (1.8632)  Acc@1: 77.7123 (58.6580)  Acc@5: 91.7453 (81.9840)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-192.pth.tar', 58.65800001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-191.pth.tar', 58.44799998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-190.pth.tar', 58.38599991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-189.pth.tar', 58.192000014648436)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-188.pth.tar', 58.018000092773434)

Train: 193 [   0/1251 (  0%)]  Loss:  4.622831 (4.6228)  Time: 12.014s,   85.23/s  (12.014s,   85.23/s)  LR: 2.592e-05  Data: 11.028 (11.028)
Train: 193 [  50/1251 (  4%)]  Loss:  4.893466 (4.7581)  Time: 0.585s, 1749.88/s  (2.556s,  400.60/s)  LR: 2.592e-05  Data: 0.021 (1.950)
Train: 193 [ 100/1251 (  8%)]  Loss:  4.395596 (4.6373)  Time: 3.400s,  301.19/s  (2.508s,  408.23/s)  LR: 2.592e-05  Data: 2.676 (1.902)
Train: 193 [ 150/1251 ( 12%)]  Loss:  4.453973 (4.5915)  Time: 0.583s, 1755.65/s  (2.405s,  425.80/s)  LR: 2.592e-05  Data: 0.020 (1.796)
Train: 193 [ 200/1251 ( 16%)]  Loss:  4.633640 (4.5999)  Time: 2.080s,  492.19/s  (2.381s,  430.14/s)  LR: 2.592e-05  Data: 1.405 (1.771)
Train: 193 [ 250/1251 ( 20%)]  Loss:  4.582274 (4.5970)  Time: 0.587s, 1745.63/s  (2.340s,  437.67/s)  LR: 2.592e-05  Data: 0.019 (1.730)
Train: 193 [ 300/1251 ( 24%)]  Loss:  4.178832 (4.5372)  Time: 3.011s,  340.12/s  (2.383s,  429.63/s)  LR: 2.592e-05  Data: 2.348 (1.772)
Train: 193 [ 350/1251 ( 28%)]  Loss:  4.336124 (4.5121)  Time: 0.583s, 1755.96/s  (2.378s,  430.64/s)  LR: 2.592e-05  Data: 0.021 (1.768)
Train: 193 [ 400/1251 ( 32%)]  Loss:  4.770631 (4.5408)  Time: 3.507s,  291.95/s  (2.390s,  428.38/s)  LR: 2.592e-05  Data: 2.750 (1.779)
Train: 193 [ 450/1251 ( 36%)]  Loss:  4.793549 (4.5661)  Time: 0.585s, 1750.59/s  (2.377s,  430.82/s)  LR: 2.592e-05  Data: 0.020 (1.766)
Train: 193 [ 500/1251 ( 40%)]  Loss:  4.244920 (4.5369)  Time: 2.550s,  401.62/s  (2.382s,  429.98/s)  LR: 2.592e-05  Data: 1.848 (1.768)
Train: 193 [ 550/1251 ( 44%)]  Loss:  4.760405 (4.5555)  Time: 1.968s,  520.32/s  (2.367s,  432.58/s)  LR: 2.592e-05  Data: 1.222 (1.753)
Train: 193 [ 600/1251 ( 48%)]  Loss:  4.905854 (4.5825)  Time: 1.958s,  522.91/s  (2.369s,  432.21/s)  LR: 2.592e-05  Data: 1.396 (1.756)
Train: 193 [ 650/1251 ( 52%)]  Loss:  4.976933 (4.6106)  Time: 1.798s,  569.40/s  (2.370s,  432.13/s)  LR: 2.592e-05  Data: 1.236 (1.757)
Train: 193 [ 700/1251 ( 56%)]  Loss:  4.866261 (4.6277)  Time: 2.264s,  452.22/s  (2.389s,  428.69/s)  LR: 2.592e-05  Data: 1.591 (1.773)
Train: 193 [ 750/1251 ( 60%)]  Loss:  4.750881 (4.6354)  Time: 0.585s, 1749.21/s  (2.395s,  427.48/s)  LR: 2.592e-05  Data: 0.021 (1.780)
Train: 193 [ 800/1251 ( 64%)]  Loss:  4.624938 (4.6348)  Time: 5.000s,  204.78/s  (2.405s,  425.83/s)  LR: 2.592e-05  Data: 4.426 (1.791)
Train: 193 [ 850/1251 ( 68%)]  Loss:  4.572494 (4.6313)  Time: 0.821s, 1247.72/s  (2.404s,  426.04/s)  LR: 2.592e-05  Data: 0.125 (1.791)
Train: 193 [ 900/1251 ( 72%)]  Loss:  4.468230 (4.6227)  Time: 3.538s,  289.41/s  (2.407s,  425.43/s)  LR: 2.592e-05  Data: 2.950 (1.794)
Train: 193 [ 950/1251 ( 76%)]  Loss:  4.801957 (4.6317)  Time: 1.496s,  684.69/s  (2.398s,  426.99/s)  LR: 2.592e-05  Data: 0.851 (1.784)
Train: 193 [1000/1251 ( 80%)]  Loss:  3.955364 (4.5995)  Time: 2.641s,  387.73/s  (2.394s,  427.74/s)  LR: 2.592e-05  Data: 1.949 (1.780)
Train: 193 [1050/1251 ( 84%)]  Loss:  4.123720 (4.5779)  Time: 0.585s, 1750.09/s  (2.402s,  426.34/s)  LR: 2.592e-05  Data: 0.020 (1.788)
Train: 193 [1100/1251 ( 88%)]  Loss:  4.949507 (4.5940)  Time: 1.585s,  646.12/s  (2.404s,  425.96/s)  LR: 2.592e-05  Data: 1.015 (1.791)
Train: 193 [1150/1251 ( 92%)]  Loss:  4.397597 (4.5858)  Time: 1.472s,  695.69/s  (2.405s,  425.74/s)  LR: 2.592e-05  Data: 0.822 (1.790)
Train: 193 [1200/1251 ( 96%)]  Loss:  4.280828 (4.5736)  Time: 2.786s,  367.55/s  (2.406s,  425.57/s)  LR: 2.592e-05  Data: 2.213 (1.790)
Train: 193 [1250/1251 (100%)]  Loss:  4.142360 (4.5570)  Time: 0.565s, 1812.40/s  (2.402s,  426.24/s)  LR: 2.592e-05  Data: 0.000 (1.785)
Test: [   0/48]  Time: 14.391 (14.391)  Loss:  1.0395 (1.0395)  Acc@1: 77.8320 (77.8320)  Acc@5: 92.9688 (92.9688)
Test: [  48/48]  Time: 0.149 (3.297)  Loss:  1.0480 (1.8786)  Acc@1: 77.7123 (58.6320)  Acc@5: 91.9811 (82.0660)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-192.pth.tar', 58.65800001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-193.pth.tar', 58.63200001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-191.pth.tar', 58.44799998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-190.pth.tar', 58.38599991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-189.pth.tar', 58.192000014648436)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-188.pth.tar', 58.018000092773434)

Train: 194 [   0/1251 (  0%)]  Loss:  4.177585 (4.1776)  Time: 11.024s,   92.88/s  (11.024s,   92.88/s)  LR: 2.411e-05  Data: 10.060 (10.060)
Train: 194 [  50/1251 (  4%)]  Loss:  4.405370 (4.2915)  Time: 0.584s, 1754.46/s  (2.362s,  433.58/s)  LR: 2.411e-05  Data: 0.020 (1.770)
Train: 194 [ 100/1251 (  8%)]  Loss:  4.429262 (4.3374)  Time: 1.139s,  899.13/s  (2.447s,  418.52/s)  LR: 2.411e-05  Data: 0.379 (1.841)
Train: 194 [ 150/1251 ( 12%)]  Loss:  4.675632 (4.4220)  Time: 0.587s, 1744.80/s  (2.393s,  427.91/s)  LR: 2.411e-05  Data: 0.020 (1.792)
Train: 194 [ 200/1251 ( 16%)]  Loss:  4.826262 (4.5028)  Time: 0.586s, 1747.05/s  (2.396s,  427.31/s)  LR: 2.411e-05  Data: 0.021 (1.797)
Train: 194 [ 250/1251 ( 20%)]  Loss:  4.795754 (4.5516)  Time: 0.588s, 1742.73/s  (2.378s,  430.70/s)  LR: 2.411e-05  Data: 0.021 (1.781)
Train: 194 [ 300/1251 ( 24%)]  Loss:  4.879535 (4.5985)  Time: 0.585s, 1749.39/s  (2.378s,  430.60/s)  LR: 2.411e-05  Data: 0.019 (1.783)
Train: 194 [ 350/1251 ( 28%)]  Loss:  4.425842 (4.5769)  Time: 1.524s,  671.90/s  (2.360s,  433.96/s)  LR: 2.411e-05  Data: 0.840 (1.763)
Train: 194 [ 400/1251 ( 32%)]  Loss:  4.443061 (4.5620)  Time: 1.842s,  555.93/s  (2.353s,  435.17/s)  LR: 2.411e-05  Data: 1.202 (1.753)
Train: 194 [ 450/1251 ( 36%)]  Loss:  4.872879 (4.5931)  Time: 0.589s, 1738.82/s  (2.329s,  439.67/s)  LR: 2.411e-05  Data: 0.021 (1.729)
Train: 194 [ 500/1251 ( 40%)]  Loss:  4.375103 (4.5733)  Time: 1.926s,  531.71/s  (2.357s,  434.51/s)  LR: 2.411e-05  Data: 1.364 (1.754)
Train: 194 [ 550/1251 ( 44%)]  Loss:  4.064561 (4.5309)  Time: 0.586s, 1746.16/s  (2.368s,  432.48/s)  LR: 2.411e-05  Data: 0.019 (1.767)
Train: 194 [ 600/1251 ( 48%)]  Loss:  4.871181 (4.5571)  Time: 0.585s, 1751.01/s  (2.397s,  427.28/s)  LR: 2.411e-05  Data: 0.022 (1.797)
Train: 194 [ 650/1251 ( 52%)]  Loss:  4.309814 (4.5394)  Time: 0.587s, 1744.70/s  (2.405s,  425.80/s)  LR: 2.411e-05  Data: 0.021 (1.806)
Train: 194 [ 700/1251 ( 56%)]  Loss:  4.723186 (4.5517)  Time: 0.586s, 1746.13/s  (2.412s,  424.54/s)  LR: 2.411e-05  Data: 0.024 (1.814)
Train: 194 [ 750/1251 ( 60%)]  Loss:  3.746465 (4.5013)  Time: 0.582s, 1760.31/s  (2.403s,  426.21/s)  LR: 2.411e-05  Data: 0.020 (1.806)
Train: 194 [ 800/1251 ( 64%)]  Loss:  4.192259 (4.4832)  Time: 1.900s,  539.03/s  (2.406s,  425.62/s)  LR: 2.411e-05  Data: 1.247 (1.809)
Train: 194 [ 850/1251 ( 68%)]  Loss:  4.122390 (4.4631)  Time: 0.587s, 1743.33/s  (2.416s,  423.78/s)  LR: 2.411e-05  Data: 0.020 (1.819)
Train: 194 [ 900/1251 ( 72%)]  Loss:  4.941455 (4.4883)  Time: 2.049s,  499.68/s  (2.423s,  422.61/s)  LR: 2.411e-05  Data: 1.398 (1.826)
Train: 194 [ 950/1251 ( 76%)]  Loss:  4.606595 (4.4942)  Time: 0.586s, 1748.06/s  (2.419s,  423.30/s)  LR: 2.411e-05  Data: 0.023 (1.822)
Train: 194 [1000/1251 ( 80%)]  Loss:  4.708686 (4.5044)  Time: 0.585s, 1751.08/s  (2.422s,  422.83/s)  LR: 2.411e-05  Data: 0.021 (1.824)
Train: 194 [1050/1251 ( 84%)]  Loss:  4.190519 (4.4902)  Time: 0.586s, 1748.67/s  (2.417s,  423.63/s)  LR: 2.411e-05  Data: 0.018 (1.819)
Train: 194 [1100/1251 ( 88%)]  Loss:  4.271152 (4.4806)  Time: 1.430s,  716.21/s  (2.416s,  423.79/s)  LR: 2.411e-05  Data: 0.867 (1.818)
Train: 194 [1150/1251 ( 92%)]  Loss:  3.762492 (4.4507)  Time: 0.586s, 1746.61/s  (2.408s,  425.33/s)  LR: 2.411e-05  Data: 0.019 (1.810)
Train: 194 [1200/1251 ( 96%)]  Loss:  5.117457 (4.4774)  Time: 1.909s,  536.29/s  (2.404s,  425.98/s)  LR: 2.411e-05  Data: 1.328 (1.807)
Train: 194 [1250/1251 (100%)]  Loss:  4.786767 (4.4893)  Time: 0.563s, 1818.38/s  (2.410s,  424.97/s)  LR: 2.411e-05  Data: 0.000 (1.812)
Test: [   0/48]  Time: 15.568 (15.568)  Loss:  1.0305 (1.0305)  Acc@1: 78.4180 (78.4180)  Acc@5: 93.0664 (93.0664)
Test: [  48/48]  Time: 0.149 (3.569)  Loss:  1.0878 (1.8750)  Acc@1: 77.5943 (58.6880)  Acc@5: 91.7453 (81.9900)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-194.pth.tar', 58.68799996337891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-192.pth.tar', 58.65800001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-193.pth.tar', 58.63200001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-191.pth.tar', 58.44799998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-190.pth.tar', 58.38599991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-189.pth.tar', 58.192000014648436)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-188.pth.tar', 58.018000092773434)

Train: 195 [   0/1251 (  0%)]  Loss:  4.669272 (4.6693)  Time: 12.106s,   84.59/s  (12.106s,   84.59/s)  LR: 2.241e-05  Data: 11.212 (11.212)
Train: 195 [  50/1251 (  4%)]  Loss:  4.436946 (4.5531)  Time: 0.585s, 1751.53/s  (2.558s,  400.28/s)  LR: 2.241e-05  Data: 0.021 (1.929)
Train: 195 [ 100/1251 (  8%)]  Loss:  4.965331 (4.6905)  Time: 2.355s,  434.79/s  (2.462s,  415.93/s)  LR: 2.241e-05  Data: 1.693 (1.828)
Train: 195 [ 150/1251 ( 12%)]  Loss:  4.387365 (4.6147)  Time: 0.588s, 1740.48/s  (2.422s,  422.85/s)  LR: 2.241e-05  Data: 0.020 (1.794)
Train: 195 [ 200/1251 ( 16%)]  Loss:  4.748105 (4.6414)  Time: 1.924s,  532.34/s  (2.391s,  428.20/s)  LR: 2.241e-05  Data: 1.249 (1.772)
Train: 195 [ 250/1251 ( 20%)]  Loss:  4.220135 (4.5712)  Time: 0.585s, 1750.01/s  (2.363s,  433.39/s)  LR: 2.241e-05  Data: 0.020 (1.747)
Train: 195 [ 300/1251 ( 24%)]  Loss:  4.457029 (4.5549)  Time: 5.026s,  203.73/s  (2.413s,  424.39/s)  LR: 2.241e-05  Data: 4.445 (1.798)
Train: 195 [ 350/1251 ( 28%)]  Loss:  4.335586 (4.5275)  Time: 0.586s, 1747.10/s  (2.410s,  424.93/s)  LR: 2.241e-05  Data: 0.021 (1.798)
Train: 195 [ 400/1251 ( 32%)]  Loss:  4.607138 (4.5363)  Time: 7.945s,  128.89/s  (2.422s,  422.73/s)  LR: 2.241e-05  Data: 7.287 (1.813)
Train: 195 [ 450/1251 ( 36%)]  Loss:  4.886180 (4.5713)  Time: 1.041s,  983.21/s  (2.414s,  424.24/s)  LR: 2.241e-05  Data: 0.356 (1.806)
Train: 195 [ 500/1251 ( 40%)]  Loss:  3.966943 (4.5164)  Time: 5.764s,  177.67/s  (2.414s,  424.24/s)  LR: 2.241e-05  Data: 5.190 (1.807)
Train: 195 [ 550/1251 ( 44%)]  Loss:  4.076345 (4.4797)  Time: 1.025s,  998.98/s  (2.402s,  426.33/s)  LR: 2.241e-05  Data: 0.361 (1.795)
Train: 195 [ 600/1251 ( 48%)]  Loss:  4.261988 (4.4630)  Time: 5.196s,  197.08/s  (2.407s,  425.42/s)  LR: 2.241e-05  Data: 4.378 (1.800)
Train: 195 [ 650/1251 ( 52%)]  Loss:  4.456843 (4.4625)  Time: 1.145s,  894.38/s  (2.430s,  421.45/s)  LR: 2.241e-05  Data: 0.489 (1.821)
Train: 195 [ 700/1251 ( 56%)]  Loss:  4.811317 (4.4858)  Time: 0.590s, 1735.51/s  (2.442s,  419.39/s)  LR: 2.241e-05  Data: 0.022 (1.835)
Train: 195 [ 750/1251 ( 60%)]  Loss:  4.799860 (4.5054)  Time: 5.688s,  180.03/s  (2.446s,  418.68/s)  LR: 2.241e-05  Data: 5.126 (1.840)
Train: 195 [ 800/1251 ( 64%)]  Loss:  4.291391 (4.4928)  Time: 2.945s,  347.70/s  (2.451s,  417.81/s)  LR: 2.241e-05  Data: 2.290 (1.843)
Train: 195 [ 850/1251 ( 68%)]  Loss:  4.713931 (4.5051)  Time: 0.585s, 1750.15/s  (2.446s,  418.56/s)  LR: 2.241e-05  Data: 0.021 (1.838)
Train: 195 [ 900/1251 ( 72%)]  Loss:  4.531805 (4.5065)  Time: 0.587s, 1744.45/s  (2.449s,  418.16/s)  LR: 2.241e-05  Data: 0.019 (1.841)
Train: 195 [ 950/1251 ( 76%)]  Loss:  3.899356 (4.4761)  Time: 0.583s, 1755.73/s  (2.439s,  419.89/s)  LR: 2.241e-05  Data: 0.020 (1.832)
Train: 195 [1000/1251 ( 80%)]  Loss:  4.181912 (4.4621)  Time: 0.586s, 1748.30/s  (2.456s,  416.96/s)  LR: 2.241e-05  Data: 0.023 (1.851)
Train: 195 [1050/1251 ( 84%)]  Loss:  4.748990 (4.4752)  Time: 0.590s, 1734.96/s  (2.456s,  416.94/s)  LR: 2.241e-05  Data: 0.020 (1.851)
Train: 195 [1100/1251 ( 88%)]  Loss:  4.400437 (4.4719)  Time: 0.586s, 1748.49/s  (2.462s,  415.93/s)  LR: 2.241e-05  Data: 0.023 (1.859)
Train: 195 [1150/1251 ( 92%)]  Loss:  4.535798 (4.4746)  Time: 0.586s, 1748.06/s  (2.463s,  415.78/s)  LR: 2.241e-05  Data: 0.018 (1.860)
Train: 195 [1200/1251 ( 96%)]  Loss:  4.617111 (4.4803)  Time: 0.585s, 1751.56/s  (2.465s,  415.45/s)  LR: 2.241e-05  Data: 0.022 (1.863)
Train: 195 [1250/1251 (100%)]  Loss:  5.067697 (4.5029)  Time: 0.566s, 1808.59/s  (2.459s,  416.42/s)  LR: 2.241e-05  Data: 0.000 (1.858)
Test: [   0/48]  Time: 14.219 (14.219)  Loss:  1.0311 (1.0311)  Acc@1: 78.9062 (78.9062)  Acc@5: 93.1641 (93.1641)
Test: [  48/48]  Time: 0.150 (3.302)  Loss:  1.0770 (1.8788)  Acc@1: 77.3585 (58.6880)  Acc@5: 92.0991 (82.2260)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-195.pth.tar', 58.687999990234374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-194.pth.tar', 58.68799996337891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-192.pth.tar', 58.65800001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-193.pth.tar', 58.63200001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-191.pth.tar', 58.44799998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-190.pth.tar', 58.38599991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-189.pth.tar', 58.192000014648436)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-188.pth.tar', 58.018000092773434)

Train: 196 [   0/1251 (  0%)]  Loss:  4.544106 (4.5441)  Time: 10.982s,   93.24/s  (10.982s,   93.24/s)  LR: 2.082e-05  Data: 10.328 (10.328)
Train: 196 [  50/1251 (  4%)]  Loss:  4.956874 (4.7505)  Time: 0.588s, 1742.29/s  (2.711s,  377.72/s)  LR: 2.082e-05  Data: 0.020 (2.117)
Train: 196 [ 100/1251 (  8%)]  Loss:  4.674858 (4.7253)  Time: 0.589s, 1738.12/s  (2.602s,  393.57/s)  LR: 2.082e-05  Data: 0.027 (2.007)
Train: 196 [ 150/1251 ( 12%)]  Loss:  4.365401 (4.6353)  Time: 0.589s, 1739.99/s  (2.514s,  407.36/s)  LR: 2.082e-05  Data: 0.021 (1.918)
Train: 196 [ 200/1251 ( 16%)]  Loss:  3.890563 (4.4864)  Time: 0.585s, 1749.99/s  (2.495s,  410.46/s)  LR: 2.082e-05  Data: 0.019 (1.902)
Train: 196 [ 250/1251 ( 20%)]  Loss:  4.910513 (4.5571)  Time: 0.584s, 1752.62/s  (2.450s,  417.99/s)  LR: 2.082e-05  Data: 0.019 (1.859)
Train: 196 [ 300/1251 ( 24%)]  Loss:  4.872570 (4.6021)  Time: 0.585s, 1750.65/s  (2.438s,  420.06/s)  LR: 2.082e-05  Data: 0.020 (1.847)
Train: 196 [ 350/1251 ( 28%)]  Loss:  4.561511 (4.5970)  Time: 0.584s, 1754.41/s  (2.397s,  427.14/s)  LR: 2.082e-05  Data: 0.020 (1.808)
Train: 196 [ 400/1251 ( 32%)]  Loss:  4.332646 (4.5677)  Time: 0.586s, 1745.96/s  (2.387s,  428.99/s)  LR: 2.082e-05  Data: 0.022 (1.797)
Train: 196 [ 450/1251 ( 36%)]  Loss:  4.601188 (4.5710)  Time: 0.587s, 1745.57/s  (2.405s,  425.78/s)  LR: 2.082e-05  Data: 0.024 (1.814)
Train: 196 [ 500/1251 ( 40%)]  Loss:  5.125401 (4.6214)  Time: 0.585s, 1750.42/s  (2.416s,  423.81/s)  LR: 2.082e-05  Data: 0.020 (1.825)
Train: 196 [ 550/1251 ( 44%)]  Loss:  4.879731 (4.6429)  Time: 0.585s, 1749.12/s  (2.413s,  424.46/s)  LR: 2.082e-05  Data: 0.020 (1.820)
Train: 196 [ 600/1251 ( 48%)]  Loss:  3.905216 (4.5862)  Time: 0.587s, 1745.20/s  (2.428s,  421.67/s)  LR: 2.082e-05  Data: 0.019 (1.836)
Train: 196 [ 650/1251 ( 52%)]  Loss:  4.483413 (4.5789)  Time: 0.584s, 1752.02/s  (2.430s,  421.40/s)  LR: 2.082e-05  Data: 0.019 (1.838)
Train: 196 [ 700/1251 ( 56%)]  Loss:  4.299430 (4.5602)  Time: 0.586s, 1746.04/s  (2.431s,  421.22/s)  LR: 2.082e-05  Data: 0.020 (1.839)
Train: 196 [ 750/1251 ( 60%)]  Loss:  4.268106 (4.5420)  Time: 0.588s, 1740.39/s  (2.420s,  423.14/s)  LR: 2.082e-05  Data: 0.019 (1.828)
Train: 196 [ 800/1251 ( 64%)]  Loss:  4.904047 (4.5633)  Time: 0.587s, 1743.36/s  (2.436s,  420.31/s)  LR: 2.082e-05  Data: 0.020 (1.844)
Train: 196 [ 850/1251 ( 68%)]  Loss:  4.565503 (4.5634)  Time: 0.596s, 1717.31/s  (2.437s,  420.27/s)  LR: 2.082e-05  Data: 0.022 (1.845)
Train: 196 [ 900/1251 ( 72%)]  Loss:  4.548016 (4.5626)  Time: 0.589s, 1739.26/s  (2.439s,  419.88/s)  LR: 2.082e-05  Data: 0.020 (1.847)
Train: 196 [ 950/1251 ( 76%)]  Loss:  4.244594 (4.5467)  Time: 0.587s, 1744.16/s  (2.431s,  421.30/s)  LR: 2.082e-05  Data: 0.022 (1.839)
Train: 196 [1000/1251 ( 80%)]  Loss:  4.510756 (4.5450)  Time: 0.584s, 1754.66/s  (2.430s,  421.37/s)  LR: 2.082e-05  Data: 0.017 (1.837)
Train: 196 [1050/1251 ( 84%)]  Loss:  4.314067 (4.5345)  Time: 0.584s, 1752.32/s  (2.420s,  423.15/s)  LR: 2.082e-05  Data: 0.021 (1.827)
Train: 196 [1100/1251 ( 88%)]  Loss:  5.103968 (4.5592)  Time: 0.586s, 1748.85/s  (2.417s,  423.64/s)  LR: 2.082e-05  Data: 0.022 (1.824)
Train: 196 [1150/1251 ( 92%)]  Loss:  4.423320 (4.5536)  Time: 5.169s,  198.09/s  (2.413s,  424.34/s)  LR: 2.082e-05  Data: 4.527 (1.820)
Train: 196 [1200/1251 ( 96%)]  Loss:  4.907610 (4.5677)  Time: 0.586s, 1747.31/s  (2.415s,  423.96/s)  LR: 2.082e-05  Data: 0.023 (1.822)
Train: 196 [1250/1251 (100%)]  Loss:  4.445550 (4.5630)  Time: 0.563s, 1818.09/s  (2.415s,  424.03/s)  LR: 2.082e-05  Data: 0.000 (1.821)
Test: [   0/48]  Time: 14.080 (14.080)  Loss:  1.0244 (1.0244)  Acc@1: 79.3945 (79.3945)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.149 (3.390)  Loss:  1.0563 (1.8646)  Acc@1: 78.3019 (58.7560)  Acc@5: 91.9811 (82.3440)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-196.pth.tar', 58.75600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-195.pth.tar', 58.687999990234374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-194.pth.tar', 58.68799996337891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-192.pth.tar', 58.65800001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-193.pth.tar', 58.63200001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-191.pth.tar', 58.44799998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-190.pth.tar', 58.38599991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-189.pth.tar', 58.192000014648436)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-188.pth.tar', 58.018000092773434)

Train: 197 [   0/1251 (  0%)]  Loss:  4.602982 (4.6030)  Time: 11.174s,   91.64/s  (11.174s,   91.64/s)  LR: 1.933e-05  Data: 9.995 (9.995)
Train: 197 [  50/1251 (  4%)]  Loss:  4.318383 (4.4607)  Time: 0.604s, 1695.36/s  (2.368s,  432.36/s)  LR: 1.933e-05  Data: 0.042 (1.773)
Train: 197 [ 100/1251 (  8%)]  Loss:  4.004170 (4.3085)  Time: 2.691s,  380.51/s  (2.336s,  438.27/s)  LR: 1.933e-05  Data: 2.096 (1.737)
Train: 197 [ 150/1251 ( 12%)]  Loss:  4.469972 (4.3489)  Time: 0.590s, 1735.98/s  (2.278s,  449.59/s)  LR: 1.933e-05  Data: 0.022 (1.678)
Train: 197 [ 200/1251 ( 16%)]  Loss:  4.204671 (4.3200)  Time: 2.261s,  452.96/s  (2.289s,  447.37/s)  LR: 1.933e-05  Data: 1.659 (1.689)
Train: 197 [ 250/1251 ( 20%)]  Loss:  4.758266 (4.3931)  Time: 0.585s, 1749.77/s  (2.337s,  438.18/s)  LR: 1.933e-05  Data: 0.019 (1.738)
Train: 197 [ 300/1251 ( 24%)]  Loss:  4.213103 (4.3674)  Time: 5.133s,  199.49/s  (2.360s,  433.83/s)  LR: 1.933e-05  Data: 4.544 (1.762)
Train: 197 [ 350/1251 ( 28%)]  Loss:  4.927292 (4.4374)  Time: 0.586s, 1746.99/s  (2.341s,  437.40/s)  LR: 1.933e-05  Data: 0.019 (1.740)
Train: 197 [ 400/1251 ( 32%)]  Loss:  4.890728 (4.4877)  Time: 0.584s, 1753.82/s  (2.342s,  437.32/s)  LR: 1.933e-05  Data: 0.020 (1.741)
Train: 197 [ 450/1251 ( 36%)]  Loss:  4.147519 (4.4537)  Time: 0.583s, 1755.36/s  (2.330s,  439.53/s)  LR: 1.933e-05  Data: 0.021 (1.732)
Train: 197 [ 500/1251 ( 40%)]  Loss:  4.712166 (4.4772)  Time: 0.585s, 1750.99/s  (2.331s,  439.32/s)  LR: 1.933e-05  Data: 0.022 (1.733)
Train: 197 [ 550/1251 ( 44%)]  Loss:  4.017554 (4.4389)  Time: 0.691s, 1482.86/s  (2.316s,  442.05/s)  LR: 1.933e-05  Data: 0.053 (1.718)
Train: 197 [ 600/1251 ( 48%)]  Loss:  5.051827 (4.4860)  Time: 0.582s, 1758.97/s  (2.355s,  434.87/s)  LR: 1.933e-05  Data: 0.019 (1.756)
Train: 197 [ 650/1251 ( 52%)]  Loss:  4.708031 (4.5019)  Time: 0.586s, 1748.81/s  (2.355s,  434.76/s)  LR: 1.933e-05  Data: 0.020 (1.758)
Train: 197 [ 700/1251 ( 56%)]  Loss:  4.889729 (4.5278)  Time: 0.586s, 1747.85/s  (2.371s,  431.96/s)  LR: 1.933e-05  Data: 0.019 (1.774)
Train: 197 [ 750/1251 ( 60%)]  Loss:  4.272861 (4.5118)  Time: 0.585s, 1751.40/s  (2.370s,  432.00/s)  LR: 1.933e-05  Data: 0.019 (1.773)
Train: 197 [ 800/1251 ( 64%)]  Loss:  4.625785 (4.5185)  Time: 0.587s, 1743.14/s  (2.376s,  430.96/s)  LR: 1.933e-05  Data: 0.020 (1.780)
Train: 197 [ 850/1251 ( 68%)]  Loss:  4.608451 (4.5235)  Time: 0.970s, 1055.18/s  (2.368s,  432.36/s)  LR: 1.933e-05  Data: 0.339 (1.772)
Train: 197 [ 900/1251 ( 72%)]  Loss:  3.749852 (4.4828)  Time: 0.589s, 1739.89/s  (2.365s,  433.03/s)  LR: 1.933e-05  Data: 0.023 (1.769)
Train: 197 [ 950/1251 ( 76%)]  Loss:  4.413461 (4.4793)  Time: 0.584s, 1754.04/s  (2.354s,  435.06/s)  LR: 1.933e-05  Data: 0.021 (1.758)
Train: 197 [1000/1251 ( 80%)]  Loss:  4.215403 (4.4668)  Time: 0.586s, 1746.10/s  (2.370s,  432.00/s)  LR: 1.933e-05  Data: 0.020 (1.776)
Train: 197 [1050/1251 ( 84%)]  Loss:  4.522836 (4.4693)  Time: 0.582s, 1758.38/s  (2.363s,  433.28/s)  LR: 1.933e-05  Data: 0.020 (1.769)
Train: 197 [1100/1251 ( 88%)]  Loss:  4.641446 (4.4768)  Time: 0.587s, 1743.20/s  (2.364s,  433.13/s)  LR: 1.933e-05  Data: 0.021 (1.771)
Train: 197 [1150/1251 ( 92%)]  Loss:  4.650491 (4.4840)  Time: 0.585s, 1750.57/s  (2.361s,  433.80/s)  LR: 1.933e-05  Data: 0.022 (1.767)
Train: 197 [1200/1251 ( 96%)]  Loss:  4.403529 (4.4808)  Time: 0.587s, 1743.91/s  (2.362s,  433.60/s)  LR: 1.933e-05  Data: 0.020 (1.768)
Train: 197 [1250/1251 (100%)]  Loss:  4.406873 (4.4780)  Time: 0.563s, 1818.71/s  (2.353s,  435.26/s)  LR: 1.933e-05  Data: 0.000 (1.759)
Test: [   0/48]  Time: 13.535 (13.535)  Loss:  0.9816 (0.9816)  Acc@1: 79.3945 (79.3945)  Acc@5: 93.4570 (93.4570)
Test: [  48/48]  Time: 0.149 (3.240)  Loss:  1.0375 (1.8563)  Acc@1: 78.1840 (58.8200)  Acc@5: 91.9811 (82.3360)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-197.pth.tar', 58.82000009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-196.pth.tar', 58.75600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-195.pth.tar', 58.687999990234374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-194.pth.tar', 58.68799996337891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-192.pth.tar', 58.65800001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-193.pth.tar', 58.63200001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-191.pth.tar', 58.44799998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-190.pth.tar', 58.38599991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-189.pth.tar', 58.192000014648436)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-188.pth.tar', 58.018000092773434)

Train: 198 [   0/1251 (  0%)]  Loss:  4.564864 (4.5649)  Time: 10.858s,   94.31/s  (10.858s,   94.31/s)  LR: 1.795e-05  Data: 9.664 (9.664)
Train: 198 [  50/1251 (  4%)]  Loss:  4.460246 (4.5126)  Time: 0.590s, 1735.17/s  (2.625s,  390.04/s)  LR: 1.795e-05  Data: 0.021 (2.012)
Train: 198 [ 100/1251 (  8%)]  Loss:  4.635802 (4.5536)  Time: 2.008s,  509.97/s  (2.537s,  403.59/s)  LR: 1.795e-05  Data: 1.387 (1.923)
Train: 198 [ 150/1251 ( 12%)]  Loss:  3.869678 (4.3826)  Time: 0.588s, 1741.60/s  (2.425s,  422.31/s)  LR: 1.795e-05  Data: 0.023 (1.817)
Train: 198 [ 200/1251 ( 16%)]  Loss:  4.751894 (4.4565)  Time: 2.929s,  349.55/s  (2.403s,  426.17/s)  LR: 1.795e-05  Data: 2.356 (1.798)
Train: 198 [ 250/1251 ( 20%)]  Loss:  4.551965 (4.4724)  Time: 0.585s, 1749.06/s  (2.365s,  433.00/s)  LR: 1.795e-05  Data: 0.019 (1.761)
Train: 198 [ 300/1251 ( 24%)]  Loss:  4.920548 (4.5364)  Time: 3.245s,  315.58/s  (2.357s,  434.44/s)  LR: 1.795e-05  Data: 2.594 (1.750)
Train: 198 [ 350/1251 ( 28%)]  Loss:  3.942929 (4.4622)  Time: 0.586s, 1748.86/s  (2.333s,  438.88/s)  LR: 1.795e-05  Data: 0.019 (1.728)
Train: 198 [ 400/1251 ( 32%)]  Loss:  4.453678 (4.4613)  Time: 4.387s,  233.43/s  (2.323s,  440.78/s)  LR: 1.795e-05  Data: 3.704 (1.717)
Train: 198 [ 450/1251 ( 36%)]  Loss:  3.861247 (4.4013)  Time: 0.584s, 1753.96/s  (2.345s,  436.68/s)  LR: 1.795e-05  Data: 0.020 (1.741)
Train: 198 [ 500/1251 ( 40%)]  Loss:  4.693487 (4.4278)  Time: 2.570s,  398.41/s  (2.345s,  436.60/s)  LR: 1.795e-05  Data: 1.876 (1.741)
Train: 198 [ 550/1251 ( 44%)]  Loss:  4.332880 (4.4199)  Time: 0.589s, 1739.20/s  (2.344s,  436.77/s)  LR: 1.795e-05  Data: 0.024 (1.739)
Train: 198 [ 600/1251 ( 48%)]  Loss:  4.948192 (4.4606)  Time: 0.898s, 1140.19/s  (2.358s,  434.20/s)  LR: 1.795e-05  Data: 0.235 (1.753)
Train: 198 [ 650/1251 ( 52%)]  Loss:  4.618518 (4.4719)  Time: 0.590s, 1734.63/s  (2.394s,  427.78/s)  LR: 1.795e-05  Data: 0.025 (1.788)
Train: 198 [ 700/1251 ( 56%)]  Loss:  4.334465 (4.4627)  Time: 0.682s, 1502.26/s  (2.386s,  429.16/s)  LR: 1.795e-05  Data: 0.098 (1.781)
Train: 198 [ 750/1251 ( 60%)]  Loss:  4.558052 (4.4687)  Time: 0.584s, 1752.05/s  (2.385s,  429.26/s)  LR: 1.795e-05  Data: 0.019 (1.780)
Train: 198 [ 800/1251 ( 64%)]  Loss:  4.560597 (4.4741)  Time: 1.007s, 1016.98/s  (2.397s,  427.13/s)  LR: 1.795e-05  Data: 0.302 (1.793)
Train: 198 [ 850/1251 ( 68%)]  Loss:  4.845023 (4.4947)  Time: 0.587s, 1745.44/s  (2.394s,  427.82/s)  LR: 1.795e-05  Data: 0.020 (1.791)
Train: 198 [ 900/1251 ( 72%)]  Loss:  4.669390 (4.5039)  Time: 0.594s, 1723.07/s  (2.392s,  428.11/s)  LR: 1.795e-05  Data: 0.021 (1.790)
Train: 198 [ 950/1251 ( 76%)]  Loss:  4.158321 (4.4866)  Time: 0.588s, 1741.84/s  (2.396s,  427.46/s)  LR: 1.795e-05  Data: 0.025 (1.794)
Train: 198 [1000/1251 ( 80%)]  Loss:  4.520033 (4.4882)  Time: 3.377s,  303.21/s  (2.389s,  428.71/s)  LR: 1.795e-05  Data: 2.728 (1.786)
Train: 198 [1050/1251 ( 84%)]  Loss:  4.421923 (4.4852)  Time: 0.585s, 1750.89/s  (2.383s,  429.66/s)  LR: 1.795e-05  Data: 0.019 (1.781)
Train: 198 [1100/1251 ( 88%)]  Loss:  4.348483 (4.4792)  Time: 4.910s,  208.54/s  (2.380s,  430.18/s)  LR: 1.795e-05  Data: 4.062 (1.778)
Train: 198 [1150/1251 ( 92%)]  Loss:  4.166095 (4.4662)  Time: 0.586s, 1746.62/s  (2.374s,  431.42/s)  LR: 1.795e-05  Data: 0.020 (1.771)
Train: 198 [1200/1251 ( 96%)]  Loss:  4.599919 (4.4715)  Time: 5.562s,  184.09/s  (2.384s,  429.49/s)  LR: 1.795e-05  Data: 4.899 (1.781)
Train: 198 [1250/1251 (100%)]  Loss:  4.669174 (4.4791)  Time: 0.562s, 1821.15/s  (2.382s,  429.96/s)  LR: 1.795e-05  Data: 0.000 (1.779)
Test: [   0/48]  Time: 14.865 (14.865)  Loss:  1.0092 (1.0092)  Acc@1: 79.1992 (79.1992)  Acc@5: 93.1641 (93.1641)
Test: [  48/48]  Time: 0.149 (3.425)  Loss:  1.0609 (1.8689)  Acc@1: 77.4764 (58.7840)  Acc@5: 92.2170 (82.2180)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-197.pth.tar', 58.82000009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-198.pth.tar', 58.78400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-196.pth.tar', 58.75600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-195.pth.tar', 58.687999990234374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-194.pth.tar', 58.68799996337891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-192.pth.tar', 58.65800001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-193.pth.tar', 58.63200001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-191.pth.tar', 58.44799998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-190.pth.tar', 58.38599991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-189.pth.tar', 58.192000014648436)

Train: 199 [   0/1251 (  0%)]  Loss:  4.661397 (4.6614)  Time: 10.613s,   96.49/s  (10.613s,   96.49/s)  LR: 1.669e-05  Data: 9.679 (9.679)
Train: 199 [  50/1251 (  4%)]  Loss:  4.393533 (4.5275)  Time: 0.588s, 1741.49/s  (2.443s,  419.20/s)  LR: 1.669e-05  Data: 0.019 (1.826)
Train: 199 [ 100/1251 (  8%)]  Loss:  4.497206 (4.5174)  Time: 5.893s,  173.76/s  (2.373s,  431.58/s)  LR: 1.669e-05  Data: 5.265 (1.764)
Train: 199 [ 150/1251 ( 12%)]  Loss:  4.896559 (4.6122)  Time: 0.589s, 1739.51/s  (2.311s,  443.15/s)  LR: 1.669e-05  Data: 0.018 (1.704)
Train: 199 [ 200/1251 ( 16%)]  Loss:  4.246403 (4.5390)  Time: 0.593s, 1727.29/s  (2.335s,  438.55/s)  LR: 1.669e-05  Data: 0.023 (1.704)
Train: 199 [ 250/1251 ( 20%)]  Loss:  4.272558 (4.4946)  Time: 0.585s, 1751.33/s  (2.414s,  424.25/s)  LR: 1.669e-05  Data: 0.020 (1.793)
Train: 199 [ 300/1251 ( 24%)]  Loss:  3.880265 (4.4068)  Time: 0.591s, 1731.88/s  (2.422s,  422.81/s)  LR: 1.669e-05  Data: 0.025 (1.807)
Train: 199 [ 350/1251 ( 28%)]  Loss:  4.682230 (4.4413)  Time: 0.586s, 1746.88/s  (2.408s,  425.20/s)  LR: 1.669e-05  Data: 0.020 (1.796)
Train: 199 [ 400/1251 ( 32%)]  Loss:  4.643129 (4.4637)  Time: 0.588s, 1742.15/s  (2.404s,  425.90/s)  LR: 1.669e-05  Data: 0.021 (1.797)
Train: 199 [ 450/1251 ( 36%)]  Loss:  4.745880 (4.4919)  Time: 0.583s, 1755.31/s  (2.387s,  429.00/s)  LR: 1.669e-05  Data: 0.021 (1.781)
Train: 199 [ 500/1251 ( 40%)]  Loss:  4.590145 (4.5008)  Time: 0.583s, 1756.62/s  (2.384s,  429.61/s)  LR: 1.669e-05  Data: 0.019 (1.781)
Train: 199 [ 550/1251 ( 44%)]  Loss:  4.441057 (4.4959)  Time: 0.589s, 1739.51/s  (2.374s,  431.40/s)  LR: 1.669e-05  Data: 0.021 (1.772)
Train: 199 [ 600/1251 ( 48%)]  Loss:  4.303977 (4.4811)  Time: 0.588s, 1742.50/s  (2.402s,  426.31/s)  LR: 1.669e-05  Data: 0.022 (1.802)
Train: 199 [ 650/1251 ( 52%)]  Loss:  4.665954 (4.4943)  Time: 0.585s, 1750.63/s  (2.424s,  422.47/s)  LR: 1.669e-05  Data: 0.022 (1.825)
Train: 199 [ 700/1251 ( 56%)]  Loss:  4.311997 (4.4822)  Time: 0.589s, 1738.00/s  (2.448s,  418.36/s)  LR: 1.669e-05  Data: 0.021 (1.849)
Train: 199 [ 750/1251 ( 60%)]  Loss:  4.251050 (4.4677)  Time: 0.586s, 1746.97/s  (2.451s,  417.71/s)  LR: 1.669e-05  Data: 0.021 (1.854)
Train: 199 [ 800/1251 ( 64%)]  Loss:  4.342186 (4.4603)  Time: 0.583s, 1755.06/s  (2.466s,  415.18/s)  LR: 1.669e-05  Data: 0.018 (1.870)
Train: 199 [ 850/1251 ( 68%)]  Loss:  4.111201 (4.4409)  Time: 0.587s, 1743.97/s  (2.461s,  416.16/s)  LR: 1.669e-05  Data: 0.020 (1.864)
Train: 199 [ 900/1251 ( 72%)]  Loss:  3.897994 (4.4124)  Time: 0.586s, 1747.41/s  (2.460s,  416.30/s)  LR: 1.669e-05  Data: 0.019 (1.864)
Train: 199 [ 950/1251 ( 76%)]  Loss:  4.528415 (4.4182)  Time: 0.585s, 1749.01/s  (2.457s,  416.85/s)  LR: 1.669e-05  Data: 0.017 (1.861)
Train: 199 [1000/1251 ( 80%)]  Loss:  4.026877 (4.3995)  Time: 0.583s, 1756.43/s  (2.462s,  415.92/s)  LR: 1.669e-05  Data: 0.019 (1.867)
Train: 199 [1050/1251 ( 84%)]  Loss:  4.751360 (4.4155)  Time: 0.584s, 1753.26/s  (2.458s,  416.63/s)  LR: 1.669e-05  Data: 0.019 (1.863)
Train: 199 [1100/1251 ( 88%)]  Loss:  4.688755 (4.4274)  Time: 0.588s, 1740.73/s  (2.478s,  413.30/s)  LR: 1.669e-05  Data: 0.018 (1.883)
Train: 199 [1150/1251 ( 92%)]  Loss:  4.534044 (4.4318)  Time: 1.461s,  700.92/s  (2.504s,  409.01/s)  LR: 1.669e-05  Data: 0.874 (1.909)
Train: 199 [1200/1251 ( 96%)]  Loss:  4.491704 (4.4342)  Time: 1.038s,  986.54/s  (2.497s,  410.10/s)  LR: 1.669e-05  Data: 0.377 (1.902)
Train: 199 [1250/1251 (100%)]  Loss:  4.037838 (4.4190)  Time: 0.564s, 1814.00/s  (2.486s,  411.86/s)  LR: 1.669e-05  Data: 0.000 (1.891)
Test: [   0/48]  Time: 14.933 (14.933)  Loss:  1.0105 (1.0105)  Acc@1: 79.1992 (79.1992)  Acc@5: 93.0664 (93.0664)
Test: [  48/48]  Time: 0.149 (3.456)  Loss:  1.0498 (1.8660)  Acc@1: 77.5943 (58.8380)  Acc@5: 91.5094 (82.3360)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-199.pth.tar', 58.837999963378905)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-197.pth.tar', 58.82000009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-198.pth.tar', 58.78400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-196.pth.tar', 58.75600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-195.pth.tar', 58.687999990234374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-194.pth.tar', 58.68799996337891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-192.pth.tar', 58.65800001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-193.pth.tar', 58.63200001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-191.pth.tar', 58.44799998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-190.pth.tar', 58.38599991210938)

Train: 200 [   0/1251 (  0%)]  Loss:  4.637046 (4.6370)  Time: 12.294s,   83.29/s  (12.294s,   83.29/s)  LR: 1.553e-05  Data: 11.175 (11.175)
Train: 200 [  50/1251 (  4%)]  Loss:  4.192095 (4.4146)  Time: 0.585s, 1750.24/s  (2.463s,  415.71/s)  LR: 1.553e-05  Data: 0.018 (1.864)
Train: 200 [ 100/1251 (  8%)]  Loss:  4.582027 (4.4704)  Time: 0.588s, 1742.82/s  (2.440s,  419.74/s)  LR: 1.553e-05  Data: 0.022 (1.843)
Train: 200 [ 150/1251 ( 12%)]  Loss:  4.216414 (4.4069)  Time: 0.589s, 1737.31/s  (2.406s,  425.56/s)  LR: 1.553e-05  Data: 0.021 (1.800)
Train: 200 [ 200/1251 ( 16%)]  Loss:  4.828517 (4.4912)  Time: 1.301s,  786.98/s  (2.404s,  425.94/s)  LR: 1.553e-05  Data: 0.726 (1.805)
Train: 200 [ 250/1251 ( 20%)]  Loss:  4.581300 (4.5062)  Time: 0.586s, 1747.94/s  (2.378s,  430.61/s)  LR: 1.553e-05  Data: 0.022 (1.781)
Train: 200 [ 300/1251 ( 24%)]  Loss:  4.970257 (4.5725)  Time: 0.594s, 1724.88/s  (2.371s,  431.90/s)  LR: 1.553e-05  Data: 0.019 (1.776)
Train: 200 [ 350/1251 ( 28%)]  Loss:  4.617503 (4.5781)  Time: 0.586s, 1748.89/s  (2.346s,  436.50/s)  LR: 1.553e-05  Data: 0.021 (1.752)
Train: 200 [ 400/1251 ( 32%)]  Loss:  4.760074 (4.5984)  Time: 1.710s,  598.79/s  (2.406s,  425.59/s)  LR: 1.553e-05  Data: 1.147 (1.810)
Train: 200 [ 450/1251 ( 36%)]  Loss:  4.572036 (4.5957)  Time: 0.590s, 1734.82/s  (2.418s,  423.53/s)  LR: 1.553e-05  Data: 0.020 (1.821)
Train: 200 [ 500/1251 ( 40%)]  Loss:  4.497539 (4.5868)  Time: 0.583s, 1757.62/s  (2.438s,  420.09/s)  LR: 1.553e-05  Data: 0.020 (1.842)
Train: 200 [ 550/1251 ( 44%)]  Loss:  4.220843 (4.5563)  Time: 0.587s, 1744.25/s  (2.434s,  420.70/s)  LR: 1.553e-05  Data: 0.022 (1.840)
Train: 200 [ 600/1251 ( 48%)]  Loss:  4.658254 (4.5641)  Time: 0.589s, 1739.88/s  (2.477s,  413.37/s)  LR: 1.553e-05  Data: 0.019 (1.884)
Train: 200 [ 650/1251 ( 52%)]  Loss:  4.446476 (4.5557)  Time: 1.271s,  805.77/s  (2.477s,  413.34/s)  LR: 1.553e-05  Data: 0.706 (1.884)
Train: 200 [ 700/1251 ( 56%)]  Loss:  4.401247 (4.5454)  Time: 0.583s, 1756.74/s  (2.533s,  404.21/s)  LR: 1.553e-05  Data: 0.019 (1.940)
Train: 200 [ 750/1251 ( 60%)]  Loss:  4.797658 (4.5612)  Time: 3.841s,  266.62/s  (2.545s,  402.37/s)  LR: 1.553e-05  Data: 3.277 (1.951)
Train: 200 [ 800/1251 ( 64%)]  Loss:  4.072364 (4.5325)  Time: 0.584s, 1752.73/s  (2.544s,  402.56/s)  LR: 1.553e-05  Data: 0.020 (1.950)

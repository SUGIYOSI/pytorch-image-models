--Start--
Mon May 24 14:51:18 JST 2021
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 2
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Added key: store_based_barrier_key:1 to store for rank: 3
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
wandb: Currently logged in as: gyama_x (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.10.30 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.10.27
wandb: Resuming run PreTraining_vit_deit_tiny_patch16_224_1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gyama_x/pytorch-image-models
wandb: üöÄ View run at https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_1k
wandb: Run data is saved locally in /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210524_145227-PreTraining_vit_deit_tiny_patch16_224_1k
wandb: Run `wandb offline` to turn off syncing.

Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar' (epoch 21)
Using native Torch DistributedDataParallel.
Scheduled epochs: 44
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Train: 22 [   0/1251 (  0%)]  Loss:  5.314925 (5.3149)  Time: 13.853s,   73.92/s  (13.853s,   73.92/s)  LR: 5.050e-04  Data: 12.075 (12.075)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 22 [  50/1251 (  4%)]  Loss:  5.510655 (5.4128)  Time: 0.977s, 1047.64/s  (2.752s,  372.11/s)  LR: 5.050e-04  Data: 0.413 (2.132)
Train: 22 [ 100/1251 (  8%)]  Loss:  5.503936 (5.4432)  Time: 0.585s, 1750.38/s  (2.681s,  382.01/s)  LR: 5.050e-04  Data: 0.020 (2.074)
Train: 22 [ 150/1251 ( 12%)]  Loss:  5.273193 (5.4007)  Time: 3.793s,  269.94/s  (2.661s,  384.76/s)  LR: 5.050e-04  Data: 3.229 (2.061)
Train: 22 [ 200/1251 ( 16%)]  Loss:  5.199213 (5.3604)  Time: 0.586s, 1748.07/s  (2.615s,  391.56/s)  LR: 5.050e-04  Data: 0.021 (2.009)
Train: 22 [ 250/1251 ( 20%)]  Loss:  5.140327 (5.3237)  Time: 6.489s,  157.80/s  (2.583s,  396.43/s)  LR: 5.050e-04  Data: 5.816 (1.975)
Train: 22 [ 300/1251 ( 24%)]  Loss:  5.534083 (5.3538)  Time: 1.148s,  891.73/s  (2.531s,  404.62/s)  LR: 5.050e-04  Data: 0.544 (1.927)
Train: 22 [ 350/1251 ( 28%)]  Loss:  5.156787 (5.3291)  Time: 7.021s,  145.84/s  (2.514s,  407.38/s)  LR: 5.050e-04  Data: 6.381 (1.910)
Train: 22 [ 400/1251 ( 32%)]  Loss:  5.606369 (5.3599)  Time: 1.240s,  825.88/s  (2.485s,  412.14/s)  LR: 5.050e-04  Data: 0.676 (1.882)
Train: 22 [ 450/1251 ( 36%)]  Loss:  5.257516 (5.3497)  Time: 5.059s,  202.39/s  (2.465s,  415.35/s)  LR: 5.050e-04  Data: 4.430 (1.861)
Train: 22 [ 500/1251 ( 40%)]  Loss:  5.147599 (5.3313)  Time: 0.584s, 1752.47/s  (2.435s,  420.46/s)  LR: 5.050e-04  Data: 0.019 (1.832)
Train: 22 [ 550/1251 ( 44%)]  Loss:  5.371653 (5.3347)  Time: 3.459s,  296.07/s  (2.445s,  418.86/s)  LR: 5.050e-04  Data: 2.773 (1.838)
Train: 22 [ 600/1251 ( 48%)]  Loss:  5.898067 (5.3780)  Time: 0.583s, 1756.02/s  (2.437s,  420.14/s)  LR: 5.050e-04  Data: 0.019 (1.829)
Train: 22 [ 650/1251 ( 52%)]  Loss:  5.045665 (5.3543)  Time: 0.585s, 1749.18/s  (2.437s,  420.24/s)  LR: 5.050e-04  Data: 0.021 (1.827)
Train: 22 [ 700/1251 ( 56%)]  Loss:  5.594708 (5.3703)  Time: 0.584s, 1752.47/s  (2.438s,  419.95/s)  LR: 5.050e-04  Data: 0.019 (1.830)
Train: 22 [ 750/1251 ( 60%)]  Loss:  5.794035 (5.3968)  Time: 0.583s, 1757.80/s  (2.453s,  417.49/s)  LR: 5.050e-04  Data: 0.019 (1.846)
Train: 22 [ 800/1251 ( 64%)]  Loss:  5.304214 (5.3913)  Time: 0.584s, 1752.40/s  (2.453s,  417.37/s)  LR: 5.050e-04  Data: 0.020 (1.848)
Train: 22 [ 850/1251 ( 68%)]  Loss:  5.360233 (5.3896)  Time: 0.585s, 1749.58/s  (2.460s,  416.24/s)  LR: 5.050e-04  Data: 0.021 (1.857)
Train: 22 [ 900/1251 ( 72%)]  Loss:  5.549239 (5.3980)  Time: 0.586s, 1748.29/s  (2.472s,  414.20/s)  LR: 5.050e-04  Data: 0.020 (1.870)
Train: 22 [ 950/1251 ( 76%)]  Loss:  5.130581 (5.3846)  Time: 0.586s, 1746.93/s  (2.482s,  412.50/s)  LR: 5.050e-04  Data: 0.021 (1.881)
Train: 22 [1000/1251 ( 80%)]  Loss:  4.840653 (5.3587)  Time: 2.285s,  448.19/s  (2.487s,  411.67/s)  LR: 5.050e-04  Data: 1.718 (1.884)
Train: 22 [1050/1251 ( 84%)]  Loss:  5.308703 (5.3565)  Time: 1.802s,  568.25/s  (2.494s,  410.62/s)  LR: 5.050e-04  Data: 1.159 (1.888)
Train: 22 [1100/1251 ( 88%)]  Loss:  5.508003 (5.3631)  Time: 6.942s,  147.51/s  (2.500s,  409.54/s)  LR: 5.050e-04  Data: 6.360 (1.895)
Train: 22 [1150/1251 ( 92%)]  Loss:  5.198984 (5.3562)  Time: 0.586s, 1746.23/s  (2.497s,  410.01/s)  LR: 5.050e-04  Data: 0.021 (1.893)
Train: 22 [1200/1251 ( 96%)]  Loss:  5.007923 (5.3423)  Time: 7.588s,  134.95/s  (2.501s,  409.38/s)  LR: 5.050e-04  Data: 6.998 (1.896)
Train: 22 [1250/1251 (100%)]  Loss:  5.253553 (5.3389)  Time: 0.565s, 1813.47/s  (2.510s,  408.04/s)  LR: 5.050e-04  Data: 0.000 (1.906)
Test: [   0/48]  Time: 17.408 (17.408)  Loss:  2.1462 (2.1462)  Acc@1: 55.6641 (55.6641)  Acc@5: 79.1992 (79.1992)
Test: [  48/48]  Time: 0.473 (3.674)  Loss:  1.9139 (3.1129)  Acc@1: 64.3868 (36.5280)  Acc@5: 80.1887 (61.6560)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-22.pth.tar', 36.52799991455078)

Train: 23 [   0/1251 (  0%)]  Loss:  5.351281 (5.3513)  Time: 8.143s,  125.75/s  (8.143s,  125.75/s)  LR: 4.697e-04  Data: 7.524 (7.524)
Train: 23 [  50/1251 (  4%)]  Loss:  5.241207 (5.2962)  Time: 0.586s, 1747.86/s  (2.624s,  390.19/s)  LR: 4.697e-04  Data: 0.022 (2.040)
Train: 23 [ 100/1251 (  8%)]  Loss:  5.549731 (5.3807)  Time: 0.587s, 1744.16/s  (2.593s,  394.89/s)  LR: 4.697e-04  Data: 0.021 (2.005)
Train: 23 [ 150/1251 ( 12%)]  Loss:  5.129942 (5.3180)  Time: 0.588s, 1742.55/s  (2.490s,  411.25/s)  LR: 4.697e-04  Data: 0.019 (1.905)
Train: 23 [ 200/1251 ( 16%)]  Loss:  5.324392 (5.3193)  Time: 0.590s, 1734.89/s  (2.476s,  413.59/s)  LR: 4.697e-04  Data: 0.023 (1.890)
Train: 23 [ 250/1251 ( 20%)]  Loss:  4.977448 (5.2623)  Time: 0.587s, 1743.86/s  (2.427s,  421.91/s)  LR: 4.697e-04  Data: 0.021 (1.842)
Train: 23 [ 300/1251 ( 24%)]  Loss:  5.741646 (5.3308)  Time: 0.583s, 1757.36/s  (2.465s,  415.47/s)  LR: 4.697e-04  Data: 0.019 (1.879)
Train: 23 [ 350/1251 ( 28%)]  Loss:  5.012530 (5.2910)  Time: 0.588s, 1742.68/s  (2.477s,  413.33/s)  LR: 4.697e-04  Data: 0.021 (1.892)
Train: 23 [ 400/1251 ( 32%)]  Loss:  5.356563 (5.2983)  Time: 0.588s, 1742.70/s  (2.506s,  408.56/s)  LR: 4.697e-04  Data: 0.021 (1.921)
Train: 23 [ 450/1251 ( 36%)]  Loss:  5.685392 (5.3370)  Time: 0.588s, 1742.32/s  (2.498s,  409.96/s)  LR: 4.697e-04  Data: 0.020 (1.913)
Train: 23 [ 500/1251 ( 40%)]  Loss:  4.908765 (5.2981)  Time: 0.588s, 1741.72/s  (2.491s,  411.12/s)  LR: 4.697e-04  Data: 0.021 (1.905)
Train: 23 [ 550/1251 ( 44%)]  Loss:  5.540088 (5.3182)  Time: 0.585s, 1751.50/s  (2.478s,  413.17/s)  LR: 4.697e-04  Data: 0.020 (1.892)
Train: 23 [ 600/1251 ( 48%)]  Loss:  5.530174 (5.3346)  Time: 0.585s, 1750.95/s  (2.477s,  413.46/s)  LR: 4.697e-04  Data: 0.019 (1.890)
Train: 23 [ 650/1251 ( 52%)]  Loss:  5.570740 (5.3514)  Time: 0.680s, 1504.92/s  (2.494s,  410.50/s)  LR: 4.697e-04  Data: 0.083 (1.907)
Train: 23 [ 700/1251 ( 56%)]  Loss:  5.207056 (5.3418)  Time: 0.584s, 1751.93/s  (2.505s,  408.73/s)  LR: 4.697e-04  Data: 0.020 (1.917)
Train: 23 [ 750/1251 ( 60%)]  Loss:  5.620409 (5.3592)  Time: 0.585s, 1750.08/s  (2.506s,  408.56/s)  LR: 4.697e-04  Data: 0.021 (1.918)
Train: 23 [ 800/1251 ( 64%)]  Loss:  5.711410 (5.3799)  Time: 0.588s, 1740.28/s  (2.505s,  408.73/s)  LR: 4.697e-04  Data: 0.019 (1.917)
Train: 23 [ 850/1251 ( 68%)]  Loss:  5.406588 (5.3814)  Time: 0.586s, 1746.33/s  (2.497s,  410.07/s)  LR: 4.697e-04  Data: 0.020 (1.909)
Train: 23 [ 900/1251 ( 72%)]  Loss:  5.577231 (5.3917)  Time: 0.588s, 1742.86/s  (2.495s,  410.42/s)  LR: 4.697e-04  Data: 0.021 (1.905)
Train: 23 [ 950/1251 ( 76%)]  Loss:  5.147474 (5.3795)  Time: 0.587s, 1744.71/s  (2.485s,  412.07/s)  LR: 4.697e-04  Data: 0.019 (1.895)
Train: 23 [1000/1251 ( 80%)]  Loss:  5.409837 (5.3809)  Time: 0.585s, 1749.14/s  (2.490s,  411.32/s)  LR: 4.697e-04  Data: 0.021 (1.899)
Train: 23 [1050/1251 ( 84%)]  Loss:  5.618341 (5.3917)  Time: 0.585s, 1750.37/s  (2.502s,  409.28/s)  LR: 4.697e-04  Data: 0.020 (1.911)
Train: 23 [1100/1251 ( 88%)]  Loss:  5.801168 (5.4095)  Time: 0.589s, 1739.70/s  (2.519s,  406.45/s)  LR: 4.697e-04  Data: 0.025 (1.927)
Train: 23 [1150/1251 ( 92%)]  Loss:  5.749020 (5.4237)  Time: 0.587s, 1744.11/s  (2.524s,  405.67/s)  LR: 4.697e-04  Data: 0.021 (1.931)
Train: 23 [1200/1251 ( 96%)]  Loss:  5.370317 (5.4216)  Time: 0.591s, 1731.31/s  (2.536s,  403.78/s)  LR: 4.697e-04  Data: 0.023 (1.943)
Train: 23 [1250/1251 (100%)]  Loss:  5.459638 (5.4230)  Time: 0.565s, 1813.05/s  (2.537s,  403.57/s)  LR: 4.697e-04  Data: 0.000 (1.944)
Test: [   0/48]  Time: 16.621 (16.621)  Loss:  2.0019 (2.0019)  Acc@1: 57.8125 (57.8125)  Acc@5: 82.1289 (82.1289)
Test: [  48/48]  Time: 0.148 (3.696)  Loss:  1.7643 (3.0276)  Acc@1: 65.3302 (37.3640)  Acc@5: 80.7783 (62.9520)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-23.pth.tar', 37.363999936523435)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-22.pth.tar', 36.52799991455078)

Train: 24 [   0/1251 (  0%)]  Loss:  5.490022 (5.4900)  Time: 18.303s,   55.95/s  (18.303s,   55.95/s)  LR: 4.346e-04  Data: 17.101 (17.101)
Train: 24 [  50/1251 (  4%)]  Loss:  4.920567 (5.2053)  Time: 0.588s, 1740.46/s  (2.838s,  360.84/s)  LR: 4.346e-04  Data: 0.020 (2.214)
Train: 24 [ 100/1251 (  8%)]  Loss:  5.101110 (5.1706)  Time: 0.917s, 1117.12/s  (2.721s,  376.39/s)  LR: 4.346e-04  Data: 0.341 (2.106)
Train: 24 [ 150/1251 ( 12%)]  Loss:  5.646961 (5.2897)  Time: 0.588s, 1742.08/s  (2.633s,  388.91/s)  LR: 4.346e-04  Data: 0.021 (2.026)
Train: 24 [ 200/1251 ( 16%)]  Loss:  5.620087 (5.3557)  Time: 0.586s, 1747.23/s  (2.638s,  388.16/s)  LR: 4.346e-04  Data: 0.020 (2.033)
Train: 24 [ 250/1251 ( 20%)]  Loss:  5.528244 (5.3845)  Time: 0.585s, 1750.90/s  (2.595s,  394.67/s)  LR: 4.346e-04  Data: 0.020 (1.995)
Train: 24 [ 300/1251 ( 24%)]  Loss:  5.385149 (5.3846)  Time: 0.584s, 1752.65/s  (2.586s,  395.95/s)  LR: 4.346e-04  Data: 0.020 (1.987)
Train: 24 [ 350/1251 ( 28%)]  Loss:  5.419325 (5.3889)  Time: 0.589s, 1739.79/s  (2.548s,  401.90/s)  LR: 4.346e-04  Data: 0.024 (1.951)
Train: 24 [ 400/1251 ( 32%)]  Loss:  5.398681 (5.3900)  Time: 0.587s, 1744.37/s  (2.603s,  393.33/s)  LR: 4.346e-04  Data: 0.021 (2.007)
Train: 24 [ 450/1251 ( 36%)]  Loss:  5.370333 (5.3880)  Time: 0.589s, 1738.55/s  (2.596s,  394.43/s)  LR: 4.346e-04  Data: 0.024 (2.000)
Train: 24 [ 500/1251 ( 40%)]  Loss:  5.063735 (5.3586)  Time: 0.589s, 1739.96/s  (2.596s,  394.40/s)  LR: 4.346e-04  Data: 0.019 (1.997)
Train: 24 [ 550/1251 ( 44%)]  Loss:  4.996723 (5.3284)  Time: 0.589s, 1738.68/s  (2.589s,  395.59/s)  LR: 4.346e-04  Data: 0.023 (1.989)
Train: 24 [ 600/1251 ( 48%)]  Loss:  5.074434 (5.3089)  Time: 0.589s, 1739.78/s  (2.591s,  395.17/s)  LR: 4.346e-04  Data: 0.021 (1.992)
Train: 24 [ 650/1251 ( 52%)]  Loss:  4.744356 (5.2686)  Time: 0.587s, 1744.98/s  (2.583s,  396.44/s)  LR: 4.346e-04  Data: 0.021 (1.985)
Train: 24 [ 700/1251 ( 56%)]  Loss:  5.183187 (5.2629)  Time: 0.588s, 1742.96/s  (2.592s,  395.08/s)  LR: 4.346e-04  Data: 0.021 (1.994)
Train: 24 [ 750/1251 ( 60%)]  Loss:  5.766454 (5.2943)  Time: 0.586s, 1746.34/s  (2.595s,  394.64/s)  LR: 4.346e-04  Data: 0.020 (1.997)
Train: 24 [ 800/1251 ( 64%)]  Loss:  5.174237 (5.2873)  Time: 0.585s, 1749.46/s  (2.605s,  393.11/s)  LR: 4.346e-04  Data: 0.020 (2.008)
Train: 24 [ 850/1251 ( 68%)]  Loss:  4.566436 (5.2472)  Time: 0.585s, 1751.72/s  (2.595s,  394.58/s)  LR: 4.346e-04  Data: 0.020 (1.999)
Train: 24 [ 900/1251 ( 72%)]  Loss:  5.543658 (5.2628)  Time: 0.587s, 1743.73/s  (2.592s,  395.06/s)  LR: 4.346e-04  Data: 0.020 (1.996)
Train: 24 [ 950/1251 ( 76%)]  Loss:  5.587150 (5.2790)  Time: 0.585s, 1749.28/s  (2.580s,  396.95/s)  LR: 4.346e-04  Data: 0.021 (1.983)
Train: 24 [1000/1251 ( 80%)]  Loss:  5.197254 (5.2751)  Time: 0.584s, 1752.89/s  (2.576s,  397.52/s)  LR: 4.346e-04  Data: 0.020 (1.980)
Train: 24 [1050/1251 ( 84%)]  Loss:  5.161047 (5.2700)  Time: 0.585s, 1750.96/s  (2.563s,  399.50/s)  LR: 4.346e-04  Data: 0.020 (1.967)
Train: 24 [1100/1251 ( 88%)]  Loss:  5.460679 (5.2783)  Time: 3.208s,  319.22/s  (2.579s,  397.01/s)  LR: 4.346e-04  Data: 2.541 (1.982)
Train: 24 [1150/1251 ( 92%)]  Loss:  5.075644 (5.2698)  Time: 0.597s, 1716.07/s  (2.578s,  397.17/s)  LR: 4.346e-04  Data: 0.021 (1.980)
Train: 24 [1200/1251 ( 96%)]  Loss:  5.229012 (5.2682)  Time: 4.628s,  221.28/s  (2.573s,  398.03/s)  LR: 4.346e-04  Data: 3.989 (1.974)
Train: 24 [1250/1251 (100%)]  Loss:  5.311144 (5.2698)  Time: 0.565s, 1811.21/s  (2.564s,  399.33/s)  LR: 4.346e-04  Data: 0.000 (1.966)
Test: [   0/48]  Time: 15.661 (15.661)  Loss:  1.8563 (1.8563)  Acc@1: 61.2305 (61.2305)  Acc@5: 83.5938 (83.5938)
Test: [  48/48]  Time: 0.148 (3.568)  Loss:  1.7161 (3.0053)  Acc@1: 65.4481 (38.0660)  Acc@5: 81.1321 (63.6000)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-24.pth.tar', 38.06599998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-23.pth.tar', 37.363999936523435)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-22.pth.tar', 36.52799991455078)

Train: 25 [   0/1251 (  0%)]  Loss:  5.333977 (5.3340)  Time: 12.573s,   81.44/s  (12.573s,   81.44/s)  LR: 3.998e-04  Data: 11.216 (11.216)
Train: 25 [  50/1251 (  4%)]  Loss:  5.152181 (5.2431)  Time: 0.588s, 1741.38/s  (2.411s,  424.65/s)  LR: 3.998e-04  Data: 0.021 (1.805)
Train: 25 [ 100/1251 (  8%)]  Loss:  5.447589 (5.3112)  Time: 7.199s,  142.25/s  (2.566s,  399.11/s)  LR: 3.998e-04  Data: 6.522 (1.942)
Train: 25 [ 150/1251 ( 12%)]  Loss:  5.487633 (5.3553)  Time: 0.585s, 1749.61/s  (2.513s,  407.48/s)  LR: 3.998e-04  Data: 0.020 (1.895)
Train: 25 [ 200/1251 ( 16%)]  Loss:  5.189046 (5.3221)  Time: 8.892s,  115.16/s  (2.563s,  399.54/s)  LR: 3.998e-04  Data: 8.160 (1.952)
Train: 25 [ 250/1251 ( 20%)]  Loss:  5.557847 (5.3614)  Time: 0.586s, 1747.83/s  (2.522s,  405.99/s)  LR: 3.998e-04  Data: 0.019 (1.909)
Train: 25 [ 300/1251 ( 24%)]  Loss:  5.462461 (5.3758)  Time: 7.794s,  131.39/s  (2.531s,  404.56/s)  LR: 3.998e-04  Data: 7.169 (1.921)
Train: 25 [ 350/1251 ( 28%)]  Loss:  5.467643 (5.3873)  Time: 0.586s, 1746.38/s  (2.498s,  409.97/s)  LR: 3.998e-04  Data: 0.021 (1.891)
Train: 25 [ 400/1251 ( 32%)]  Loss:  5.315000 (5.3793)  Time: 8.381s,  122.18/s  (2.490s,  411.23/s)  LR: 3.998e-04  Data: 7.817 (1.884)
Train: 25 [ 450/1251 ( 36%)]  Loss:  4.645586 (5.3059)  Time: 0.587s, 1745.24/s  (2.470s,  414.53/s)  LR: 3.998e-04  Data: 0.021 (1.867)
Train: 25 [ 500/1251 ( 40%)]  Loss:  5.750817 (5.3463)  Time: 6.891s,  148.61/s  (2.511s,  407.77/s)  LR: 3.998e-04  Data: 6.255 (1.909)
Train: 25 [ 550/1251 ( 44%)]  Loss:  5.481812 (5.3576)  Time: 0.635s, 1611.35/s  (2.517s,  406.77/s)  LR: 3.998e-04  Data: 0.019 (1.914)
Train: 25 [ 600/1251 ( 48%)]  Loss:  5.538113 (5.3715)  Time: 10.080s,  101.59/s  (2.516s,  406.97/s)  LR: 3.998e-04  Data: 9.493 (1.913)
Train: 25 [ 650/1251 ( 52%)]  Loss:  5.454148 (5.3774)  Time: 0.592s, 1729.39/s  (2.517s,  406.83/s)  LR: 3.998e-04  Data: 0.027 (1.914)
Train: 25 [ 700/1251 ( 56%)]  Loss:  5.631329 (5.3943)  Time: 8.215s,  124.65/s  (2.523s,  405.83/s)  LR: 3.998e-04  Data: 7.548 (1.921)
Train: 25 [ 750/1251 ( 60%)]  Loss:  5.389727 (5.3941)  Time: 0.592s, 1730.93/s  (2.512s,  407.71/s)  LR: 3.998e-04  Data: 0.027 (1.910)
Train: 25 [ 800/1251 ( 64%)]  Loss:  5.430918 (5.3962)  Time: 9.147s,  111.94/s  (2.512s,  407.65/s)  LR: 3.998e-04  Data: 8.556 (1.910)
Train: 25 [ 850/1251 ( 68%)]  Loss:  4.792155 (5.3627)  Time: 0.587s, 1744.24/s  (2.528s,  405.05/s)  LR: 3.998e-04  Data: 0.021 (1.928)
Train: 25 [ 900/1251 ( 72%)]  Loss:  4.832082 (5.3347)  Time: 8.018s,  127.71/s  (2.537s,  403.62/s)  LR: 3.998e-04  Data: 7.331 (1.936)
Train: 25 [ 950/1251 ( 76%)]  Loss:  4.936905 (5.3148)  Time: 0.589s, 1739.74/s  (2.534s,  404.09/s)  LR: 3.998e-04  Data: 0.021 (1.934)
Train: 25 [1000/1251 ( 80%)]  Loss:  4.971679 (5.2985)  Time: 8.540s,  119.90/s  (2.541s,  403.06/s)  LR: 3.998e-04  Data: 7.886 (1.940)
Train: 25 [1050/1251 ( 84%)]  Loss:  5.588109 (5.3117)  Time: 0.585s, 1750.37/s  (2.529s,  404.91/s)  LR: 3.998e-04  Data: 0.020 (1.929)
Train: 25 [1100/1251 ( 88%)]  Loss:  5.381023 (5.3147)  Time: 4.119s,  248.63/s  (2.525s,  405.50/s)  LR: 3.998e-04  Data: 3.470 (1.925)
Train: 25 [1150/1251 ( 92%)]  Loss:  5.443202 (5.3200)  Time: 0.586s, 1748.04/s  (2.515s,  407.16/s)  LR: 3.998e-04  Data: 0.020 (1.915)
Train: 25 [1200/1251 ( 96%)]  Loss:  4.999010 (5.3072)  Time: 4.492s,  227.94/s  (2.531s,  404.58/s)  LR: 3.998e-04  Data: 3.930 (1.932)
Train: 25 [1250/1251 (100%)]  Loss:  5.104349 (5.2994)  Time: 0.565s, 1810.83/s  (2.533s,  404.34/s)  LR: 3.998e-04  Data: 0.000 (1.932)
Test: [   0/48]  Time: 16.068 (16.068)  Loss:  1.9517 (1.9517)  Acc@1: 59.5703 (59.5703)  Acc@5: 81.5430 (81.5430)
Test: [  48/48]  Time: 0.149 (3.598)  Loss:  1.6799 (2.9369)  Acc@1: 68.0425 (39.3540)  Acc@5: 81.7217 (64.6300)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-25.pth.tar', 39.35400008056641)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-24.pth.tar', 38.06599998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-23.pth.tar', 37.363999936523435)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-22.pth.tar', 36.52799991455078)

Train: 26 [   0/1251 (  0%)]  Loss:  5.284110 (5.2841)  Time: 12.579s,   81.41/s  (12.579s,   81.41/s)  LR: 3.655e-04  Data: 11.407 (11.407)
Train: 26 [  50/1251 (  4%)]  Loss:  4.992397 (5.1383)  Time: 0.587s, 1744.38/s  (2.585s,  396.20/s)  LR: 3.655e-04  Data: 0.022 (1.979)
Train: 26 [ 100/1251 (  8%)]  Loss:  5.586435 (5.2876)  Time: 0.589s, 1738.99/s  (2.544s,  402.48/s)  LR: 3.655e-04  Data: 0.021 (1.946)
Train: 26 [ 150/1251 ( 12%)]  Loss:  4.693759 (5.1392)  Time: 0.585s, 1751.53/s  (2.485s,  412.03/s)  LR: 3.655e-04  Data: 0.020 (1.891)
Train: 26 [ 200/1251 ( 16%)]  Loss:  5.545203 (5.2204)  Time: 0.587s, 1744.96/s  (2.559s,  400.23/s)  LR: 3.655e-04  Data: 0.022 (1.963)
Train: 26 [ 250/1251 ( 20%)]  Loss:  5.508068 (5.2683)  Time: 0.586s, 1746.49/s  (2.546s,  402.23/s)  LR: 3.655e-04  Data: 0.021 (1.953)
Train: 26 [ 300/1251 ( 24%)]  Loss:  5.360866 (5.2815)  Time: 0.590s, 1736.70/s  (2.581s,  396.67/s)  LR: 3.655e-04  Data: 0.025 (1.988)
Train: 26 [ 350/1251 ( 28%)]  Loss:  4.935859 (5.2383)  Time: 0.585s, 1749.39/s  (2.557s,  400.47/s)  LR: 3.655e-04  Data: 0.020 (1.965)
Train: 26 [ 400/1251 ( 32%)]  Loss:  5.816563 (5.3026)  Time: 0.587s, 1745.14/s  (2.562s,  399.64/s)  LR: 3.655e-04  Data: 0.020 (1.972)
Train: 26 [ 450/1251 ( 36%)]  Loss:  5.037324 (5.2761)  Time: 0.587s, 1744.31/s  (2.547s,  402.11/s)  LR: 3.655e-04  Data: 0.020 (1.955)
Train: 26 [ 500/1251 ( 40%)]  Loss:  5.012126 (5.2521)  Time: 0.584s, 1753.54/s  (2.541s,  402.96/s)  LR: 3.655e-04  Data: 0.020 (1.950)
Train: 26 [ 550/1251 ( 44%)]  Loss:  4.971255 (5.2287)  Time: 0.589s, 1739.35/s  (2.546s,  402.17/s)  LR: 3.655e-04  Data: 0.021 (1.954)
Train: 26 [ 600/1251 ( 48%)]  Loss:  5.566246 (5.2546)  Time: 0.583s, 1755.15/s  (2.577s,  397.39/s)  LR: 3.655e-04  Data: 0.019 (1.984)
Train: 26 [ 650/1251 ( 52%)]  Loss:  5.020252 (5.2379)  Time: 0.588s, 1740.61/s  (2.596s,  394.38/s)  LR: 3.655e-04  Data: 0.023 (2.004)
Train: 26 [ 700/1251 ( 56%)]  Loss:  4.887105 (5.2145)  Time: 0.584s, 1752.14/s  (2.598s,  394.09/s)  LR: 3.655e-04  Data: 0.020 (2.005)
Train: 26 [ 750/1251 ( 60%)]  Loss:  5.231011 (5.2155)  Time: 0.586s, 1745.99/s  (2.587s,  395.75/s)  LR: 3.655e-04  Data: 0.019 (1.994)
Train: 26 [ 800/1251 ( 64%)]  Loss:  5.188355 (5.2139)  Time: 0.585s, 1749.92/s  (2.587s,  395.82/s)  LR: 3.655e-04  Data: 0.021 (1.993)
Train: 26 [ 850/1251 ( 68%)]  Loss:  5.267448 (5.2169)  Time: 0.588s, 1742.33/s  (2.569s,  398.53/s)  LR: 3.655e-04  Data: 0.021 (1.976)
Train: 26 [ 900/1251 ( 72%)]  Loss:  4.990039 (5.2050)  Time: 0.584s, 1754.24/s  (2.585s,  396.17/s)  LR: 3.655e-04  Data: 0.019 (1.991)
Train: 26 [ 950/1251 ( 76%)]  Loss:  5.423487 (5.2159)  Time: 0.585s, 1751.81/s  (2.577s,  397.33/s)  LR: 3.655e-04  Data: 0.019 (1.984)
Train: 26 [1000/1251 ( 80%)]  Loss:  5.452005 (5.2271)  Time: 0.586s, 1747.78/s  (2.586s,  396.00/s)  LR: 3.655e-04  Data: 0.021 (1.993)
Train: 26 [1050/1251 ( 84%)]  Loss:  4.904824 (5.2125)  Time: 0.586s, 1746.07/s  (2.579s,  397.04/s)  LR: 3.655e-04  Data: 0.021 (1.985)
Train: 26 [1100/1251 ( 88%)]  Loss:  5.188499 (5.2114)  Time: 0.593s, 1727.01/s  (2.580s,  396.89/s)  LR: 3.655e-04  Data: 0.026 (1.987)
Train: 26 [1150/1251 ( 92%)]  Loss:  5.675257 (5.2308)  Time: 0.589s, 1737.49/s  (2.572s,  398.18/s)  LR: 3.655e-04  Data: 0.025 (1.979)
Train: 26 [1200/1251 ( 96%)]  Loss:  5.828570 (5.2547)  Time: 0.586s, 1748.41/s  (2.566s,  399.01/s)  LR: 3.655e-04  Data: 0.023 (1.974)
Train: 26 [1250/1251 (100%)]  Loss:  5.286930 (5.2559)  Time: 0.564s, 1815.12/s  (2.569s,  398.61/s)  LR: 3.655e-04  Data: 0.000 (1.977)
Test: [   0/48]  Time: 17.054 (17.054)  Loss:  1.8263 (1.8263)  Acc@1: 63.2812 (63.2812)  Acc@5: 84.8633 (84.8633)
Test: [  48/48]  Time: 0.149 (3.661)  Loss:  1.6778 (2.8598)  Acc@1: 67.4528 (40.5480)  Acc@5: 82.0755 (66.0620)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-26.pth.tar', 40.54799995361328)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-25.pth.tar', 39.35400008056641)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-24.pth.tar', 38.06599998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-23.pth.tar', 37.363999936523435)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-22.pth.tar', 36.52799991455078)

Train: 27 [   0/1251 (  0%)]  Loss:  5.185458 (5.1855)  Time: 12.604s,   81.24/s  (12.604s,   81.24/s)  LR: 3.320e-04  Data: 11.444 (11.444)
Train: 27 [  50/1251 (  4%)]  Loss:  5.571801 (5.3786)  Time: 0.588s, 1740.45/s  (2.681s,  381.94/s)  LR: 3.320e-04  Data: 0.020 (2.077)
Train: 27 [ 100/1251 (  8%)]  Loss:  5.083998 (5.2804)  Time: 0.583s, 1755.90/s  (2.645s,  387.20/s)  LR: 3.320e-04  Data: 0.019 (2.047)
Train: 27 [ 150/1251 ( 12%)]  Loss:  5.205829 (5.2618)  Time: 0.589s, 1737.39/s  (2.564s,  399.34/s)  LR: 3.320e-04  Data: 0.024 (1.964)
Train: 27 [ 200/1251 ( 16%)]  Loss:  5.386341 (5.2867)  Time: 0.587s, 1743.12/s  (2.537s,  403.68/s)  LR: 3.320e-04  Data: 0.020 (1.940)
Train: 27 [ 250/1251 ( 20%)]  Loss:  5.372408 (5.3010)  Time: 0.586s, 1747.85/s  (2.497s,  410.05/s)  LR: 3.320e-04  Data: 0.022 (1.895)
Train: 27 [ 300/1251 ( 24%)]  Loss:  4.938773 (5.2492)  Time: 0.587s, 1744.14/s  (2.573s,  397.92/s)  LR: 3.320e-04  Data: 0.023 (1.969)
Train: 27 [ 350/1251 ( 28%)]  Loss:  5.119624 (5.2330)  Time: 0.584s, 1753.98/s  (2.529s,  404.94/s)  LR: 3.320e-04  Data: 0.019 (1.926)
Train: 27 [ 400/1251 ( 32%)]  Loss:  5.524049 (5.2654)  Time: 0.588s, 1740.27/s  (2.532s,  404.38/s)  LR: 3.320e-04  Data: 0.019 (1.932)
Train: 27 [ 450/1251 ( 36%)]  Loss:  5.608552 (5.2997)  Time: 0.714s, 1434.48/s  (2.519s,  406.44/s)  LR: 3.320e-04  Data: 0.134 (1.919)
Train: 27 [ 500/1251 ( 40%)]  Loss:  4.959070 (5.2687)  Time: 0.589s, 1738.54/s  (2.519s,  406.44/s)  LR: 3.320e-04  Data: 0.024 (1.920)
Train: 27 [ 550/1251 ( 44%)]  Loss:  5.414890 (5.2809)  Time: 0.624s, 1641.69/s  (2.505s,  408.79/s)  LR: 3.320e-04  Data: 0.021 (1.906)
Train: 27 [ 600/1251 ( 48%)]  Loss:  5.620327 (5.3070)  Time: 0.588s, 1742.96/s  (2.512s,  407.69/s)  LR: 3.320e-04  Data: 0.023 (1.913)
Train: 27 [ 650/1251 ( 52%)]  Loss:  5.458000 (5.3178)  Time: 0.587s, 1743.56/s  (2.541s,  403.03/s)  LR: 3.320e-04  Data: 0.022 (1.944)
Train: 27 [ 700/1251 ( 56%)]  Loss:  5.527907 (5.3318)  Time: 0.587s, 1745.52/s  (2.551s,  401.42/s)  LR: 3.320e-04  Data: 0.023 (1.952)
Train: 27 [ 750/1251 ( 60%)]  Loss:  5.408359 (5.3366)  Time: 0.584s, 1752.61/s  (2.553s,  401.06/s)  LR: 3.320e-04  Data: 0.018 (1.955)
Train: 27 [ 800/1251 ( 64%)]  Loss:  5.368658 (5.3385)  Time: 0.588s, 1740.90/s  (2.559s,  400.20/s)  LR: 3.320e-04  Data: 0.021 (1.961)
Train: 27 [ 850/1251 ( 68%)]  Loss:  5.203968 (5.3310)  Time: 0.585s, 1750.93/s  (2.547s,  402.10/s)  LR: 3.320e-04  Data: 0.018 (1.949)
Train: 27 [ 900/1251 ( 72%)]  Loss:  5.278792 (5.3283)  Time: 0.589s, 1739.66/s  (2.549s,  401.78/s)  LR: 3.320e-04  Data: 0.020 (1.952)
Train: 27 [ 950/1251 ( 76%)]  Loss:  5.486060 (5.3361)  Time: 0.584s, 1754.36/s  (2.538s,  403.50/s)  LR: 3.320e-04  Data: 0.019 (1.941)
Train: 27 [1000/1251 ( 80%)]  Loss:  4.721455 (5.3069)  Time: 0.583s, 1755.31/s  (2.555s,  400.80/s)  LR: 3.320e-04  Data: 0.019 (1.958)
Train: 27 [1050/1251 ( 84%)]  Loss:  4.923351 (5.2894)  Time: 0.587s, 1744.21/s  (2.550s,  401.58/s)  LR: 3.320e-04  Data: 0.020 (1.954)
Train: 27 [1100/1251 ( 88%)]  Loss:  5.532872 (5.3000)  Time: 0.586s, 1746.86/s  (2.559s,  400.17/s)  LR: 3.320e-04  Data: 0.022 (1.963)
Train: 27 [1150/1251 ( 92%)]  Loss:  5.123978 (5.2927)  Time: 0.586s, 1747.05/s  (2.557s,  400.40/s)  LR: 3.320e-04  Data: 0.022 (1.962)
Train: 27 [1200/1251 ( 96%)]  Loss:  5.064213 (5.2835)  Time: 0.593s, 1727.82/s  (2.556s,  400.57/s)  LR: 3.320e-04  Data: 0.017 (1.961)
Train: 27 [1250/1251 (100%)]  Loss:  4.993100 (5.2724)  Time: 0.567s, 1806.84/s  (2.552s,  401.29/s)  LR: 3.320e-04  Data: 0.000 (1.956)
Test: [   0/48]  Time: 14.063 (14.063)  Loss:  1.8725 (1.8725)  Acc@1: 63.7695 (63.7695)  Acc@5: 84.1797 (84.1797)
Test: [  48/48]  Time: 0.148 (3.388)  Loss:  1.7354 (2.8588)  Acc@1: 67.6887 (41.3340)  Acc@5: 82.0755 (66.6780)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-27.pth.tar', 41.33400005615234)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-26.pth.tar', 40.54799995361328)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-25.pth.tar', 39.35400008056641)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-24.pth.tar', 38.06599998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-23.pth.tar', 37.363999936523435)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-22.pth.tar', 36.52799991455078)

Train: 28 [   0/1251 (  0%)]  Loss:  4.996094 (4.9961)  Time: 12.281s,   83.38/s  (12.281s,   83.38/s)  LR: 2.994e-04  Data: 10.925 (10.925)
Train: 28 [  50/1251 (  4%)]  Loss:  5.121099 (5.0586)  Time: 0.589s, 1739.15/s  (2.864s,  357.51/s)  LR: 2.994e-04  Data: 0.024 (2.263)
Train: 28 [ 100/1251 (  8%)]  Loss:  5.143316 (5.0868)  Time: 0.957s, 1070.11/s  (2.726s,  375.68/s)  LR: 2.994e-04  Data: 0.271 (2.112)
Train: 28 [ 150/1251 ( 12%)]  Loss:  5.366803 (5.1568)  Time: 1.767s,  579.44/s  (2.613s,  391.94/s)  LR: 2.994e-04  Data: 1.204 (2.000)
Train: 28 [ 200/1251 ( 16%)]  Loss:  5.448762 (5.2152)  Time: 0.782s, 1309.61/s  (2.591s,  395.25/s)  LR: 2.994e-04  Data: 0.065 (1.976)
Train: 28 [ 250/1251 ( 20%)]  Loss:  5.356146 (5.2387)  Time: 0.587s, 1743.27/s  (2.552s,  401.22/s)  LR: 2.994e-04  Data: 0.022 (1.939)
Train: 28 [ 300/1251 ( 24%)]  Loss:  5.464691 (5.2710)  Time: 0.589s, 1739.57/s  (2.542s,  402.85/s)  LR: 2.994e-04  Data: 0.026 (1.928)
Train: 28 [ 350/1251 ( 28%)]  Loss:  5.225914 (5.2654)  Time: 0.590s, 1735.45/s  (2.502s,  409.21/s)  LR: 2.994e-04  Data: 0.019 (1.892)
Train: 28 [ 400/1251 ( 32%)]  Loss:  5.104240 (5.2475)  Time: 4.467s,  229.26/s  (2.551s,  401.37/s)  LR: 2.994e-04  Data: 3.865 (1.938)
Train: 28 [ 450/1251 ( 36%)]  Loss:  5.463115 (5.2690)  Time: 0.588s, 1740.16/s  (2.540s,  403.15/s)  LR: 2.994e-04  Data: 0.019 (1.926)
Train: 28 [ 500/1251 ( 40%)]  Loss:  4.927958 (5.2380)  Time: 3.426s,  298.91/s  (2.534s,  404.10/s)  LR: 2.994e-04  Data: 2.857 (1.919)
Train: 28 [ 550/1251 ( 44%)]  Loss:  4.774628 (5.1994)  Time: 1.932s,  530.15/s  (2.531s,  404.55/s)  LR: 2.994e-04  Data: 1.344 (1.917)
Train: 28 [ 600/1251 ( 48%)]  Loss:  5.521794 (5.2242)  Time: 1.513s,  676.72/s  (2.546s,  402.26/s)  LR: 2.994e-04  Data: 0.870 (1.931)
Train: 28 [ 650/1251 ( 52%)]  Loss:  4.983418 (5.2070)  Time: 0.703s, 1457.42/s  (2.540s,  403.15/s)  LR: 2.994e-04  Data: 0.036 (1.926)
Train: 28 [ 700/1251 ( 56%)]  Loss:  5.380492 (5.2186)  Time: 0.585s, 1751.59/s  (2.540s,  403.11/s)  LR: 2.994e-04  Data: 0.021 (1.928)
Train: 28 [ 750/1251 ( 60%)]  Loss:  4.601291 (5.1800)  Time: 1.775s,  577.01/s  (2.557s,  400.39/s)  LR: 2.994e-04  Data: 1.188 (1.946)
Train: 28 [ 800/1251 ( 64%)]  Loss:  4.905476 (5.1638)  Time: 1.949s,  525.37/s  (2.561s,  399.82/s)  LR: 2.994e-04  Data: 1.254 (1.950)
Train: 28 [ 850/1251 ( 68%)]  Loss:  4.748620 (5.1408)  Time: 0.587s, 1744.06/s  (2.559s,  400.20/s)  LR: 2.994e-04  Data: 0.023 (1.948)
Train: 28 [ 900/1251 ( 72%)]  Loss:  5.549833 (5.1623)  Time: 3.679s,  278.32/s  (2.567s,  398.86/s)  LR: 2.994e-04  Data: 2.994 (1.957)
Train: 28 [ 950/1251 ( 76%)]  Loss:  5.293438 (5.1689)  Time: 0.589s, 1737.68/s  (2.557s,  400.49/s)  LR: 2.994e-04  Data: 0.025 (1.946)
Train: 28 [1000/1251 ( 80%)]  Loss:  5.194252 (5.1701)  Time: 7.374s,  138.87/s  (2.555s,  400.81/s)  LR: 2.994e-04  Data: 6.808 (1.943)
Train: 28 [1050/1251 ( 84%)]  Loss:  4.856736 (5.1558)  Time: 0.588s, 1742.56/s  (2.546s,  402.26/s)  LR: 2.994e-04  Data: 0.019 (1.935)
Train: 28 [1100/1251 ( 88%)]  Loss:  4.906538 (5.1450)  Time: 7.935s,  129.05/s  (2.566s,  399.06/s)  LR: 2.994e-04  Data: 7.372 (1.955)
Train: 28 [1150/1251 ( 92%)]  Loss:  4.517550 (5.1188)  Time: 0.590s, 1737.05/s  (2.560s,  399.99/s)  LR: 2.994e-04  Data: 0.019 (1.950)
Train: 28 [1200/1251 ( 96%)]  Loss:  5.547770 (5.1360)  Time: 7.542s,  135.77/s  (2.565s,  399.20/s)  LR: 2.994e-04  Data: 6.962 (1.956)
Train: 28 [1250/1251 (100%)]  Loss:  5.308331 (5.1426)  Time: 0.568s, 1802.24/s  (2.563s,  399.57/s)  LR: 2.994e-04  Data: 0.000 (1.955)
Test: [   0/48]  Time: 17.409 (17.409)  Loss:  1.7650 (1.7650)  Acc@1: 64.1602 (64.1602)  Acc@5: 84.8633 (84.8633)
Test: [  48/48]  Time: 0.149 (3.579)  Loss:  1.6230 (2.7790)  Acc@1: 67.3349 (42.3080)  Acc@5: 83.1368 (67.6860)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-28.pth.tar', 42.30800003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-27.pth.tar', 41.33400005615234)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-26.pth.tar', 40.54799995361328)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-25.pth.tar', 39.35400008056641)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-24.pth.tar', 38.06599998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-23.pth.tar', 37.363999936523435)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-22.pth.tar', 36.52799991455078)

Train: 29 [   0/1251 (  0%)]  Loss:  5.358501 (5.3585)  Time: 13.453s,   76.11/s  (13.453s,   76.11/s)  LR: 2.678e-04  Data: 12.308 (12.308)
Train: 29 [  50/1251 (  4%)]  Loss:  5.183133 (5.2708)  Time: 0.588s, 1741.23/s  (2.593s,  394.84/s)  LR: 2.678e-04  Data: 0.024 (1.979)
Train: 29 [ 100/1251 (  8%)]  Loss:  5.713577 (5.4184)  Time: 2.240s,  457.22/s  (2.650s,  386.44/s)  LR: 2.678e-04  Data: 1.587 (2.036)
Train: 29 [ 150/1251 ( 12%)]  Loss:  4.897202 (5.2881)  Time: 0.586s, 1746.95/s  (2.592s,  395.07/s)  LR: 2.678e-04  Data: 0.022 (1.983)
Train: 29 [ 200/1251 ( 16%)]  Loss:  5.430033 (5.3165)  Time: 0.586s, 1748.13/s  (2.631s,  389.25/s)  LR: 2.678e-04  Data: 0.021 (2.024)
Train: 29 [ 250/1251 ( 20%)]  Loss:  4.953411 (5.2560)  Time: 0.599s, 1708.88/s  (2.598s,  394.15/s)  LR: 2.678e-04  Data: 0.024 (1.994)
Train: 29 [ 300/1251 ( 24%)]  Loss:  5.219694 (5.2508)  Time: 0.584s, 1752.04/s  (2.592s,  395.05/s)  LR: 2.678e-04  Data: 0.020 (1.991)
Train: 29 [ 350/1251 ( 28%)]  Loss:  5.218762 (5.2468)  Time: 0.591s, 1732.91/s  (2.568s,  398.68/s)  LR: 2.678e-04  Data: 0.024 (1.969)
Train: 29 [ 400/1251 ( 32%)]  Loss:  5.351767 (5.2585)  Time: 0.584s, 1751.95/s  (2.564s,  399.45/s)  LR: 2.678e-04  Data: 0.018 (1.967)
Train: 29 [ 450/1251 ( 36%)]  Loss:  5.519955 (5.2846)  Time: 0.588s, 1740.69/s  (2.552s,  401.24/s)  LR: 2.678e-04  Data: 0.024 (1.956)
Train: 29 [ 500/1251 ( 40%)]  Loss:  4.818522 (5.2422)  Time: 0.588s, 1742.02/s  (2.581s,  396.74/s)  LR: 2.678e-04  Data: 0.020 (1.985)
Train: 29 [ 550/1251 ( 44%)]  Loss:  4.720571 (5.1988)  Time: 0.971s, 1054.52/s  (2.585s,  396.06/s)  LR: 2.678e-04  Data: 0.130 (1.990)
Train: 29 [ 600/1251 ( 48%)]  Loss:  4.800150 (5.1681)  Time: 0.586s, 1748.70/s  (2.596s,  394.40/s)  LR: 2.678e-04  Data: 0.021 (2.001)
Train: 29 [ 650/1251 ( 52%)]  Loss:  5.182222 (5.1691)  Time: 0.584s, 1752.48/s  (2.593s,  394.86/s)  LR: 2.678e-04  Data: 0.020 (1.998)
Train: 29 [ 700/1251 ( 56%)]  Loss:  5.504706 (5.1915)  Time: 1.821s,  562.44/s  (2.599s,  393.94/s)  LR: 2.678e-04  Data: 1.152 (2.004)
Train: 29 [ 750/1251 ( 60%)]  Loss:  5.385481 (5.2036)  Time: 0.585s, 1749.02/s  (2.591s,  395.25/s)  LR: 2.678e-04  Data: 0.020 (1.995)
Train: 29 [ 800/1251 ( 64%)]  Loss:  4.996950 (5.1914)  Time: 0.586s, 1747.30/s  (2.608s,  392.57/s)  LR: 2.678e-04  Data: 0.022 (2.012)
Train: 29 [ 850/1251 ( 68%)]  Loss:  5.335069 (5.1994)  Time: 0.585s, 1749.93/s  (2.602s,  393.61/s)  LR: 2.678e-04  Data: 0.020 (2.005)
Train: 29 [ 900/1251 ( 72%)]  Loss:  5.223891 (5.2007)  Time: 1.753s,  584.25/s  (2.610s,  392.41/s)  LR: 2.678e-04  Data: 1.093 (2.012)
Train: 29 [ 950/1251 ( 76%)]  Loss:  4.623303 (5.1718)  Time: 0.588s, 1742.70/s  (2.603s,  393.34/s)  LR: 2.678e-04  Data: 0.019 (2.004)
Train: 29 [1000/1251 ( 80%)]  Loss:  4.785707 (5.1535)  Time: 2.858s,  358.35/s  (2.606s,  392.92/s)  LR: 2.678e-04  Data: 2.278 (2.007)
Train: 29 [1050/1251 ( 84%)]  Loss:  5.291176 (5.1597)  Time: 0.589s, 1739.09/s  (2.601s,  393.69/s)  LR: 2.678e-04  Data: 0.022 (2.002)
Train: 29 [1100/1251 ( 88%)]  Loss:  5.023642 (5.1538)  Time: 1.685s,  607.84/s  (2.594s,  394.83/s)  LR: 2.678e-04  Data: 1.114 (1.992)
Train: 29 [1150/1251 ( 92%)]  Loss:  5.160422 (5.1541)  Time: 0.589s, 1739.94/s  (2.603s,  393.37/s)  LR: 2.678e-04  Data: 0.023 (2.002)
Train: 29 [1200/1251 ( 96%)]  Loss:  5.170081 (5.1547)  Time: 1.743s,  587.49/s  (2.602s,  393.47/s)  LR: 2.678e-04  Data: 1.180 (2.001)
Train: 29 [1250/1251 (100%)]  Loss:  5.517279 (5.1687)  Time: 0.565s, 1812.36/s  (2.602s,  393.54/s)  LR: 2.678e-04  Data: 0.000 (2.000)
Test: [   0/48]  Time: 15.829 (15.829)  Loss:  1.7633 (1.7633)  Acc@1: 65.5273 (65.5273)  Acc@5: 86.1328 (86.1328)
Test: [  48/48]  Time: 0.150 (3.636)  Loss:  1.5882 (2.7820)  Acc@1: 69.1038 (43.0080)  Acc@5: 84.0802 (68.1720)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-29.pth.tar', 43.00800002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-28.pth.tar', 42.30800003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-27.pth.tar', 41.33400005615234)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-26.pth.tar', 40.54799995361328)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-25.pth.tar', 39.35400008056641)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-24.pth.tar', 38.06599998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-23.pth.tar', 37.363999936523435)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-22.pth.tar', 36.52799991455078)

Train: 30 [   0/1251 (  0%)]  Loss:  5.219844 (5.2198)  Time: 11.901s,   86.04/s  (11.901s,   86.04/s)  LR: 2.374e-04  Data: 10.681 (10.681)
Train: 30 [  50/1251 (  4%)]  Loss:  5.429226 (5.3245)  Time: 0.584s, 1753.18/s  (2.645s,  387.14/s)  LR: 2.374e-04  Data: 0.020 (2.029)
Train: 30 [ 100/1251 (  8%)]  Loss:  5.495204 (5.3814)  Time: 6.462s,  158.48/s  (2.625s,  390.11/s)  LR: 2.374e-04  Data: 5.697 (2.016)
Train: 30 [ 150/1251 ( 12%)]  Loss:  4.987027 (5.2828)  Time: 0.587s, 1743.14/s  (2.577s,  397.41/s)  LR: 2.374e-04  Data: 0.022 (1.965)
Train: 30 [ 200/1251 ( 16%)]  Loss:  4.445112 (5.1153)  Time: 8.723s,  117.39/s  (2.700s,  379.29/s)  LR: 2.374e-04  Data: 8.112 (2.088)
Train: 30 [ 250/1251 ( 20%)]  Loss:  5.507750 (5.1807)  Time: 0.587s, 1745.76/s  (2.673s,  383.13/s)  LR: 2.374e-04  Data: 0.022 (2.063)
Train: 30 [ 300/1251 ( 24%)]  Loss:  5.458152 (5.2203)  Time: 9.427s,  108.63/s  (2.666s,  384.05/s)  LR: 2.374e-04  Data: 8.816 (2.060)
Train: 30 [ 350/1251 ( 28%)]  Loss:  5.294470 (5.2296)  Time: 0.590s, 1736.51/s  (2.630s,  389.33/s)  LR: 2.374e-04  Data: 0.025 (2.027)
Train: 30 [ 400/1251 ( 32%)]  Loss:  4.870227 (5.1897)  Time: 8.057s,  127.10/s  (2.627s,  389.75/s)  LR: 2.374e-04  Data: 7.450 (2.024)
Train: 30 [ 450/1251 ( 36%)]  Loss:  5.272374 (5.1979)  Time: 0.588s, 1740.31/s  (2.604s,  393.28/s)  LR: 2.374e-04  Data: 0.023 (2.002)
Train: 30 [ 500/1251 ( 40%)]  Loss:  5.653456 (5.2393)  Time: 14.423s,   71.00/s  (2.609s,  392.55/s)  LR: 2.374e-04  Data: 13.387 (2.007)
Train: 30 [ 550/1251 ( 44%)]  Loss:  5.428295 (5.2551)  Time: 0.588s, 1741.20/s  (2.621s,  390.66/s)  LR: 2.374e-04  Data: 0.023 (2.019)
Train: 30 [ 600/1251 ( 48%)]  Loss:  4.609290 (5.2054)  Time: 7.250s,  141.24/s  (2.647s,  386.92/s)  LR: 2.374e-04  Data: 6.659 (2.042)
Train: 30 [ 650/1251 ( 52%)]  Loss:  5.210300 (5.2058)  Time: 0.584s, 1752.57/s  (2.662s,  384.73/s)  LR: 2.374e-04  Data: 0.021 (2.055)
Train: 30 [ 700/1251 ( 56%)]  Loss:  4.959852 (5.1894)  Time: 2.705s,  378.54/s  (2.657s,  385.37/s)  LR: 2.374e-04  Data: 1.691 (2.050)
Train: 30 [ 750/1251 ( 60%)]  Loss:  4.920654 (5.1726)  Time: 0.588s, 1741.08/s  (2.648s,  386.64/s)  LR: 2.374e-04  Data: 0.023 (2.042)
Train: 30 [ 800/1251 ( 64%)]  Loss:  5.544981 (5.1945)  Time: 0.586s, 1746.10/s  (2.636s,  388.50/s)  LR: 2.374e-04  Data: 0.022 (2.029)
Train: 30 [ 850/1251 ( 68%)]  Loss:  5.043351 (5.1861)  Time: 0.588s, 1741.36/s  (2.653s,  386.00/s)  LR: 2.374e-04  Data: 0.019 (2.046)
Train: 30 [ 900/1251 ( 72%)]  Loss:  5.189714 (5.1863)  Time: 2.053s,  498.81/s  (2.653s,  386.04/s)  LR: 2.374e-04  Data: 1.487 (2.046)
Train: 30 [ 950/1251 ( 76%)]  Loss:  4.870802 (5.1705)  Time: 0.589s, 1739.08/s  (2.661s,  384.80/s)  LR: 2.374e-04  Data: 0.024 (2.054)
Train: 30 [1000/1251 ( 80%)]  Loss:  5.153874 (5.1697)  Time: 0.586s, 1746.36/s  (2.653s,  385.95/s)  LR: 2.374e-04  Data: 0.021 (2.046)
Train: 30 [1050/1251 ( 84%)]  Loss:  5.056128 (5.1645)  Time: 0.586s, 1748.53/s  (2.649s,  386.55/s)  LR: 2.374e-04  Data: 0.022 (2.043)
Train: 30 [1100/1251 ( 88%)]  Loss:  5.745973 (5.1898)  Time: 0.629s, 1626.94/s  (2.644s,  387.33/s)  LR: 2.374e-04  Data: 0.064 (2.036)
Train: 30 [1150/1251 ( 92%)]  Loss:  5.030634 (5.1832)  Time: 0.588s, 1740.33/s  (2.640s,  387.87/s)  LR: 2.374e-04  Data: 0.019 (2.033)
Train: 30 [1200/1251 ( 96%)]  Loss:  5.440022 (5.1935)  Time: 2.345s,  436.60/s  (2.648s,  386.65/s)  LR: 2.374e-04  Data: 1.684 (2.042)
Train: 30 [1250/1251 (100%)]  Loss:  5.055199 (5.1882)  Time: 0.565s, 1813.95/s  (2.649s,  386.56/s)  LR: 2.374e-04  Data: 0.000 (2.042)
Test: [   0/48]  Time: 17.025 (17.025)  Loss:  1.6795 (1.6795)  Acc@1: 67.9688 (67.9688)  Acc@5: 86.1328 (86.1328)
Test: [  48/48]  Time: 0.148 (3.626)  Loss:  1.5926 (2.6907)  Acc@1: 69.9292 (44.1140)  Acc@5: 83.9623 (69.2340)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-30.pth.tar', 44.11399999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-29.pth.tar', 43.00800002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-28.pth.tar', 42.30800003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-27.pth.tar', 41.33400005615234)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-26.pth.tar', 40.54799995361328)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-25.pth.tar', 39.35400008056641)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-24.pth.tar', 38.06599998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-23.pth.tar', 37.363999936523435)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-22.pth.tar', 36.52799991455078)

Train: 31 [   0/1251 (  0%)]  Loss:  5.041869 (5.0419)  Time: 12.543s,   81.64/s  (12.543s,   81.64/s)  LR: 2.084e-04  Data: 11.326 (11.326)
Train: 31 [  50/1251 (  4%)]  Loss:  4.880656 (4.9613)  Time: 0.588s, 1742.66/s  (2.563s,  399.59/s)  LR: 2.084e-04  Data: 0.020 (1.966)
Train: 31 [ 100/1251 (  8%)]  Loss:  4.575327 (4.8326)  Time: 4.562s,  224.48/s  (2.531s,  404.58/s)  LR: 2.084e-04  Data: 3.981 (1.930)
Train: 31 [ 150/1251 ( 12%)]  Loss:  5.099732 (4.8994)  Time: 0.587s, 1744.95/s  (2.470s,  414.52/s)  LR: 2.084e-04  Data: 0.021 (1.865)
Train: 31 [ 200/1251 ( 16%)]  Loss:  4.873694 (4.8943)  Time: 4.742s,  215.96/s  (2.461s,  416.14/s)  LR: 2.084e-04  Data: 4.179 (1.852)
Train: 31 [ 250/1251 ( 20%)]  Loss:  5.407558 (4.9798)  Time: 0.586s, 1748.51/s  (2.542s,  402.76/s)  LR: 2.084e-04  Data: 0.020 (1.934)
Train: 31 [ 300/1251 ( 24%)]  Loss:  4.792886 (4.9531)  Time: 6.498s,  157.60/s  (2.586s,  396.05/s)  LR: 2.084e-04  Data: 5.935 (1.979)
Train: 31 [ 350/1251 ( 28%)]  Loss:  5.565736 (5.0297)  Time: 0.590s, 1736.80/s  (2.565s,  399.27/s)  LR: 2.084e-04  Data: 0.021 (1.956)
Train: 31 [ 400/1251 ( 32%)]  Loss:  5.390244 (5.0697)  Time: 2.213s,  462.76/s  (2.580s,  396.93/s)  LR: 2.084e-04  Data: 1.501 (1.968)
Train: 31 [ 450/1251 ( 36%)]  Loss:  4.898607 (5.0526)  Time: 0.588s, 1740.82/s  (2.567s,  398.86/s)  LR: 2.084e-04  Data: 0.022 (1.955)
Train: 31 [ 500/1251 ( 40%)]  Loss:  5.338650 (5.0786)  Time: 1.406s,  728.13/s  (2.564s,  399.40/s)  LR: 2.084e-04  Data: 0.838 (1.951)
Train: 31 [ 550/1251 ( 44%)]  Loss:  4.804431 (5.0558)  Time: 0.586s, 1747.80/s  (2.566s,  399.07/s)  LR: 2.084e-04  Data: 0.022 (1.953)
Train: 31 [ 600/1251 ( 48%)]  Loss:  5.484850 (5.0888)  Time: 5.483s,  186.75/s  (2.609s,  392.42/s)  LR: 2.084e-04  Data: 4.835 (1.996)
Train: 31 [ 650/1251 ( 52%)]  Loss:  5.268116 (5.1016)  Time: 0.589s, 1738.35/s  (2.626s,  389.95/s)  LR: 2.084e-04  Data: 0.019 (2.013)
Train: 31 [ 700/1251 ( 56%)]  Loss:  5.358134 (5.1187)  Time: 3.349s,  305.74/s  (2.632s,  389.09/s)  LR: 2.084e-04  Data: 2.556 (2.019)
Train: 31 [ 750/1251 ( 60%)]  Loss:  4.803437 (5.0990)  Time: 0.590s, 1736.18/s  (2.634s,  388.77/s)  LR: 2.084e-04  Data: 0.025 (2.022)
Train: 31 [ 800/1251 ( 64%)]  Loss:  5.257485 (5.1083)  Time: 1.385s,  739.31/s  (2.623s,  390.39/s)  LR: 2.084e-04  Data: 0.821 (2.010)
Train: 31 [ 850/1251 ( 68%)]  Loss:  5.071878 (5.1063)  Time: 0.589s, 1739.56/s  (2.622s,  390.52/s)  LR: 2.084e-04  Data: 0.023 (2.010)
Train: 31 [ 900/1251 ( 72%)]  Loss:  4.522768 (5.0756)  Time: 4.463s,  229.43/s  (2.641s,  387.71/s)  LR: 2.084e-04  Data: 3.899 (2.030)
Train: 31 [ 950/1251 ( 76%)]  Loss:  4.968749 (5.0702)  Time: 0.592s, 1728.97/s  (2.644s,  387.25/s)  LR: 2.084e-04  Data: 0.024 (2.033)
Train: 31 [1000/1251 ( 80%)]  Loss:  4.823300 (5.0585)  Time: 0.587s, 1744.16/s  (2.653s,  385.96/s)  LR: 2.084e-04  Data: 0.023 (2.042)
Train: 31 [1050/1251 ( 84%)]  Loss:  5.035480 (5.0574)  Time: 0.597s, 1715.04/s  (2.659s,  385.16/s)  LR: 2.084e-04  Data: 0.026 (2.049)
Train: 31 [1100/1251 ( 88%)]  Loss:  5.181126 (5.0628)  Time: 0.589s, 1737.25/s  (2.653s,  386.04/s)  LR: 2.084e-04  Data: 0.025 (2.044)
Train: 31 [1150/1251 ( 92%)]  Loss:  5.244634 (5.0704)  Time: 0.589s, 1738.49/s  (2.652s,  386.14/s)  LR: 2.084e-04  Data: 0.023 (2.044)
Train: 31 [1200/1251 ( 96%)]  Loss:  4.943214 (5.0653)  Time: 0.588s, 1742.07/s  (2.643s,  387.42/s)  LR: 2.084e-04  Data: 0.023 (2.035)
Train: 31 [1250/1251 (100%)]  Loss:  5.191019 (5.0701)  Time: 0.564s, 1816.33/s  (2.657s,  385.34/s)  LR: 2.084e-04  Data: 0.000 (2.050)
Test: [   0/48]  Time: 17.297 (17.297)  Loss:  1.5764 (1.5764)  Acc@1: 67.1875 (67.1875)  Acc@5: 87.9883 (87.9883)
Test: [  48/48]  Time: 0.149 (3.856)  Loss:  1.4821 (2.6067)  Acc@1: 70.8726 (45.1940)  Acc@5: 85.4953 (70.2580)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-31.pth.tar', 45.19400001708984)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-30.pth.tar', 44.11399999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-29.pth.tar', 43.00800002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-28.pth.tar', 42.30800003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-27.pth.tar', 41.33400005615234)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-26.pth.tar', 40.54799995361328)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-25.pth.tar', 39.35400008056641)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-24.pth.tar', 38.06599998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-23.pth.tar', 37.363999936523435)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-22.pth.tar', 36.52799991455078)

Train: 32 [   0/1251 (  0%)]  Loss:  5.188394 (5.1884)  Time: 13.641s,   75.07/s  (13.641s,   75.07/s)  LR: 1.808e-04  Data: 12.791 (12.791)
Train: 32 [  50/1251 (  4%)]  Loss:  5.147600 (5.1680)  Time: 0.586s, 1747.80/s  (2.906s,  352.33/s)  LR: 1.808e-04  Data: 0.019 (2.322)
Train: 32 [ 100/1251 (  8%)]  Loss:  5.259083 (5.1984)  Time: 0.586s, 1748.14/s  (2.798s,  365.95/s)  LR: 1.808e-04  Data: 0.020 (2.214)
Train: 32 [ 150/1251 ( 12%)]  Loss:  4.517038 (5.0280)  Time: 0.588s, 1742.71/s  (2.680s,  382.04/s)  LR: 1.808e-04  Data: 0.018 (2.094)
Train: 32 [ 200/1251 ( 16%)]  Loss:  5.343133 (5.0910)  Time: 0.585s, 1750.67/s  (2.671s,  383.37/s)  LR: 1.808e-04  Data: 0.020 (2.083)
Train: 32 [ 250/1251 ( 20%)]  Loss:  5.025996 (5.0802)  Time: 0.588s, 1742.78/s  (2.700s,  379.24/s)  LR: 1.808e-04  Data: 0.017 (2.110)
Train: 32 [ 300/1251 ( 24%)]  Loss:  5.494712 (5.1394)  Time: 0.584s, 1752.75/s  (2.713s,  377.40/s)  LR: 1.808e-04  Data: 0.020 (2.124)
Train: 32 [ 350/1251 ( 28%)]  Loss:  4.663269 (5.0799)  Time: 0.590s, 1736.10/s  (2.700s,  379.19/s)  LR: 1.808e-04  Data: 0.025 (2.110)
Train: 32 [ 400/1251 ( 32%)]  Loss:  5.166941 (5.0896)  Time: 0.590s, 1735.13/s  (2.692s,  380.42/s)  LR: 1.808e-04  Data: 0.019 (2.101)
Train: 32 [ 450/1251 ( 36%)]  Loss:  4.479923 (5.0286)  Time: 0.586s, 1746.42/s  (2.668s,  383.79/s)  LR: 1.808e-04  Data: 0.022 (2.077)
Train: 32 [ 500/1251 ( 40%)]  Loss:  5.350161 (5.0578)  Time: 0.585s, 1748.98/s  (2.662s,  384.70/s)  LR: 1.808e-04  Data: 0.021 (2.070)
Train: 32 [ 550/1251 ( 44%)]  Loss:  5.113251 (5.0625)  Time: 0.586s, 1747.56/s  (2.643s,  387.41/s)  LR: 1.808e-04  Data: 0.021 (2.053)
Train: 32 [ 600/1251 ( 48%)]  Loss:  5.418846 (5.0899)  Time: 0.586s, 1747.40/s  (2.691s,  380.58/s)  LR: 1.808e-04  Data: 0.023 (2.101)
Train: 32 [ 650/1251 ( 52%)]  Loss:  5.147441 (5.0940)  Time: 0.586s, 1746.30/s  (2.697s,  379.72/s)  LR: 1.808e-04  Data: 0.019 (2.106)
Train: 32 [ 700/1251 ( 56%)]  Loss:  4.926406 (5.0828)  Time: 0.585s, 1750.61/s  (2.704s,  378.66/s)  LR: 1.808e-04  Data: 0.022 (2.114)
Train: 32 [ 750/1251 ( 60%)]  Loss:  5.188718 (5.0894)  Time: 0.588s, 1742.68/s  (2.698s,  379.50/s)  LR: 1.808e-04  Data: 0.019 (2.107)
Train: 32 [ 800/1251 ( 64%)]  Loss:  5.159639 (5.0936)  Time: 0.589s, 1737.19/s  (2.696s,  379.76/s)  LR: 1.808e-04  Data: 0.025 (2.105)
Train: 32 [ 850/1251 ( 68%)]  Loss:  5.439438 (5.1128)  Time: 0.589s, 1738.46/s  (2.679s,  382.16/s)  LR: 1.808e-04  Data: 0.024 (2.088)
Train: 32 [ 900/1251 ( 72%)]  Loss:  5.220098 (5.1184)  Time: 0.586s, 1748.92/s  (2.681s,  382.01/s)  LR: 1.808e-04  Data: 0.021 (2.089)
Train: 32 [ 950/1251 ( 76%)]  Loss:  4.659447 (5.0955)  Time: 0.585s, 1749.15/s  (2.678s,  382.33/s)  LR: 1.808e-04  Data: 0.021 (2.087)
Train: 32 [1000/1251 ( 80%)]  Loss:  5.110950 (5.0962)  Time: 0.585s, 1750.50/s  (2.682s,  381.80/s)  LR: 1.808e-04  Data: 0.020 (2.091)
Train: 32 [1050/1251 ( 84%)]  Loss:  5.019765 (5.0927)  Time: 0.588s, 1740.98/s  (2.675s,  382.83/s)  LR: 1.808e-04  Data: 0.021 (2.084)
Train: 32 [1100/1251 ( 88%)]  Loss:  4.813399 (5.0806)  Time: 0.710s, 1442.50/s  (2.672s,  383.24/s)  LR: 1.808e-04  Data: 0.029 (2.081)
Train: 32 [1150/1251 ( 92%)]  Loss:  4.881899 (5.0723)  Time: 1.628s,  629.15/s  (2.661s,  384.85/s)  LR: 1.808e-04  Data: 0.926 (2.069)
Train: 32 [1200/1251 ( 96%)]  Loss:  5.162858 (5.0759)  Time: 2.511s,  407.84/s  (2.654s,  385.78/s)  LR: 1.808e-04  Data: 1.842 (2.061)
Train: 32 [1250/1251 (100%)]  Loss:  5.231228 (5.0819)  Time: 0.565s, 1811.19/s  (2.649s,  386.53/s)  LR: 1.808e-04  Data: 0.000 (2.056)
Test: [   0/48]  Time: 16.079 (16.079)  Loss:  1.5467 (1.5467)  Acc@1: 68.7500 (68.7500)  Acc@5: 87.1094 (87.1094)
Test: [  48/48]  Time: 0.150 (3.718)  Loss:  1.4886 (2.6137)  Acc@1: 70.0472 (45.3980)  Acc@5: 86.3208 (70.6700)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-32.pth.tar', 45.39800004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-31.pth.tar', 45.19400001708984)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-30.pth.tar', 44.11399999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-29.pth.tar', 43.00800002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-28.pth.tar', 42.30800003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-27.pth.tar', 41.33400005615234)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-26.pth.tar', 40.54799995361328)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-25.pth.tar', 39.35400008056641)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-24.pth.tar', 38.06599998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-23.pth.tar', 37.363999936523435)

Train: 33 [   0/1251 (  0%)]  Loss:  5.280580 (5.2806)  Time: 12.324s,   83.09/s  (12.324s,   83.09/s)  LR: 1.550e-04  Data: 11.509 (11.509)
Train: 33 [  50/1251 (  4%)]  Loss:  4.868041 (5.0743)  Time: 0.588s, 1741.26/s  (2.724s,  375.87/s)  LR: 1.550e-04  Data: 0.022 (2.102)
Train: 33 [ 100/1251 (  8%)]  Loss:  5.041903 (5.0635)  Time: 0.911s, 1124.25/s  (2.655s,  385.66/s)  LR: 1.550e-04  Data: 0.339 (2.042)
Train: 33 [ 150/1251 ( 12%)]  Loss:  5.504078 (5.1737)  Time: 0.589s, 1737.30/s  (2.593s,  394.92/s)  LR: 1.550e-04  Data: 0.020 (1.975)
Train: 33 [ 200/1251 ( 16%)]  Loss:  4.820818 (5.1031)  Time: 0.586s, 1746.00/s  (2.566s,  399.01/s)  LR: 1.550e-04  Data: 0.022 (1.949)
Train: 33 [ 250/1251 ( 20%)]  Loss:  4.858274 (5.0623)  Time: 0.590s, 1735.62/s  (2.529s,  404.86/s)  LR: 1.550e-04  Data: 0.022 (1.915)
Train: 33 [ 300/1251 ( 24%)]  Loss:  4.417113 (4.9701)  Time: 6.157s,  166.33/s  (2.587s,  395.78/s)  LR: 1.550e-04  Data: 5.579 (1.975)
Train: 33 [ 350/1251 ( 28%)]  Loss:  5.270627 (5.0077)  Time: 0.585s, 1750.95/s  (2.583s,  396.43/s)  LR: 1.550e-04  Data: 0.021 (1.970)
Train: 33 [ 400/1251 ( 32%)]  Loss:  5.319673 (5.0423)  Time: 4.033s,  253.91/s  (2.585s,  396.13/s)  LR: 1.550e-04  Data: 3.435 (1.973)
Train: 33 [ 450/1251 ( 36%)]  Loss:  5.103757 (5.0485)  Time: 0.587s, 1743.11/s  (2.586s,  396.01/s)  LR: 1.550e-04  Data: 0.023 (1.973)
Train: 33 [ 500/1251 ( 40%)]  Loss:  5.148112 (5.0575)  Time: 0.588s, 1740.45/s  (2.572s,  398.15/s)  LR: 1.550e-04  Data: 0.023 (1.962)
Train: 33 [ 550/1251 ( 44%)]  Loss:  5.064963 (5.0582)  Time: 0.585s, 1749.44/s  (2.565s,  399.27/s)  LR: 1.550e-04  Data: 0.023 (1.955)
Train: 33 [ 600/1251 ( 48%)]  Loss:  4.914053 (5.0471)  Time: 0.587s, 1743.70/s  (2.558s,  400.29/s)  LR: 1.550e-04  Data: 0.021 (1.951)
Train: 33 [ 650/1251 ( 52%)]  Loss:  5.287436 (5.0642)  Time: 0.584s, 1753.45/s  (2.588s,  395.62/s)  LR: 1.550e-04  Data: 0.021 (1.981)
Train: 33 [ 700/1251 ( 56%)]  Loss:  4.826101 (5.0484)  Time: 1.703s,  601.19/s  (2.601s,  393.69/s)  LR: 1.550e-04  Data: 1.131 (1.993)
Train: 33 [ 750/1251 ( 60%)]  Loss:  4.911791 (5.0398)  Time: 2.274s,  450.32/s  (2.610s,  392.37/s)  LR: 1.550e-04  Data: 1.601 (2.000)
Train: 33 [ 800/1251 ( 64%)]  Loss:  4.816184 (5.0267)  Time: 1.940s,  527.90/s  (2.607s,  392.71/s)  LR: 1.550e-04  Data: 1.374 (1.997)
Train: 33 [ 850/1251 ( 68%)]  Loss:  4.633862 (5.0049)  Time: 7.370s,  138.93/s  (2.609s,  392.54/s)  LR: 1.550e-04  Data: 6.714 (1.997)
Train: 33 [ 900/1251 ( 72%)]  Loss:  4.451361 (4.9757)  Time: 0.593s, 1726.11/s  (2.596s,  394.40/s)  LR: 1.550e-04  Data: 0.022 (1.985)
Train: 33 [ 950/1251 ( 76%)]  Loss:  5.056308 (4.9798)  Time: 7.531s,  135.97/s  (2.595s,  394.67/s)  LR: 1.550e-04  Data: 6.966 (1.984)
Train: 33 [1000/1251 ( 80%)]  Loss:  4.500009 (4.9569)  Time: 0.589s, 1739.95/s  (2.604s,  393.30/s)  LR: 1.550e-04  Data: 0.021 (1.994)
Train: 33 [1050/1251 ( 84%)]  Loss:  5.196301 (4.9678)  Time: 8.980s,  114.03/s  (2.613s,  391.96/s)  LR: 1.550e-04  Data: 8.334 (2.004)
Train: 33 [1100/1251 ( 88%)]  Loss:  5.228575 (4.9791)  Time: 0.585s, 1749.54/s  (2.608s,  392.64/s)  LR: 1.550e-04  Data: 0.021 (2.000)
Train: 33 [1150/1251 ( 92%)]  Loss:  5.168531 (4.9870)  Time: 8.750s,  117.03/s  (2.605s,  393.05/s)  LR: 1.550e-04  Data: 8.128 (1.998)
Train: 33 [1200/1251 ( 96%)]  Loss:  5.030910 (4.9888)  Time: 2.018s,  507.32/s  (2.601s,  393.68/s)  LR: 1.550e-04  Data: 1.367 (1.994)
Train: 33 [1250/1251 (100%)]  Loss:  4.612955 (4.9743)  Time: 0.565s, 1812.90/s  (2.592s,  395.00/s)  LR: 1.550e-04  Data: 0.000 (1.986)
Test: [   0/48]  Time: 14.909 (14.909)  Loss:  1.5567 (1.5567)  Acc@1: 69.2383 (69.2383)  Acc@5: 86.7188 (86.7188)
Test: [  48/48]  Time: 0.150 (3.630)  Loss:  1.4498 (2.5503)  Acc@1: 71.2264 (46.4040)  Acc@5: 86.0849 (71.6360)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-33.pth.tar', 46.40399991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-32.pth.tar', 45.39800004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-31.pth.tar', 45.19400001708984)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-30.pth.tar', 44.11399999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-29.pth.tar', 43.00800002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-28.pth.tar', 42.30800003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-27.pth.tar', 41.33400005615234)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-26.pth.tar', 40.54799995361328)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-25.pth.tar', 39.35400008056641)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-24.pth.tar', 38.06599998779297)

Train: 34 [   0/1251 (  0%)]  Loss:  5.124995 (5.1250)  Time: 15.766s,   64.95/s  (15.766s,   64.95/s)  LR: 1.309e-04  Data: 14.441 (14.441)
Train: 34 [  50/1251 (  4%)]  Loss:  4.703329 (4.9142)  Time: 0.584s, 1753.33/s  (2.881s,  355.43/s)  LR: 1.309e-04  Data: 0.019 (2.280)
Train: 34 [ 100/1251 (  8%)]  Loss:  5.099941 (4.9761)  Time: 0.587s, 1744.98/s  (2.777s,  368.79/s)  LR: 1.309e-04  Data: 0.022 (2.165)
Train: 34 [ 150/1251 ( 12%)]  Loss:  4.802839 (4.9328)  Time: 0.590s, 1735.53/s  (2.708s,  378.13/s)  LR: 1.309e-04  Data: 0.025 (2.096)
Train: 34 [ 200/1251 ( 16%)]  Loss:  5.296524 (5.0055)  Time: 0.588s, 1742.60/s  (2.657s,  385.38/s)  LR: 1.309e-04  Data: 0.020 (2.048)
Train: 34 [ 250/1251 ( 20%)]  Loss:  5.114792 (5.0237)  Time: 0.591s, 1732.30/s  (2.635s,  388.54/s)  LR: 1.309e-04  Data: 0.025 (2.028)
Train: 34 [ 300/1251 ( 24%)]  Loss:  5.534552 (5.0967)  Time: 0.590s, 1736.92/s  (2.590s,  395.39/s)  LR: 1.309e-04  Data: 0.025 (1.985)
Train: 34 [ 350/1251 ( 28%)]  Loss:  5.275064 (5.1190)  Time: 0.587s, 1745.86/s  (2.630s,  389.33/s)  LR: 1.309e-04  Data: 0.023 (2.028)
Train: 34 [ 400/1251 ( 32%)]  Loss:  5.262602 (5.1350)  Time: 0.585s, 1749.85/s  (2.633s,  388.89/s)  LR: 1.309e-04  Data: 0.019 (2.033)
Train: 34 [ 450/1251 ( 36%)]  Loss:  5.275881 (5.1491)  Time: 0.589s, 1739.11/s  (2.638s,  388.24/s)  LR: 1.309e-04  Data: 0.024 (2.036)
Train: 34 [ 500/1251 ( 40%)]  Loss:  4.978909 (5.1336)  Time: 0.583s, 1756.72/s  (2.624s,  390.29/s)  LR: 1.309e-04  Data: 0.018 (2.023)
Train: 34 [ 550/1251 ( 44%)]  Loss:  4.696099 (5.0971)  Time: 0.590s, 1735.48/s  (2.629s,  389.48/s)  LR: 1.309e-04  Data: 0.025 (2.029)
Train: 34 [ 600/1251 ( 48%)]  Loss:  5.064048 (5.0946)  Time: 0.585s, 1749.17/s  (2.630s,  389.38/s)  LR: 1.309e-04  Data: 0.021 (2.030)
Train: 34 [ 650/1251 ( 52%)]  Loss:  4.913578 (5.0817)  Time: 0.590s, 1735.13/s  (2.632s,  389.01/s)  LR: 1.309e-04  Data: 0.025 (2.034)
Train: 34 [ 700/1251 ( 56%)]  Loss:  4.943671 (5.0725)  Time: 0.586s, 1747.20/s  (2.641s,  387.74/s)  LR: 1.309e-04  Data: 0.021 (2.043)
Train: 34 [ 750/1251 ( 60%)]  Loss:  5.196771 (5.0802)  Time: 0.589s, 1738.81/s  (2.652s,  386.11/s)  LR: 1.309e-04  Data: 0.024 (2.056)
Train: 34 [ 800/1251 ( 64%)]  Loss:  4.479791 (5.0449)  Time: 0.589s, 1739.74/s  (2.651s,  386.20/s)  LR: 1.309e-04  Data: 0.024 (2.055)
Train: 34 [ 850/1251 ( 68%)]  Loss:  5.014413 (5.0432)  Time: 0.590s, 1736.93/s  (2.658s,  385.19/s)  LR: 1.309e-04  Data: 0.025 (2.062)
Train: 34 [ 900/1251 ( 72%)]  Loss:  4.595957 (5.0197)  Time: 0.589s, 1739.99/s  (2.646s,  387.06/s)  LR: 1.309e-04  Data: 0.023 (2.049)
Train: 34 [ 950/1251 ( 76%)]  Loss:  4.887299 (5.0131)  Time: 0.590s, 1735.48/s  (2.642s,  387.64/s)  LR: 1.309e-04  Data: 0.023 (2.045)
Train: 34 [1000/1251 ( 80%)]  Loss:  5.053279 (5.0150)  Time: 0.587s, 1743.41/s  (2.628s,  389.63/s)  LR: 1.309e-04  Data: 0.021 (2.031)
Train: 34 [1050/1251 ( 84%)]  Loss:  4.988883 (5.0138)  Time: 0.589s, 1737.26/s  (2.649s,  386.61/s)  LR: 1.309e-04  Data: 0.023 (2.052)
Train: 34 [1100/1251 ( 88%)]  Loss:  5.171136 (5.0206)  Time: 0.586s, 1746.35/s  (2.654s,  385.84/s)  LR: 1.309e-04  Data: 0.022 (2.057)
Train: 34 [1150/1251 ( 92%)]  Loss:  5.057579 (5.0222)  Time: 0.590s, 1736.89/s  (2.662s,  384.68/s)  LR: 1.309e-04  Data: 0.023 (2.066)
Train: 34 [1200/1251 ( 96%)]  Loss:  4.730709 (5.0105)  Time: 0.586s, 1748.19/s  (2.654s,  385.77/s)  LR: 1.309e-04  Data: 0.021 (2.059)
Train: 34 [1250/1251 (100%)]  Loss:  5.304714 (5.0218)  Time: 0.568s, 1803.50/s  (2.653s,  385.93/s)  LR: 1.309e-04  Data: 0.000 (2.058)
Test: [   0/48]  Time: 15.028 (15.028)  Loss:  1.5286 (1.5286)  Acc@1: 70.0195 (70.0195)  Acc@5: 88.4766 (88.4766)
Test: [  48/48]  Time: 0.148 (3.554)  Loss:  1.4866 (2.5121)  Acc@1: 70.9906 (46.8840)  Acc@5: 85.7311 (72.1100)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-34.pth.tar', 46.88400006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-33.pth.tar', 46.40399991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-32.pth.tar', 45.39800004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-31.pth.tar', 45.19400001708984)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-30.pth.tar', 44.11399999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-29.pth.tar', 43.00800002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-28.pth.tar', 42.30800003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-27.pth.tar', 41.33400005615234)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-26.pth.tar', 40.54799995361328)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-25.pth.tar', 39.35400008056641)

Train: 35 [   0/1251 (  0%)]  Loss:  5.331799 (5.3318)  Time: 11.721s,   87.36/s  (11.721s,   87.36/s)  LR: 1.087e-04  Data: 10.987 (10.987)
Train: 35 [  50/1251 (  4%)]  Loss:  5.282693 (5.3072)  Time: 0.589s, 1737.99/s  (2.952s,  346.83/s)  LR: 1.087e-04  Data: 0.023 (2.346)
Train: 35 [ 100/1251 (  8%)]  Loss:  4.826000 (5.1468)  Time: 4.029s,  254.13/s  (2.860s,  357.99/s)  LR: 1.087e-04  Data: 3.442 (2.258)
Train: 35 [ 150/1251 ( 12%)]  Loss:  4.824254 (5.0662)  Time: 0.587s, 1745.67/s  (2.764s,  370.53/s)  LR: 1.087e-04  Data: 0.022 (2.161)
Train: 35 [ 200/1251 ( 16%)]  Loss:  4.725506 (4.9981)  Time: 8.465s,  120.98/s  (2.741s,  373.57/s)  LR: 1.087e-04  Data: 7.866 (2.140)
Train: 35 [ 250/1251 ( 20%)]  Loss:  4.596737 (4.9312)  Time: 0.586s, 1746.78/s  (2.689s,  380.85/s)  LR: 1.087e-04  Data: 0.018 (2.087)
Train: 35 [ 300/1251 ( 24%)]  Loss:  4.596014 (4.8833)  Time: 4.577s,  223.72/s  (2.662s,  384.67/s)  LR: 1.087e-04  Data: 3.619 (2.059)
Train: 35 [ 350/1251 ( 28%)]  Loss:  5.089573 (4.9091)  Time: 4.318s,  237.15/s  (2.626s,  389.97/s)  LR: 1.087e-04  Data: 3.644 (2.021)
Train: 35 [ 400/1251 ( 32%)]  Loss:  5.374255 (4.9608)  Time: 4.278s,  239.38/s  (2.653s,  386.04/s)  LR: 1.087e-04  Data: 3.688 (2.047)
Train: 35 [ 450/1251 ( 36%)]  Loss:  5.503988 (5.0151)  Time: 2.704s,  378.67/s  (2.655s,  385.71/s)  LR: 1.087e-04  Data: 2.055 (2.048)
Train: 35 [ 500/1251 ( 40%)]  Loss:  5.052650 (5.0185)  Time: 5.926s,  172.78/s  (2.667s,  383.97/s)  LR: 1.087e-04  Data: 5.264 (2.057)
Train: 35 [ 550/1251 ( 44%)]  Loss:  4.934190 (5.0115)  Time: 0.589s, 1737.65/s  (2.666s,  384.06/s)  LR: 1.087e-04  Data: 0.023 (2.058)
Train: 35 [ 600/1251 ( 48%)]  Loss:  4.777288 (4.9935)  Time: 9.208s,  111.21/s  (2.676s,  382.63/s)  LR: 1.087e-04  Data: 8.551 (2.069)
Train: 35 [ 650/1251 ( 52%)]  Loss:  5.050776 (4.9976)  Time: 0.586s, 1747.66/s  (2.665s,  384.29/s)  LR: 1.087e-04  Data: 0.020 (2.060)
Train: 35 [ 700/1251 ( 56%)]  Loss:  4.535927 (4.9668)  Time: 7.578s,  135.13/s  (2.659s,  385.08/s)  LR: 1.087e-04  Data: 6.858 (2.055)
Train: 35 [ 750/1251 ( 60%)]  Loss:  5.305984 (4.9880)  Time: 3.314s,  308.96/s  (2.679s,  382.28/s)  LR: 1.087e-04  Data: 2.736 (2.073)
Train: 35 [ 800/1251 ( 64%)]  Loss:  5.099978 (4.9946)  Time: 0.656s, 1562.16/s  (2.686s,  381.27/s)  LR: 1.087e-04  Data: 0.017 (2.079)
Train: 35 [ 850/1251 ( 68%)]  Loss:  5.207853 (5.0064)  Time: 2.941s,  348.20/s  (2.688s,  381.00/s)  LR: 1.087e-04  Data: 2.345 (2.081)
Train: 35 [ 900/1251 ( 72%)]  Loss:  4.986666 (5.0054)  Time: 6.248s,  163.90/s  (2.686s,  381.19/s)  LR: 1.087e-04  Data: 5.665 (2.077)
Train: 35 [ 950/1251 ( 76%)]  Loss:  5.609014 (5.0356)  Time: 2.427s,  421.84/s  (2.679s,  382.26/s)  LR: 1.087e-04  Data: 1.844 (2.070)
Train: 35 [1000/1251 ( 80%)]  Loss:  5.003340 (5.0340)  Time: 7.819s,  130.96/s  (2.671s,  383.33/s)  LR: 1.087e-04  Data: 7.073 (2.063)
Train: 35 [1050/1251 ( 84%)]  Loss:  5.138385 (5.0388)  Time: 6.608s,  154.97/s  (2.674s,  382.97/s)  LR: 1.087e-04  Data: 6.027 (2.066)
Train: 35 [1100/1251 ( 88%)]  Loss:  5.567118 (5.0617)  Time: 4.923s,  207.98/s  (2.676s,  382.69/s)  LR: 1.087e-04  Data: 4.358 (2.067)
Train: 35 [1150/1251 ( 92%)]  Loss:  5.090359 (5.0629)  Time: 2.617s,  391.34/s  (2.673s,  383.07/s)  LR: 1.087e-04  Data: 2.005 (2.064)
Train: 35 [1200/1251 ( 96%)]  Loss:  5.292315 (5.0721)  Time: 6.407s,  159.83/s  (2.672s,  383.26/s)  LR: 1.087e-04  Data: 5.844 (2.062)
Train: 35 [1250/1251 (100%)]  Loss:  5.151822 (5.0752)  Time: 0.565s, 1812.77/s  (2.665s,  384.19/s)  LR: 1.087e-04  Data: 0.000 (2.056)
Test: [   0/48]  Time: 14.708 (14.708)  Loss:  1.5243 (1.5243)  Acc@1: 69.4336 (69.4336)  Acc@5: 86.8164 (86.8164)
Test: [  48/48]  Time: 0.148 (3.506)  Loss:  1.4491 (2.4850)  Acc@1: 70.0472 (47.2920)  Acc@5: 86.0849 (72.5360)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-35.pth.tar', 47.29200004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-34.pth.tar', 46.88400006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-33.pth.tar', 46.40399991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-32.pth.tar', 45.39800004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-31.pth.tar', 45.19400001708984)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-30.pth.tar', 44.11399999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-29.pth.tar', 43.00800002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-28.pth.tar', 42.30800003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-27.pth.tar', 41.33400005615234)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-26.pth.tar', 40.54799995361328)

Train: 36 [   0/1251 (  0%)]  Loss:  5.319686 (5.3197)  Time: 12.719s,   80.51/s  (12.719s,   80.51/s)  LR: 8.858e-05  Data: 11.967 (11.967)
Train: 36 [  50/1251 (  4%)]  Loss:  4.713634 (5.0167)  Time: 0.589s, 1739.15/s  (2.546s,  402.16/s)  LR: 8.858e-05  Data: 0.024 (1.943)
Train: 36 [ 100/1251 (  8%)]  Loss:  5.114054 (5.0491)  Time: 0.592s, 1729.97/s  (2.731s,  374.93/s)  LR: 8.858e-05  Data: 0.020 (2.132)
Train: 36 [ 150/1251 ( 12%)]  Loss:  5.080489 (5.0570)  Time: 0.588s, 1742.19/s  (2.686s,  381.19/s)  LR: 8.858e-05  Data: 0.022 (2.091)
Train: 36 [ 200/1251 ( 16%)]  Loss:  5.009830 (5.0475)  Time: 5.768s,  177.53/s  (2.729s,  375.29/s)  LR: 8.858e-05  Data: 5.188 (2.130)
Train: 36 [ 250/1251 ( 20%)]  Loss:  5.158200 (5.0660)  Time: 0.591s, 1733.32/s  (2.684s,  381.58/s)  LR: 8.858e-05  Data: 0.019 (2.082)
Train: 36 [ 300/1251 ( 24%)]  Loss:  5.207649 (5.0862)  Time: 8.079s,  126.75/s  (2.660s,  384.91/s)  LR: 8.858e-05  Data: 7.412 (2.057)
Train: 36 [ 350/1251 ( 28%)]  Loss:  4.850388 (5.0567)  Time: 0.590s, 1734.26/s  (2.624s,  390.31/s)  LR: 8.858e-05  Data: 0.025 (2.022)
Train: 36 [ 400/1251 ( 32%)]  Loss:  5.262782 (5.0796)  Time: 5.533s,  185.07/s  (2.609s,  392.52/s)  LR: 8.858e-05  Data: 4.858 (2.008)
Train: 36 [ 450/1251 ( 36%)]  Loss:  5.428984 (5.1146)  Time: 0.586s, 1746.35/s  (2.629s,  389.44/s)  LR: 8.858e-05  Data: 0.019 (2.029)
Train: 36 [ 500/1251 ( 40%)]  Loss:  5.333585 (5.1345)  Time: 5.447s,  188.01/s  (2.646s,  386.98/s)  LR: 8.858e-05  Data: 4.789 (2.045)
Train: 36 [ 550/1251 ( 44%)]  Loss:  5.430986 (5.1592)  Time: 0.589s, 1739.50/s  (2.640s,  387.93/s)  LR: 8.858e-05  Data: 0.023 (2.038)
Train: 36 [ 600/1251 ( 48%)]  Loss:  4.993720 (5.1465)  Time: 3.082s,  332.29/s  (2.655s,  385.74/s)  LR: 8.858e-05  Data: 2.495 (2.053)
Train: 36 [ 650/1251 ( 52%)]  Loss:  5.276264 (5.1557)  Time: 0.587s, 1744.95/s  (2.649s,  386.63/s)  LR: 8.858e-05  Data: 0.021 (2.047)
Train: 36 [ 700/1251 ( 56%)]  Loss:  5.130083 (5.1540)  Time: 5.949s,  172.14/s  (2.654s,  385.89/s)  LR: 8.858e-05  Data: 5.372 (2.052)
Train: 36 [ 750/1251 ( 60%)]  Loss:  4.873669 (5.1365)  Time: 0.591s, 1733.37/s  (2.662s,  384.62/s)  LR: 8.858e-05  Data: 0.023 (2.061)
Train: 36 [ 800/1251 ( 64%)]  Loss:  5.076229 (5.1330)  Time: 9.565s,  107.06/s  (2.672s,  383.29/s)  LR: 8.858e-05  Data: 8.966 (2.071)
Train: 36 [ 850/1251 ( 68%)]  Loss:  4.599657 (5.1033)  Time: 0.586s, 1746.95/s  (2.670s,  383.53/s)  LR: 8.858e-05  Data: 0.019 (2.070)
Train: 36 [ 900/1251 ( 72%)]  Loss:  4.529421 (5.0731)  Time: 8.063s,  127.00/s  (2.671s,  383.34/s)  LR: 8.858e-05  Data: 7.500 (2.072)
Train: 36 [ 950/1251 ( 76%)]  Loss:  5.488978 (5.0939)  Time: 0.586s, 1747.07/s  (2.658s,  385.30/s)  LR: 8.858e-05  Data: 0.020 (2.060)
Train: 36 [1000/1251 ( 80%)]  Loss:  4.883161 (5.0839)  Time: 7.703s,  132.94/s  (2.655s,  385.71/s)  LR: 8.858e-05  Data: 7.082 (2.056)
Train: 36 [1050/1251 ( 84%)]  Loss:  5.041801 (5.0820)  Time: 0.585s, 1750.87/s  (2.642s,  387.51/s)  LR: 8.858e-05  Data: 0.020 (2.043)
Train: 36 [1100/1251 ( 88%)]  Loss:  5.256320 (5.0895)  Time: 5.146s,  198.97/s  (2.652s,  386.18/s)  LR: 8.858e-05  Data: 4.582 (2.052)
Train: 36 [1150/1251 ( 92%)]  Loss:  5.390081 (5.1021)  Time: 0.585s, 1749.93/s  (2.653s,  385.96/s)  LR: 8.858e-05  Data: 0.023 (2.053)
Train: 36 [1200/1251 ( 96%)]  Loss:  5.032422 (5.0993)  Time: 0.986s, 1038.86/s  (2.651s,  386.32/s)  LR: 8.858e-05  Data: 0.412 (2.050)
Train: 36 [1250/1251 (100%)]  Loss:  4.970980 (5.0943)  Time: 0.565s, 1813.22/s  (2.651s,  386.31/s)  LR: 8.858e-05  Data: 0.000 (2.048)
Test: [   0/48]  Time: 16.064 (16.064)  Loss:  1.4803 (1.4803)  Acc@1: 69.8242 (69.8242)  Acc@5: 88.7695 (88.7695)
Test: [  48/48]  Time: 0.149 (3.558)  Loss:  1.3760 (2.4423)  Acc@1: 71.8160 (48.2120)  Acc@5: 86.0849 (73.0980)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-36.pth.tar', 48.2120000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-35.pth.tar', 47.29200004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-34.pth.tar', 46.88400006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-33.pth.tar', 46.40399991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-32.pth.tar', 45.39800004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-31.pth.tar', 45.19400001708984)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-30.pth.tar', 44.11399999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-29.pth.tar', 43.00800002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-28.pth.tar', 42.30800003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-27.pth.tar', 41.33400005615234)

Train: 37 [   0/1251 (  0%)]  Loss:  5.051219 (5.0512)  Time: 11.474s,   89.24/s  (11.474s,   89.24/s)  LR: 7.055e-05  Data: 10.300 (10.300)
Train: 37 [  50/1251 (  4%)]  Loss:  5.277467 (5.1643)  Time: 0.586s, 1747.36/s  (2.583s,  396.38/s)  LR: 7.055e-05  Data: 0.019 (1.979)
Train: 37 [ 100/1251 (  8%)]  Loss:  4.998530 (5.1091)  Time: 0.586s, 1747.74/s  (2.536s,  403.71/s)  LR: 7.055e-05  Data: 0.021 (1.932)
Train: 37 [ 150/1251 ( 12%)]  Loss:  5.521573 (5.2122)  Time: 0.588s, 1740.04/s  (2.623s,  390.43/s)  LR: 7.055e-05  Data: 0.021 (2.015)
Train: 37 [ 200/1251 ( 16%)]  Loss:  5.395148 (5.2488)  Time: 0.584s, 1752.78/s  (2.658s,  385.21/s)  LR: 7.055e-05  Data: 0.020 (2.056)
Train: 37 [ 250/1251 ( 20%)]  Loss:  4.834394 (5.1797)  Time: 0.591s, 1734.03/s  (2.644s,  387.30/s)  LR: 7.055e-05  Data: 0.026 (2.044)
Train: 37 [ 300/1251 ( 24%)]  Loss:  5.139866 (5.1740)  Time: 0.586s, 1746.99/s  (2.647s,  386.90/s)  LR: 7.055e-05  Data: 0.022 (2.050)
Train: 37 [ 350/1251 ( 28%)]  Loss:  4.520299 (5.0923)  Time: 0.587s, 1745.45/s  (2.621s,  390.62/s)  LR: 7.055e-05  Data: 0.022 (2.027)
Train: 37 [ 400/1251 ( 32%)]  Loss:  5.345054 (5.1204)  Time: 0.589s, 1737.62/s  (2.606s,  392.89/s)  LR: 7.055e-05  Data: 0.026 (2.009)
Train: 37 [ 450/1251 ( 36%)]  Loss:  4.212908 (5.0296)  Time: 0.586s, 1748.01/s  (2.597s,  394.36/s)  LR: 7.055e-05  Data: 0.021 (1.999)
Train: 37 [ 500/1251 ( 40%)]  Loss:  4.556523 (4.9866)  Time: 0.587s, 1743.24/s  (2.628s,  389.65/s)  LR: 7.055e-05  Data: 0.023 (2.033)
Train: 37 [ 550/1251 ( 44%)]  Loss:  4.734216 (4.9656)  Time: 0.586s, 1747.51/s  (2.652s,  386.16/s)  LR: 7.055e-05  Data: 0.019 (2.057)
Train: 37 [ 600/1251 ( 48%)]  Loss:  5.312140 (4.9923)  Time: 0.589s, 1737.98/s  (2.657s,  385.35/s)  LR: 7.055e-05  Data: 0.024 (2.063)
Train: 37 [ 650/1251 ( 52%)]  Loss:  4.950241 (4.9893)  Time: 0.587s, 1744.52/s  (2.672s,  383.30/s)  LR: 7.055e-05  Data: 0.021 (2.079)
Train: 37 [ 700/1251 ( 56%)]  Loss:  4.919028 (4.9846)  Time: 0.588s, 1742.12/s  (2.661s,  384.86/s)  LR: 7.055e-05  Data: 0.023 (2.068)
Train: 37 [ 750/1251 ( 60%)]  Loss:  4.282506 (4.9407)  Time: 0.586s, 1748.39/s  (2.657s,  385.44/s)  LR: 7.055e-05  Data: 0.021 (2.064)
Train: 37 [ 800/1251 ( 64%)]  Loss:  4.788165 (4.9317)  Time: 0.588s, 1740.48/s  (2.656s,  385.53/s)  LR: 7.055e-05  Data: 0.024 (2.063)
Train: 37 [ 850/1251 ( 68%)]  Loss:  5.284412 (4.9513)  Time: 0.588s, 1742.09/s  (2.658s,  385.19/s)  LR: 7.055e-05  Data: 0.021 (2.066)
Train: 37 [ 900/1251 ( 72%)]  Loss:  4.787203 (4.9427)  Time: 0.588s, 1740.46/s  (2.645s,  387.16/s)  LR: 7.055e-05  Data: 0.021 (2.053)
Train: 37 [ 950/1251 ( 76%)]  Loss:  5.231171 (4.9571)  Time: 0.588s, 1741.75/s  (2.638s,  388.15/s)  LR: 7.055e-05  Data: 0.024 (2.046)
Train: 37 [1000/1251 ( 80%)]  Loss:  5.184015 (4.9679)  Time: 0.587s, 1743.76/s  (2.623s,  390.33/s)  LR: 7.055e-05  Data: 0.019 (2.032)
Train: 37 [1050/1251 ( 84%)]  Loss:  4.660483 (4.9539)  Time: 0.586s, 1746.76/s  (2.611s,  392.22/s)  LR: 7.055e-05  Data: 0.022 (2.020)
Train: 37 [1100/1251 ( 88%)]  Loss:  4.792428 (4.9469)  Time: 0.585s, 1749.12/s  (2.594s,  394.68/s)  LR: 7.055e-05  Data: 0.020 (2.003)
Train: 37 [1150/1251 ( 92%)]  Loss:  5.105696 (4.9535)  Time: 0.587s, 1743.50/s  (2.582s,  396.63/s)  LR: 7.055e-05  Data: 0.019 (1.990)
Train: 37 [1200/1251 ( 96%)]  Loss:  4.887391 (4.9509)  Time: 0.585s, 1750.47/s  (2.587s,  395.88/s)  LR: 7.055e-05  Data: 0.020 (1.994)
Train: 37 [1250/1251 (100%)]  Loss:  4.812893 (4.9456)  Time: 0.567s, 1805.78/s  (2.584s,  396.24/s)  LR: 7.055e-05  Data: 0.000 (1.991)
Test: [   0/48]  Time: 16.484 (16.484)  Loss:  1.4697 (1.4697)  Acc@1: 70.2148 (70.2148)  Acc@5: 88.9648 (88.9648)
Test: [  48/48]  Time: 0.148 (3.514)  Loss:  1.4012 (2.4410)  Acc@1: 71.8160 (48.6140)  Acc@5: 87.0283 (73.2960)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-37.pth.tar', 48.6140000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-36.pth.tar', 48.2120000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-35.pth.tar', 47.29200004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-34.pth.tar', 46.88400006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-33.pth.tar', 46.40399991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-32.pth.tar', 45.39800004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-31.pth.tar', 45.19400001708984)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-30.pth.tar', 44.11399999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-29.pth.tar', 43.00800002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-28.pth.tar', 42.30800003173828)

Train: 38 [   0/1251 (  0%)]  Loss:  4.297204 (4.2972)  Time: 12.055s,   84.94/s  (12.055s,   84.94/s)  LR: 5.473e-05  Data: 11.377 (11.377)
Train: 38 [  50/1251 (  4%)]  Loss:  4.541885 (4.4195)  Time: 0.600s, 1707.81/s  (2.538s,  403.49/s)  LR: 5.473e-05  Data: 0.020 (1.926)
Train: 38 [ 100/1251 (  8%)]  Loss:  5.079763 (4.6396)  Time: 0.591s, 1734.00/s  (2.425s,  422.26/s)  LR: 5.473e-05  Data: 0.021 (1.819)
Train: 38 [ 150/1251 ( 12%)]  Loss:  5.017509 (4.7341)  Time: 0.584s, 1754.40/s  (2.398s,  426.96/s)  LR: 5.473e-05  Data: 0.019 (1.792)
Train: 38 [ 200/1251 ( 16%)]  Loss:  4.662988 (4.7199)  Time: 0.588s, 1741.89/s  (2.419s,  423.30/s)  LR: 5.473e-05  Data: 0.020 (1.813)
Train: 38 [ 250/1251 ( 20%)]  Loss:  4.771550 (4.7285)  Time: 0.585s, 1751.34/s  (2.461s,  416.03/s)  LR: 5.473e-05  Data: 0.020 (1.858)
Train: 38 [ 300/1251 ( 24%)]  Loss:  5.216694 (4.7982)  Time: 0.591s, 1731.25/s  (2.450s,  418.00/s)  LR: 5.473e-05  Data: 0.021 (1.846)
Train: 38 [ 350/1251 ( 28%)]  Loss:  5.345223 (4.8666)  Time: 0.585s, 1751.13/s  (2.451s,  417.75/s)  LR: 5.473e-05  Data: 0.020 (1.847)
Train: 38 [ 400/1251 ( 32%)]  Loss:  4.522208 (4.8283)  Time: 0.588s, 1742.48/s  (2.446s,  418.72/s)  LR: 5.473e-05  Data: 0.020 (1.842)
Train: 38 [ 450/1251 ( 36%)]  Loss:  4.809113 (4.8264)  Time: 0.586s, 1748.91/s  (2.441s,  419.57/s)  LR: 5.473e-05  Data: 0.019 (1.836)
Train: 38 [ 500/1251 ( 40%)]  Loss:  4.621791 (4.8078)  Time: 0.584s, 1752.22/s  (2.439s,  419.77/s)  LR: 5.473e-05  Data: 0.019 (1.837)
Train: 38 [ 550/1251 ( 44%)]  Loss:  5.111716 (4.8331)  Time: 0.584s, 1752.97/s  (2.426s,  422.14/s)  LR: 5.473e-05  Data: 0.019 (1.821)
Train: 38 [ 600/1251 ( 48%)]  Loss:  5.024198 (4.8478)  Time: 0.585s, 1751.16/s  (2.459s,  416.44/s)  LR: 5.473e-05  Data: 0.020 (1.854)
Train: 38 [ 650/1251 ( 52%)]  Loss:  4.919435 (4.8529)  Time: 0.583s, 1757.39/s  (2.481s,  412.68/s)  LR: 5.473e-05  Data: 0.018 (1.877)
Train: 38 [ 700/1251 ( 56%)]  Loss:  5.037922 (4.8653)  Time: 0.589s, 1738.17/s  (2.481s,  412.73/s)  LR: 5.473e-05  Data: 0.022 (1.877)
Train: 38 [ 750/1251 ( 60%)]  Loss:  4.828975 (4.8630)  Time: 0.586s, 1747.29/s  (2.486s,  411.88/s)  LR: 5.473e-05  Data: 0.022 (1.881)
Train: 38 [ 800/1251 ( 64%)]  Loss:  4.725519 (4.8549)  Time: 0.587s, 1745.94/s  (2.480s,  412.84/s)  LR: 5.473e-05  Data: 0.022 (1.875)
Train: 38 [ 850/1251 ( 68%)]  Loss:  4.981849 (4.8620)  Time: 0.586s, 1746.21/s  (2.474s,  413.84/s)  LR: 5.473e-05  Data: 0.022 (1.869)
Train: 38 [ 900/1251 ( 72%)]  Loss:  4.854603 (4.8616)  Time: 0.586s, 1746.01/s  (2.464s,  415.50/s)  LR: 5.473e-05  Data: 0.022 (1.859)
Train: 38 [ 950/1251 ( 76%)]  Loss:  4.794109 (4.8582)  Time: 0.587s, 1743.79/s  (2.486s,  411.98/s)  LR: 5.473e-05  Data: 0.024 (1.880)
Train: 38 [1000/1251 ( 80%)]  Loss:  4.558833 (4.8440)  Time: 0.587s, 1745.10/s  (2.484s,  412.20/s)  LR: 5.473e-05  Data: 0.022 (1.880)
Train: 38 [1050/1251 ( 84%)]  Loss:  5.230438 (4.8615)  Time: 0.591s, 1732.80/s  (2.485s,  412.12/s)  LR: 5.473e-05  Data: 0.026 (1.881)
Train: 38 [1100/1251 ( 88%)]  Loss:  5.245059 (4.8782)  Time: 0.587s, 1745.51/s  (2.480s,  412.83/s)  LR: 5.473e-05  Data: 0.022 (1.878)
Train: 38 [1150/1251 ( 92%)]  Loss:  4.642023 (4.8684)  Time: 0.586s, 1746.03/s  (2.479s,  413.12/s)  LR: 5.473e-05  Data: 0.022 (1.877)
Train: 38 [1200/1251 ( 96%)]  Loss:  4.886909 (4.8691)  Time: 0.589s, 1738.34/s  (2.468s,  414.92/s)  LR: 5.473e-05  Data: 0.024 (1.867)
Train: 38 [1250/1251 (100%)]  Loss:  4.865471 (4.8690)  Time: 0.564s, 1815.41/s  (2.465s,  415.44/s)  LR: 5.473e-05  Data: 0.000 (1.864)
Test: [   0/48]  Time: 15.393 (15.393)  Loss:  1.4250 (1.4250)  Acc@1: 71.8750 (71.8750)  Acc@5: 88.8672 (88.8672)
Test: [  48/48]  Time: 0.148 (3.655)  Loss:  1.3888 (2.4102)  Acc@1: 72.8773 (48.9900)  Acc@5: 87.6179 (73.7300)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-38.pth.tar', 48.98999985351563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-37.pth.tar', 48.6140000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-36.pth.tar', 48.2120000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-35.pth.tar', 47.29200004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-34.pth.tar', 46.88400006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-33.pth.tar', 46.40399991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-32.pth.tar', 45.39800004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-31.pth.tar', 45.19400001708984)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-30.pth.tar', 44.11399999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-29.pth.tar', 43.00800002441406)

Train: 39 [   0/1251 (  0%)]  Loss:  5.098051 (5.0981)  Time: 12.578s,   81.41/s  (12.578s,   81.41/s)  LR: 4.121e-05  Data: 11.067 (11.067)
Train: 39 [  50/1251 (  4%)]  Loss:  4.961590 (5.0298)  Time: 1.107s,  925.09/s  (2.585s,  396.06/s)  LR: 4.121e-05  Data: 0.525 (1.962)
Train: 39 [ 100/1251 (  8%)]  Loss:  5.253247 (5.1043)  Time: 2.903s,  352.74/s  (2.552s,  401.23/s)  LR: 4.121e-05  Data: 2.022 (1.939)
Train: 39 [ 150/1251 ( 12%)]  Loss:  4.799133 (5.0280)  Time: 0.593s, 1725.94/s  (2.483s,  412.37/s)  LR: 4.121e-05  Data: 0.025 (1.871)
Train: 39 [ 200/1251 ( 16%)]  Loss:  5.338232 (5.0901)  Time: 0.589s, 1737.75/s  (2.469s,  414.68/s)  LR: 4.121e-05  Data: 0.019 (1.861)
Train: 39 [ 250/1251 ( 20%)]  Loss:  4.520490 (4.9951)  Time: 0.588s, 1741.41/s  (2.419s,  423.34/s)  LR: 4.121e-05  Data: 0.024 (1.812)
Train: 39 [ 300/1251 ( 24%)]  Loss:  5.160788 (5.0188)  Time: 0.583s, 1756.51/s  (2.402s,  426.36/s)  LR: 4.121e-05  Data: 0.019 (1.800)
Train: 39 [ 350/1251 ( 28%)]  Loss:  4.898256 (5.0037)  Time: 2.767s,  370.04/s  (2.423s,  422.67/s)  LR: 4.121e-05  Data: 2.181 (1.823)
Train: 39 [ 400/1251 ( 32%)]  Loss:  5.287873 (5.0353)  Time: 0.589s, 1738.59/s  (2.432s,  421.09/s)  LR: 4.121e-05  Data: 0.022 (1.832)
Train: 39 [ 450/1251 ( 36%)]  Loss:  4.302925 (4.9621)  Time: 0.602s, 1701.20/s  (2.432s,  421.01/s)  LR: 4.121e-05  Data: 0.037 (1.833)
Train: 39 [ 500/1251 ( 40%)]  Loss:  4.781959 (4.9457)  Time: 0.593s, 1727.37/s  (2.442s,  419.33/s)  LR: 4.121e-05  Data: 0.021 (1.844)
Train: 39 [ 550/1251 ( 44%)]  Loss:  5.194553 (4.9664)  Time: 0.586s, 1746.24/s  (2.439s,  419.86/s)  LR: 4.121e-05  Data: 0.022 (1.841)
Train: 39 [ 600/1251 ( 48%)]  Loss:  5.132548 (4.9792)  Time: 0.590s, 1736.78/s  (2.445s,  418.77/s)  LR: 4.121e-05  Data: 0.020 (1.844)
Train: 39 [ 650/1251 ( 52%)]  Loss:  4.579668 (4.9507)  Time: 1.195s,  857.24/s  (2.438s,  420.08/s)  LR: 4.121e-05  Data: 0.619 (1.837)
Train: 39 [ 700/1251 ( 56%)]  Loss:  4.578952 (4.9259)  Time: 0.585s, 1750.00/s  (2.444s,  419.03/s)  LR: 4.121e-05  Data: 0.021 (1.842)
Train: 39 [ 750/1251 ( 60%)]  Loss:  4.890482 (4.9237)  Time: 1.739s,  588.88/s  (2.455s,  417.18/s)  LR: 4.121e-05  Data: 0.988 (1.852)
Train: 39 [ 800/1251 ( 64%)]  Loss:  5.230243 (4.9417)  Time: 0.585s, 1751.48/s  (2.462s,  415.87/s)  LR: 4.121e-05  Data: 0.020 (1.860)
Train: 39 [ 850/1251 ( 68%)]  Loss:  4.947122 (4.9420)  Time: 1.577s,  649.17/s  (2.459s,  416.46/s)  LR: 4.121e-05  Data: 0.905 (1.857)
Train: 39 [ 900/1251 ( 72%)]  Loss:  5.433747 (4.9679)  Time: 0.585s, 1750.20/s  (2.459s,  416.48/s)  LR: 4.121e-05  Data: 0.019 (1.856)
Train: 39 [ 950/1251 ( 76%)]  Loss:  4.875315 (4.9633)  Time: 3.353s,  305.42/s  (2.453s,  417.39/s)  LR: 4.121e-05  Data: 2.677 (1.851)
Train: 39 [1000/1251 ( 80%)]  Loss:  5.490623 (4.9884)  Time: 0.589s, 1738.52/s  (2.447s,  418.48/s)  LR: 4.121e-05  Data: 0.021 (1.844)
Train: 39 [1050/1251 ( 84%)]  Loss:  4.629851 (4.9721)  Time: 5.042s,  203.11/s  (2.440s,  419.66/s)  LR: 4.121e-05  Data: 4.464 (1.837)
Train: 39 [1100/1251 ( 88%)]  Loss:  4.510319 (4.9520)  Time: 0.585s, 1750.96/s  (2.461s,  416.11/s)  LR: 4.121e-05  Data: 0.019 (1.857)
Train: 39 [1150/1251 ( 92%)]  Loss:  4.952223 (4.9520)  Time: 8.138s,  125.82/s  (2.481s,  412.77/s)  LR: 4.121e-05  Data: 7.522 (1.878)
Train: 39 [1200/1251 ( 96%)]  Loss:  4.716200 (4.9426)  Time: 0.589s, 1738.33/s  (2.483s,  412.36/s)  LR: 4.121e-05  Data: 0.024 (1.880)
Train: 39 [1250/1251 (100%)]  Loss:  5.391496 (4.9598)  Time: 0.565s, 1811.44/s  (2.485s,  411.99/s)  LR: 4.121e-05  Data: 0.000 (1.883)
Test: [   0/48]  Time: 15.709 (15.709)  Loss:  1.4169 (1.4169)  Acc@1: 71.7773 (71.7773)  Acc@5: 89.5508 (89.5508)
Test: [  48/48]  Time: 0.151 (3.607)  Loss:  1.3439 (2.3958)  Acc@1: 72.4057 (49.0960)  Acc@5: 88.4434 (73.9620)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-39.pth.tar', 49.09600003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-38.pth.tar', 48.98999985351563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-37.pth.tar', 48.6140000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-36.pth.tar', 48.2120000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-35.pth.tar', 47.29200004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-34.pth.tar', 46.88400006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-33.pth.tar', 46.40399991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-32.pth.tar', 45.39800004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-31.pth.tar', 45.19400001708984)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-30.pth.tar', 44.11399999511719)

Train: 40 [   0/1251 (  0%)]  Loss:  5.359643 (5.3596)  Time: 11.773s,   86.98/s  (11.773s,   86.98/s)  LR: 3.005e-05  Data: 11.094 (11.094)
Train: 40 [  50/1251 (  4%)]  Loss:  5.169421 (5.2645)  Time: 0.586s, 1747.16/s  (2.598s,  394.15/s)  LR: 3.005e-05  Data: 0.020 (2.000)
Train: 40 [ 100/1251 (  8%)]  Loss:  5.239690 (5.2563)  Time: 0.586s, 1748.44/s  (2.741s,  373.56/s)  LR: 3.005e-05  Data: 0.020 (2.147)
Train: 40 [ 150/1251 ( 12%)]  Loss:  5.308950 (5.2694)  Time: 0.587s, 1743.42/s  (2.758s,  371.35/s)  LR: 3.005e-05  Data: 0.021 (2.165)
Train: 40 [ 200/1251 ( 16%)]  Loss:  5.327177 (5.2810)  Time: 0.585s, 1749.59/s  (2.750s,  372.39/s)  LR: 3.005e-05  Data: 0.021 (2.154)
Train: 40 [ 250/1251 ( 20%)]  Loss:  4.774061 (5.1965)  Time: 1.430s,  716.24/s  (2.712s,  377.54/s)  LR: 3.005e-05  Data: 0.782 (2.113)
Train: 40 [ 300/1251 ( 24%)]  Loss:  5.225624 (5.2007)  Time: 0.585s, 1751.71/s  (2.701s,  379.06/s)  LR: 3.005e-05  Data: 0.019 (2.101)
Train: 40 [ 350/1251 ( 28%)]  Loss:  5.255256 (5.2075)  Time: 2.567s,  398.92/s  (2.665s,  384.25/s)  LR: 3.005e-05  Data: 1.873 (2.063)
Train: 40 [ 400/1251 ( 32%)]  Loss:  4.993781 (5.1837)  Time: 0.587s, 1745.20/s  (2.643s,  387.51/s)  LR: 3.005e-05  Data: 0.023 (2.041)
Train: 40 [ 450/1251 ( 36%)]  Loss:  5.311847 (5.1965)  Time: 6.511s,  157.27/s  (2.663s,  384.59/s)  LR: 3.005e-05  Data: 5.946 (2.061)
Train: 40 [ 500/1251 ( 40%)]  Loss:  5.155013 (5.1928)  Time: 0.584s, 1753.40/s  (2.664s,  384.42/s)  LR: 3.005e-05  Data: 0.019 (2.062)
Train: 40 [ 550/1251 ( 44%)]  Loss:  4.845710 (5.1638)  Time: 6.277s,  163.14/s  (2.673s,  383.02/s)  LR: 3.005e-05  Data: 5.653 (2.071)
Train: 40 [ 600/1251 ( 48%)]  Loss:  4.872951 (5.1415)  Time: 0.590s, 1734.65/s  (2.681s,  382.00/s)  LR: 3.005e-05  Data: 0.022 (2.078)
Train: 40 [ 650/1251 ( 52%)]  Loss:  4.185046 (5.0732)  Time: 5.290s,  193.56/s  (2.687s,  381.08/s)  LR: 3.005e-05  Data: 4.616 (2.083)
Train: 40 [ 700/1251 ( 56%)]  Loss:  5.205865 (5.0820)  Time: 0.589s, 1738.59/s  (2.678s,  382.42/s)  LR: 3.005e-05  Data: 0.024 (2.072)
Train: 40 [ 750/1251 ( 60%)]  Loss:  4.729070 (5.0599)  Time: 4.687s,  218.45/s  (2.676s,  382.72/s)  LR: 3.005e-05  Data: 4.123 (2.070)
Train: 40 [ 800/1251 ( 64%)]  Loss:  5.169927 (5.0664)  Time: 0.589s, 1738.04/s  (2.690s,  380.61/s)  LR: 3.005e-05  Data: 0.018 (2.085)
Train: 40 [ 850/1251 ( 68%)]  Loss:  5.515807 (5.0914)  Time: 3.876s,  264.22/s  (2.700s,  379.20/s)  LR: 3.005e-05  Data: 3.208 (2.094)
Train: 40 [ 900/1251 ( 72%)]  Loss:  5.186271 (5.0964)  Time: 0.586s, 1746.85/s  (2.693s,  380.28/s)  LR: 3.005e-05  Data: 0.020 (2.087)
Train: 40 [ 950/1251 ( 76%)]  Loss:  4.783573 (5.0807)  Time: 2.928s,  349.74/s  (2.690s,  380.67/s)  LR: 3.005e-05  Data: 2.277 (2.084)
Train: 40 [1000/1251 ( 80%)]  Loss:  4.556095 (5.0558)  Time: 0.589s, 1739.13/s  (2.683s,  381.59/s)  LR: 3.005e-05  Data: 0.021 (2.076)
Train: 40 [1050/1251 ( 84%)]  Loss:  4.723512 (5.0406)  Time: 2.424s,  422.43/s  (2.674s,  382.94/s)  LR: 3.005e-05  Data: 1.853 (2.066)
Train: 40 [1100/1251 ( 88%)]  Loss:  5.305463 (5.0522)  Time: 0.588s, 1741.14/s  (2.678s,  382.37/s)  LR: 3.005e-05  Data: 0.020 (2.069)
Train: 40 [1150/1251 ( 92%)]  Loss:  5.260358 (5.0608)  Time: 0.588s, 1741.12/s  (2.680s,  382.15/s)  LR: 3.005e-05  Data: 0.024 (2.072)
Train: 40 [1200/1251 ( 96%)]  Loss:  5.118049 (5.0631)  Time: 0.586s, 1746.74/s  (2.684s,  381.49/s)  LR: 3.005e-05  Data: 0.019 (2.077)
Train: 40 [1250/1251 (100%)]  Loss:  5.206133 (5.0686)  Time: 0.565s, 1812.83/s  (2.680s,  382.10/s)  LR: 3.005e-05  Data: 0.000 (2.072)
Test: [   0/48]  Time: 16.309 (16.309)  Loss:  1.4305 (1.4305)  Acc@1: 71.8750 (71.8750)  Acc@5: 89.3555 (89.3555)
Test: [  48/48]  Time: 0.150 (3.695)  Loss:  1.3736 (2.3948)  Acc@1: 72.0519 (49.3460)  Acc@5: 87.5000 (74.0900)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-40.pth.tar', 49.34600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-39.pth.tar', 49.09600003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-38.pth.tar', 48.98999985351563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-37.pth.tar', 48.6140000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-36.pth.tar', 48.2120000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-35.pth.tar', 47.29200004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-34.pth.tar', 46.88400006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-33.pth.tar', 46.40399991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-32.pth.tar', 45.39800004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-31.pth.tar', 45.19400001708984)

Train: 41 [   0/1251 (  0%)]  Loss:  5.279735 (5.2797)  Time: 12.208s,   83.88/s  (12.208s,   83.88/s)  LR: 2.131e-05  Data: 11.176 (11.176)
Train: 41 [  50/1251 (  4%)]  Loss:  5.176671 (5.2282)  Time: 0.592s, 1730.51/s  (2.609s,  392.54/s)  LR: 2.131e-05  Data: 0.021 (1.969)
Train: 41 [ 100/1251 (  8%)]  Loss:  4.940722 (5.1324)  Time: 0.588s, 1741.68/s  (2.558s,  400.37/s)  LR: 2.131e-05  Data: 0.024 (1.935)
Train: 41 [ 150/1251 ( 12%)]  Loss:  5.029366 (5.1066)  Time: 0.588s, 1742.36/s  (2.641s,  387.70/s)  LR: 2.131e-05  Data: 0.021 (2.025)
Train: 41 [ 200/1251 ( 16%)]  Loss:  4.568038 (4.9989)  Time: 0.843s, 1215.08/s  (2.685s,  381.31/s)  LR: 2.131e-05  Data: 0.019 (2.076)
Train: 41 [ 250/1251 ( 20%)]  Loss:  4.913922 (4.9847)  Time: 3.897s,  262.79/s  (2.669s,  383.64/s)  LR: 2.131e-05  Data: 3.174 (2.061)
Train: 41 [ 300/1251 ( 24%)]  Loss:  4.341889 (4.8929)  Time: 0.585s, 1749.61/s  (2.671s,  383.43/s)  LR: 2.131e-05  Data: 0.020 (2.064)
Train: 41 [ 350/1251 ( 28%)]  Loss:  5.193132 (4.9304)  Time: 1.988s,  515.13/s  (2.641s,  387.72/s)  LR: 2.131e-05  Data: 1.335 (2.032)
Train: 41 [ 400/1251 ( 32%)]  Loss:  5.300855 (4.9716)  Time: 0.584s, 1753.52/s  (2.635s,  388.62/s)  LR: 2.131e-05  Data: 0.020 (2.026)
Train: 41 [ 450/1251 ( 36%)]  Loss:  4.591448 (4.9336)  Time: 0.589s, 1738.40/s  (2.606s,  392.87/s)  LR: 2.131e-05  Data: 0.020 (1.998)
Train: 41 [ 500/1251 ( 40%)]  Loss:  4.608045 (4.9040)  Time: 0.588s, 1742.60/s  (2.648s,  386.65/s)  LR: 2.131e-05  Data: 0.021 (2.041)
Train: 41 [ 550/1251 ( 44%)]  Loss:  5.119555 (4.9219)  Time: 1.764s,  580.34/s  (2.645s,  387.10/s)  LR: 2.131e-05  Data: 1.171 (2.040)
Train: 41 [ 600/1251 ( 48%)]  Loss:  5.308300 (4.9517)  Time: 0.584s, 1752.46/s  (2.656s,  385.47/s)  LR: 2.131e-05  Data: 0.022 (2.050)
Train: 41 [ 650/1251 ( 52%)]  Loss:  5.170698 (4.9673)  Time: 0.726s, 1410.36/s  (2.665s,  384.19/s)  LR: 2.131e-05  Data: 0.078 (2.060)
Train: 41 [ 700/1251 ( 56%)]  Loss:  5.457164 (5.0000)  Time: 0.589s, 1739.95/s  (2.661s,  384.77/s)  LR: 2.131e-05  Data: 0.021 (2.055)
Train: 41 [ 750/1251 ( 60%)]  Loss:  4.631074 (4.9769)  Time: 2.211s,  463.22/s  (2.660s,  385.03/s)  LR: 2.131e-05  Data: 1.642 (2.053)
Train: 41 [ 800/1251 ( 64%)]  Loss:  5.159004 (4.9876)  Time: 0.586s, 1747.21/s  (2.668s,  383.85/s)  LR: 2.131e-05  Data: 0.020 (2.063)
Train: 41 [ 850/1251 ( 68%)]  Loss:  4.963784 (4.9863)  Time: 2.026s,  505.45/s  (2.678s,  382.43/s)  LR: 2.131e-05  Data: 1.461 (2.072)
Train: 41 [ 900/1251 ( 72%)]  Loss:  4.581750 (4.9650)  Time: 0.854s, 1198.78/s  (2.674s,  382.95/s)  LR: 2.131e-05  Data: 0.289 (2.070)
Train: 41 [ 950/1251 ( 76%)]  Loss:  4.974175 (4.9655)  Time: 0.586s, 1746.96/s  (2.679s,  382.24/s)  LR: 2.131e-05  Data: 0.022 (2.076)
Train: 41 [1000/1251 ( 80%)]  Loss:  5.429055 (4.9875)  Time: 1.860s,  550.63/s  (2.671s,  383.34/s)  LR: 2.131e-05  Data: 1.170 (2.067)
Train: 41 [1050/1251 ( 84%)]  Loss:  4.457717 (4.9635)  Time: 2.501s,  409.36/s  (2.665s,  384.27/s)  LR: 2.131e-05  Data: 1.845 (2.060)
Train: 41 [1100/1251 ( 88%)]  Loss:  5.051457 (4.9673)  Time: 4.409s,  232.25/s  (2.657s,  385.33/s)  LR: 2.131e-05  Data: 3.766 (2.052)
Train: 41 [1150/1251 ( 92%)]  Loss:  4.260616 (4.9378)  Time: 4.191s,  244.32/s  (2.668s,  383.85/s)  LR: 2.131e-05  Data: 3.512 (2.062)
Train: 41 [1200/1251 ( 96%)]  Loss:  4.888210 (4.9359)  Time: 2.765s,  370.33/s  (2.668s,  383.76/s)  LR: 2.131e-05  Data: 2.121 (2.063)
Train: 41 [1250/1251 (100%)]  Loss:  5.255951 (4.9482)  Time: 0.568s, 1803.72/s  (2.667s,  383.88/s)  LR: 2.131e-05  Data: 0.000 (2.061)
Test: [   0/48]  Time: 16.145 (16.145)  Loss:  1.3923 (1.3923)  Acc@1: 72.0703 (72.0703)  Acc@5: 89.4531 (89.4531)
Test: [  48/48]  Time: 0.148 (3.621)  Loss:  1.3554 (2.3845)  Acc@1: 72.4057 (49.4540)  Acc@5: 88.0896 (74.1140)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-41.pth.tar', 49.454000036621096)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-40.pth.tar', 49.34600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-39.pth.tar', 49.09600003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-38.pth.tar', 48.98999985351563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-37.pth.tar', 48.6140000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-36.pth.tar', 48.2120000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-35.pth.tar', 47.29200004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-34.pth.tar', 46.88400006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-33.pth.tar', 46.40399991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-32.pth.tar', 45.39800004638672)

Train: 42 [   0/1251 (  0%)]  Loss:  4.678289 (4.6783)  Time: 12.499s,   81.93/s  (12.499s,   81.93/s)  LR: 1.504e-05  Data: 11.900 (11.900)
Train: 42 [  50/1251 (  4%)]  Loss:  5.068444 (4.8734)  Time: 0.585s, 1750.49/s  (2.644s,  387.33/s)  LR: 1.504e-05  Data: 0.021 (2.049)
Train: 42 [ 100/1251 (  8%)]  Loss:  4.972711 (4.9065)  Time: 0.585s, 1750.02/s  (2.603s,  393.37/s)  LR: 1.504e-05  Data: 0.021 (2.013)
Train: 42 [ 150/1251 ( 12%)]  Loss:  5.255554 (4.9937)  Time: 0.587s, 1745.30/s  (2.603s,  393.37/s)  LR: 1.504e-05  Data: 0.020 (2.017)
Train: 42 [ 200/1251 ( 16%)]  Loss:  5.337670 (5.0625)  Time: 0.585s, 1751.37/s  (2.684s,  381.48/s)  LR: 1.504e-05  Data: 0.020 (2.099)
Train: 42 [ 250/1251 ( 20%)]  Loss:  4.816806 (5.0216)  Time: 0.586s, 1747.46/s  (2.667s,  383.96/s)  LR: 1.504e-05  Data: 0.022 (2.080)
Train: 42 [ 300/1251 ( 24%)]  Loss:  4.665436 (4.9707)  Time: 0.585s, 1749.40/s  (2.669s,  383.69/s)  LR: 1.504e-05  Data: 0.019 (2.081)
Train: 42 [ 350/1251 ( 28%)]  Loss:  5.280201 (5.0094)  Time: 0.589s, 1739.04/s  (2.653s,  385.95/s)  LR: 1.504e-05  Data: 0.023 (2.065)
Train: 42 [ 400/1251 ( 32%)]  Loss:  4.788836 (4.9849)  Time: 0.588s, 1740.72/s  (2.647s,  386.88/s)  LR: 1.504e-05  Data: 0.022 (2.058)
Train: 42 [ 450/1251 ( 36%)]  Loss:  4.343913 (4.9208)  Time: 1.068s,  958.73/s  (2.614s,  391.76/s)  LR: 1.504e-05  Data: 0.494 (2.026)
Train: 42 [ 500/1251 ( 40%)]  Loss:  4.979100 (4.9261)  Time: 0.586s, 1747.77/s  (2.636s,  388.46/s)  LR: 1.504e-05  Data: 0.021 (2.044)
Train: 42 [ 550/1251 ( 44%)]  Loss:  4.927266 (4.9262)  Time: 2.601s,  393.64/s  (2.637s,  388.38/s)  LR: 1.504e-05  Data: 1.979 (2.043)
Train: 42 [ 600/1251 ( 48%)]  Loss:  5.042679 (4.9351)  Time: 0.588s, 1741.29/s  (2.658s,  385.32/s)  LR: 1.504e-05  Data: 0.022 (2.063)
Train: 42 [ 650/1251 ( 52%)]  Loss:  4.725289 (4.9202)  Time: 0.590s, 1734.54/s  (2.660s,  384.93/s)  LR: 1.504e-05  Data: 0.022 (2.067)
Train: 42 [ 700/1251 ( 56%)]  Loss:  5.002396 (4.9256)  Time: 0.587s, 1744.69/s  (2.663s,  384.59/s)  LR: 1.504e-05  Data: 0.023 (2.070)
Train: 42 [ 750/1251 ( 60%)]  Loss:  4.882238 (4.9229)  Time: 0.588s, 1740.88/s  (2.651s,  386.22/s)  LR: 1.504e-05  Data: 0.021 (2.059)
Train: 42 [ 800/1251 ( 64%)]  Loss:  5.276671 (4.9437)  Time: 0.584s, 1753.48/s  (2.644s,  387.23/s)  LR: 1.504e-05  Data: 0.017 (2.052)
Train: 42 [ 850/1251 ( 68%)]  Loss:  5.017158 (4.9478)  Time: 0.586s, 1748.00/s  (2.653s,  386.02/s)  LR: 1.504e-05  Data: 0.020 (2.060)
Train: 42 [ 900/1251 ( 72%)]  Loss:  4.438978 (4.9210)  Time: 0.585s, 1749.38/s  (2.658s,  385.22/s)  LR: 1.504e-05  Data: 0.022 (2.066)
Train: 42 [ 950/1251 ( 76%)]  Loss:  4.711424 (4.9106)  Time: 0.586s, 1746.18/s  (2.652s,  386.10/s)  LR: 1.504e-05  Data: 0.020 (2.059)
Train: 42 [1000/1251 ( 80%)]  Loss:  5.078803 (4.9186)  Time: 1.145s,  894.65/s  (2.652s,  386.15/s)  LR: 1.504e-05  Data: 0.580 (2.059)
Train: 42 [1050/1251 ( 84%)]  Loss:  4.619563 (4.9050)  Time: 1.080s,  948.00/s  (2.641s,  387.72/s)  LR: 1.504e-05  Data: 0.404 (2.048)
Train: 42 [1100/1251 ( 88%)]  Loss:  4.951468 (4.9070)  Time: 0.589s, 1738.26/s  (2.640s,  387.84/s)  LR: 1.504e-05  Data: 0.025 (2.046)
Train: 42 [1150/1251 ( 92%)]  Loss:  4.637984 (4.8958)  Time: 0.590s, 1736.52/s  (2.631s,  389.28/s)  LR: 1.504e-05  Data: 0.025 (2.037)
Train: 42 [1200/1251 ( 96%)]  Loss:  5.136991 (4.9054)  Time: 0.589s, 1737.57/s  (2.641s,  387.74/s)  LR: 1.504e-05  Data: 0.020 (2.048)
Train: 42 [1250/1251 (100%)]  Loss:  5.281002 (4.9199)  Time: 0.564s, 1814.78/s  (2.638s,  388.11/s)  LR: 1.504e-05  Data: 0.000 (2.046)
Test: [   0/48]  Time: 15.220 (15.220)  Loss:  1.4137 (1.4137)  Acc@1: 71.9727 (71.9727)  Acc@5: 89.3555 (89.3555)
Test: [  48/48]  Time: 0.148 (3.764)  Loss:  1.3582 (2.3814)  Acc@1: 72.2877 (49.5600)  Acc@5: 88.5613 (74.2700)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-42.pth.tar', 49.55999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-41.pth.tar', 49.454000036621096)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-40.pth.tar', 49.34600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-39.pth.tar', 49.09600003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-38.pth.tar', 48.98999985351563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-37.pth.tar', 48.6140000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-36.pth.tar', 48.2120000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-35.pth.tar', 47.29200004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-34.pth.tar', 46.88400006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-33.pth.tar', 46.40399991210938)

Train: 43 [   0/1251 (  0%)]  Loss:  5.266869 (5.2669)  Time: 12.265s,   83.49/s  (12.265s,   83.49/s)  LR: 1.126e-05  Data: 11.010 (11.010)
Train: 43 [  50/1251 (  4%)]  Loss:  5.141057 (5.2040)  Time: 0.586s, 1746.88/s  (2.748s,  372.58/s)  LR: 1.126e-05  Data: 0.019 (2.135)
Train: 43 [ 100/1251 (  8%)]  Loss:  5.093926 (5.1673)  Time: 0.591s, 1731.72/s  (2.655s,  385.75/s)  LR: 1.126e-05  Data: 0.021 (2.049)
Train: 43 [ 150/1251 ( 12%)]  Loss:  4.990164 (5.1230)  Time: 0.590s, 1735.55/s  (2.583s,  396.41/s)  LR: 1.126e-05  Data: 0.020 (1.973)
Train: 43 [ 200/1251 ( 16%)]  Loss:  4.559729 (5.0103)  Time: 0.585s, 1751.47/s  (2.656s,  385.61/s)  LR: 1.126e-05  Data: 0.020 (2.051)
Train: 43 [ 250/1251 ( 20%)]  Loss:  4.939995 (4.9986)  Time: 1.727s,  593.02/s  (2.652s,  386.16/s)  LR: 1.126e-05  Data: 1.075 (2.051)
Train: 43 [ 300/1251 ( 24%)]  Loss:  4.866199 (4.9797)  Time: 0.590s, 1734.86/s  (2.703s,  378.86/s)  LR: 1.126e-05  Data: 0.021 (2.099)
Train: 43 [ 350/1251 ( 28%)]  Loss:  4.807668 (4.9582)  Time: 0.589s, 1739.57/s  (2.716s,  376.99/s)  LR: 1.126e-05  Data: 0.021 (2.112)
Train: 43 [ 400/1251 ( 32%)]  Loss:  5.115307 (4.9757)  Time: 0.589s, 1739.29/s  (2.732s,  374.88/s)  LR: 1.126e-05  Data: 0.020 (2.130)
Train: 43 [ 450/1251 ( 36%)]  Loss:  5.238589 (5.0020)  Time: 3.108s,  329.47/s  (2.727s,  375.53/s)  LR: 1.126e-05  Data: 2.519 (2.123)
Train: 43 [ 500/1251 ( 40%)]  Loss:  4.645356 (4.9695)  Time: 0.590s, 1735.37/s  (2.720s,  376.52/s)  LR: 1.126e-05  Data: 0.019 (2.116)
Train: 43 [ 550/1251 ( 44%)]  Loss:  5.024979 (4.9742)  Time: 4.209s,  243.28/s  (2.769s,  369.80/s)  LR: 1.126e-05  Data: 3.523 (2.164)
Train: 43 [ 600/1251 ( 48%)]  Loss:  5.394801 (5.0065)  Time: 0.589s, 1739.71/s  (2.782s,  368.04/s)  LR: 1.126e-05  Data: 0.019 (2.178)
Train: 43 [ 650/1251 ( 52%)]  Loss:  4.643691 (4.9806)  Time: 3.426s,  298.90/s  (2.781s,  368.18/s)  LR: 1.126e-05  Data: 2.655 (2.176)
Train: 43 [ 700/1251 ( 56%)]  Loss:  4.641833 (4.9580)  Time: 4.041s,  253.43/s  (2.780s,  368.39/s)  LR: 1.126e-05  Data: 3.477 (2.175)
Train: 43 [ 750/1251 ( 60%)]  Loss:  4.941257 (4.9570)  Time: 1.407s,  728.03/s  (2.766s,  370.24/s)  LR: 1.126e-05  Data: 0.843 (2.159)
Train: 43 [ 800/1251 ( 64%)]  Loss:  5.172941 (4.9697)  Time: 3.753s,  272.87/s  (2.754s,  371.80/s)  LR: 1.126e-05  Data: 3.173 (2.149)
Train: 43 [ 850/1251 ( 68%)]  Loss:  4.401996 (4.9381)  Time: 4.565s,  224.33/s  (2.759s,  371.12/s)  LR: 1.126e-05  Data: 3.974 (2.153)
Train: 43 [ 900/1251 ( 72%)]  Loss:  4.241702 (4.9015)  Time: 0.585s, 1750.80/s  (2.757s,  371.45/s)  LR: 1.126e-05  Data: 0.019 (2.152)
Train: 43 [ 950/1251 ( 76%)]  Loss:  4.728058 (4.8928)  Time: 8.813s,  116.19/s  (2.755s,  371.71/s)  LR: 1.126e-05  Data: 8.130 (2.151)
Train: 43 [1000/1251 ( 80%)]  Loss:  5.069192 (4.9012)  Time: 0.587s, 1743.55/s  (2.745s,  373.02/s)  LR: 1.126e-05  Data: 0.020 (2.142)
Train: 43 [1050/1251 ( 84%)]  Loss:  5.168129 (4.9133)  Time: 8.692s,  117.81/s  (2.747s,  372.82/s)  LR: 1.126e-05  Data: 8.129 (2.144)
Train: 43 [1100/1251 ( 88%)]  Loss:  4.975592 (4.9160)  Time: 0.586s, 1746.40/s  (2.732s,  374.81/s)  LR: 1.126e-05  Data: 0.021 (2.130)
Train: 43 [1150/1251 ( 92%)]  Loss:  5.146730 (4.9257)  Time: 8.458s,  121.07/s  (2.724s,  375.89/s)  LR: 1.126e-05  Data: 7.891 (2.123)
Train: 43 [1200/1251 ( 96%)]  Loss:  5.138765 (4.9342)  Time: 0.588s, 1741.48/s  (2.726s,  375.69/s)  LR: 1.126e-05  Data: 0.024 (2.125)
Train: 43 [1250/1251 (100%)]  Loss:  5.000694 (4.9367)  Time: 0.564s, 1815.10/s  (2.723s,  376.11/s)  LR: 1.126e-05  Data: 0.000 (2.122)
Test: [   0/48]  Time: 17.139 (17.139)  Loss:  1.4092 (1.4092)  Acc@1: 71.8750 (71.8750)  Acc@5: 89.1602 (89.1602)
Test: [  48/48]  Time: 0.148 (3.782)  Loss:  1.3503 (2.3749)  Acc@1: 72.8774 (49.7240)  Acc@5: 88.4434 (74.4120)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-43.pth.tar', 49.72399998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-42.pth.tar', 49.55999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-41.pth.tar', 49.454000036621096)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-40.pth.tar', 49.34600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-39.pth.tar', 49.09600003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-38.pth.tar', 48.98999985351563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-37.pth.tar', 48.6140000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-36.pth.tar', 48.2120000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-35.pth.tar', 47.29200004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-34.pth.tar', 46.88400006835938)

*** Best metric: 49.72399998291016 (epoch 43)

wandb: Waiting for W&B process to finish, PID 37084
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210524_145227-PreTraining_vit_deit_tiny_patch16_224_1k/logs/debug.log
wandb: Find internal logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210524_145227-PreTraining_vit_deit_tiny_patch16_224_1k/logs/debug-internal.log
wandb: Run summary:
wandb:    eval_top1 49.724
wandb:    eval_top5 74.412
wandb:   _timestamp 1621911119
wandb:   train_loss 4.93674
wandb:        _step 43
wandb:        epoch 43
wandb:     _runtime 148242
wandb:    eval_loss 2.37493
wandb: Run history:
wandb:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   train_loss ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ
wandb:    eval_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    eval_top1 ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:    eval_top5 ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced PreTraining_vit_deit_tiny_patch16_224_1k: https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_1k
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
--End--
Tue May 25 11:52:10 JST 2021

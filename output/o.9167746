--Start--
Sat May 29 12:13:33 JST 2021
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 0
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
wandb: Currently logged in as: gyama_x (use `wandb login --relogin` to force relogin)
wandb: WARNING Tried to auto resume run with id PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1 but id PreTraining_vit_deit_tiny_patch16_224_1k is set.
wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
wandb: Tracking run with wandb version 0.10.27
wandb: Resuming run PreTraining_vit_deit_tiny_patch16_224_1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gyama_x/pytorch-image-models
wandb: üöÄ View run at https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_1k
wandb: Run data is saved locally in /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210529_121414-PreTraining_vit_deit_tiny_patch16_224_1k
wandb: Run `wandb offline` to turn off syncing.

Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar' (epoch 99)
Using native Torch DistributedDataParallel.
Scheduled epochs: 122
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Train: 100 [   0/1251 (  0%)]  Loss:  4.711967 (4.7120)  Time: 13.649s,   75.02/s  (13.649s,   75.02/s)  LR: 8.733e-05  Data: 11.652 (11.652)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 100 [  50/1251 (  4%)]  Loss:  4.858493 (4.7852)  Time: 0.583s, 1755.11/s  (1.983s,  516.38/s)  LR: 8.733e-05  Data: 0.021 (1.373)
Train: 100 [ 100/1251 (  8%)]  Loss:  4.907115 (4.8259)  Time: 1.416s,  723.03/s  (1.972s,  519.15/s)  LR: 8.733e-05  Data: 0.832 (1.365)
Train: 100 [ 150/1251 ( 12%)]  Loss:  4.393381 (4.7177)  Time: 0.584s, 1752.34/s  (1.925s,  531.87/s)  LR: 8.733e-05  Data: 0.021 (1.325)
Train: 100 [ 200/1251 ( 16%)]  Loss:  4.581526 (4.6905)  Time: 0.583s, 1755.21/s  (1.922s,  532.91/s)  LR: 8.733e-05  Data: 0.019 (1.325)
Train: 100 [ 250/1251 ( 20%)]  Loss:  4.188045 (4.6068)  Time: 0.589s, 1739.89/s  (1.890s,  541.74/s)  LR: 8.733e-05  Data: 0.021 (1.294)
Train: 100 [ 300/1251 ( 24%)]  Loss:  4.729129 (4.6242)  Time: 0.594s, 1723.63/s  (1.886s,  542.81/s)  LR: 8.733e-05  Data: 0.028 (1.293)
Train: 100 [ 350/1251 ( 28%)]  Loss:  4.429640 (4.5999)  Time: 0.588s, 1742.03/s  (1.868s,  548.28/s)  LR: 8.733e-05  Data: 0.023 (1.276)
Train: 100 [ 400/1251 ( 32%)]  Loss:  4.864542 (4.6293)  Time: 0.588s, 1741.53/s  (1.860s,  550.39/s)  LR: 8.733e-05  Data: 0.021 (1.265)
Train: 100 [ 450/1251 ( 36%)]  Loss:  4.543449 (4.6207)  Time: 0.584s, 1753.35/s  (1.847s,  554.29/s)  LR: 8.733e-05  Data: 0.020 (1.251)
Train: 100 [ 500/1251 ( 40%)]  Loss:  4.394417 (4.6002)  Time: 0.587s, 1744.98/s  (1.868s,  548.21/s)  LR: 8.733e-05  Data: 0.019 (1.272)
Train: 100 [ 550/1251 ( 44%)]  Loss:  4.606894 (4.6007)  Time: 1.537s,  666.14/s  (1.861s,  550.24/s)  LR: 8.733e-05  Data: 0.859 (1.264)
Train: 100 [ 600/1251 ( 48%)]  Loss:  5.210984 (4.6477)  Time: 0.591s, 1733.34/s  (1.860s,  550.63/s)  LR: 8.733e-05  Data: 0.020 (1.262)
Train: 100 [ 650/1251 ( 52%)]  Loss:  4.485343 (4.6361)  Time: 0.584s, 1752.13/s  (1.856s,  551.64/s)  LR: 8.733e-05  Data: 0.021 (1.258)
Train: 100 [ 700/1251 ( 56%)]  Loss:  4.680641 (4.6390)  Time: 0.583s, 1757.79/s  (1.859s,  550.73/s)  LR: 8.733e-05  Data: 0.019 (1.261)
Train: 100 [ 750/1251 ( 60%)]  Loss:  5.050043 (4.6647)  Time: 1.222s,  838.26/s  (1.858s,  551.24/s)  LR: 8.733e-05  Data: 0.631 (1.259)
Train: 100 [ 800/1251 ( 64%)]  Loss:  4.466614 (4.6531)  Time: 0.585s, 1750.07/s  (1.870s,  547.52/s)  LR: 8.733e-05  Data: 0.021 (1.272)
Train: 100 [ 850/1251 ( 68%)]  Loss:  4.711984 (4.6563)  Time: 0.683s, 1498.96/s  (1.875s,  545.99/s)  LR: 8.733e-05  Data: 0.083 (1.279)
Train: 100 [ 900/1251 ( 72%)]  Loss:  5.132802 (4.6814)  Time: 0.586s, 1748.41/s  (1.883s,  543.85/s)  LR: 8.733e-05  Data: 0.019 (1.285)
Train: 100 [ 950/1251 ( 76%)]  Loss:  4.442468 (4.6695)  Time: 0.668s, 1531.93/s  (1.895s,  540.43/s)  LR: 8.733e-05  Data: 0.088 (1.297)
Train: 100 [1000/1251 ( 80%)]  Loss:  4.161529 (4.6453)  Time: 0.587s, 1743.12/s  (1.908s,  536.79/s)  LR: 8.733e-05  Data: 0.019 (1.310)
Train: 100 [1050/1251 ( 84%)]  Loss:  4.587264 (4.6426)  Time: 0.583s, 1756.20/s  (1.913s,  535.38/s)  LR: 8.733e-05  Data: 0.018 (1.316)
Train: 100 [1100/1251 ( 88%)]  Loss:  4.780001 (4.6486)  Time: 0.584s, 1752.64/s  (1.922s,  532.91/s)  LR: 8.733e-05  Data: 0.021 (1.325)
Train: 100 [1150/1251 ( 92%)]  Loss:  4.268295 (4.6328)  Time: 0.585s, 1750.90/s  (1.926s,  531.66/s)  LR: 8.733e-05  Data: 0.023 (1.329)
Train: 100 [1200/1251 ( 96%)]  Loss:  4.247864 (4.6174)  Time: 0.586s, 1747.15/s  (1.931s,  530.34/s)  LR: 8.733e-05  Data: 0.021 (1.335)
Train: 100 [1250/1251 (100%)]  Loss:  4.628304 (4.6178)  Time: 0.566s, 1809.67/s  (1.930s,  530.64/s)  LR: 8.733e-05  Data: 0.000 (1.333)
Test: [   0/48]  Time: 12.487 (12.487)  Loss:  1.2046 (1.2046)  Acc@1: 75.0977 (75.0977)  Acc@5: 91.5039 (91.5039)
Test: [  48/48]  Time: 0.447 (2.857)  Loss:  1.1390 (2.0869)  Acc@1: 75.5896 (54.5380)  Acc@5: 90.2123 (78.8640)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-100.pth.tar', 54.537999868164064)

Train: 101 [   0/1251 (  0%)]  Loss:  4.668137 (4.6681)  Time: 8.464s,  120.99/s  (8.464s,  120.99/s)  LR: 8.063e-05  Data: 7.788 (7.788)
Train: 101 [  50/1251 (  4%)]  Loss:  4.464754 (4.5664)  Time: 0.584s, 1754.36/s  (1.952s,  524.54/s)  LR: 8.063e-05  Data: 0.019 (1.375)
Train: 101 [ 100/1251 (  8%)]  Loss:  4.957160 (4.6967)  Time: 0.586s, 1746.58/s  (2.097s,  488.31/s)  LR: 8.063e-05  Data: 0.023 (1.513)
Train: 101 [ 150/1251 ( 12%)]  Loss:  4.412388 (4.6256)  Time: 0.588s, 1742.12/s  (2.083s,  491.58/s)  LR: 8.063e-05  Data: 0.021 (1.496)
Train: 101 [ 200/1251 ( 16%)]  Loss:  4.655652 (4.6316)  Time: 2.048s,  500.06/s  (2.090s,  490.05/s)  LR: 8.063e-05  Data: 1.444 (1.500)
Train: 101 [ 250/1251 ( 20%)]  Loss:  4.125831 (4.5473)  Time: 0.589s, 1739.56/s  (2.066s,  495.64/s)  LR: 8.063e-05  Data: 0.020 (1.471)
Train: 101 [ 300/1251 ( 24%)]  Loss:  5.274676 (4.6512)  Time: 2.086s,  490.97/s  (2.065s,  495.81/s)  LR: 8.063e-05  Data: 1.513 (1.469)
Train: 101 [ 350/1251 ( 28%)]  Loss:  4.388463 (4.6184)  Time: 0.584s, 1754.56/s  (2.043s,  501.10/s)  LR: 8.063e-05  Data: 0.020 (1.448)
Train: 101 [ 400/1251 ( 32%)]  Loss:  4.642755 (4.6211)  Time: 1.815s,  564.34/s  (2.032s,  503.93/s)  LR: 8.063e-05  Data: 1.175 (1.433)
Train: 101 [ 450/1251 ( 36%)]  Loss:  5.007896 (4.6598)  Time: 0.583s, 1755.92/s  (2.020s,  507.00/s)  LR: 8.063e-05  Data: 0.019 (1.421)
Train: 101 [ 500/1251 ( 40%)]  Loss:  4.171047 (4.6153)  Time: 0.587s, 1745.15/s  (2.006s,  510.41/s)  LR: 8.063e-05  Data: 0.022 (1.407)
Train: 101 [ 550/1251 ( 44%)]  Loss:  4.931773 (4.6417)  Time: 0.584s, 1753.20/s  (2.033s,  503.67/s)  LR: 8.063e-05  Data: 0.019 (1.433)
Train: 101 [ 600/1251 ( 48%)]  Loss:  4.959898 (4.6662)  Time: 1.324s,  773.31/s  (2.047s,  500.35/s)  LR: 8.063e-05  Data: 0.735 (1.445)
Train: 101 [ 650/1251 ( 52%)]  Loss:  4.891195 (4.6823)  Time: 0.584s, 1753.83/s  (2.058s,  497.61/s)  LR: 8.063e-05  Data: 0.019 (1.457)
Train: 101 [ 700/1251 ( 56%)]  Loss:  4.504217 (4.6704)  Time: 0.588s, 1741.54/s  (2.058s,  497.53/s)  LR: 8.063e-05  Data: 0.025 (1.457)
Train: 101 [ 750/1251 ( 60%)]  Loss:  4.872656 (4.6830)  Time: 0.582s, 1757.97/s  (2.057s,  497.77/s)  LR: 8.063e-05  Data: 0.018 (1.456)
Train: 101 [ 800/1251 ( 64%)]  Loss:  5.128520 (4.7092)  Time: 0.587s, 1743.71/s  (2.051s,  499.24/s)  LR: 8.063e-05  Data: 0.023 (1.450)
Train: 101 [ 850/1251 ( 68%)]  Loss:  4.737027 (4.7108)  Time: 0.590s, 1736.96/s  (2.048s,  500.02/s)  LR: 8.063e-05  Data: 0.023 (1.447)
Train: 101 [ 900/1251 ( 72%)]  Loss:  4.793490 (4.7151)  Time: 1.032s,  992.53/s  (2.039s,  502.17/s)  LR: 8.063e-05  Data: 0.355 (1.439)
Train: 101 [ 950/1251 ( 76%)]  Loss:  4.522413 (4.7055)  Time: 0.588s, 1742.63/s  (2.045s,  500.72/s)  LR: 8.063e-05  Data: 0.023 (1.444)
Train: 101 [1000/1251 ( 80%)]  Loss:  4.760224 (4.7081)  Time: 1.711s,  598.61/s  (2.046s,  500.50/s)  LR: 8.063e-05  Data: 1.011 (1.445)
Train: 101 [1050/1251 ( 84%)]  Loss:  4.934375 (4.7184)  Time: 0.587s, 1744.20/s  (2.051s,  499.28/s)  LR: 8.063e-05  Data: 0.024 (1.450)
Train: 101 [1100/1251 ( 88%)]  Loss:  4.985868 (4.7300)  Time: 2.821s,  363.00/s  (2.052s,  499.03/s)  LR: 8.063e-05  Data: 2.259 (1.451)
Train: 101 [1150/1251 ( 92%)]  Loss:  5.237526 (4.7512)  Time: 0.587s, 1743.93/s  (2.057s,  497.93/s)  LR: 8.063e-05  Data: 0.024 (1.456)
Train: 101 [1200/1251 ( 96%)]  Loss:  4.742744 (4.7508)  Time: 0.585s, 1750.40/s  (2.055s,  498.35/s)  LR: 8.063e-05  Data: 0.019 (1.455)
Train: 101 [1250/1251 (100%)]  Loss:  4.903498 (4.7567)  Time: 0.563s, 1818.09/s  (2.054s,  498.65/s)  LR: 8.063e-05  Data: 0.000 (1.455)
Test: [   0/48]  Time: 11.798 (11.798)  Loss:  1.1603 (1.1603)  Acc@1: 76.1719 (76.1719)  Acc@5: 91.9922 (91.9922)
Test: [  48/48]  Time: 0.149 (2.895)  Loss:  1.1505 (2.0725)  Acc@1: 76.4151 (54.8180)  Acc@5: 90.3302 (78.9940)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-101.pth.tar', 54.81800009765625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-100.pth.tar', 54.537999868164064)

Train: 102 [   0/1251 (  0%)]  Loss:  4.890142 (4.8901)  Time: 9.237s,  110.86/s  (9.237s,  110.86/s)  LR: 7.421e-05  Data: 8.664 (8.664)
Train: 102 [  50/1251 (  4%)]  Loss:  4.241219 (4.5657)  Time: 0.583s, 1755.90/s  (2.004s,  510.98/s)  LR: 7.421e-05  Data: 0.019 (1.422)
Train: 102 [ 100/1251 (  8%)]  Loss:  4.264666 (4.4653)  Time: 0.584s, 1753.91/s  (2.130s,  480.78/s)  LR: 7.421e-05  Data: 0.019 (1.544)
Train: 102 [ 150/1251 ( 12%)]  Loss:  5.060841 (4.6142)  Time: 0.588s, 1740.63/s  (2.093s,  489.26/s)  LR: 7.421e-05  Data: 0.019 (1.507)
Train: 102 [ 200/1251 ( 16%)]  Loss:  4.831424 (4.6577)  Time: 0.586s, 1747.46/s  (2.105s,  486.54/s)  LR: 7.421e-05  Data: 0.023 (1.519)
Train: 102 [ 250/1251 ( 20%)]  Loss:  4.973047 (4.7102)  Time: 0.581s, 1761.84/s  (2.086s,  490.98/s)  LR: 7.421e-05  Data: 0.018 (1.501)
Train: 102 [ 300/1251 ( 24%)]  Loss:  4.808258 (4.7242)  Time: 0.583s, 1757.10/s  (2.077s,  493.04/s)  LR: 7.421e-05  Data: 0.018 (1.491)
Train: 102 [ 350/1251 ( 28%)]  Loss:  4.634103 (4.7130)  Time: 0.582s, 1758.88/s  (2.050s,  499.43/s)  LR: 7.421e-05  Data: 0.019 (1.465)
Train: 102 [ 400/1251 ( 32%)]  Loss:  4.686822 (4.7101)  Time: 0.583s, 1755.45/s  (2.045s,  500.63/s)  LR: 7.421e-05  Data: 0.018 (1.461)
Train: 102 [ 450/1251 ( 36%)]  Loss:  4.932199 (4.7323)  Time: 0.581s, 1762.19/s  (2.021s,  506.63/s)  LR: 7.421e-05  Data: 0.018 (1.437)
Train: 102 [ 500/1251 ( 40%)]  Loss:  4.499549 (4.7111)  Time: 0.590s, 1735.61/s  (2.023s,  506.29/s)  LR: 7.421e-05  Data: 0.021 (1.438)
Train: 102 [ 550/1251 ( 44%)]  Loss:  4.252281 (4.6729)  Time: 0.582s, 1760.37/s  (2.034s,  503.35/s)  LR: 7.421e-05  Data: 0.018 (1.450)
Train: 102 [ 600/1251 ( 48%)]  Loss:  4.303074 (4.6444)  Time: 0.594s, 1723.44/s  (2.056s,  498.16/s)  LR: 7.421e-05  Data: 0.018 (1.471)
Train: 102 [ 650/1251 ( 52%)]  Loss:  4.104383 (4.6059)  Time: 0.581s, 1761.10/s  (2.059s,  497.30/s)  LR: 7.421e-05  Data: 0.018 (1.475)
Train: 102 [ 700/1251 ( 56%)]  Loss:  4.399717 (4.5921)  Time: 0.583s, 1757.30/s  (2.068s,  495.17/s)  LR: 7.421e-05  Data: 0.019 (1.483)
Train: 102 [ 750/1251 ( 60%)]  Loss:  5.070438 (4.6220)  Time: 0.583s, 1754.99/s  (2.062s,  496.70/s)  LR: 7.421e-05  Data: 0.020 (1.477)
Train: 102 [ 800/1251 ( 64%)]  Loss:  4.498754 (4.6148)  Time: 0.591s, 1731.44/s  (2.063s,  496.30/s)  LR: 7.421e-05  Data: 0.022 (1.479)
Train: 102 [ 850/1251 ( 68%)]  Loss:  3.760158 (4.5673)  Time: 0.586s, 1747.94/s  (2.054s,  498.60/s)  LR: 7.421e-05  Data: 0.024 (1.469)
Train: 102 [ 900/1251 ( 72%)]  Loss:  4.789684 (4.5790)  Time: 0.585s, 1751.52/s  (2.052s,  498.94/s)  LR: 7.421e-05  Data: 0.021 (1.468)
Train: 102 [ 950/1251 ( 76%)]  Loss:  4.924818 (4.5963)  Time: 0.587s, 1744.37/s  (2.058s,  497.62/s)  LR: 7.421e-05  Data: 0.023 (1.474)
Train: 102 [1000/1251 ( 80%)]  Loss:  4.612903 (4.5971)  Time: 0.584s, 1754.56/s  (2.066s,  495.55/s)  LR: 7.421e-05  Data: 0.019 (1.482)
Train: 102 [1050/1251 ( 84%)]  Loss:  4.589115 (4.5967)  Time: 0.585s, 1749.93/s  (2.064s,  496.18/s)  LR: 7.421e-05  Data: 0.022 (1.479)
Train: 102 [1100/1251 ( 88%)]  Loss:  4.850556 (4.6077)  Time: 0.585s, 1749.29/s  (2.068s,  495.20/s)  LR: 7.421e-05  Data: 0.021 (1.483)
Train: 102 [1150/1251 ( 92%)]  Loss:  4.358200 (4.5973)  Time: 0.586s, 1747.79/s  (2.067s,  495.36/s)  LR: 7.421e-05  Data: 0.023 (1.482)
Train: 102 [1200/1251 ( 96%)]  Loss:  4.633062 (4.5988)  Time: 0.584s, 1752.44/s  (2.071s,  494.54/s)  LR: 7.421e-05  Data: 0.020 (1.486)
Train: 102 [1250/1251 (100%)]  Loss:  4.671136 (4.6016)  Time: 0.564s, 1815.09/s  (2.066s,  495.71/s)  LR: 7.421e-05  Data: 0.000 (1.481)
Test: [   0/48]  Time: 11.541 (11.541)  Loss:  1.1624 (1.1624)  Acc@1: 75.6836 (75.6836)  Acc@5: 91.5039 (91.5039)
Test: [  48/48]  Time: 0.149 (2.875)  Loss:  1.1698 (2.0724)  Acc@1: 75.5896 (55.0160)  Acc@5: 89.7406 (78.9800)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-102.pth.tar', 55.015999997558595)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-101.pth.tar', 54.81800009765625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-100.pth.tar', 54.537999868164064)

Train: 103 [   0/1251 (  0%)]  Loss:  4.609539 (4.6095)  Time: 9.235s,  110.88/s  (9.235s,  110.88/s)  LR: 6.807e-05  Data: 8.285 (8.285)
Train: 103 [  50/1251 (  4%)]  Loss:  4.360876 (4.4852)  Time: 0.583s, 1756.61/s  (2.237s,  457.68/s)  LR: 6.807e-05  Data: 0.019 (1.648)
Train: 103 [ 100/1251 (  8%)]  Loss:  4.832248 (4.6009)  Time: 0.587s, 1744.43/s  (2.235s,  458.09/s)  LR: 6.807e-05  Data: 0.020 (1.645)
Train: 103 [ 150/1251 ( 12%)]  Loss:  4.741917 (4.6361)  Time: 0.582s, 1759.75/s  (2.195s,  466.58/s)  LR: 6.807e-05  Data: 0.018 (1.605)
Train: 103 [ 200/1251 ( 16%)]  Loss:  4.496994 (4.6083)  Time: 0.586s, 1746.70/s  (2.179s,  470.02/s)  LR: 6.807e-05  Data: 0.021 (1.589)
Train: 103 [ 250/1251 ( 20%)]  Loss:  4.982091 (4.6706)  Time: 0.586s, 1746.10/s  (2.144s,  477.57/s)  LR: 6.807e-05  Data: 0.019 (1.552)
Train: 103 [ 300/1251 ( 24%)]  Loss:  4.924318 (4.7069)  Time: 0.586s, 1746.24/s  (2.122s,  482.66/s)  LR: 6.807e-05  Data: 0.021 (1.530)
Train: 103 [ 350/1251 ( 28%)]  Loss:  4.880021 (4.7285)  Time: 0.588s, 1740.60/s  (2.091s,  489.79/s)  LR: 6.807e-05  Data: 0.021 (1.499)
Train: 103 [ 400/1251 ( 32%)]  Loss:  4.506388 (4.7038)  Time: 0.587s, 1744.89/s  (2.071s,  494.35/s)  LR: 6.807e-05  Data: 0.021 (1.481)
Train: 103 [ 450/1251 ( 36%)]  Loss:  4.035184 (4.6370)  Time: 1.481s,  691.58/s  (2.049s,  499.76/s)  LR: 6.807e-05  Data: 0.919 (1.459)
Train: 103 [ 500/1251 ( 40%)]  Loss:  5.254223 (4.6931)  Time: 0.586s, 1746.00/s  (2.073s,  494.06/s)  LR: 6.807e-05  Data: 0.021 (1.482)
Train: 103 [ 550/1251 ( 44%)]  Loss:  4.792569 (4.7014)  Time: 3.916s,  261.46/s  (2.077s,  493.12/s)  LR: 6.807e-05  Data: 3.354 (1.486)
Train: 103 [ 600/1251 ( 48%)]  Loss:  4.958724 (4.7212)  Time: 0.586s, 1747.15/s  (2.077s,  493.06/s)  LR: 6.807e-05  Data: 0.020 (1.485)
Train: 103 [ 650/1251 ( 52%)]  Loss:  4.961120 (4.7383)  Time: 6.564s,  156.01/s  (2.082s,  491.85/s)  LR: 6.807e-05  Data: 5.862 (1.491)
Train: 103 [ 700/1251 ( 56%)]  Loss:  4.990020 (4.7551)  Time: 0.586s, 1748.60/s  (2.074s,  493.74/s)  LR: 6.807e-05  Data: 0.021 (1.484)
Train: 103 [ 750/1251 ( 60%)]  Loss:  4.893717 (4.7637)  Time: 5.148s,  198.93/s  (2.071s,  494.51/s)  LR: 6.807e-05  Data: 4.571 (1.479)
Train: 103 [ 800/1251 ( 64%)]  Loss:  4.771829 (4.7642)  Time: 0.588s, 1742.69/s  (2.059s,  497.28/s)  LR: 6.807e-05  Data: 0.024 (1.467)
Train: 103 [ 850/1251 ( 68%)]  Loss:  4.311429 (4.7391)  Time: 4.302s,  238.00/s  (2.051s,  499.21/s)  LR: 6.807e-05  Data: 3.718 (1.459)
Train: 103 [ 900/1251 ( 72%)]  Loss:  4.138837 (4.7075)  Time: 0.587s, 1744.91/s  (2.046s,  500.60/s)  LR: 6.807e-05  Data: 0.020 (1.453)
Train: 103 [ 950/1251 ( 76%)]  Loss:  4.587655 (4.7015)  Time: 3.101s,  330.23/s  (2.057s,  497.81/s)  LR: 6.807e-05  Data: 2.538 (1.465)
Train: 103 [1000/1251 ( 80%)]  Loss:  4.280497 (4.6814)  Time: 0.583s, 1757.42/s  (2.061s,  496.74/s)  LR: 6.807e-05  Data: 0.018 (1.469)
Train: 103 [1050/1251 ( 84%)]  Loss:  5.029011 (4.6972)  Time: 6.068s,  168.75/s  (2.065s,  495.79/s)  LR: 6.807e-05  Data: 5.188 (1.472)
Train: 103 [1100/1251 ( 88%)]  Loss:  4.749263 (4.6995)  Time: 0.586s, 1748.75/s  (2.065s,  495.93/s)  LR: 6.807e-05  Data: 0.021 (1.471)
Train: 103 [1150/1251 ( 92%)]  Loss:  4.800092 (4.7037)  Time: 6.805s,  150.49/s  (2.071s,  494.54/s)  LR: 6.807e-05  Data: 6.223 (1.477)
Train: 103 [1200/1251 ( 96%)]  Loss:  4.220132 (4.6843)  Time: 0.583s, 1756.75/s  (2.069s,  495.00/s)  LR: 6.807e-05  Data: 0.020 (1.475)
Train: 103 [1250/1251 (100%)]  Loss:  4.514137 (4.6778)  Time: 0.565s, 1813.57/s  (2.064s,  496.16/s)  LR: 6.807e-05  Data: 0.000 (1.471)
Test: [   0/48]  Time: 11.634 (11.634)  Loss:  1.1724 (1.1724)  Acc@1: 75.9766 (75.9766)  Acc@5: 91.6016 (91.6016)
Test: [  48/48]  Time: 0.149 (2.856)  Loss:  1.1774 (2.0666)  Acc@1: 76.0613 (55.0400)  Acc@5: 90.0943 (79.1220)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-103.pth.tar', 55.04000007324219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-102.pth.tar', 55.015999997558595)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-101.pth.tar', 54.81800009765625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-100.pth.tar', 54.537999868164064)

Train: 104 [   0/1251 (  0%)]  Loss:  4.470989 (4.4710)  Time: 9.331s,  109.74/s  (9.331s,  109.74/s)  LR: 6.223e-05  Data: 8.526 (8.526)
Train: 104 [  50/1251 (  4%)]  Loss:  4.213431 (4.3422)  Time: 0.588s, 1740.56/s  (2.238s,  457.62/s)  LR: 6.223e-05  Data: 0.020 (1.630)
Train: 104 [ 100/1251 (  8%)]  Loss:  4.888739 (4.5244)  Time: 1.077s,  950.52/s  (2.208s,  463.83/s)  LR: 6.223e-05  Data: 0.376 (1.603)
Train: 104 [ 150/1251 ( 12%)]  Loss:  3.848278 (4.3554)  Time: 0.586s, 1747.36/s  (2.145s,  477.34/s)  LR: 6.223e-05  Data: 0.021 (1.547)
Train: 104 [ 200/1251 ( 16%)]  Loss:  5.139817 (4.5123)  Time: 0.584s, 1754.32/s  (2.153s,  475.62/s)  LR: 6.223e-05  Data: 0.021 (1.559)
Train: 104 [ 250/1251 ( 20%)]  Loss:  4.951001 (4.5854)  Time: 0.585s, 1750.57/s  (2.122s,  482.66/s)  LR: 6.223e-05  Data: 0.021 (1.530)
Train: 104 [ 300/1251 ( 24%)]  Loss:  4.723122 (4.6051)  Time: 0.583s, 1756.75/s  (2.104s,  486.64/s)  LR: 6.223e-05  Data: 0.020 (1.511)
Train: 104 [ 350/1251 ( 28%)]  Loss:  4.307280 (4.5678)  Time: 1.912s,  535.69/s  (2.083s,  491.58/s)  LR: 6.223e-05  Data: 1.331 (1.489)
Train: 104 [ 400/1251 ( 32%)]  Loss:  5.292273 (4.6483)  Time: 0.584s, 1752.31/s  (2.062s,  496.65/s)  LR: 6.223e-05  Data: 0.022 (1.468)
Train: 104 [ 450/1251 ( 36%)]  Loss:  4.294100 (4.6129)  Time: 3.664s,  279.48/s  (2.047s,  500.20/s)  LR: 6.223e-05  Data: 2.999 (1.453)
Train: 104 [ 500/1251 ( 40%)]  Loss:  4.392159 (4.5928)  Time: 2.176s,  470.65/s  (2.066s,  495.54/s)  LR: 6.223e-05  Data: 1.493 (1.471)
Train: 104 [ 550/1251 ( 44%)]  Loss:  4.273914 (4.5663)  Time: 3.776s,  271.21/s  (2.073s,  494.01/s)  LR: 6.223e-05  Data: 3.198 (1.476)
Train: 104 [ 600/1251 ( 48%)]  Loss:  4.898842 (4.5918)  Time: 2.618s,  391.16/s  (2.077s,  493.09/s)  LR: 6.223e-05  Data: 2.056 (1.477)
Train: 104 [ 650/1251 ( 52%)]  Loss:  4.365816 (4.5757)  Time: 2.717s,  376.87/s  (2.075s,  493.46/s)  LR: 6.223e-05  Data: 2.155 (1.475)
Train: 104 [ 700/1251 ( 56%)]  Loss:  4.241260 (4.5534)  Time: 1.105s,  926.83/s  (2.075s,  493.43/s)  LR: 6.223e-05  Data: 0.501 (1.473)
Train: 104 [ 750/1251 ( 60%)]  Loss:  4.521885 (4.5514)  Time: 2.040s,  501.89/s  (2.068s,  495.11/s)  LR: 6.223e-05  Data: 1.443 (1.467)
Train: 104 [ 800/1251 ( 64%)]  Loss:  4.522338 (4.5497)  Time: 0.591s, 1732.13/s  (2.064s,  496.02/s)  LR: 6.223e-05  Data: 0.023 (1.463)
Train: 104 [ 850/1251 ( 68%)]  Loss:  4.473578 (4.5455)  Time: 1.471s,  696.14/s  (2.056s,  498.00/s)  LR: 6.223e-05  Data: 0.808 (1.455)
Train: 104 [ 900/1251 ( 72%)]  Loss:  4.267998 (4.5309)  Time: 0.581s, 1761.41/s  (2.051s,  499.19/s)  LR: 6.223e-05  Data: 0.018 (1.451)
Train: 104 [ 950/1251 ( 76%)]  Loss:  4.857694 (4.5472)  Time: 2.516s,  407.00/s  (2.061s,  496.74/s)  LR: 6.223e-05  Data: 1.941 (1.461)
Train: 104 [1000/1251 ( 80%)]  Loss:  4.758729 (4.5573)  Time: 0.585s, 1750.41/s  (2.069s,  494.88/s)  LR: 6.223e-05  Data: 0.021 (1.469)
Train: 104 [1050/1251 ( 84%)]  Loss:  4.152551 (4.5389)  Time: 4.173s,  245.36/s  (2.070s,  494.57/s)  LR: 6.223e-05  Data: 3.516 (1.471)
Train: 104 [1100/1251 ( 88%)]  Loss:  4.532261 (4.5386)  Time: 0.586s, 1747.26/s  (2.071s,  494.45/s)  LR: 6.223e-05  Data: 0.020 (1.472)
Train: 104 [1150/1251 ( 92%)]  Loss:  5.215867 (4.5668)  Time: 6.849s,  149.50/s  (2.076s,  493.23/s)  LR: 6.223e-05  Data: 6.205 (1.477)
Train: 104 [1200/1251 ( 96%)]  Loss:  5.236444 (4.5936)  Time: 0.587s, 1743.39/s  (2.072s,  494.22/s)  LR: 6.223e-05  Data: 0.020 (1.474)
Train: 104 [1250/1251 (100%)]  Loss:  4.660517 (4.5962)  Time: 0.565s, 1812.77/s  (2.067s,  495.41/s)  LR: 6.223e-05  Data: 0.000 (1.470)
Test: [   0/48]  Time: 11.628 (11.628)  Loss:  1.1582 (1.1582)  Acc@1: 75.4883 (75.4883)  Acc@5: 92.3828 (92.3828)
Test: [  48/48]  Time: 0.148 (2.855)  Loss:  1.1405 (2.0375)  Acc@1: 75.7076 (55.4480)  Acc@5: 90.9198 (79.5340)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-104.pth.tar', 55.44800004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-103.pth.tar', 55.04000007324219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-102.pth.tar', 55.015999997558595)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-101.pth.tar', 54.81800009765625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-100.pth.tar', 54.537999868164064)

Train: 105 [   0/1251 (  0%)]  Loss:  4.744138 (4.7441)  Time: 8.986s,  113.95/s  (8.986s,  113.95/s)  LR: 5.668e-05  Data: 8.040 (8.040)
Train: 105 [  50/1251 (  4%)]  Loss:  5.021131 (4.8826)  Time: 0.585s, 1749.41/s  (2.332s,  439.20/s)  LR: 5.668e-05  Data: 0.023 (1.740)
Train: 105 [ 100/1251 (  8%)]  Loss:  4.378690 (4.7147)  Time: 0.933s, 1096.98/s  (2.280s,  449.11/s)  LR: 5.668e-05  Data: 0.081 (1.685)
Train: 105 [ 150/1251 ( 12%)]  Loss:  4.669751 (4.7034)  Time: 0.584s, 1752.68/s  (2.195s,  466.53/s)  LR: 5.668e-05  Data: 0.020 (1.602)
Train: 105 [ 200/1251 ( 16%)]  Loss:  4.679070 (4.6986)  Time: 0.583s, 1756.05/s  (2.178s,  470.05/s)  LR: 5.668e-05  Data: 0.020 (1.588)
Train: 105 [ 250/1251 ( 20%)]  Loss:  4.794721 (4.7146)  Time: 0.585s, 1749.29/s  (2.137s,  479.12/s)  LR: 5.668e-05  Data: 0.021 (1.549)
Train: 105 [ 300/1251 ( 24%)]  Loss:  4.419159 (4.6724)  Time: 0.584s, 1752.31/s  (2.130s,  480.70/s)  LR: 5.668e-05  Data: 0.022 (1.541)
Train: 105 [ 350/1251 ( 28%)]  Loss:  4.640474 (4.6684)  Time: 0.585s, 1751.26/s  (2.101s,  487.33/s)  LR: 5.668e-05  Data: 0.021 (1.512)
Train: 105 [ 400/1251 ( 32%)]  Loss:  4.896177 (4.6937)  Time: 0.585s, 1751.28/s  (2.092s,  489.38/s)  LR: 5.668e-05  Data: 0.021 (1.503)
Train: 105 [ 450/1251 ( 36%)]  Loss:  4.990734 (4.7234)  Time: 0.587s, 1744.37/s  (2.077s,  493.05/s)  LR: 5.668e-05  Data: 0.019 (1.488)
Train: 105 [ 500/1251 ( 40%)]  Loss:  4.306474 (4.6855)  Time: 0.587s, 1744.35/s  (2.101s,  487.38/s)  LR: 5.668e-05  Data: 0.020 (1.511)
Train: 105 [ 550/1251 ( 44%)]  Loss:  4.898399 (4.7032)  Time: 0.585s, 1750.10/s  (2.103s,  486.98/s)  LR: 5.668e-05  Data: 0.019 (1.514)
Train: 105 [ 600/1251 ( 48%)]  Loss:  4.958923 (4.7229)  Time: 0.585s, 1751.58/s  (2.108s,  485.70/s)  LR: 5.668e-05  Data: 0.021 (1.520)
Train: 105 [ 650/1251 ( 52%)]  Loss:  5.145974 (4.7531)  Time: 0.585s, 1750.10/s  (2.103s,  486.95/s)  LR: 5.668e-05  Data: 0.021 (1.516)
Train: 105 [ 700/1251 ( 56%)]  Loss:  5.031521 (4.7717)  Time: 0.583s, 1755.82/s  (2.105s,  486.35/s)  LR: 5.668e-05  Data: 0.020 (1.518)
Train: 105 [ 750/1251 ( 60%)]  Loss:  4.847735 (4.7764)  Time: 0.583s, 1756.85/s  (2.092s,  489.57/s)  LR: 5.668e-05  Data: 0.019 (1.505)
Train: 105 [ 800/1251 ( 64%)]  Loss:  4.811814 (4.7785)  Time: 0.583s, 1755.19/s  (2.085s,  491.21/s)  LR: 5.668e-05  Data: 0.020 (1.497)
Train: 105 [ 850/1251 ( 68%)]  Loss:  4.664164 (4.7722)  Time: 0.584s, 1754.23/s  (2.073s,  493.96/s)  LR: 5.668e-05  Data: 0.021 (1.487)
Train: 105 [ 900/1251 ( 72%)]  Loss:  4.726292 (4.7698)  Time: 0.586s, 1747.24/s  (2.079s,  492.53/s)  LR: 5.668e-05  Data: 0.021 (1.493)
Train: 105 [ 950/1251 ( 76%)]  Loss:  4.888206 (4.7757)  Time: 0.585s, 1749.67/s  (2.075s,  493.41/s)  LR: 5.668e-05  Data: 0.020 (1.489)
Train: 105 [1000/1251 ( 80%)]  Loss:  4.027773 (4.7401)  Time: 0.594s, 1725.15/s  (2.080s,  492.28/s)  LR: 5.668e-05  Data: 0.018 (1.493)
Train: 105 [1050/1251 ( 84%)]  Loss:  4.362627 (4.7229)  Time: 4.579s,  223.62/s  (2.079s,  492.45/s)  LR: 5.668e-05  Data: 3.900 (1.492)
Train: 105 [1100/1251 ( 88%)]  Loss:  5.035307 (4.7365)  Time: 0.585s, 1749.40/s  (2.079s,  492.44/s)  LR: 5.668e-05  Data: 0.022 (1.491)
Train: 105 [1150/1251 ( 92%)]  Loss:  4.593814 (4.7305)  Time: 2.269s,  451.21/s  (2.079s,  492.65/s)  LR: 5.668e-05  Data: 1.649 (1.490)
Train: 105 [1200/1251 ( 96%)]  Loss:  4.317420 (4.7140)  Time: 0.585s, 1749.14/s  (2.076s,  493.24/s)  LR: 5.668e-05  Data: 0.020 (1.487)
Train: 105 [1250/1251 (100%)]  Loss:  4.491948 (4.7055)  Time: 0.563s, 1818.20/s  (2.072s,  494.32/s)  LR: 5.668e-05  Data: 0.000 (1.482)
Test: [   0/48]  Time: 11.799 (11.799)  Loss:  1.1497 (1.1497)  Acc@1: 76.9531 (76.9531)  Acc@5: 91.7969 (91.7969)
Test: [  48/48]  Time: 0.149 (2.882)  Loss:  1.1776 (2.0488)  Acc@1: 75.9434 (55.5120)  Acc@5: 90.3302 (79.4960)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-105.pth.tar', 55.51200002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-104.pth.tar', 55.44800004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-103.pth.tar', 55.04000007324219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-102.pth.tar', 55.015999997558595)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-101.pth.tar', 54.81800009765625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-100.pth.tar', 54.537999868164064)

Train: 106 [   0/1251 (  0%)]  Loss:  4.387496 (4.3875)  Time: 14.445s,   70.89/s  (14.445s,   70.89/s)  LR: 5.142e-05  Data: 13.377 (13.377)
Train: 106 [  50/1251 (  4%)]  Loss:  4.435740 (4.4116)  Time: 0.582s, 1760.12/s  (2.274s,  450.34/s)  LR: 5.142e-05  Data: 0.019 (1.684)
Train: 106 [ 100/1251 (  8%)]  Loss:  4.678692 (4.5006)  Time: 0.583s, 1755.33/s  (2.219s,  461.38/s)  LR: 5.142e-05  Data: 0.019 (1.632)
Train: 106 [ 150/1251 ( 12%)]  Loss:  4.843358 (4.5863)  Time: 0.585s, 1749.52/s  (2.156s,  474.95/s)  LR: 5.142e-05  Data: 0.019 (1.568)
Train: 106 [ 200/1251 ( 16%)]  Loss:  5.034154 (4.6759)  Time: 1.061s,  964.84/s  (2.150s,  476.25/s)  LR: 5.142e-05  Data: 0.386 (1.560)
Train: 106 [ 250/1251 ( 20%)]  Loss:  4.947006 (4.7211)  Time: 0.586s, 1748.26/s  (2.112s,  484.82/s)  LR: 5.142e-05  Data: 0.021 (1.522)
Train: 106 [ 300/1251 ( 24%)]  Loss:  4.969375 (4.7565)  Time: 0.586s, 1748.12/s  (2.111s,  485.15/s)  LR: 5.142e-05  Data: 0.017 (1.518)
Train: 106 [ 350/1251 ( 28%)]  Loss:  4.581132 (4.7346)  Time: 0.583s, 1755.82/s  (2.085s,  491.23/s)  LR: 5.142e-05  Data: 0.018 (1.491)
Train: 106 [ 400/1251 ( 32%)]  Loss:  4.659256 (4.7262)  Time: 0.580s, 1764.27/s  (2.078s,  492.67/s)  LR: 5.142e-05  Data: 0.018 (1.483)
Train: 106 [ 450/1251 ( 36%)]  Loss:  4.934380 (4.7471)  Time: 0.587s, 1744.19/s  (2.091s,  489.74/s)  LR: 5.142e-05  Data: 0.020 (1.495)
Train: 106 [ 500/1251 ( 40%)]  Loss:  4.535359 (4.7278)  Time: 0.585s, 1751.36/s  (2.109s,  485.62/s)  LR: 5.142e-05  Data: 0.021 (1.513)
Train: 106 [ 550/1251 ( 44%)]  Loss:  4.308766 (4.6929)  Time: 0.584s, 1753.26/s  (2.117s,  483.70/s)  LR: 5.142e-05  Data: 0.020 (1.521)
Train: 106 [ 600/1251 ( 48%)]  Loss:  5.020627 (4.7181)  Time: 0.590s, 1736.87/s  (2.117s,  483.75/s)  LR: 5.142e-05  Data: 0.022 (1.520)
Train: 106 [ 650/1251 ( 52%)]  Loss:  4.482906 (4.7013)  Time: 0.591s, 1731.60/s  (2.116s,  483.99/s)  LR: 5.142e-05  Data: 0.021 (1.519)
Train: 106 [ 700/1251 ( 56%)]  Loss:  4.935184 (4.7169)  Time: 1.209s,  846.96/s  (2.115s,  484.17/s)  LR: 5.142e-05  Data: 0.521 (1.517)
Train: 106 [ 750/1251 ( 60%)]  Loss:  3.876016 (4.6643)  Time: 0.586s, 1747.28/s  (2.105s,  486.40/s)  LR: 5.142e-05  Data: 0.019 (1.508)
Train: 106 [ 800/1251 ( 64%)]  Loss:  4.230062 (4.6388)  Time: 0.585s, 1751.14/s  (2.099s,  487.91/s)  LR: 5.142e-05  Data: 0.021 (1.501)
Train: 106 [ 850/1251 ( 68%)]  Loss:  4.256095 (4.6175)  Time: 0.587s, 1744.55/s  (2.091s,  489.66/s)  LR: 5.142e-05  Data: 0.020 (1.494)
Train: 106 [ 900/1251 ( 72%)]  Loss:  5.181373 (4.6472)  Time: 0.588s, 1741.25/s  (2.098s,  487.97/s)  LR: 5.142e-05  Data: 0.025 (1.501)
Train: 106 [ 950/1251 ( 76%)]  Loss:  4.835076 (4.6566)  Time: 0.588s, 1740.14/s  (2.100s,  487.69/s)  LR: 5.142e-05  Data: 0.026 (1.502)
Train: 106 [1000/1251 ( 80%)]  Loss:  4.642924 (4.6560)  Time: 0.585s, 1750.78/s  (2.102s,  487.25/s)  LR: 5.142e-05  Data: 0.021 (1.503)
Train: 106 [1050/1251 ( 84%)]  Loss:  4.348233 (4.6420)  Time: 1.168s,  877.00/s  (2.097s,  488.36/s)  LR: 5.142e-05  Data: 0.595 (1.498)
Train: 106 [1100/1251 ( 88%)]  Loss:  4.523641 (4.6368)  Time: 0.588s, 1740.47/s  (2.095s,  488.76/s)  LR: 5.142e-05  Data: 0.020 (1.496)
Train: 106 [1150/1251 ( 92%)]  Loss:  4.015259 (4.6109)  Time: 0.584s, 1751.98/s  (2.091s,  489.75/s)  LR: 5.142e-05  Data: 0.018 (1.493)
Train: 106 [1200/1251 ( 96%)]  Loss:  5.175781 (4.6335)  Time: 0.587s, 1745.89/s  (2.090s,  490.05/s)  LR: 5.142e-05  Data: 0.021 (1.492)
Train: 106 [1250/1251 (100%)]  Loss:  4.795752 (4.6398)  Time: 0.563s, 1819.41/s  (2.082s,  491.87/s)  LR: 5.142e-05  Data: 0.000 (1.485)
Test: [   0/48]  Time: 11.476 (11.476)  Loss:  1.1950 (1.1950)  Acc@1: 75.9766 (75.9766)  Acc@5: 91.9922 (91.9922)
Test: [  48/48]  Time: 0.148 (3.038)  Loss:  1.1422 (2.0522)  Acc@1: 76.7689 (55.5360)  Acc@5: 90.9198 (79.6440)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-106.pth.tar', 55.535999992675784)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-105.pth.tar', 55.51200002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-104.pth.tar', 55.44800004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-103.pth.tar', 55.04000007324219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-102.pth.tar', 55.015999997558595)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-101.pth.tar', 54.81800009765625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-100.pth.tar', 54.537999868164064)

Train: 107 [   0/1251 (  0%)]  Loss:  4.920766 (4.9208)  Time: 9.715s,  105.41/s  (9.715s,  105.41/s)  LR: 4.647e-05  Data: 8.361 (8.361)
Train: 107 [  50/1251 (  4%)]  Loss:  4.728674 (4.8247)  Time: 0.584s, 1752.61/s  (2.183s,  469.13/s)  LR: 4.647e-05  Data: 0.021 (1.568)
Train: 107 [ 100/1251 (  8%)]  Loss:  5.067441 (4.9056)  Time: 0.584s, 1752.83/s  (2.148s,  476.65/s)  LR: 4.647e-05  Data: 0.021 (1.550)
Train: 107 [ 150/1251 ( 12%)]  Loss:  4.501432 (4.8046)  Time: 0.583s, 1757.38/s  (2.094s,  488.92/s)  LR: 4.647e-05  Data: 0.020 (1.503)
Train: 107 [ 200/1251 ( 16%)]  Loss:  4.895799 (4.8228)  Time: 0.586s, 1748.61/s  (2.092s,  489.47/s)  LR: 4.647e-05  Data: 0.022 (1.504)
Train: 107 [ 250/1251 ( 20%)]  Loss:  4.418560 (4.7554)  Time: 0.583s, 1755.62/s  (2.066s,  495.62/s)  LR: 4.647e-05  Data: 0.020 (1.480)
Train: 107 [ 300/1251 ( 24%)]  Loss:  4.719884 (4.7504)  Time: 0.583s, 1756.50/s  (2.065s,  495.81/s)  LR: 4.647e-05  Data: 0.019 (1.480)
Train: 107 [ 350/1251 ( 28%)]  Loss:  4.651113 (4.7380)  Time: 0.584s, 1753.28/s  (2.041s,  501.73/s)  LR: 4.647e-05  Data: 0.019 (1.456)
Train: 107 [ 400/1251 ( 32%)]  Loss:  4.755855 (4.7399)  Time: 0.586s, 1747.74/s  (2.031s,  504.29/s)  LR: 4.647e-05  Data: 0.020 (1.446)
Train: 107 [ 450/1251 ( 36%)]  Loss:  5.104663 (4.7764)  Time: 0.583s, 1755.58/s  (2.050s,  499.53/s)  LR: 4.647e-05  Data: 0.021 (1.465)
Train: 107 [ 500/1251 ( 40%)]  Loss:  4.234667 (4.7272)  Time: 0.590s, 1736.82/s  (2.076s,  493.32/s)  LR: 4.647e-05  Data: 0.020 (1.491)
Train: 107 [ 550/1251 ( 44%)]  Loss:  4.087945 (4.6739)  Time: 0.582s, 1759.85/s  (2.086s,  490.98/s)  LR: 4.647e-05  Data: 0.019 (1.501)
Train: 107 [ 600/1251 ( 48%)]  Loss:  4.325151 (4.6471)  Time: 0.586s, 1747.48/s  (2.098s,  488.06/s)  LR: 4.647e-05  Data: 0.021 (1.513)
Train: 107 [ 650/1251 ( 52%)]  Loss:  4.646454 (4.6470)  Time: 0.586s, 1746.48/s  (2.097s,  488.37/s)  LR: 4.647e-05  Data: 0.021 (1.510)
Train: 107 [ 700/1251 ( 56%)]  Loss:  4.932240 (4.6660)  Time: 0.586s, 1748.19/s  (2.102s,  487.07/s)  LR: 4.647e-05  Data: 0.019 (1.515)
Train: 107 [ 750/1251 ( 60%)]  Loss:  4.954744 (4.6841)  Time: 0.583s, 1755.10/s  (2.093s,  489.27/s)  LR: 4.647e-05  Data: 0.021 (1.505)
Train: 107 [ 800/1251 ( 64%)]  Loss:  4.582345 (4.6781)  Time: 0.584s, 1753.79/s  (2.091s,  489.74/s)  LR: 4.647e-05  Data: 0.021 (1.503)
Train: 107 [ 850/1251 ( 68%)]  Loss:  4.853066 (4.6878)  Time: 0.587s, 1744.59/s  (2.100s,  487.71/s)  LR: 4.647e-05  Data: 0.022 (1.511)
Train: 107 [ 900/1251 ( 72%)]  Loss:  4.781452 (4.6928)  Time: 0.584s, 1751.95/s  (2.107s,  486.10/s)  LR: 4.647e-05  Data: 0.022 (1.518)
Train: 107 [ 950/1251 ( 76%)]  Loss:  4.036588 (4.6599)  Time: 0.585s, 1750.37/s  (2.103s,  486.84/s)  LR: 4.647e-05  Data: 0.019 (1.515)
Train: 107 [1000/1251 ( 80%)]  Loss:  4.348675 (4.6451)  Time: 0.584s, 1752.57/s  (2.107s,  486.04/s)  LR: 4.647e-05  Data: 0.020 (1.519)
Train: 107 [1050/1251 ( 84%)]  Loss:  4.778847 (4.6512)  Time: 0.583s, 1755.24/s  (2.100s,  487.60/s)  LR: 4.647e-05  Data: 0.018 (1.512)
Train: 107 [1100/1251 ( 88%)]  Loss:  4.650153 (4.6512)  Time: 0.584s, 1753.79/s  (2.099s,  487.77/s)  LR: 4.647e-05  Data: 0.019 (1.512)
Train: 107 [1150/1251 ( 92%)]  Loss:  4.683353 (4.6525)  Time: 0.583s, 1755.82/s  (2.093s,  489.27/s)  LR: 4.647e-05  Data: 0.019 (1.506)
Train: 107 [1200/1251 ( 96%)]  Loss:  4.669306 (4.6532)  Time: 0.586s, 1747.57/s  (2.091s,  489.73/s)  LR: 4.647e-05  Data: 0.017 (1.504)
Train: 107 [1250/1251 (100%)]  Loss:  5.150704 (4.6723)  Time: 0.565s, 1813.05/s  (2.084s,  491.29/s)  LR: 4.647e-05  Data: 0.000 (1.498)
Test: [   0/48]  Time: 11.223 (11.223)  Loss:  1.1611 (1.1611)  Acc@1: 77.1484 (77.1484)  Acc@5: 91.7969 (91.7969)
Test: [  48/48]  Time: 0.149 (3.342)  Loss:  1.1665 (2.0572)  Acc@1: 76.7689 (55.6440)  Acc@5: 90.3302 (79.6520)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-107.pth.tar', 55.64399999267578)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-106.pth.tar', 55.535999992675784)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-105.pth.tar', 55.51200002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-104.pth.tar', 55.44800004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-103.pth.tar', 55.04000007324219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-102.pth.tar', 55.015999997558595)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-101.pth.tar', 54.81800009765625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-100.pth.tar', 54.537999868164064)

Train: 108 [   0/1251 (  0%)]  Loss:  4.814035 (4.8140)  Time: 10.677s,   95.91/s  (10.677s,   95.91/s)  LR: 4.182e-05  Data: 9.514 (9.514)
Train: 108 [  50/1251 (  4%)]  Loss:  4.938779 (4.8764)  Time: 0.586s, 1746.75/s  (2.233s,  458.51/s)  LR: 4.182e-05  Data: 0.020 (1.632)
Train: 108 [ 100/1251 (  8%)]  Loss:  5.012450 (4.9218)  Time: 1.137s,  900.74/s  (2.181s,  469.55/s)  LR: 4.182e-05  Data: 0.475 (1.581)
Train: 108 [ 150/1251 ( 12%)]  Loss:  4.487693 (4.8132)  Time: 0.587s, 1744.45/s  (2.111s,  484.98/s)  LR: 4.182e-05  Data: 0.020 (1.509)
Train: 108 [ 200/1251 ( 16%)]  Loss:  4.015975 (4.6538)  Time: 1.477s,  693.13/s  (2.078s,  492.83/s)  LR: 4.182e-05  Data: 0.876 (1.476)
Train: 108 [ 250/1251 ( 20%)]  Loss:  4.976278 (4.7075)  Time: 0.585s, 1751.19/s  (2.036s,  502.95/s)  LR: 4.182e-05  Data: 0.021 (1.433)
Train: 108 [ 300/1251 ( 24%)]  Loss:  5.046228 (4.7559)  Time: 3.465s,  295.55/s  (2.019s,  507.10/s)  LR: 4.182e-05  Data: 2.886 (1.417)
Train: 108 [ 350/1251 ( 28%)]  Loss:  4.821404 (4.7641)  Time: 0.584s, 1752.47/s  (1.985s,  515.96/s)  LR: 4.182e-05  Data: 0.022 (1.385)
Train: 108 [ 400/1251 ( 32%)]  Loss:  4.387511 (4.7223)  Time: 2.680s,  382.09/s  (2.004s,  511.00/s)  LR: 4.182e-05  Data: 2.102 (1.405)
Train: 108 [ 450/1251 ( 36%)]  Loss:  4.807091 (4.7307)  Time: 0.584s, 1753.01/s  (2.017s,  507.77/s)  LR: 4.182e-05  Data: 0.021 (1.418)
Train: 108 [ 500/1251 ( 40%)]  Loss:  5.097926 (4.7641)  Time: 6.750s,  151.71/s  (2.041s,  501.74/s)  LR: 4.182e-05  Data: 6.167 (1.441)
Train: 108 [ 550/1251 ( 44%)]  Loss:  5.042956 (4.7874)  Time: 0.585s, 1751.21/s  (2.047s,  500.29/s)  LR: 4.182e-05  Data: 0.020 (1.449)
Train: 108 [ 600/1251 ( 48%)]  Loss:  4.110511 (4.7353)  Time: 7.456s,  137.34/s  (2.056s,  498.07/s)  LR: 4.182e-05  Data: 6.305 (1.458)
Train: 108 [ 650/1251 ( 52%)]  Loss:  4.626506 (4.7275)  Time: 0.584s, 1752.96/s  (2.051s,  499.37/s)  LR: 4.182e-05  Data: 0.020 (1.453)
Train: 108 [ 700/1251 ( 56%)]  Loss:  4.565686 (4.7167)  Time: 5.795s,  176.71/s  (2.049s,  499.64/s)  LR: 4.182e-05  Data: 5.183 (1.453)
Train: 108 [ 750/1251 ( 60%)]  Loss:  4.356177 (4.6942)  Time: 0.584s, 1753.36/s  (2.035s,  503.24/s)  LR: 4.182e-05  Data: 0.021 (1.439)
Train: 108 [ 800/1251 ( 64%)]  Loss:  4.991861 (4.7117)  Time: 5.553s,  184.41/s  (2.024s,  505.92/s)  LR: 4.182e-05  Data: 4.984 (1.429)
Train: 108 [ 850/1251 ( 68%)]  Loss:  4.590989 (4.7050)  Time: 0.585s, 1751.70/s  (2.027s,  505.19/s)  LR: 4.182e-05  Data: 0.021 (1.432)
Train: 108 [ 900/1251 ( 72%)]  Loss:  4.705753 (4.7050)  Time: 6.259s,  163.60/s  (2.035s,  503.26/s)  LR: 4.182e-05  Data: 5.679 (1.440)
Train: 108 [ 950/1251 ( 76%)]  Loss:  4.116209 (4.6756)  Time: 0.585s, 1750.13/s  (2.033s,  503.75/s)  LR: 4.182e-05  Data: 0.021 (1.437)
Train: 108 [1000/1251 ( 80%)]  Loss:  4.644556 (4.6741)  Time: 3.654s,  280.25/s  (2.036s,  503.05/s)  LR: 4.182e-05  Data: 2.995 (1.439)
Train: 108 [1050/1251 ( 84%)]  Loss:  4.476274 (4.6651)  Time: 0.586s, 1747.66/s  (2.032s,  503.94/s)  LR: 4.182e-05  Data: 0.021 (1.435)
Train: 108 [1100/1251 ( 88%)]  Loss:  5.366831 (4.6956)  Time: 1.970s,  519.79/s  (2.030s,  504.31/s)  LR: 4.182e-05  Data: 1.400 (1.432)
Train: 108 [1150/1251 ( 92%)]  Loss:  4.598569 (4.6916)  Time: 0.584s, 1752.57/s  (2.024s,  505.87/s)  LR: 4.182e-05  Data: 0.019 (1.426)
Train: 108 [1200/1251 ( 96%)]  Loss:  5.011103 (4.7044)  Time: 0.588s, 1742.12/s  (2.019s,  507.22/s)  LR: 4.182e-05  Data: 0.025 (1.420)
Train: 108 [1250/1251 (100%)]  Loss:  4.795468 (4.7079)  Time: 0.562s, 1820.59/s  (2.010s,  509.49/s)  LR: 4.182e-05  Data: 0.000 (1.412)
Test: [   0/48]  Time: 10.984 (10.984)  Loss:  1.1516 (1.1516)  Acc@1: 76.9531 (76.9531)  Acc@5: 91.7969 (91.7969)
Test: [  48/48]  Time: 0.149 (3.120)  Loss:  1.1624 (2.0384)  Acc@1: 76.6509 (55.6380)  Acc@5: 90.0943 (79.7840)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-107.pth.tar', 55.64399999267578)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-108.pth.tar', 55.63799994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-106.pth.tar', 55.535999992675784)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-105.pth.tar', 55.51200002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-104.pth.tar', 55.44800004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-103.pth.tar', 55.04000007324219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-102.pth.tar', 55.015999997558595)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-101.pth.tar', 54.81800009765625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-100.pth.tar', 54.537999868164064)

Train: 109 [   0/1251 (  0%)]  Loss:  4.564200 (4.5642)  Time: 10.837s,   94.49/s  (10.837s,   94.49/s)  LR: 3.748e-05  Data: 10.178 (10.178)
Train: 109 [  50/1251 (  4%)]  Loss:  4.380095 (4.4721)  Time: 0.589s, 1739.42/s  (2.194s,  466.63/s)  LR: 3.748e-05  Data: 0.026 (1.596)
Train: 109 [ 100/1251 (  8%)]  Loss:  4.191180 (4.3785)  Time: 0.586s, 1747.10/s  (2.133s,  480.15/s)  LR: 3.748e-05  Data: 0.020 (1.533)
Train: 109 [ 150/1251 ( 12%)]  Loss:  4.522714 (4.4145)  Time: 0.585s, 1750.15/s  (2.071s,  494.56/s)  LR: 3.748e-05  Data: 0.022 (1.476)
Train: 109 [ 200/1251 ( 16%)]  Loss:  4.370060 (4.4056)  Time: 0.584s, 1753.37/s  (2.043s,  501.21/s)  LR: 3.748e-05  Data: 0.019 (1.444)
Train: 109 [ 250/1251 ( 20%)]  Loss:  4.902213 (4.4884)  Time: 1.296s,  790.25/s  (2.004s,  511.09/s)  LR: 3.748e-05  Data: 0.698 (1.404)
Train: 109 [ 300/1251 ( 24%)]  Loss:  4.405884 (4.4766)  Time: 0.694s, 1474.98/s  (1.987s,  515.40/s)  LR: 3.748e-05  Data: 0.116 (1.388)
Train: 109 [ 350/1251 ( 28%)]  Loss:  5.118403 (4.5568)  Time: 0.835s, 1227.06/s  (1.957s,  523.18/s)  LR: 3.748e-05  Data: 0.172 (1.359)
Train: 109 [ 400/1251 ( 32%)]  Loss:  4.995914 (4.6056)  Time: 1.721s,  594.85/s  (1.943s,  527.00/s)  LR: 3.748e-05  Data: 1.055 (1.343)
Train: 109 [ 450/1251 ( 36%)]  Loss:  4.387472 (4.5838)  Time: 2.001s,  511.66/s  (1.965s,  521.18/s)  LR: 3.748e-05  Data: 1.439 (1.363)
Train: 109 [ 500/1251 ( 40%)]  Loss:  4.963221 (4.6183)  Time: 3.133s,  326.83/s  (1.991s,  514.36/s)  LR: 3.748e-05  Data: 2.569 (1.389)
Train: 109 [ 550/1251 ( 44%)]  Loss:  4.002329 (4.5670)  Time: 0.589s, 1739.62/s  (2.004s,  510.87/s)  LR: 3.748e-05  Data: 0.023 (1.403)
Train: 109 [ 600/1251 ( 48%)]  Loss:  5.060487 (4.6049)  Time: 0.592s, 1729.80/s  (2.017s,  507.63/s)  LR: 3.748e-05  Data: 0.024 (1.416)
Train: 109 [ 650/1251 ( 52%)]  Loss:  4.781356 (4.6175)  Time: 0.586s, 1746.73/s  (2.021s,  506.60/s)  LR: 3.748e-05  Data: 0.017 (1.421)
Train: 109 [ 700/1251 ( 56%)]  Loss:  4.863389 (4.6339)  Time: 1.659s,  617.30/s  (2.031s,  504.07/s)  LR: 3.748e-05  Data: 0.980 (1.431)
Train: 109 [ 750/1251 ( 60%)]  Loss:  4.329458 (4.6149)  Time: 0.584s, 1752.30/s  (2.030s,  504.39/s)  LR: 3.748e-05  Data: 0.022 (1.430)
Train: 109 [ 800/1251 ( 64%)]  Loss:  4.958204 (4.6351)  Time: 1.613s,  634.73/s  (2.031s,  504.26/s)  LR: 3.748e-05  Data: 1.043 (1.430)
Train: 109 [ 850/1251 ( 68%)]  Loss:  4.712149 (4.6394)  Time: 0.587s, 1744.16/s  (2.039s,  502.27/s)  LR: 3.748e-05  Data: 0.022 (1.438)
Train: 109 [ 900/1251 ( 72%)]  Loss:  3.928434 (4.6020)  Time: 1.955s,  523.89/s  (2.042s,  501.55/s)  LR: 3.748e-05  Data: 1.374 (1.441)
Train: 109 [ 950/1251 ( 76%)]  Loss:  4.518835 (4.5978)  Time: 0.587s, 1744.14/s  (2.045s,  500.84/s)  LR: 3.748e-05  Data: 0.025 (1.444)
Train: 109 [1000/1251 ( 80%)]  Loss:  4.298228 (4.5835)  Time: 0.582s, 1759.47/s  (2.050s,  499.45/s)  LR: 3.748e-05  Data: 0.019 (1.450)
Train: 109 [1050/1251 ( 84%)]  Loss:  4.730836 (4.5902)  Time: 0.589s, 1737.97/s  (2.048s,  499.98/s)  LR: 3.748e-05  Data: 0.024 (1.448)
Train: 109 [1100/1251 ( 88%)]  Loss:  4.735186 (4.5965)  Time: 0.587s, 1743.63/s  (2.049s,  499.71/s)  LR: 3.748e-05  Data: 0.020 (1.450)
Train: 109 [1150/1251 ( 92%)]  Loss:  4.802501 (4.6051)  Time: 0.587s, 1744.15/s  (2.045s,  500.68/s)  LR: 3.748e-05  Data: 0.021 (1.447)
Train: 109 [1200/1251 ( 96%)]  Loss:  4.374374 (4.5959)  Time: 0.587s, 1745.79/s  (2.046s,  500.37/s)  LR: 3.748e-05  Data: 0.022 (1.448)
Train: 109 [1250/1251 (100%)]  Loss:  4.545477 (4.5939)  Time: 0.563s, 1817.99/s  (2.041s,  501.69/s)  LR: 3.748e-05  Data: 0.000 (1.443)
Test: [   0/48]  Time: 12.186 (12.186)  Loss:  1.0989 (1.0989)  Acc@1: 78.3203 (78.3203)  Acc@5: 92.4805 (92.4805)
Test: [  48/48]  Time: 0.149 (3.179)  Loss:  1.1286 (2.0139)  Acc@1: 75.9434 (56.0080)  Acc@5: 90.9198 (79.9760)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-109.pth.tar', 56.008000021972656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-107.pth.tar', 55.64399999267578)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-108.pth.tar', 55.63799994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-106.pth.tar', 55.535999992675784)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-105.pth.tar', 55.51200002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-104.pth.tar', 55.44800004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-103.pth.tar', 55.04000007324219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-102.pth.tar', 55.015999997558595)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-101.pth.tar', 54.81800009765625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-100.pth.tar', 54.537999868164064)

Train: 110 [   0/1251 (  0%)]  Loss:  4.588252 (4.5883)  Time: 10.033s,  102.06/s  (10.033s,  102.06/s)  LR: 3.345e-05  Data: 9.385 (9.385)
Train: 110 [  50/1251 (  4%)]  Loss:  4.656353 (4.6223)  Time: 1.031s,  993.44/s  (2.195s,  466.48/s)  LR: 3.345e-05  Data: 0.450 (1.595)
Train: 110 [ 100/1251 (  8%)]  Loss:  4.795341 (4.6800)  Time: 0.783s, 1308.39/s  (2.120s,  483.02/s)  LR: 3.345e-05  Data: 0.221 (1.511)
Train: 110 [ 150/1251 ( 12%)]  Loss:  4.132047 (4.5430)  Time: 2.551s,  401.40/s  (2.064s,  496.10/s)  LR: 3.345e-05  Data: 1.967 (1.446)
Train: 110 [ 200/1251 ( 16%)]  Loss:  4.871933 (4.6088)  Time: 1.683s,  608.42/s  (2.037s,  502.71/s)  LR: 3.345e-05  Data: 1.118 (1.423)
Train: 110 [ 250/1251 ( 20%)]  Loss:  4.667718 (4.6186)  Time: 3.765s,  271.96/s  (2.031s,  504.18/s)  LR: 3.345e-05  Data: 3.184 (1.419)
Train: 110 [ 300/1251 ( 24%)]  Loss:  5.033656 (4.6779)  Time: 0.586s, 1748.41/s  (2.019s,  507.18/s)  LR: 3.345e-05  Data: 0.021 (1.406)
Train: 110 [ 350/1251 ( 28%)]  Loss:  4.093500 (4.6048)  Time: 2.642s,  387.53/s  (2.010s,  509.42/s)  LR: 3.345e-05  Data: 2.037 (1.399)
Train: 110 [ 400/1251 ( 32%)]  Loss:  4.627511 (4.6074)  Time: 0.810s, 1264.73/s  (2.028s,  504.82/s)  LR: 3.345e-05  Data: 0.151 (1.419)
Train: 110 [ 450/1251 ( 36%)]  Loss:  4.095228 (4.5562)  Time: 6.563s,  156.03/s  (2.074s,  493.83/s)  LR: 3.345e-05  Data: 5.986 (1.468)
Train: 110 [ 500/1251 ( 40%)]  Loss:  4.854938 (4.5833)  Time: 0.589s, 1738.35/s  (2.100s,  487.64/s)  LR: 3.345e-05  Data: 0.023 (1.493)
Train: 110 [ 550/1251 ( 44%)]  Loss:  4.545352 (4.5802)  Time: 3.379s,  303.01/s  (2.108s,  485.75/s)  LR: 3.345e-05  Data: 2.768 (1.502)
Train: 110 [ 600/1251 ( 48%)]  Loss:  4.996291 (4.6122)  Time: 0.588s, 1742.42/s  (2.107s,  485.91/s)  LR: 3.345e-05  Data: 0.022 (1.502)
Train: 110 [ 650/1251 ( 52%)]  Loss:  4.911314 (4.6335)  Time: 1.187s,  862.64/s  (2.102s,  487.16/s)  LR: 3.345e-05  Data: 0.514 (1.497)
Train: 110 [ 700/1251 ( 56%)]  Loss:  4.483979 (4.6236)  Time: 0.586s, 1747.86/s  (2.102s,  487.18/s)  LR: 3.345e-05  Data: 0.024 (1.497)
Train: 110 [ 750/1251 ( 60%)]  Loss:  4.786772 (4.6338)  Time: 0.586s, 1748.56/s  (2.092s,  489.57/s)  LR: 3.345e-05  Data: 0.022 (1.487)
Train: 110 [ 800/1251 ( 64%)]  Loss:  4.584252 (4.6308)  Time: 0.588s, 1740.95/s  (2.087s,  490.72/s)  LR: 3.345e-05  Data: 0.022 (1.480)
Train: 110 [ 850/1251 ( 68%)]  Loss:  4.990438 (4.6508)  Time: 0.678s, 1510.96/s  (2.096s,  488.52/s)  LR: 3.345e-05  Data: 0.022 (1.491)
Train: 110 [ 900/1251 ( 72%)]  Loss:  4.710724 (4.6540)  Time: 0.732s, 1399.64/s  (2.105s,  486.43/s)  LR: 3.345e-05  Data: 0.071 (1.500)
Train: 110 [ 950/1251 ( 76%)]  Loss:  4.230998 (4.6328)  Time: 0.583s, 1755.59/s  (2.105s,  486.53/s)  LR: 3.345e-05  Data: 0.019 (1.500)
Train: 110 [1000/1251 ( 80%)]  Loss:  4.598384 (4.6312)  Time: 0.587s, 1745.29/s  (2.107s,  485.92/s)  LR: 3.345e-05  Data: 0.024 (1.502)
Train: 110 [1050/1251 ( 84%)]  Loss:  4.547578 (4.6274)  Time: 0.592s, 1728.54/s  (2.104s,  486.77/s)  LR: 3.345e-05  Data: 0.025 (1.499)
Train: 110 [1100/1251 ( 88%)]  Loss:  4.292310 (4.6128)  Time: 0.587s, 1744.08/s  (2.101s,  487.32/s)  LR: 3.345e-05  Data: 0.024 (1.497)
Train: 110 [1150/1251 ( 92%)]  Loss:  4.352721 (4.6020)  Time: 0.947s, 1081.74/s  (2.097s,  488.34/s)  LR: 3.345e-05  Data: 0.236 (1.491)
Train: 110 [1200/1251 ( 96%)]  Loss:  4.724579 (4.6069)  Time: 0.589s, 1739.23/s  (2.093s,  489.28/s)  LR: 3.345e-05  Data: 0.025 (1.486)
Train: 110 [1250/1251 (100%)]  Loss:  4.767482 (4.6131)  Time: 0.564s, 1814.94/s  (2.096s,  488.60/s)  LR: 3.345e-05  Data: 0.000 (1.488)
Test: [   0/48]  Time: 13.821 (13.821)  Loss:  1.1590 (1.1590)  Acc@1: 76.6602 (76.6602)  Acc@5: 91.9922 (91.9922)
Test: [  48/48]  Time: 0.149 (3.234)  Loss:  1.1423 (2.0324)  Acc@1: 77.1227 (55.9100)  Acc@5: 90.5660 (79.8860)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-109.pth.tar', 56.008000021972656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-110.pth.tar', 55.91000014648438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-107.pth.tar', 55.64399999267578)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-108.pth.tar', 55.63799994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-106.pth.tar', 55.535999992675784)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-105.pth.tar', 55.51200002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-104.pth.tar', 55.44800004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-103.pth.tar', 55.04000007324219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-102.pth.tar', 55.015999997558595)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-101.pth.tar', 54.81800009765625)

Train: 111 [   0/1251 (  0%)]  Loss:  4.839591 (4.8396)  Time: 11.101s,   92.25/s  (11.101s,   92.25/s)  LR: 2.973e-05  Data: 10.196 (10.196)
Train: 111 [  50/1251 (  4%)]  Loss:  4.632348 (4.7360)  Time: 0.586s, 1747.51/s  (2.241s,  457.03/s)  LR: 2.973e-05  Data: 0.022 (1.629)
Train: 111 [ 100/1251 (  8%)]  Loss:  4.641644 (4.7045)  Time: 0.587s, 1744.45/s  (2.179s,  469.88/s)  LR: 2.973e-05  Data: 0.018 (1.567)
Train: 111 [ 150/1251 ( 12%)]  Loss:  5.014665 (4.7821)  Time: 0.591s, 1733.37/s  (2.114s,  484.43/s)  LR: 2.973e-05  Data: 0.021 (1.510)
Train: 111 [ 200/1251 ( 16%)]  Loss:  4.287194 (4.6831)  Time: 0.586s, 1747.99/s  (2.096s,  488.61/s)  LR: 2.973e-05  Data: 0.019 (1.496)
Train: 111 [ 250/1251 ( 20%)]  Loss:  4.407646 (4.6372)  Time: 0.585s, 1751.26/s  (2.055s,  498.21/s)  LR: 2.973e-05  Data: 0.020 (1.461)
Train: 111 [ 300/1251 ( 24%)]  Loss:  3.917910 (4.5344)  Time: 0.585s, 1751.73/s  (2.049s,  499.84/s)  LR: 2.973e-05  Data: 0.021 (1.454)
Train: 111 [ 350/1251 ( 28%)]  Loss:  4.895959 (4.5796)  Time: 3.471s,  295.04/s  (2.048s,  500.05/s)  LR: 2.973e-05  Data: 2.734 (1.451)
Train: 111 [ 400/1251 ( 32%)]  Loss:  4.850290 (4.6097)  Time: 0.588s, 1740.27/s  (2.062s,  496.70/s)  LR: 2.973e-05  Data: 0.018 (1.465)
Train: 111 [ 450/1251 ( 36%)]  Loss:  4.779961 (4.6267)  Time: 2.929s,  349.55/s  (2.071s,  494.50/s)  LR: 2.973e-05  Data: 2.367 (1.474)
Train: 111 [ 500/1251 ( 40%)]  Loss:  4.649706 (4.6288)  Time: 0.586s, 1748.52/s  (2.076s,  493.25/s)  LR: 2.973e-05  Data: 0.020 (1.480)
Train: 111 [ 550/1251 ( 44%)]  Loss:  4.641592 (4.6299)  Time: 2.205s,  464.32/s  (2.072s,  494.26/s)  LR: 2.973e-05  Data: 1.633 (1.475)
Train: 111 [ 600/1251 ( 48%)]  Loss:  4.603207 (4.6278)  Time: 0.585s, 1751.35/s  (2.073s,  493.91/s)  LR: 2.973e-05  Data: 0.021 (1.477)
Train: 111 [ 650/1251 ( 52%)]  Loss:  4.962379 (4.6517)  Time: 1.191s,  859.75/s  (2.076s,  493.32/s)  LR: 2.973e-05  Data: 0.601 (1.477)
Train: 111 [ 700/1251 ( 56%)]  Loss:  4.552848 (4.6451)  Time: 2.646s,  386.97/s  (2.071s,  494.35/s)  LR: 2.973e-05  Data: 2.069 (1.472)
Train: 111 [ 750/1251 ( 60%)]  Loss:  4.324347 (4.6251)  Time: 0.588s, 1741.41/s  (2.067s,  495.52/s)  LR: 2.973e-05  Data: 0.022 (1.466)
Train: 111 [ 800/1251 ( 64%)]  Loss:  4.438734 (4.6141)  Time: 4.102s,  249.61/s  (2.081s,  491.98/s)  LR: 2.973e-05  Data: 3.539 (1.481)
Train: 111 [ 850/1251 ( 68%)]  Loss:  4.233658 (4.5930)  Time: 0.586s, 1747.80/s  (2.081s,  491.97/s)  LR: 2.973e-05  Data: 0.023 (1.482)
Train: 111 [ 900/1251 ( 72%)]  Loss:  4.050313 (4.5644)  Time: 4.135s,  247.66/s  (2.087s,  490.77/s)  LR: 2.973e-05  Data: 3.552 (1.487)
Train: 111 [ 950/1251 ( 76%)]  Loss:  4.628921 (4.5676)  Time: 0.587s, 1744.94/s  (2.090s,  489.96/s)  LR: 2.973e-05  Data: 0.020 (1.490)
Train: 111 [1000/1251 ( 80%)]  Loss:  4.158563 (4.5482)  Time: 3.343s,  306.29/s  (2.091s,  489.77/s)  LR: 2.973e-05  Data: 2.754 (1.491)
Train: 111 [1050/1251 ( 84%)]  Loss:  4.846131 (4.5617)  Time: 0.586s, 1747.25/s  (2.088s,  490.41/s)  LR: 2.973e-05  Data: 0.020 (1.488)
Train: 111 [1100/1251 ( 88%)]  Loss:  4.812172 (4.5726)  Time: 2.443s,  419.23/s  (2.084s,  491.43/s)  LR: 2.973e-05  Data: 1.867 (1.484)
Train: 111 [1150/1251 ( 92%)]  Loss:  4.783408 (4.5814)  Time: 0.586s, 1746.64/s  (2.082s,  491.91/s)  LR: 2.973e-05  Data: 0.022 (1.482)
Train: 111 [1200/1251 ( 96%)]  Loss:  4.631142 (4.5834)  Time: 6.085s,  168.30/s  (2.082s,  491.75/s)  LR: 2.973e-05  Data: 5.413 (1.483)
Train: 111 [1250/1251 (100%)]  Loss:  4.348106 (4.5743)  Time: 0.563s, 1818.96/s  (2.089s,  490.08/s)  LR: 2.973e-05  Data: 0.000 (1.491)
Test: [   0/48]  Time: 13.357 (13.357)  Loss:  1.1209 (1.1209)  Acc@1: 78.0273 (78.0273)  Acc@5: 92.5781 (92.5781)
Test: [  48/48]  Time: 0.149 (3.151)  Loss:  1.1884 (2.0217)  Acc@1: 76.6509 (56.3080)  Acc@5: 90.3302 (80.0300)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-111.pth.tar', 56.30799994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-109.pth.tar', 56.008000021972656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-110.pth.tar', 55.91000014648438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-107.pth.tar', 55.64399999267578)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-108.pth.tar', 55.63799994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-106.pth.tar', 55.535999992675784)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-105.pth.tar', 55.51200002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-104.pth.tar', 55.44800004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-103.pth.tar', 55.04000007324219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-102.pth.tar', 55.015999997558595)

Train: 112 [   0/1251 (  0%)]  Loss:  4.731513 (4.7315)  Time: 9.650s,  106.12/s  (9.650s,  106.12/s)  LR: 2.632e-05  Data: 8.925 (8.925)
Train: 112 [  50/1251 (  4%)]  Loss:  4.363096 (4.5473)  Time: 0.587s, 1744.61/s  (2.158s,  474.54/s)  LR: 2.632e-05  Data: 0.022 (1.573)
Train: 112 [ 100/1251 (  8%)]  Loss:  4.748613 (4.6144)  Time: 0.699s, 1465.04/s  (2.097s,  488.42/s)  LR: 2.632e-05  Data: 0.017 (1.506)
Train: 112 [ 150/1251 ( 12%)]  Loss:  4.428437 (4.5679)  Time: 3.968s,  258.04/s  (2.067s,  495.41/s)  LR: 2.632e-05  Data: 3.305 (1.466)
Train: 112 [ 200/1251 ( 16%)]  Loss:  4.927593 (4.6399)  Time: 1.028s,  996.20/s  (2.034s,  503.44/s)  LR: 2.632e-05  Data: 0.449 (1.428)
Train: 112 [ 250/1251 ( 20%)]  Loss:  4.607149 (4.6344)  Time: 2.613s,  391.85/s  (2.014s,  508.53/s)  LR: 2.632e-05  Data: 2.038 (1.407)
Train: 112 [ 300/1251 ( 24%)]  Loss:  5.106441 (4.7018)  Time: 3.810s,  268.80/s  (2.000s,  511.96/s)  LR: 2.632e-05  Data: 3.221 (1.392)
Train: 112 [ 350/1251 ( 28%)]  Loss:  4.814713 (4.7159)  Time: 1.917s,  534.04/s  (2.034s,  503.56/s)  LR: 2.632e-05  Data: 1.355 (1.427)
Train: 112 [ 400/1251 ( 32%)]  Loss:  4.894691 (4.7358)  Time: 7.311s,  140.07/s  (2.041s,  501.68/s)  LR: 2.632e-05  Data: 6.728 (1.434)
Train: 112 [ 450/1251 ( 36%)]  Loss:  4.872299 (4.7495)  Time: 0.586s, 1745.96/s  (2.038s,  502.52/s)  LR: 2.632e-05  Data: 0.022 (1.432)
Train: 112 [ 500/1251 ( 40%)]  Loss:  4.507503 (4.7275)  Time: 6.890s,  148.62/s  (2.051s,  499.33/s)  LR: 2.632e-05  Data: 6.212 (1.448)
Train: 112 [ 550/1251 ( 44%)]  Loss:  4.227212 (4.6858)  Time: 0.585s, 1750.37/s  (2.046s,  500.49/s)  LR: 2.632e-05  Data: 0.021 (1.445)
Train: 112 [ 600/1251 ( 48%)]  Loss:  4.706015 (4.6873)  Time: 6.428s,  159.29/s  (2.053s,  498.90/s)  LR: 2.632e-05  Data: 5.835 (1.452)
Train: 112 [ 650/1251 ( 52%)]  Loss:  4.653766 (4.6849)  Time: 0.588s, 1742.57/s  (2.048s,  500.00/s)  LR: 2.632e-05  Data: 0.022 (1.448)
Train: 112 [ 700/1251 ( 56%)]  Loss:  4.541965 (4.6754)  Time: 4.108s,  249.28/s  (2.047s,  500.33/s)  LR: 2.632e-05  Data: 3.415 (1.447)
Train: 112 [ 750/1251 ( 60%)]  Loss:  4.805397 (4.6835)  Time: 2.898s,  353.29/s  (2.041s,  501.71/s)  LR: 2.632e-05  Data: 2.335 (1.441)
Train: 112 [ 800/1251 ( 64%)]  Loss:  4.336091 (4.6631)  Time: 3.327s,  307.74/s  (2.061s,  496.95/s)  LR: 2.632e-05  Data: 2.627 (1.460)
Train: 112 [ 850/1251 ( 68%)]  Loss:  4.610237 (4.6602)  Time: 2.286s,  447.97/s  (2.064s,  496.03/s)  LR: 2.632e-05  Data: 1.570 (1.464)
Train: 112 [ 900/1251 ( 72%)]  Loss:  4.203474 (4.6361)  Time: 3.813s,  268.56/s  (2.066s,  495.64/s)  LR: 2.632e-05  Data: 3.137 (1.464)
Train: 112 [ 950/1251 ( 76%)]  Loss:  4.485918 (4.6286)  Time: 1.197s,  855.27/s  (2.066s,  495.62/s)  LR: 2.632e-05  Data: 0.634 (1.464)
Train: 112 [1000/1251 ( 80%)]  Loss:  4.638887 (4.6291)  Time: 5.444s,  188.11/s  (2.066s,  495.68/s)  LR: 2.632e-05  Data: 4.687 (1.464)
Train: 112 [1050/1251 ( 84%)]  Loss:  4.478600 (4.6223)  Time: 1.004s, 1020.02/s  (2.063s,  496.48/s)  LR: 2.632e-05  Data: 0.442 (1.461)
Train: 112 [1100/1251 ( 88%)]  Loss:  5.027054 (4.6399)  Time: 5.489s,  186.56/s  (2.063s,  496.31/s)  LR: 2.632e-05  Data: 4.912 (1.462)
Train: 112 [1150/1251 ( 92%)]  Loss:  4.759212 (4.6448)  Time: 1.037s,  987.74/s  (2.058s,  497.55/s)  LR: 2.632e-05  Data: 0.430 (1.457)
Train: 112 [1200/1251 ( 96%)]  Loss:  4.392352 (4.6347)  Time: 6.946s,  147.43/s  (2.069s,  494.84/s)  LR: 2.632e-05  Data: 6.319 (1.467)
Train: 112 [1250/1251 (100%)]  Loss:  4.989576 (4.6484)  Time: 0.563s, 1819.74/s  (2.064s,  496.06/s)  LR: 2.632e-05  Data: 0.000 (1.463)
Test: [   0/48]  Time: 13.777 (13.777)  Loss:  1.1215 (1.1215)  Acc@1: 77.0508 (77.0508)  Acc@5: 92.2852 (92.2852)
Test: [  48/48]  Time: 0.149 (3.138)  Loss:  1.1443 (2.0097)  Acc@1: 76.1792 (56.0660)  Acc@5: 90.4481 (80.0800)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-111.pth.tar', 56.30799994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-112.pth.tar', 56.06599999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-109.pth.tar', 56.008000021972656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-110.pth.tar', 55.91000014648438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-107.pth.tar', 55.64399999267578)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-108.pth.tar', 55.63799994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-106.pth.tar', 55.535999992675784)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-105.pth.tar', 55.51200002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-104.pth.tar', 55.44800004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-103.pth.tar', 55.04000007324219)

Train: 113 [   0/1251 (  0%)]  Loss:  5.052399 (5.0524)  Time: 9.701s,  105.55/s  (9.701s,  105.55/s)  LR: 2.323e-05  Data: 9.053 (9.053)
Train: 113 [  50/1251 (  4%)]  Loss:  4.997550 (5.0250)  Time: 0.587s, 1743.61/s  (2.099s,  487.83/s)  LR: 2.323e-05  Data: 0.022 (1.517)
Train: 113 [ 100/1251 (  8%)]  Loss:  4.303291 (4.7844)  Time: 0.586s, 1746.36/s  (2.055s,  498.28/s)  LR: 2.323e-05  Data: 0.022 (1.465)
Train: 113 [ 150/1251 ( 12%)]  Loss:  4.515585 (4.7172)  Time: 0.586s, 1746.42/s  (1.993s,  513.79/s)  LR: 2.323e-05  Data: 0.020 (1.406)
Train: 113 [ 200/1251 ( 16%)]  Loss:  4.252120 (4.6242)  Time: 1.104s,  927.66/s  (1.996s,  513.05/s)  LR: 2.323e-05  Data: 0.444 (1.405)
Train: 113 [ 250/1251 ( 20%)]  Loss:  4.224372 (4.5576)  Time: 0.586s, 1746.79/s  (1.970s,  519.90/s)  LR: 2.323e-05  Data: 0.020 (1.376)
Train: 113 [ 300/1251 ( 24%)]  Loss:  4.052215 (4.4854)  Time: 2.214s,  462.55/s  (1.966s,  520.88/s)  LR: 2.323e-05  Data: 1.537 (1.370)
Train: 113 [ 350/1251 ( 28%)]  Loss:  4.636202 (4.5042)  Time: 0.587s, 1744.79/s  (2.007s,  510.23/s)  LR: 2.323e-05  Data: 0.022 (1.408)
Train: 113 [ 400/1251 ( 32%)]  Loss:  5.108933 (4.5714)  Time: 1.920s,  533.31/s  (1.999s,  512.14/s)  LR: 2.323e-05  Data: 1.307 (1.401)
Train: 113 [ 450/1251 ( 36%)]  Loss:  4.997449 (4.6140)  Time: 0.586s, 1747.35/s  (2.009s,  509.70/s)  LR: 2.323e-05  Data: 0.023 (1.410)
Train: 113 [ 500/1251 ( 40%)]  Loss:  4.656012 (4.6178)  Time: 4.507s,  227.22/s  (2.010s,  509.49/s)  LR: 2.323e-05  Data: 3.828 (1.410)
Train: 113 [ 550/1251 ( 44%)]  Loss:  4.608053 (4.6170)  Time: 0.584s, 1752.19/s  (2.000s,  511.89/s)  LR: 2.323e-05  Data: 0.022 (1.401)
Train: 113 [ 600/1251 ( 48%)]  Loss:  4.138347 (4.5802)  Time: 2.022s,  506.34/s  (2.004s,  511.06/s)  LR: 2.323e-05  Data: 1.363 (1.405)
Train: 113 [ 650/1251 ( 52%)]  Loss:  4.591739 (4.5810)  Time: 0.589s, 1738.25/s  (2.003s,  511.23/s)  LR: 2.323e-05  Data: 0.024 (1.405)
Train: 113 [ 700/1251 ( 56%)]  Loss:  4.211259 (4.5564)  Time: 3.262s,  313.95/s  (2.004s,  510.88/s)  LR: 2.323e-05  Data: 2.651 (1.406)
Train: 113 [ 750/1251 ( 60%)]  Loss:  4.993471 (4.5837)  Time: 0.591s, 1733.60/s  (2.002s,  511.48/s)  LR: 2.323e-05  Data: 0.024 (1.404)
Train: 113 [ 800/1251 ( 64%)]  Loss:  4.738435 (4.5928)  Time: 3.753s,  272.84/s  (2.026s,  505.38/s)  LR: 2.323e-05  Data: 3.097 (1.427)
Train: 113 [ 850/1251 ( 68%)]  Loss:  4.795101 (4.6040)  Time: 0.586s, 1746.68/s  (2.025s,  505.75/s)  LR: 2.323e-05  Data: 0.020 (1.426)
Train: 113 [ 900/1251 ( 72%)]  Loss:  4.653625 (4.6066)  Time: 3.268s,  313.33/s  (2.028s,  504.84/s)  LR: 2.323e-05  Data: 2.695 (1.430)
Train: 113 [ 950/1251 ( 76%)]  Loss:  5.246387 (4.6386)  Time: 0.591s, 1733.27/s  (2.022s,  506.48/s)  LR: 2.323e-05  Data: 0.021 (1.423)
Train: 113 [1000/1251 ( 80%)]  Loss:  4.674516 (4.6403)  Time: 5.461s,  187.52/s  (2.025s,  505.80/s)  LR: 2.323e-05  Data: 4.742 (1.426)
Train: 113 [1050/1251 ( 84%)]  Loss:  4.671650 (4.6418)  Time: 0.586s, 1747.62/s  (2.019s,  507.31/s)  LR: 2.323e-05  Data: 0.019 (1.420)
Train: 113 [1100/1251 ( 88%)]  Loss:  5.008058 (4.6577)  Time: 5.509s,  185.87/s  (2.018s,  507.45/s)  LR: 2.323e-05  Data: 4.933 (1.420)
Train: 113 [1150/1251 ( 92%)]  Loss:  4.676994 (4.6585)  Time: 0.583s, 1755.42/s  (2.013s,  508.69/s)  LR: 2.323e-05  Data: 0.020 (1.415)
Train: 113 [1200/1251 ( 96%)]  Loss:  5.021330 (4.6730)  Time: 4.329s,  236.55/s  (2.024s,  505.86/s)  LR: 2.323e-05  Data: 3.761 (1.425)
Train: 113 [1250/1251 (100%)]  Loss:  5.044465 (4.6873)  Time: 0.562s, 1820.80/s  (2.024s,  506.05/s)  LR: 2.323e-05  Data: 0.000 (1.425)
Test: [   0/48]  Time: 12.737 (12.737)  Loss:  1.1129 (1.1129)  Acc@1: 76.8555 (76.8555)  Acc@5: 91.8945 (91.8945)
Test: [  48/48]  Time: 0.151 (3.086)  Loss:  1.1329 (1.9958)  Acc@1: 77.0047 (56.2760)  Acc@5: 90.3302 (80.0960)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-111.pth.tar', 56.30799994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-113.pth.tar', 56.27599996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-112.pth.tar', 56.06599999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-109.pth.tar', 56.008000021972656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-110.pth.tar', 55.91000014648438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-107.pth.tar', 55.64399999267578)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-108.pth.tar', 55.63799994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-106.pth.tar', 55.535999992675784)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-105.pth.tar', 55.51200002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-104.pth.tar', 55.44800004882813)

Train: 114 [   0/1251 (  0%)]  Loss:  4.856903 (4.8569)  Time: 9.309s,  110.00/s  (9.309s,  110.00/s)  LR: 2.047e-05  Data: 8.632 (8.632)
Train: 114 [  50/1251 (  4%)]  Loss:  4.301645 (4.5793)  Time: 0.588s, 1742.30/s  (2.052s,  498.94/s)  LR: 2.047e-05  Data: 0.022 (1.468)
Train: 114 [ 100/1251 (  8%)]  Loss:  4.747265 (4.6353)  Time: 0.587s, 1745.73/s  (2.042s,  501.42/s)  LR: 2.047e-05  Data: 0.023 (1.447)
Train: 114 [ 150/1251 ( 12%)]  Loss:  4.529824 (4.6089)  Time: 0.585s, 1749.05/s  (1.985s,  515.78/s)  LR: 2.047e-05  Data: 0.020 (1.392)
Train: 114 [ 200/1251 ( 16%)]  Loss:  4.472963 (4.5817)  Time: 2.361s,  433.63/s  (1.986s,  515.62/s)  LR: 2.047e-05  Data: 1.782 (1.392)
Train: 114 [ 250/1251 ( 20%)]  Loss:  4.732717 (4.6069)  Time: 0.583s, 1755.60/s  (1.956s,  523.58/s)  LR: 2.047e-05  Data: 0.021 (1.364)
Train: 114 [ 300/1251 ( 24%)]  Loss:  4.755928 (4.6282)  Time: 0.766s, 1336.45/s  (1.951s,  524.82/s)  LR: 2.047e-05  Data: 0.084 (1.357)
Train: 114 [ 350/1251 ( 28%)]  Loss:  4.528267 (4.6157)  Time: 0.585s, 1749.58/s  (1.986s,  515.57/s)  LR: 2.047e-05  Data: 0.020 (1.393)
Train: 114 [ 400/1251 ( 32%)]  Loss:  5.056580 (4.6647)  Time: 2.248s,  455.61/s  (1.985s,  515.75/s)  LR: 2.047e-05  Data: 1.667 (1.392)
Train: 114 [ 450/1251 ( 36%)]  Loss:  5.144341 (4.7126)  Time: 0.585s, 1749.21/s  (1.993s,  513.90/s)  LR: 2.047e-05  Data: 0.022 (1.400)
Train: 114 [ 500/1251 ( 40%)]  Loss:  4.867397 (4.7267)  Time: 2.480s,  412.83/s  (1.995s,  513.16/s)  LR: 2.047e-05  Data: 1.832 (1.400)
Train: 114 [ 550/1251 ( 44%)]  Loss:  5.075367 (4.7558)  Time: 0.589s, 1738.13/s  (1.987s,  515.38/s)  LR: 2.047e-05  Data: 0.024 (1.391)
Train: 114 [ 600/1251 ( 48%)]  Loss:  4.507046 (4.7366)  Time: 1.354s,  756.15/s  (1.993s,  513.77/s)  LR: 2.047e-05  Data: 0.792 (1.396)
Train: 114 [ 650/1251 ( 52%)]  Loss:  4.745349 (4.7373)  Time: 0.587s, 1745.42/s  (1.994s,  513.61/s)  LR: 2.047e-05  Data: 0.022 (1.395)
Train: 114 [ 700/1251 ( 56%)]  Loss:  4.864505 (4.7457)  Time: 1.134s,  903.10/s  (1.995s,  513.34/s)  LR: 2.047e-05  Data: 0.571 (1.395)
Train: 114 [ 750/1251 ( 60%)]  Loss:  4.543043 (4.7331)  Time: 0.590s, 1736.57/s  (1.991s,  514.20/s)  LR: 2.047e-05  Data: 0.026 (1.392)
Train: 114 [ 800/1251 ( 64%)]  Loss:  4.829070 (4.7387)  Time: 0.588s, 1742.56/s  (2.010s,  509.57/s)  LR: 2.047e-05  Data: 0.022 (1.409)
Train: 114 [ 850/1251 ( 68%)]  Loss:  4.237344 (4.7109)  Time: 0.587s, 1743.02/s  (2.006s,  510.52/s)  LR: 2.047e-05  Data: 0.024 (1.406)
Train: 114 [ 900/1251 ( 72%)]  Loss:  4.387537 (4.6938)  Time: 0.584s, 1752.02/s  (2.011s,  509.30/s)  LR: 2.047e-05  Data: 0.021 (1.410)
Train: 114 [ 950/1251 ( 76%)]  Loss:  5.066022 (4.7125)  Time: 0.591s, 1733.88/s  (2.009s,  509.60/s)  LR: 2.047e-05  Data: 0.026 (1.410)
Train: 114 [1000/1251 ( 80%)]  Loss:  4.463176 (4.7006)  Time: 0.585s, 1750.42/s  (2.009s,  509.63/s)  LR: 2.047e-05  Data: 0.022 (1.409)
Train: 114 [1050/1251 ( 84%)]  Loss:  4.721618 (4.7015)  Time: 0.591s, 1733.99/s  (2.008s,  510.07/s)  LR: 2.047e-05  Data: 0.020 (1.408)
Train: 114 [1100/1251 ( 88%)]  Loss:  4.926563 (4.7113)  Time: 0.586s, 1747.38/s  (2.005s,  510.63/s)  LR: 2.047e-05  Data: 0.021 (1.406)
Train: 114 [1150/1251 ( 92%)]  Loss:  5.088890 (4.7271)  Time: 0.589s, 1738.97/s  (2.005s,  510.82/s)  LR: 2.047e-05  Data: 0.025 (1.406)
Train: 114 [1200/1251 ( 96%)]  Loss:  4.839302 (4.7315)  Time: 0.587s, 1744.35/s  (2.002s,  511.55/s)  LR: 2.047e-05  Data: 0.023 (1.403)
Train: 114 [1250/1251 (100%)]  Loss:  4.660766 (4.7288)  Time: 0.563s, 1817.73/s  (2.018s,  507.38/s)  LR: 2.047e-05  Data: 0.000 (1.420)
Test: [   0/48]  Time: 13.292 (13.292)  Loss:  1.0854 (1.0854)  Acc@1: 77.7344 (77.7344)  Acc@5: 92.5781 (92.5781)
Test: [  48/48]  Time: 0.149 (3.142)  Loss:  1.1083 (1.9826)  Acc@1: 76.7689 (56.6180)  Acc@5: 90.6840 (80.2860)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-114.pth.tar', 56.617999992675784)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-111.pth.tar', 56.30799994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-113.pth.tar', 56.27599996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-112.pth.tar', 56.06599999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-109.pth.tar', 56.008000021972656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-110.pth.tar', 55.91000014648438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-107.pth.tar', 55.64399999267578)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-108.pth.tar', 55.63799994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-106.pth.tar', 55.535999992675784)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-105.pth.tar', 55.51200002197265)

Train: 115 [   0/1251 (  0%)]  Loss:  4.713178 (4.7132)  Time: 9.943s,  102.99/s  (9.943s,  102.99/s)  LR: 1.802e-05  Data: 8.627 (8.627)
Train: 115 [  50/1251 (  4%)]  Loss:  5.015567 (4.8644)  Time: 0.584s, 1753.39/s  (2.093s,  489.35/s)  LR: 1.802e-05  Data: 0.018 (1.488)
Train: 115 [ 100/1251 (  8%)]  Loss:  4.641801 (4.7902)  Time: 0.586s, 1746.98/s  (2.059s,  497.39/s)  LR: 1.802e-05  Data: 0.021 (1.467)
Train: 115 [ 150/1251 ( 12%)]  Loss:  5.108523 (4.8698)  Time: 0.583s, 1755.16/s  (2.002s,  511.51/s)  LR: 1.802e-05  Data: 0.019 (1.412)
Train: 115 [ 200/1251 ( 16%)]  Loss:  5.044609 (4.9047)  Time: 0.587s, 1743.94/s  (1.994s,  513.55/s)  LR: 1.802e-05  Data: 0.023 (1.407)
Train: 115 [ 250/1251 ( 20%)]  Loss:  4.412298 (4.8227)  Time: 0.586s, 1746.76/s  (1.971s,  519.46/s)  LR: 1.802e-05  Data: 0.022 (1.381)
Train: 115 [ 300/1251 ( 24%)]  Loss:  4.721098 (4.8082)  Time: 0.636s, 1609.64/s  (1.963s,  521.71/s)  LR: 1.802e-05  Data: 0.069 (1.375)
Train: 115 [ 350/1251 ( 28%)]  Loss:  4.166534 (4.7280)  Time: 0.585s, 1749.10/s  (1.997s,  512.87/s)  LR: 1.802e-05  Data: 0.023 (1.410)
Train: 115 [ 400/1251 ( 32%)]  Loss:  4.990335 (4.7571)  Time: 5.134s,  199.45/s  (2.000s,  512.00/s)  LR: 1.802e-05  Data: 4.433 (1.410)
Train: 115 [ 450/1251 ( 36%)]  Loss:  3.948577 (4.6763)  Time: 0.585s, 1750.94/s  (1.998s,  512.61/s)  LR: 1.802e-05  Data: 0.021 (1.408)
Train: 115 [ 500/1251 ( 40%)]  Loss:  4.107886 (4.6246)  Time: 5.644s,  181.43/s  (2.011s,  509.09/s)  LR: 1.802e-05  Data: 5.063 (1.421)
Train: 115 [ 550/1251 ( 44%)]  Loss:  4.209307 (4.5900)  Time: 0.585s, 1751.91/s  (2.000s,  512.04/s)  LR: 1.802e-05  Data: 0.020 (1.409)
Train: 115 [ 600/1251 ( 48%)]  Loss:  4.848310 (4.6098)  Time: 3.654s,  280.25/s  (2.006s,  510.45/s)  LR: 1.802e-05  Data: 2.983 (1.413)
Train: 115 [ 650/1251 ( 52%)]  Loss:  4.710158 (4.6170)  Time: 2.368s,  432.35/s  (2.007s,  510.26/s)  LR: 1.802e-05  Data: 1.695 (1.414)
Train: 115 [ 700/1251 ( 56%)]  Loss:  4.691323 (4.6220)  Time: 4.614s,  221.95/s  (2.008s,  510.05/s)  LR: 1.802e-05  Data: 4.051 (1.414)
Train: 115 [ 750/1251 ( 60%)]  Loss:  4.025873 (4.5847)  Time: 0.585s, 1751.43/s  (2.000s,  511.93/s)  LR: 1.802e-05  Data: 0.020 (1.404)
Train: 115 [ 800/1251 ( 64%)]  Loss:  4.621793 (4.5869)  Time: 3.945s,  259.59/s  (2.023s,  506.21/s)  LR: 1.802e-05  Data: 3.249 (1.425)
Train: 115 [ 850/1251 ( 68%)]  Loss:  4.829642 (4.6004)  Time: 0.586s, 1747.64/s  (2.016s,  507.90/s)  LR: 1.802e-05  Data: 0.020 (1.418)
Train: 115 [ 900/1251 ( 72%)]  Loss:  4.451253 (4.5925)  Time: 4.871s,  210.21/s  (2.024s,  506.03/s)  LR: 1.802e-05  Data: 4.213 (1.426)
Train: 115 [ 950/1251 ( 76%)]  Loss:  4.888508 (4.6073)  Time: 0.582s, 1758.55/s  (2.018s,  507.38/s)  LR: 1.802e-05  Data: 0.020 (1.421)
Train: 115 [1000/1251 ( 80%)]  Loss:  4.883594 (4.6205)  Time: 4.524s,  226.33/s  (2.019s,  507.24/s)  LR: 1.802e-05  Data: 3.960 (1.422)
Train: 115 [1050/1251 ( 84%)]  Loss:  4.231568 (4.6028)  Time: 0.586s, 1746.92/s  (2.016s,  507.85/s)  LR: 1.802e-05  Data: 0.020 (1.419)
Train: 115 [1100/1251 ( 88%)]  Loss:  4.414027 (4.5946)  Time: 4.840s,  211.58/s  (2.015s,  508.20/s)  LR: 1.802e-05  Data: 4.150 (1.417)
Train: 115 [1150/1251 ( 92%)]  Loss:  4.719249 (4.5998)  Time: 0.584s, 1753.43/s  (2.010s,  509.41/s)  LR: 1.802e-05  Data: 0.019 (1.413)
Train: 115 [1200/1251 ( 96%)]  Loss:  4.514423 (4.5964)  Time: 3.321s,  308.34/s  (2.010s,  509.40/s)  LR: 1.802e-05  Data: 2.729 (1.412)
Train: 115 [1250/1251 (100%)]  Loss:  4.466039 (4.5914)  Time: 0.564s, 1815.54/s  (2.020s,  506.96/s)  LR: 1.802e-05  Data: 0.000 (1.421)
Test: [   0/48]  Time: 13.546 (13.546)  Loss:  1.1405 (1.1405)  Acc@1: 77.2461 (77.2461)  Acc@5: 91.6016 (91.6016)
Test: [  48/48]  Time: 0.149 (3.084)  Loss:  1.1364 (1.9947)  Acc@1: 76.2972 (56.6280)  Acc@5: 91.0377 (80.3420)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-115.pth.tar', 56.62800004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-114.pth.tar', 56.617999992675784)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-111.pth.tar', 56.30799994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-113.pth.tar', 56.27599996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-112.pth.tar', 56.06599999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-109.pth.tar', 56.008000021972656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-110.pth.tar', 55.91000014648438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-107.pth.tar', 55.64399999267578)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-108.pth.tar', 55.63799994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-106.pth.tar', 55.535999992675784)

Train: 116 [   0/1251 (  0%)]  Loss:  3.826945 (3.8269)  Time: 9.390s,  109.05/s  (9.390s,  109.05/s)  LR: 1.590e-05  Data: 8.468 (8.468)
Train: 116 [  50/1251 (  4%)]  Loss:  4.148921 (3.9879)  Time: 0.585s, 1748.99/s  (2.018s,  507.43/s)  LR: 1.590e-05  Data: 0.019 (1.439)
Train: 116 [ 100/1251 (  8%)]  Loss:  4.668778 (4.2149)  Time: 1.332s,  768.71/s  (2.005s,  510.61/s)  LR: 1.590e-05  Data: 0.749 (1.425)
Train: 116 [ 150/1251 ( 12%)]  Loss:  4.737185 (4.3455)  Time: 0.587s, 1745.64/s  (1.955s,  523.72/s)  LR: 1.590e-05  Data: 0.020 (1.370)
Train: 116 [ 200/1251 ( 16%)]  Loss:  4.300688 (4.3365)  Time: 4.655s,  219.96/s  (1.949s,  525.46/s)  LR: 1.590e-05  Data: 4.009 (1.359)
Train: 116 [ 250/1251 ( 20%)]  Loss:  4.481759 (4.3607)  Time: 0.587s, 1745.31/s  (1.921s,  533.00/s)  LR: 1.590e-05  Data: 0.019 (1.329)
Train: 116 [ 300/1251 ( 24%)]  Loss:  4.864860 (4.4327)  Time: 4.485s,  228.30/s  (1.911s,  535.76/s)  LR: 1.590e-05  Data: 3.896 (1.317)
Train: 116 [ 350/1251 ( 28%)]  Loss:  4.982216 (4.5014)  Time: 0.588s, 1742.55/s  (1.896s,  540.18/s)  LR: 1.590e-05  Data: 0.019 (1.301)
Train: 116 [ 400/1251 ( 32%)]  Loss:  4.231916 (4.4715)  Time: 5.805s,  176.40/s  (1.925s,  532.08/s)  LR: 1.590e-05  Data: 5.236 (1.330)
Train: 116 [ 450/1251 ( 36%)]  Loss:  4.448059 (4.4691)  Time: 0.591s, 1734.06/s  (1.929s,  530.94/s)  LR: 1.590e-05  Data: 0.019 (1.336)
Train: 116 [ 500/1251 ( 40%)]  Loss:  4.482273 (4.4703)  Time: 6.352s,  161.20/s  (1.937s,  528.66/s)  LR: 1.590e-05  Data: 5.677 (1.342)
Train: 116 [ 550/1251 ( 44%)]  Loss:  4.789692 (4.4969)  Time: 0.588s, 1741.02/s  (1.937s,  528.53/s)  LR: 1.590e-05  Data: 0.019 (1.344)
Train: 116 [ 600/1251 ( 48%)]  Loss:  4.561503 (4.5019)  Time: 5.687s,  180.07/s  (1.937s,  528.57/s)  LR: 1.590e-05  Data: 5.027 (1.345)
Train: 116 [ 650/1251 ( 52%)]  Loss:  4.598345 (4.5088)  Time: 0.587s, 1744.82/s  (1.929s,  530.96/s)  LR: 1.590e-05  Data: 0.020 (1.337)
Train: 116 [ 700/1251 ( 56%)]  Loss:  4.508616 (4.5088)  Time: 5.581s,  183.48/s  (1.932s,  529.98/s)  LR: 1.590e-05  Data: 4.898 (1.341)
Train: 116 [ 750/1251 ( 60%)]  Loss:  4.370447 (4.5001)  Time: 0.583s, 1755.08/s  (1.923s,  532.54/s)  LR: 1.590e-05  Data: 0.021 (1.332)
Train: 116 [ 800/1251 ( 64%)]  Loss:  4.337950 (4.4906)  Time: 5.350s,  191.42/s  (1.916s,  534.44/s)  LR: 1.590e-05  Data: 4.755 (1.325)
Train: 116 [ 850/1251 ( 68%)]  Loss:  4.635715 (4.4987)  Time: 0.590s, 1734.50/s  (1.929s,  530.85/s)  LR: 1.590e-05  Data: 0.018 (1.338)
Train: 116 [ 900/1251 ( 72%)]  Loss:  4.535326 (4.5006)  Time: 7.261s,  141.03/s  (1.939s,  528.05/s)  LR: 1.590e-05  Data: 6.681 (1.348)
Train: 116 [ 950/1251 ( 76%)]  Loss:  4.571968 (4.5042)  Time: 0.590s, 1736.01/s  (1.940s,  527.83/s)  LR: 1.590e-05  Data: 0.019 (1.349)
Train: 116 [1000/1251 ( 80%)]  Loss:  4.257704 (4.4924)  Time: 6.101s,  167.85/s  (1.947s,  525.97/s)  LR: 1.590e-05  Data: 5.516 (1.356)
Train: 116 [1050/1251 ( 84%)]  Loss:  4.803610 (4.5066)  Time: 0.586s, 1747.65/s  (1.941s,  527.46/s)  LR: 1.590e-05  Data: 0.021 (1.351)
Train: 116 [1100/1251 ( 88%)]  Loss:  4.902620 (4.5238)  Time: 5.388s,  190.06/s  (1.941s,  527.49/s)  LR: 1.590e-05  Data: 4.806 (1.351)
Train: 116 [1150/1251 ( 92%)]  Loss:  4.210799 (4.5107)  Time: 0.589s, 1737.80/s  (1.935s,  529.30/s)  LR: 1.590e-05  Data: 0.020 (1.345)
Train: 116 [1200/1251 ( 96%)]  Loss:  4.598658 (4.5143)  Time: 5.357s,  191.15/s  (1.934s,  529.59/s)  LR: 1.590e-05  Data: 4.655 (1.344)
Train: 116 [1250/1251 (100%)]  Loss:  4.608142 (4.5179)  Time: 0.565s, 1811.03/s  (1.924s,  532.23/s)  LR: 1.590e-05  Data: 0.000 (1.335)
Test: [   0/48]  Time: 10.753 (10.753)  Loss:  1.0995 (1.0995)  Acc@1: 78.5156 (78.5156)  Acc@5: 91.9922 (91.9922)
Test: [  48/48]  Time: 0.149 (3.043)  Loss:  1.1105 (1.9795)  Acc@1: 76.7689 (56.6560)  Acc@5: 91.1557 (80.3540)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-116.pth.tar', 56.65599999267578)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-115.pth.tar', 56.62800004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-114.pth.tar', 56.617999992675784)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-111.pth.tar', 56.30799994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-113.pth.tar', 56.27599996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-112.pth.tar', 56.06599999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-109.pth.tar', 56.008000021972656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-110.pth.tar', 55.91000014648438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-107.pth.tar', 55.64399999267578)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-108.pth.tar', 55.63799994140625)

Train: 117 [   0/1251 (  0%)]  Loss:  4.674392 (4.6744)  Time: 8.898s,  115.08/s  (8.898s,  115.08/s)  LR: 1.410e-05  Data: 7.933 (7.933)
Train: 117 [  50/1251 (  4%)]  Loss:  4.390894 (4.5326)  Time: 0.590s, 1736.12/s  (2.150s,  476.34/s)  LR: 1.410e-05  Data: 0.020 (1.559)
Train: 117 [ 100/1251 (  8%)]  Loss:  4.895500 (4.6536)  Time: 0.587s, 1744.02/s  (2.060s,  497.04/s)  LR: 1.410e-05  Data: 0.019 (1.462)
Train: 117 [ 150/1251 ( 12%)]  Loss:  4.396810 (4.5894)  Time: 0.586s, 1746.97/s  (1.992s,  513.93/s)  LR: 1.410e-05  Data: 0.021 (1.401)
Train: 117 [ 200/1251 ( 16%)]  Loss:  5.029886 (4.6775)  Time: 0.585s, 1751.64/s  (1.970s,  519.84/s)  LR: 1.410e-05  Data: 0.019 (1.378)
Train: 117 [ 250/1251 ( 20%)]  Loss:  4.202322 (4.5983)  Time: 0.587s, 1744.24/s  (1.924s,  532.32/s)  LR: 1.410e-05  Data: 0.022 (1.334)
Train: 117 [ 300/1251 ( 24%)]  Loss:  4.719692 (4.6156)  Time: 0.584s, 1754.31/s  (1.906s,  537.33/s)  LR: 1.410e-05  Data: 0.018 (1.316)
Train: 117 [ 350/1251 ( 28%)]  Loss:  4.502064 (4.6014)  Time: 0.592s, 1731.17/s  (1.883s,  543.83/s)  LR: 1.410e-05  Data: 0.019 (1.294)
Train: 117 [ 400/1251 ( 32%)]  Loss:  4.959294 (4.6412)  Time: 0.587s, 1745.58/s  (1.871s,  547.38/s)  LR: 1.410e-05  Data: 0.022 (1.282)
Train: 117 [ 450/1251 ( 36%)]  Loss:  4.262683 (4.6034)  Time: 0.588s, 1740.61/s  (1.886s,  542.88/s)  LR: 1.410e-05  Data: 0.019 (1.298)
Train: 117 [ 500/1251 ( 40%)]  Loss:  4.417166 (4.5864)  Time: 0.586s, 1748.82/s  (1.899s,  539.36/s)  LR: 1.410e-05  Data: 0.018 (1.311)
Train: 117 [ 550/1251 ( 44%)]  Loss:  4.882013 (4.6111)  Time: 0.666s, 1538.56/s  (1.906s,  537.14/s)  LR: 1.410e-05  Data: 0.065 (1.319)
Train: 117 [ 600/1251 ( 48%)]  Loss:  4.709977 (4.6187)  Time: 0.588s, 1741.79/s  (1.918s,  533.96/s)  LR: 1.410e-05  Data: 0.020 (1.329)
Train: 117 [ 650/1251 ( 52%)]  Loss:  4.248439 (4.5922)  Time: 0.585s, 1751.58/s  (1.912s,  535.69/s)  LR: 1.410e-05  Data: 0.021 (1.324)
Train: 117 [ 700/1251 ( 56%)]  Loss:  4.197440 (4.5659)  Time: 0.587s, 1743.10/s  (1.913s,  535.18/s)  LR: 1.410e-05  Data: 0.024 (1.325)
Train: 117 [ 750/1251 ( 60%)]  Loss:  4.632953 (4.5701)  Time: 0.587s, 1745.22/s  (1.906s,  537.26/s)  LR: 1.410e-05  Data: 0.022 (1.317)
Train: 117 [ 800/1251 ( 64%)]  Loss:  4.976117 (4.5940)  Time: 0.586s, 1747.26/s  (1.902s,  538.40/s)  LR: 1.410e-05  Data: 0.020 (1.313)
Train: 117 [ 850/1251 ( 68%)]  Loss:  4.610744 (4.5949)  Time: 0.583s, 1755.17/s  (1.891s,  541.46/s)  LR: 1.410e-05  Data: 0.019 (1.303)
Train: 117 [ 900/1251 ( 72%)]  Loss:  5.038872 (4.6183)  Time: 0.584s, 1752.51/s  (1.895s,  540.47/s)  LR: 1.410e-05  Data: 0.020 (1.306)
Train: 117 [ 950/1251 ( 76%)]  Loss:  4.656676 (4.6202)  Time: 0.584s, 1753.83/s  (1.907s,  536.98/s)  LR: 1.410e-05  Data: 0.018 (1.318)
Train: 117 [1000/1251 ( 80%)]  Loss:  5.010633 (4.6388)  Time: 0.584s, 1754.65/s  (1.919s,  533.52/s)  LR: 1.410e-05  Data: 0.020 (1.329)
Train: 117 [1050/1251 ( 84%)]  Loss:  4.274962 (4.6223)  Time: 0.585s, 1748.98/s  (1.922s,  532.84/s)  LR: 1.410e-05  Data: 0.022 (1.332)
Train: 117 [1100/1251 ( 88%)]  Loss:  4.113047 (4.6001)  Time: 0.585s, 1750.93/s  (1.931s,  530.27/s)  LR: 1.410e-05  Data: 0.020 (1.342)
Train: 117 [1150/1251 ( 92%)]  Loss:  4.616006 (4.6008)  Time: 0.588s, 1740.82/s  (1.935s,  529.21/s)  LR: 1.410e-05  Data: 0.023 (1.346)
Train: 117 [1200/1251 ( 96%)]  Loss:  4.439331 (4.5943)  Time: 0.588s, 1740.22/s  (1.946s,  526.29/s)  LR: 1.410e-05  Data: 0.021 (1.357)
Train: 117 [1250/1251 (100%)]  Loss:  5.048836 (4.6118)  Time: 0.564s, 1816.04/s  (1.945s,  526.55/s)  LR: 1.410e-05  Data: 0.000 (1.357)
Test: [   0/48]  Time: 12.619 (12.619)  Loss:  1.1124 (1.1124)  Acc@1: 78.1250 (78.1250)  Acc@5: 92.0898 (92.0898)
Test: [  48/48]  Time: 0.149 (2.936)  Loss:  1.1296 (1.9834)  Acc@1: 76.5330 (56.6860)  Acc@5: 90.6840 (80.4340)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-117.pth.tar', 56.68600001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-116.pth.tar', 56.65599999267578)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-115.pth.tar', 56.62800004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-114.pth.tar', 56.617999992675784)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-111.pth.tar', 56.30799994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-113.pth.tar', 56.27599996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-112.pth.tar', 56.06599999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-109.pth.tar', 56.008000021972656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-110.pth.tar', 55.91000014648438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-107.pth.tar', 55.64399999267578)

Train: 118 [   0/1251 (  0%)]  Loss:  5.075986 (5.0760)  Time: 9.885s,  103.59/s  (9.885s,  103.59/s)  LR: 1.262e-05  Data: 8.743 (8.743)
Train: 118 [  50/1251 (  4%)]  Loss:  4.844976 (4.9605)  Time: 0.587s, 1745.77/s  (2.406s,  425.52/s)  LR: 1.262e-05  Data: 0.020 (1.812)
Train: 118 [ 100/1251 (  8%)]  Loss:  5.098475 (5.0065)  Time: 0.583s, 1755.08/s  (2.252s,  454.71/s)  LR: 1.262e-05  Data: 0.019 (1.656)
Train: 118 [ 150/1251 ( 12%)]  Loss:  4.917341 (4.9842)  Time: 0.585s, 1751.78/s  (2.166s,  472.67/s)  LR: 1.262e-05  Data: 0.022 (1.578)
Train: 118 [ 200/1251 ( 16%)]  Loss:  5.070269 (5.0014)  Time: 0.584s, 1752.23/s  (2.120s,  482.96/s)  LR: 1.262e-05  Data: 0.020 (1.532)
Train: 118 [ 250/1251 ( 20%)]  Loss:  4.380985 (4.8980)  Time: 0.584s, 1752.12/s  (2.088s,  490.45/s)  LR: 1.262e-05  Data: 0.022 (1.496)
Train: 118 [ 300/1251 ( 24%)]  Loss:  4.963807 (4.9074)  Time: 3.573s,  286.59/s  (2.073s,  494.03/s)  LR: 1.262e-05  Data: 3.011 (1.475)
Train: 118 [ 350/1251 ( 28%)]  Loss:  4.842635 (4.8993)  Time: 0.583s, 1755.78/s  (2.047s,  500.18/s)  LR: 1.262e-05  Data: 0.021 (1.445)
Train: 118 [ 400/1251 ( 32%)]  Loss:  4.596312 (4.8656)  Time: 3.327s,  307.79/s  (2.038s,  502.48/s)  LR: 1.262e-05  Data: 2.639 (1.435)
Train: 118 [ 450/1251 ( 36%)]  Loss:  4.845344 (4.8636)  Time: 0.585s, 1748.98/s  (2.016s,  507.89/s)  LR: 1.262e-05  Data: 0.022 (1.416)
Train: 118 [ 500/1251 ( 40%)]  Loss:  4.824485 (4.8601)  Time: 2.149s,  476.55/s  (2.053s,  498.89/s)  LR: 1.262e-05  Data: 1.586 (1.452)
Train: 118 [ 550/1251 ( 44%)]  Loss:  4.523751 (4.8320)  Time: 0.789s, 1297.98/s  (2.034s,  503.46/s)  LR: 1.262e-05  Data: 0.054 (1.434)
Train: 118 [ 600/1251 ( 48%)]  Loss:  4.568064 (4.8117)  Time: 0.586s, 1748.70/s  (2.052s,  498.97/s)  LR: 1.262e-05  Data: 0.022 (1.453)
Train: 118 [ 650/1251 ( 52%)]  Loss:  3.964391 (4.7512)  Time: 0.585s, 1750.25/s  (2.049s,  499.87/s)  LR: 1.262e-05  Data: 0.022 (1.451)
Train: 118 [ 700/1251 ( 56%)]  Loss:  4.927185 (4.7629)  Time: 0.583s, 1755.45/s  (2.054s,  498.46/s)  LR: 1.262e-05  Data: 0.018 (1.458)
Train: 118 [ 750/1251 ( 60%)]  Loss:  4.441759 (4.7429)  Time: 0.583s, 1756.93/s  (2.044s,  500.90/s)  LR: 1.262e-05  Data: 0.021 (1.449)
Train: 118 [ 800/1251 ( 64%)]  Loss:  4.742869 (4.7429)  Time: 0.653s, 1568.07/s  (2.041s,  501.71/s)  LR: 1.262e-05  Data: 0.081 (1.446)
Train: 118 [ 850/1251 ( 68%)]  Loss:  5.093153 (4.7623)  Time: 0.589s, 1739.22/s  (2.032s,  503.85/s)  LR: 1.262e-05  Data: 0.022 (1.437)
Train: 118 [ 900/1251 ( 72%)]  Loss:  4.906687 (4.7699)  Time: 2.473s,  414.03/s  (2.038s,  502.54/s)  LR: 1.262e-05  Data: 1.911 (1.442)
Train: 118 [ 950/1251 ( 76%)]  Loss:  4.576721 (4.7603)  Time: 0.585s, 1749.85/s  (2.039s,  502.12/s)  LR: 1.262e-05  Data: 0.023 (1.444)
Train: 118 [1000/1251 ( 80%)]  Loss:  4.149089 (4.7312)  Time: 0.586s, 1747.09/s  (2.039s,  502.24/s)  LR: 1.262e-05  Data: 0.020 (1.444)
Train: 118 [1050/1251 ( 84%)]  Loss:  4.491375 (4.7203)  Time: 0.587s, 1745.04/s  (2.037s,  502.69/s)  LR: 1.262e-05  Data: 0.025 (1.443)
Train: 118 [1100/1251 ( 88%)]  Loss:  4.986772 (4.7318)  Time: 0.734s, 1394.68/s  (2.035s,  503.13/s)  LR: 1.262e-05  Data: 0.172 (1.441)
Train: 118 [1150/1251 ( 92%)]  Loss:  4.980961 (4.7422)  Time: 0.583s, 1754.94/s  (2.033s,  503.60/s)  LR: 1.262e-05  Data: 0.017 (1.440)
Train: 118 [1200/1251 ( 96%)]  Loss:  4.695838 (4.7404)  Time: 2.229s,  459.38/s  (2.036s,  503.01/s)  LR: 1.262e-05  Data: 1.667 (1.442)
Train: 118 [1250/1251 (100%)]  Loss:  4.935752 (4.7479)  Time: 0.565s, 1811.66/s  (2.031s,  504.23/s)  LR: 1.262e-05  Data: 0.000 (1.437)
Test: [   0/48]  Time: 11.336 (11.336)  Loss:  1.0837 (1.0837)  Acc@1: 77.9297 (77.9297)  Acc@5: 92.5781 (92.5781)
Test: [  48/48]  Time: 0.149 (2.846)  Loss:  1.0884 (1.9775)  Acc@1: 77.4764 (56.6900)  Acc@5: 91.7453 (80.5480)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-118.pth.tar', 56.69000004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-117.pth.tar', 56.68600001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-116.pth.tar', 56.65599999267578)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-115.pth.tar', 56.62800004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-114.pth.tar', 56.617999992675784)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-111.pth.tar', 56.30799994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-113.pth.tar', 56.27599996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-112.pth.tar', 56.06599999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-109.pth.tar', 56.008000021972656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-110.pth.tar', 55.91000014648438)

Train: 119 [   0/1251 (  0%)]  Loss:  4.967958 (4.9680)  Time: 8.338s,  122.81/s  (8.338s,  122.81/s)  LR: 1.148e-05  Data: 7.762 (7.762)
Train: 119 [  50/1251 (  4%)]  Loss:  4.905883 (4.9369)  Time: 0.586s, 1747.07/s  (2.290s,  447.23/s)  LR: 1.148e-05  Data: 0.024 (1.697)
Train: 119 [ 100/1251 (  8%)]  Loss:  4.596347 (4.8234)  Time: 0.965s, 1061.29/s  (2.100s,  487.72/s)  LR: 1.148e-05  Data: 0.265 (1.509)
Train: 119 [ 150/1251 ( 12%)]  Loss:  4.611202 (4.7703)  Time: 0.585s, 1749.19/s  (2.060s,  497.18/s)  LR: 1.148e-05  Data: 0.023 (1.463)
Train: 119 [ 200/1251 ( 16%)]  Loss:  4.224858 (4.6612)  Time: 0.587s, 1745.66/s  (2.039s,  502.25/s)  LR: 1.148e-05  Data: 0.023 (1.437)
Train: 119 [ 250/1251 ( 20%)]  Loss:  4.684993 (4.6652)  Time: 0.586s, 1747.95/s  (2.009s,  509.80/s)  LR: 1.148e-05  Data: 0.023 (1.406)
Train: 119 [ 300/1251 ( 24%)]  Loss:  3.911041 (4.5575)  Time: 2.677s,  382.50/s  (1.999s,  512.38/s)  LR: 1.148e-05  Data: 2.115 (1.399)
Train: 119 [ 350/1251 ( 28%)]  Loss:  5.031525 (4.6167)  Time: 0.588s, 1742.61/s  (1.974s,  518.66/s)  LR: 1.148e-05  Data: 0.022 (1.374)
Train: 119 [ 400/1251 ( 32%)]  Loss:  4.980330 (4.6571)  Time: 5.316s,  192.62/s  (1.971s,  519.60/s)  LR: 1.148e-05  Data: 4.657 (1.370)
Train: 119 [ 450/1251 ( 36%)]  Loss:  4.414037 (4.6328)  Time: 0.589s, 1737.66/s  (1.953s,  524.45/s)  LR: 1.148e-05  Data: 0.021 (1.352)
Train: 119 [ 500/1251 ( 40%)]  Loss:  4.019893 (4.5771)  Time: 5.533s,  185.07/s  (1.985s,  515.97/s)  LR: 1.148e-05  Data: 4.868 (1.381)
Train: 119 [ 550/1251 ( 44%)]  Loss:  4.750360 (4.5915)  Time: 0.588s, 1742.89/s  (1.977s,  517.95/s)  LR: 1.148e-05  Data: 0.022 (1.374)
Train: 119 [ 600/1251 ( 48%)]  Loss:  4.962977 (4.6201)  Time: 4.176s,  245.21/s  (1.988s,  515.17/s)  LR: 1.148e-05  Data: 3.598 (1.385)
Train: 119 [ 650/1251 ( 52%)]  Loss:  4.586597 (4.6177)  Time: 0.587s, 1744.18/s  (1.992s,  513.93/s)  LR: 1.148e-05  Data: 0.022 (1.391)
Train: 119 [ 700/1251 ( 56%)]  Loss:  5.179725 (4.6552)  Time: 5.067s,  202.08/s  (1.997s,  512.78/s)  LR: 1.148e-05  Data: 4.472 (1.395)
Train: 119 [ 750/1251 ( 60%)]  Loss:  4.323468 (4.6344)  Time: 0.586s, 1747.27/s  (1.993s,  513.92/s)  LR: 1.148e-05  Data: 0.022 (1.390)
Train: 119 [ 800/1251 ( 64%)]  Loss:  4.955798 (4.6534)  Time: 5.590s,  183.20/s  (1.991s,  514.42/s)  LR: 1.148e-05  Data: 5.017 (1.389)
Train: 119 [ 850/1251 ( 68%)]  Loss:  4.738591 (4.6581)  Time: 0.587s, 1745.26/s  (1.982s,  516.70/s)  LR: 1.148e-05  Data: 0.023 (1.381)
Train: 119 [ 900/1251 ( 72%)]  Loss:  4.231114 (4.6356)  Time: 5.526s,  185.32/s  (1.980s,  517.28/s)  LR: 1.148e-05  Data: 4.963 (1.380)
Train: 119 [ 950/1251 ( 76%)]  Loss:  4.611073 (4.6344)  Time: 0.589s, 1738.33/s  (1.988s,  515.03/s)  LR: 1.148e-05  Data: 0.024 (1.389)
Train: 119 [1000/1251 ( 80%)]  Loss:  5.147183 (4.6588)  Time: 4.371s,  234.28/s  (1.993s,  513.87/s)  LR: 1.148e-05  Data: 3.711 (1.394)
Train: 119 [1050/1251 ( 84%)]  Loss:  4.049298 (4.6311)  Time: 0.586s, 1747.35/s  (1.992s,  514.16/s)  LR: 1.148e-05  Data: 0.024 (1.393)
Train: 119 [1100/1251 ( 88%)]  Loss:  4.719615 (4.6350)  Time: 0.806s, 1270.79/s  (1.994s,  513.54/s)  LR: 1.148e-05  Data: 0.158 (1.395)
Train: 119 [1150/1251 ( 92%)]  Loss:  3.935052 (4.6058)  Time: 0.587s, 1744.05/s  (1.992s,  513.97/s)  LR: 1.148e-05  Data: 0.025 (1.394)
Train: 119 [1200/1251 ( 96%)]  Loss:  4.456620 (4.5998)  Time: 0.587s, 1745.88/s  (1.998s,  512.61/s)  LR: 1.148e-05  Data: 0.023 (1.400)
Train: 119 [1250/1251 (100%)]  Loss:  4.974134 (4.6142)  Time: 0.564s, 1815.74/s  (1.994s,  513.52/s)  LR: 1.148e-05  Data: 0.000 (1.396)
Test: [   0/48]  Time: 11.701 (11.701)  Loss:  1.1010 (1.1010)  Acc@1: 77.6367 (77.6367)  Acc@5: 91.9922 (91.9922)
Test: [  48/48]  Time: 0.149 (2.900)  Loss:  1.1162 (1.9769)  Acc@1: 77.2406 (56.8140)  Acc@5: 90.8019 (80.5000)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-119.pth.tar', 56.81399993896484)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-118.pth.tar', 56.69000004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-117.pth.tar', 56.68600001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-116.pth.tar', 56.65599999267578)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-115.pth.tar', 56.62800004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-114.pth.tar', 56.617999992675784)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-111.pth.tar', 56.30799994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-113.pth.tar', 56.27599996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-112.pth.tar', 56.06599999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-109.pth.tar', 56.008000021972656)

Train: 120 [   0/1251 (  0%)]  Loss:  4.300272 (4.3003)  Time: 8.600s,  119.07/s  (8.600s,  119.07/s)  LR: 1.066e-05  Data: 8.006 (8.006)
Train: 120 [  50/1251 (  4%)]  Loss:  4.687450 (4.4939)  Time: 0.586s, 1746.24/s  (2.015s,  508.16/s)  LR: 1.066e-05  Data: 0.021 (1.435)
Train: 120 [ 100/1251 (  8%)]  Loss:  4.569450 (4.5191)  Time: 0.591s, 1732.64/s  (2.149s,  476.49/s)  LR: 1.066e-05  Data: 0.021 (1.560)
Train: 120 [ 150/1251 ( 12%)]  Loss:  4.884440 (4.6104)  Time: 0.586s, 1748.24/s  (2.043s,  501.22/s)  LR: 1.066e-05  Data: 0.023 (1.455)
Train: 120 [ 200/1251 ( 16%)]  Loss:  5.064919 (4.7013)  Time: 0.584s, 1754.05/s  (2.066s,  495.71/s)  LR: 1.066e-05  Data: 0.020 (1.476)
Train: 120 [ 250/1251 ( 20%)]  Loss:  4.578277 (4.6808)  Time: 0.596s, 1717.12/s  (2.022s,  506.36/s)  LR: 1.066e-05  Data: 0.026 (1.435)
Train: 120 [ 300/1251 ( 24%)]  Loss:  4.416732 (4.6431)  Time: 0.587s, 1745.74/s  (2.023s,  506.12/s)  LR: 1.066e-05  Data: 0.021 (1.436)
Train: 120 [ 350/1251 ( 28%)]  Loss:  4.915130 (4.6771)  Time: 1.834s,  558.49/s  (2.002s,  511.40/s)  LR: 1.066e-05  Data: 1.271 (1.415)
Train: 120 [ 400/1251 ( 32%)]  Loss:  4.464073 (4.6534)  Time: 0.585s, 1749.99/s  (1.994s,  513.58/s)  LR: 1.066e-05  Data: 0.021 (1.404)
Train: 120 [ 450/1251 ( 36%)]  Loss:  4.068926 (4.5950)  Time: 2.000s,  512.05/s  (1.982s,  516.75/s)  LR: 1.066e-05  Data: 1.419 (1.391)
Train: 120 [ 500/1251 ( 40%)]  Loss:  4.566325 (4.5924)  Time: 0.587s, 1744.84/s  (1.972s,  519.34/s)  LR: 1.066e-05  Data: 0.022 (1.380)
Train: 120 [ 550/1251 ( 44%)]  Loss:  4.569153 (4.5904)  Time: 6.186s,  165.54/s  (2.008s,  510.05/s)  LR: 1.066e-05  Data: 5.619 (1.413)
Train: 120 [ 600/1251 ( 48%)]  Loss:  4.769248 (4.6042)  Time: 0.657s, 1557.68/s  (2.000s,  512.02/s)  LR: 1.066e-05  Data: 0.047 (1.404)
Train: 120 [ 650/1251 ( 52%)]  Loss:  4.445138 (4.5928)  Time: 3.013s,  339.88/s  (2.015s,  508.11/s)  LR: 1.066e-05  Data: 2.344 (1.419)
Train: 120 [ 700/1251 ( 56%)]  Loss:  4.602489 (4.5935)  Time: 2.621s,  390.76/s  (2.016s,  507.89/s)  LR: 1.066e-05  Data: 2.034 (1.418)
Train: 120 [ 750/1251 ( 60%)]  Loss:  4.554670 (4.5910)  Time: 1.612s,  635.07/s  (2.014s,  508.40/s)  LR: 1.066e-05  Data: 1.048 (1.416)
Train: 120 [ 800/1251 ( 64%)]  Loss:  5.004449 (4.6154)  Time: 1.405s,  729.00/s  (2.012s,  509.01/s)  LR: 1.066e-05  Data: 0.739 (1.412)
Train: 120 [ 850/1251 ( 68%)]  Loss:  4.668845 (4.6183)  Time: 2.331s,  439.23/s  (2.007s,  510.30/s)  LR: 1.066e-05  Data: 1.658 (1.406)
Train: 120 [ 900/1251 ( 72%)]  Loss:  4.061522 (4.5890)  Time: 3.456s,  296.32/s  (2.003s,  511.31/s)  LR: 1.066e-05  Data: 2.893 (1.401)
Train: 120 [ 950/1251 ( 76%)]  Loss:  4.317687 (4.5755)  Time: 3.959s,  258.65/s  (1.998s,  512.51/s)  LR: 1.066e-05  Data: 3.397 (1.397)
Train: 120 [1000/1251 ( 80%)]  Loss:  4.824670 (4.5873)  Time: 6.234s,  164.25/s  (2.015s,  508.31/s)  LR: 1.066e-05  Data: 5.672 (1.414)
Train: 120 [1050/1251 ( 84%)]  Loss:  4.163852 (4.5681)  Time: 0.591s, 1732.23/s  (2.007s,  510.26/s)  LR: 1.066e-05  Data: 0.023 (1.407)
Train: 120 [1100/1251 ( 88%)]  Loss:  4.376599 (4.5598)  Time: 5.837s,  175.43/s  (2.012s,  508.93/s)  LR: 1.066e-05  Data: 5.152 (1.413)
Train: 120 [1150/1251 ( 92%)]  Loss:  4.427127 (4.5542)  Time: 0.587s, 1743.68/s  (2.009s,  509.77/s)  LR: 1.066e-05  Data: 0.023 (1.410)
Train: 120 [1200/1251 ( 96%)]  Loss:  4.863183 (4.5666)  Time: 6.121s,  167.29/s  (2.015s,  508.27/s)  LR: 1.066e-05  Data: 5.546 (1.417)
Train: 120 [1250/1251 (100%)]  Loss:  5.100624 (4.5871)  Time: 0.563s, 1819.73/s  (2.011s,  509.15/s)  LR: 1.066e-05  Data: 0.000 (1.413)
Test: [   0/48]  Time: 12.375 (12.375)  Loss:  1.0948 (1.0948)  Acc@1: 77.4414 (77.4414)  Acc@5: 92.4805 (92.4805)
Test: [  48/48]  Time: 0.149 (2.845)  Loss:  1.0918 (1.9737)  Acc@1: 77.2406 (56.7160)  Acc@5: 91.3915 (80.5800)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-119.pth.tar', 56.81399993896484)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-120.pth.tar', 56.71600006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-118.pth.tar', 56.69000004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-117.pth.tar', 56.68600001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-116.pth.tar', 56.65599999267578)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-115.pth.tar', 56.62800004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-114.pth.tar', 56.617999992675784)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-111.pth.tar', 56.30799994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-113.pth.tar', 56.27599996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-112.pth.tar', 56.06599999511719)

Train: 121 [   0/1251 (  0%)]  Loss:  5.056591 (5.0566)  Time: 9.844s,  104.02/s  (9.844s,  104.02/s)  LR: 1.016e-05  Data: 8.778 (8.778)
Train: 121 [  50/1251 (  4%)]  Loss:  4.764775 (4.9107)  Time: 0.584s, 1753.02/s  (1.990s,  514.47/s)  LR: 1.016e-05  Data: 0.020 (1.401)
Train: 121 [ 100/1251 (  8%)]  Loss:  4.690633 (4.8373)  Time: 0.587s, 1745.35/s  (2.142s,  478.12/s)  LR: 1.016e-05  Data: 0.020 (1.556)
Train: 121 [ 150/1251 ( 12%)]  Loss:  4.610544 (4.7806)  Time: 0.586s, 1748.71/s  (2.059s,  497.26/s)  LR: 1.016e-05  Data: 0.023 (1.468)
Train: 121 [ 200/1251 ( 16%)]  Loss:  4.433427 (4.7112)  Time: 0.586s, 1747.19/s  (2.061s,  496.77/s)  LR: 1.016e-05  Data: 0.019 (1.469)
Train: 121 [ 250/1251 ( 20%)]  Loss:  4.583307 (4.6899)  Time: 0.584s, 1753.78/s  (2.030s,  504.37/s)  LR: 1.016e-05  Data: 0.019 (1.439)
Train: 121 [ 300/1251 ( 24%)]  Loss:  4.636944 (4.6823)  Time: 0.586s, 1748.29/s  (2.023s,  506.23/s)  LR: 1.016e-05  Data: 0.021 (1.434)
Train: 121 [ 350/1251 ( 28%)]  Loss:  4.481095 (4.6572)  Time: 0.585s, 1749.67/s  (2.003s,  511.21/s)  LR: 1.016e-05  Data: 0.020 (1.415)
Train: 121 [ 400/1251 ( 32%)]  Loss:  4.847183 (4.6783)  Time: 0.586s, 1746.76/s  (1.997s,  512.86/s)  LR: 1.016e-05  Data: 0.020 (1.408)
Train: 121 [ 450/1251 ( 36%)]  Loss:  4.879617 (4.6984)  Time: 0.857s, 1194.58/s  (1.981s,  516.91/s)  LR: 1.016e-05  Data: 0.296 (1.393)
Train: 121 [ 500/1251 ( 40%)]  Loss:  4.445663 (4.6754)  Time: 0.588s, 1741.93/s  (1.974s,  518.75/s)  LR: 1.016e-05  Data: 0.018 (1.385)
Train: 121 [ 550/1251 ( 44%)]  Loss:  4.749783 (4.6816)  Time: 0.586s, 1746.57/s  (1.998s,  512.40/s)  LR: 1.016e-05  Data: 0.019 (1.407)
Train: 121 [ 600/1251 ( 48%)]  Loss:  5.217271 (4.7228)  Time: 0.590s, 1736.06/s  (2.002s,  511.57/s)  LR: 1.016e-05  Data: 0.020 (1.410)
Train: 121 [ 650/1251 ( 52%)]  Loss:  4.311365 (4.6934)  Time: 0.587s, 1744.31/s  (2.012s,  508.94/s)  LR: 1.016e-05  Data: 0.020 (1.418)
Train: 121 [ 700/1251 ( 56%)]  Loss:  4.393825 (4.6735)  Time: 0.587s, 1744.77/s  (2.016s,  507.87/s)  LR: 1.016e-05  Data: 0.020 (1.422)
Train: 121 [ 750/1251 ( 60%)]  Loss:  4.713923 (4.6760)  Time: 0.585s, 1749.00/s  (2.012s,  508.86/s)  LR: 1.016e-05  Data: 0.021 (1.418)
Train: 121 [ 800/1251 ( 64%)]  Loss:  4.875733 (4.6877)  Time: 0.585s, 1749.48/s  (2.012s,  508.86/s)  LR: 1.016e-05  Data: 0.020 (1.418)
Train: 121 [ 850/1251 ( 68%)]  Loss:  4.070057 (4.6534)  Time: 3.138s,  326.35/s  (2.007s,  510.21/s)  LR: 1.016e-05  Data: 2.521 (1.411)
Train: 121 [ 900/1251 ( 72%)]  Loss:  4.000887 (4.6191)  Time: 0.590s, 1734.85/s  (2.001s,  511.79/s)  LR: 1.016e-05  Data: 0.026 (1.403)
Train: 121 [ 950/1251 ( 76%)]  Loss:  4.449945 (4.6106)  Time: 3.132s,  326.93/s  (1.995s,  513.16/s)  LR: 1.016e-05  Data: 2.489 (1.397)
Train: 121 [1000/1251 ( 80%)]  Loss:  4.893587 (4.6241)  Time: 0.589s, 1738.88/s  (2.009s,  509.80/s)  LR: 1.016e-05  Data: 0.025 (1.409)
Train: 121 [1050/1251 ( 84%)]  Loss:  4.818706 (4.6329)  Time: 0.923s, 1109.98/s  (2.006s,  510.58/s)  LR: 1.016e-05  Data: 0.354 (1.406)
Train: 121 [1100/1251 ( 88%)]  Loss:  4.653131 (4.6338)  Time: 0.588s, 1741.25/s  (2.010s,  509.58/s)  LR: 1.016e-05  Data: 0.026 (1.410)
Train: 121 [1150/1251 ( 92%)]  Loss:  4.701371 (4.6366)  Time: 0.587s, 1743.69/s  (2.006s,  510.53/s)  LR: 1.016e-05  Data: 0.025 (1.407)
Train: 121 [1200/1251 ( 96%)]  Loss:  4.992911 (4.6509)  Time: 0.588s, 1740.57/s  (2.010s,  509.37/s)  LR: 1.016e-05  Data: 0.025 (1.412)
Train: 121 [1250/1251 (100%)]  Loss:  4.752279 (4.6548)  Time: 0.562s, 1820.56/s  (2.006s,  510.52/s)  LR: 1.016e-05  Data: 0.000 (1.408)
Test: [   0/48]  Time: 11.572 (11.572)  Loss:  1.0949 (1.0949)  Acc@1: 77.7344 (77.7344)  Acc@5: 91.9922 (91.9922)
Test: [  48/48]  Time: 0.149 (2.800)  Loss:  1.1267 (1.9771)  Acc@1: 77.0047 (56.8320)  Acc@5: 91.1557 (80.5520)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-121.pth.tar', 56.832000095214845)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-119.pth.tar', 56.81399993896484)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-120.pth.tar', 56.71600006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-118.pth.tar', 56.69000004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-117.pth.tar', 56.68600001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-116.pth.tar', 56.65599999267578)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-115.pth.tar', 56.62800004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-114.pth.tar', 56.617999992675784)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-111.pth.tar', 56.30799994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-113.pth.tar', 56.27599996582031)

*** Best metric: 56.832000095214845 (epoch 121)

wandb: Waiting for W&B process to finish, PID 53045
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210529_121414-PreTraining_vit_deit_tiny_patch16_224_1k/logs/debug.log
wandb: Find internal logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210529_121414-PreTraining_vit_deit_tiny_patch16_224_1k/logs/debug-internal.log
wandb: Run summary:
wandb:    eval_loss 1.97713
wandb:    eval_top1 56.832
wandb:    eval_top5 80.552
wandb:   _timestamp 1622317310
wandb:   train_loss 4.65479
wandb:        _step 121
wandb:        epoch 121
wandb:     _runtime 388515
wandb: Run history:
wandb:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   train_loss ‚ñÑ‚ñà‚ñÉ‚ñÜ‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñÉ‚ñÅ‚ñÑ‚ñà‚ñÑ‚ñÉ‚ñÖ
wandb:    eval_loss ‚ñà‚ñá‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    eval_top1 ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:    eval_top5 ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced PreTraining_vit_deit_tiny_patch16_224_1k: https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_1k
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
--End--
Sun May 30 04:42:02 JST 2021

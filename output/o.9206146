--Start--
Sun Jun 6 13:51:21 JST 2021
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 3
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
wandb: Currently logged in as: gyama_x (use `wandb login --relogin` to force relogin)
wandb: WARNING Tried to auto resume run with id PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k but id PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1 is set.
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.10.27
wandb: Resuming run PreTraining_vit_deit_tiny_patch16_224_fake_1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gyama_x/pytorch-image-models
wandb: üöÄ View run at https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run data is saved locally in /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210606_135127-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run `wandb offline` to turn off syncing.

Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar' (epoch 187)
Using native Torch DistributedDataParallel.
Scheduled epochs: 210
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Train: 188 [   0/1171 (  0%)]  Loss:  2.869595 (2.8696)  Time: 13.961s,   73.34/s  (13.961s,   73.34/s)  LR: 3.657e-05  Data: 12.781 (12.781)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 188 [  50/1171 (  4%)]  Loss:  2.913282 (2.8914)  Time: 0.583s, 1757.28/s  (2.334s,  438.70/s)  LR: 3.657e-05  Data: 0.018 (1.723)
Train: 188 [ 100/1171 (  9%)]  Loss:  2.947426 (2.9101)  Time: 2.637s,  388.30/s  (2.138s,  479.06/s)  LR: 3.657e-05  Data: 1.986 (1.531)
Train: 188 [ 150/1171 ( 13%)]  Loss:  2.509738 (2.8100)  Time: 0.583s, 1756.40/s  (2.117s,  483.73/s)  LR: 3.657e-05  Data: 0.020 (1.515)
Train: 188 [ 200/1171 ( 17%)]  Loss:  2.534971 (2.7550)  Time: 3.232s,  316.86/s  (2.078s,  492.86/s)  LR: 3.657e-05  Data: 2.426 (1.475)
Train: 188 [ 250/1171 ( 21%)]  Loss:  2.368723 (2.6906)  Time: 0.583s, 1755.25/s  (2.067s,  495.47/s)  LR: 3.657e-05  Data: 0.019 (1.464)
Train: 188 [ 300/1171 ( 26%)]  Loss:  2.942859 (2.7267)  Time: 1.703s,  601.20/s  (2.061s,  496.81/s)  LR: 3.657e-05  Data: 1.140 (1.457)
Train: 188 [ 350/1171 ( 30%)]  Loss:  2.526462 (2.7016)  Time: 0.583s, 1755.74/s  (2.071s,  494.54/s)  LR: 3.657e-05  Data: 0.021 (1.468)
Train: 188 [ 400/1171 ( 34%)]  Loss:  2.999870 (2.7348)  Time: 2.527s,  405.22/s  (2.077s,  493.05/s)  LR: 3.657e-05  Data: 1.965 (1.475)
Train: 188 [ 450/1171 ( 38%)]  Loss:  2.580977 (2.7194)  Time: 0.584s, 1753.13/s  (2.072s,  494.24/s)  LR: 3.657e-05  Data: 0.019 (1.471)
Train: 188 [ 500/1171 ( 43%)]  Loss:  2.592140 (2.7078)  Time: 6.286s,  162.90/s  (2.078s,  492.83/s)  LR: 3.657e-05  Data: 5.617 (1.477)
Train: 188 [ 550/1171 ( 47%)]  Loss:  2.658777 (2.7037)  Time: 0.585s, 1750.88/s  (2.068s,  495.11/s)  LR: 3.657e-05  Data: 0.018 (1.469)
Train: 188 [ 600/1171 ( 51%)]  Loss:  3.248043 (2.7456)  Time: 6.327s,  161.84/s  (2.067s,  495.44/s)  LR: 3.657e-05  Data: 5.668 (1.469)
Train: 188 [ 650/1171 ( 56%)]  Loss:  2.726938 (2.7443)  Time: 0.582s, 1759.42/s  (2.058s,  497.62/s)  LR: 3.657e-05  Data: 0.020 (1.461)
Train: 188 [ 700/1171 ( 60%)]  Loss:  2.847359 (2.7511)  Time: 6.811s,  150.35/s  (2.088s,  490.52/s)  LR: 3.657e-05  Data: 6.249 (1.491)
Train: 188 [ 750/1171 ( 64%)]  Loss:  3.151624 (2.7762)  Time: 0.583s, 1755.40/s  (2.092s,  489.38/s)  LR: 3.657e-05  Data: 0.020 (1.497)
Train: 188 [ 800/1171 ( 68%)]  Loss:  2.737724 (2.7739)  Time: 7.365s,  139.04/s  (2.109s,  485.64/s)  LR: 3.657e-05  Data: 6.803 (1.514)
Train: 188 [ 850/1171 ( 73%)]  Loss:  2.820691 (2.7765)  Time: 0.585s, 1751.10/s  (2.116s,  483.93/s)  LR: 3.657e-05  Data: 0.021 (1.522)
Train: 188 [ 900/1171 ( 77%)]  Loss:  2.964424 (2.7864)  Time: 6.497s,  157.62/s  (2.122s,  482.66/s)  LR: 3.657e-05  Data: 5.934 (1.529)
Train: 188 [ 950/1171 ( 81%)]  Loss:  2.583149 (2.7762)  Time: 0.585s, 1750.33/s  (2.123s,  482.44/s)  LR: 3.657e-05  Data: 0.019 (1.529)
Train: 188 [1000/1171 ( 85%)]  Loss:  2.322231 (2.7546)  Time: 6.171s,  165.94/s  (2.127s,  481.35/s)  LR: 3.657e-05  Data: 5.472 (1.535)
Train: 188 [1050/1171 ( 90%)]  Loss:  2.803712 (2.7569)  Time: 0.583s, 1755.91/s  (2.124s,  482.04/s)  LR: 3.657e-05  Data: 0.019 (1.531)
Train: 188 [1100/1171 ( 94%)]  Loss:  2.757770 (2.7569)  Time: 6.363s,  160.93/s  (2.145s,  477.35/s)  LR: 3.657e-05  Data: 5.802 (1.551)
Train: 188 [1150/1171 ( 98%)]  Loss:  2.431695 (2.7433)  Time: 0.584s, 1752.72/s  (2.149s,  476.43/s)  LR: 3.657e-05  Data: 0.017 (1.555)
Train: 188 [1170/1171 (100%)]  Loss:  2.807525 (2.7459)  Time: 0.563s, 1817.90/s  (2.152s,  475.90/s)  LR: 3.657e-05  Data: 0.000 (1.558)
Test: [   0/97]  Time: 13.725 (13.725)  Loss:  0.2682 (0.2682)  Acc@1: 97.2656 (97.2656)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.196 (3.095)  Loss:  0.4357 (0.3349)  Acc@1: 91.8945 (95.3393)  Acc@5: 98.6328 (98.9717)
Test: [  97/97]  Time: 0.494 (2.992)  Loss:  0.3099 (0.3490)  Acc@1: 94.7917 (94.8060)  Acc@5: 99.4048 (98.8440)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 189 [   0/1171 (  0%)]  Loss:  2.730507 (2.7305)  Time: 10.378s,   98.67/s  (10.378s,   98.67/s)  LR: 3.423e-05  Data: 9.291 (9.291)
Train: 189 [  50/1171 (  4%)]  Loss:  3.104414 (2.9175)  Time: 0.581s, 1762.78/s  (2.439s,  419.78/s)  LR: 3.423e-05  Data: 0.018 (1.850)
Train: 189 [ 100/1171 (  9%)]  Loss:  2.944406 (2.9264)  Time: 4.212s,  243.14/s  (2.410s,  424.91/s)  LR: 3.423e-05  Data: 3.635 (1.811)
Train: 189 [ 150/1171 ( 13%)]  Loss:  2.753321 (2.8832)  Time: 0.585s, 1751.13/s  (2.274s,  450.34/s)  LR: 3.423e-05  Data: 0.022 (1.675)
Train: 189 [ 200/1171 ( 17%)]  Loss:  2.643484 (2.8352)  Time: 3.088s,  331.55/s  (2.335s,  438.51/s)  LR: 3.423e-05  Data: 2.523 (1.735)
Train: 189 [ 250/1171 ( 21%)]  Loss:  2.557747 (2.7890)  Time: 0.584s, 1754.64/s  (2.328s,  439.87/s)  LR: 3.423e-05  Data: 0.021 (1.729)
Train: 189 [ 300/1171 ( 26%)]  Loss:  2.688493 (2.7746)  Time: 1.686s,  607.50/s  (2.346s,  436.46/s)  LR: 3.423e-05  Data: 1.116 (1.746)
Train: 189 [ 350/1171 ( 30%)]  Loss:  2.665340 (2.7610)  Time: 0.584s, 1754.51/s  (2.331s,  439.39/s)  LR: 3.423e-05  Data: 0.021 (1.731)
Train: 189 [ 400/1171 ( 34%)]  Loss:  3.162632 (2.8056)  Time: 0.586s, 1746.01/s  (2.326s,  440.29/s)  LR: 3.423e-05  Data: 0.017 (1.728)
Train: 189 [ 450/1171 ( 38%)]  Loss:  2.599531 (2.7850)  Time: 0.582s, 1758.66/s  (2.307s,  443.89/s)  LR: 3.423e-05  Data: 0.019 (1.710)
Train: 189 [ 500/1171 ( 43%)]  Loss:  2.946583 (2.7997)  Time: 0.585s, 1751.40/s  (2.307s,  443.96/s)  LR: 3.423e-05  Data: 0.019 (1.710)
Train: 189 [ 550/1171 ( 47%)]  Loss:  2.688604 (2.7904)  Time: 0.581s, 1762.59/s  (2.307s,  443.78/s)  LR: 3.423e-05  Data: 0.018 (1.711)
Train: 189 [ 600/1171 ( 51%)]  Loss:  3.027462 (2.8087)  Time: 0.584s, 1752.45/s  (2.341s,  437.37/s)  LR: 3.423e-05  Data: 0.018 (1.745)
Train: 189 [ 650/1171 ( 56%)]  Loss:  2.845961 (2.8113)  Time: 0.584s, 1752.95/s  (2.350s,  435.71/s)  LR: 3.423e-05  Data: 0.022 (1.756)
Train: 189 [ 700/1171 ( 60%)]  Loss:  2.798045 (2.8104)  Time: 0.584s, 1752.00/s  (2.355s,  434.76/s)  LR: 3.423e-05  Data: 0.019 (1.761)
Train: 189 [ 750/1171 ( 64%)]  Loss:  2.780400 (2.8086)  Time: 0.584s, 1754.17/s  (2.348s,  436.13/s)  LR: 3.423e-05  Data: 0.018 (1.754)
Train: 189 [ 800/1171 ( 68%)]  Loss:  2.743222 (2.8047)  Time: 0.586s, 1747.58/s  (2.341s,  437.40/s)  LR: 3.423e-05  Data: 0.022 (1.747)
Train: 189 [ 850/1171 ( 73%)]  Loss:  2.795224 (2.8042)  Time: 0.585s, 1749.55/s  (2.333s,  438.89/s)  LR: 3.423e-05  Data: 0.019 (1.739)
Train: 189 [ 900/1171 ( 77%)]  Loss:  2.892958 (2.8089)  Time: 0.584s, 1754.21/s  (2.325s,  440.44/s)  LR: 3.423e-05  Data: 0.021 (1.731)
Train: 189 [ 950/1171 ( 81%)]  Loss:  2.950021 (2.8159)  Time: 0.585s, 1751.65/s  (2.332s,  439.10/s)  LR: 3.423e-05  Data: 0.018 (1.738)
Train: 189 [1000/1171 ( 85%)]  Loss:  3.284010 (2.8382)  Time: 2.937s,  348.64/s  (2.334s,  438.75/s)  LR: 3.423e-05  Data: 2.204 (1.738)
Train: 189 [1050/1171 ( 90%)]  Loss:  3.247530 (2.8568)  Time: 2.793s,  366.60/s  (2.334s,  438.68/s)  LR: 3.423e-05  Data: 2.223 (1.738)
Train: 189 [1100/1171 ( 94%)]  Loss:  2.878079 (2.8577)  Time: 1.400s,  731.19/s  (2.333s,  438.98/s)  LR: 3.423e-05  Data: 0.740 (1.734)
Train: 189 [1150/1171 ( 98%)]  Loss:  2.633932 (2.8484)  Time: 1.053s,  972.75/s  (2.330s,  439.43/s)  LR: 3.423e-05  Data: 0.476 (1.731)
Train: 189 [1170/1171 (100%)]  Loss:  2.743854 (2.8442)  Time: 0.565s, 1812.05/s  (2.330s,  439.48/s)  LR: 3.423e-05  Data: 0.000 (1.730)
Test: [   0/97]  Time: 13.206 (13.206)  Loss:  0.2807 (0.2807)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.197 (3.002)  Loss:  0.4075 (0.3365)  Acc@1: 92.9688 (95.3719)  Acc@5: 98.5352 (98.9698)
Test: [  97/97]  Time: 0.120 (2.890)  Loss:  0.3224 (0.3483)  Acc@1: 94.0476 (94.8740)  Acc@5: 99.5536 (98.8480)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 190 [   0/1171 (  0%)]  Loss:  2.669693 (2.6697)  Time: 9.875s,  103.70/s  (9.875s,  103.70/s)  LR: 3.199e-05  Data: 9.253 (9.253)
Train: 190 [  50/1171 (  4%)]  Loss:  2.702344 (2.6860)  Time: 0.589s, 1738.78/s  (2.650s,  386.37/s)  LR: 3.199e-05  Data: 0.020 (2.072)
Train: 190 [ 100/1171 (  9%)]  Loss:  3.222105 (2.8647)  Time: 0.587s, 1745.47/s  (2.550s,  401.51/s)  LR: 3.199e-05  Data: 0.024 (1.970)
Train: 190 [ 150/1171 ( 13%)]  Loss:  2.293488 (2.7219)  Time: 0.586s, 1748.19/s  (2.449s,  418.07/s)  LR: 3.199e-05  Data: 0.024 (1.872)
Train: 190 [ 200/1171 ( 17%)]  Loss:  2.562088 (2.6899)  Time: 0.587s, 1744.01/s  (2.412s,  424.48/s)  LR: 3.199e-05  Data: 0.018 (1.833)
Train: 190 [ 250/1171 ( 21%)]  Loss:  2.669061 (2.6865)  Time: 0.585s, 1751.45/s  (2.354s,  435.00/s)  LR: 3.199e-05  Data: 0.020 (1.773)
Train: 190 [ 300/1171 ( 26%)]  Loss:  2.431005 (2.6500)  Time: 0.584s, 1752.77/s  (2.351s,  435.63/s)  LR: 3.199e-05  Data: 0.017 (1.768)
Train: 190 [ 350/1171 ( 30%)]  Loss:  2.728569 (2.6598)  Time: 0.597s, 1714.95/s  (2.313s,  442.68/s)  LR: 3.199e-05  Data: 0.034 (1.729)
Train: 190 [ 400/1171 ( 34%)]  Loss:  2.787885 (2.6740)  Time: 0.585s, 1749.86/s  (2.298s,  445.63/s)  LR: 3.199e-05  Data: 0.021 (1.714)
Train: 190 [ 450/1171 ( 38%)]  Loss:  2.656718 (2.6723)  Time: 0.582s, 1758.32/s  (2.320s,  441.31/s)  LR: 3.199e-05  Data: 0.020 (1.733)
Train: 190 [ 500/1171 ( 43%)]  Loss:  2.804749 (2.6843)  Time: 2.146s,  477.10/s  (2.360s,  433.83/s)  LR: 3.199e-05  Data: 1.483 (1.772)
Train: 190 [ 550/1171 ( 47%)]  Loss:  2.823846 (2.6960)  Time: 0.585s, 1750.60/s  (2.376s,  430.95/s)  LR: 3.199e-05  Data: 0.018 (1.785)
Train: 190 [ 600/1171 ( 51%)]  Loss:  2.650625 (2.6925)  Time: 7.618s,  134.42/s  (2.395s,  427.58/s)  LR: 3.199e-05  Data: 6.960 (1.803)
Train: 190 [ 650/1171 ( 56%)]  Loss:  2.852142 (2.7039)  Time: 0.583s, 1756.96/s  (2.386s,  429.20/s)  LR: 3.199e-05  Data: 0.020 (1.795)
Train: 190 [ 700/1171 ( 60%)]  Loss:  2.891826 (2.7164)  Time: 6.924s,  147.90/s  (2.387s,  429.02/s)  LR: 3.199e-05  Data: 6.259 (1.796)
Train: 190 [ 750/1171 ( 64%)]  Loss:  2.772033 (2.7199)  Time: 0.585s, 1751.37/s  (2.374s,  431.28/s)  LR: 3.199e-05  Data: 0.018 (1.783)
Train: 190 [ 800/1171 ( 68%)]  Loss:  2.779440 (2.7234)  Time: 7.306s,  140.15/s  (2.392s,  428.15/s)  LR: 3.199e-05  Data: 6.729 (1.800)
Train: 190 [ 850/1171 ( 73%)]  Loss:  2.259297 (2.6976)  Time: 0.588s, 1741.32/s  (2.390s,  428.53/s)  LR: 3.199e-05  Data: 0.021 (1.799)
Train: 190 [ 900/1171 ( 77%)]  Loss:  2.577216 (2.6913)  Time: 7.260s,  141.04/s  (2.396s,  427.37/s)  LR: 3.199e-05  Data: 6.582 (1.806)
Train: 190 [ 950/1171 ( 81%)]  Loss:  3.126130 (2.7130)  Time: 0.585s, 1750.27/s  (2.389s,  428.56/s)  LR: 3.199e-05  Data: 0.020 (1.800)
Train: 190 [1000/1171 ( 85%)]  Loss:  2.653971 (2.7102)  Time: 7.188s,  142.46/s  (2.388s,  428.77/s)  LR: 3.199e-05  Data: 6.597 (1.799)
Train: 190 [1050/1171 ( 90%)]  Loss:  2.854133 (2.7167)  Time: 0.583s, 1756.83/s  (2.380s,  430.25/s)  LR: 3.199e-05  Data: 0.020 (1.791)
Train: 190 [1100/1171 ( 94%)]  Loss:  3.210702 (2.7382)  Time: 6.895s,  148.51/s  (2.377s,  430.88/s)  LR: 3.199e-05  Data: 6.259 (1.788)
Train: 190 [1150/1171 ( 98%)]  Loss:  2.809616 (2.7412)  Time: 0.591s, 1733.33/s  (2.365s,  433.02/s)  LR: 3.199e-05  Data: 0.018 (1.776)
Train: 190 [1170/1171 (100%)]  Loss:  2.990417 (2.7512)  Time: 0.565s, 1810.88/s  (2.373s,  431.50/s)  LR: 3.199e-05  Data: 0.000 (1.785)
Test: [   0/97]  Time: 15.139 (15.139)  Loss:  0.3123 (0.3123)  Acc@1: 96.3867 (96.3867)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (3.239)  Loss:  0.4477 (0.3766)  Acc@1: 92.8711 (95.2742)  Acc@5: 98.5352 (98.9622)
Test: [  97/97]  Time: 0.120 (3.144)  Loss:  0.3539 (0.3830)  Acc@1: 94.1964 (94.8600)  Acc@5: 99.2560 (98.8580)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-190.pth.tar', 94.85999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 191 [   0/1171 (  0%)]  Loss:  3.005894 (3.0059)  Time: 11.201s,   91.42/s  (11.201s,   91.42/s)  LR: 2.986e-05  Data: 10.383 (10.383)
Train: 191 [  50/1171 (  4%)]  Loss:  2.864635 (2.9353)  Time: 0.590s, 1735.56/s  (2.357s,  434.54/s)  LR: 2.986e-05  Data: 0.020 (1.768)
Train: 191 [ 100/1171 (  9%)]  Loss:  2.749014 (2.8732)  Time: 0.732s, 1399.31/s  (2.309s,  443.56/s)  LR: 2.986e-05  Data: 0.050 (1.708)
Train: 191 [ 150/1171 ( 13%)]  Loss:  2.872023 (2.8729)  Time: 0.873s, 1173.45/s  (2.260s,  453.01/s)  LR: 2.986e-05  Data: 0.310 (1.654)
Train: 191 [ 200/1171 ( 17%)]  Loss:  2.435916 (2.7855)  Time: 3.880s,  263.92/s  (2.266s,  452.00/s)  LR: 2.986e-05  Data: 3.195 (1.660)
Train: 191 [ 250/1171 ( 21%)]  Loss:  2.827721 (2.7925)  Time: 0.583s, 1756.00/s  (2.226s,  459.97/s)  LR: 2.986e-05  Data: 0.019 (1.622)
Train: 191 [ 300/1171 ( 26%)]  Loss:  3.084616 (2.8343)  Time: 4.299s,  238.17/s  (2.319s,  441.57/s)  LR: 2.986e-05  Data: 3.696 (1.717)
Train: 191 [ 350/1171 ( 30%)]  Loss:  3.116214 (2.8695)  Time: 0.586s, 1747.64/s  (2.329s,  439.77/s)  LR: 2.986e-05  Data: 0.019 (1.727)
Train: 191 [ 400/1171 ( 34%)]  Loss:  2.716761 (2.8525)  Time: 1.246s,  821.78/s  (2.341s,  437.46/s)  LR: 2.986e-05  Data: 0.648 (1.741)
Train: 191 [ 450/1171 ( 38%)]  Loss:  2.999547 (2.8672)  Time: 0.585s, 1751.08/s  (2.339s,  437.73/s)  LR: 2.986e-05  Data: 0.019 (1.740)
Train: 191 [ 500/1171 ( 43%)]  Loss:  2.743469 (2.8560)  Time: 6.392s,  160.19/s  (2.347s,  436.32/s)  LR: 2.986e-05  Data: 5.812 (1.747)
Train: 191 [ 550/1171 ( 47%)]  Loss:  2.489735 (2.8255)  Time: 0.584s, 1752.72/s  (2.340s,  437.68/s)  LR: 2.986e-05  Data: 0.020 (1.739)
Train: 191 [ 600/1171 ( 51%)]  Loss:  2.338063 (2.7880)  Time: 6.711s,  152.59/s  (2.345s,  436.72/s)  LR: 2.986e-05  Data: 6.149 (1.745)
Train: 191 [ 650/1171 ( 56%)]  Loss:  3.008670 (2.8037)  Time: 0.582s, 1760.32/s  (2.364s,  433.24/s)  LR: 2.986e-05  Data: 0.018 (1.765)
Train: 191 [ 700/1171 ( 60%)]  Loss:  2.714504 (2.7978)  Time: 7.587s,  134.97/s  (2.379s,  430.37/s)  LR: 2.986e-05  Data: 6.856 (1.781)
Train: 191 [ 750/1171 ( 64%)]  Loss:  2.906489 (2.8046)  Time: 0.584s, 1752.46/s  (2.378s,  430.53/s)  LR: 2.986e-05  Data: 0.019 (1.782)
Train: 191 [ 800/1171 ( 68%)]  Loss:  3.028430 (2.8177)  Time: 7.908s,  129.49/s  (2.382s,  429.96/s)  LR: 2.986e-05  Data: 7.229 (1.785)
Train: 191 [ 850/1171 ( 73%)]  Loss:  3.066564 (2.8316)  Time: 0.586s, 1747.40/s  (2.376s,  430.89/s)  LR: 2.986e-05  Data: 0.019 (1.780)

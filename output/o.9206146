--Start--
Sun Jun 6 13:51:21 JST 2021
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 3
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
wandb: Currently logged in as: gyama_x (use `wandb login --relogin` to force relogin)
wandb: WARNING Tried to auto resume run with id PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k but id PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1 is set.
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.10.27
wandb: Resuming run PreTraining_vit_deit_tiny_patch16_224_fake_1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gyama_x/pytorch-image-models
wandb: üöÄ View run at https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run data is saved locally in /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210606_135127-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run `wandb offline` to turn off syncing.

Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar' (epoch 187)
Using native Torch DistributedDataParallel.
Scheduled epochs: 210
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Train: 188 [   0/1171 (  0%)]  Loss:  2.869595 (2.8696)  Time: 13.961s,   73.34/s  (13.961s,   73.34/s)  LR: 3.657e-05  Data: 12.781 (12.781)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 188 [  50/1171 (  4%)]  Loss:  2.913282 (2.8914)  Time: 0.583s, 1757.28/s  (2.334s,  438.70/s)  LR: 3.657e-05  Data: 0.018 (1.723)
Train: 188 [ 100/1171 (  9%)]  Loss:  2.947426 (2.9101)  Time: 2.637s,  388.30/s  (2.138s,  479.06/s)  LR: 3.657e-05  Data: 1.986 (1.531)
Train: 188 [ 150/1171 ( 13%)]  Loss:  2.509738 (2.8100)  Time: 0.583s, 1756.40/s  (2.117s,  483.73/s)  LR: 3.657e-05  Data: 0.020 (1.515)
Train: 188 [ 200/1171 ( 17%)]  Loss:  2.534971 (2.7550)  Time: 3.232s,  316.86/s  (2.078s,  492.86/s)  LR: 3.657e-05  Data: 2.426 (1.475)
Train: 188 [ 250/1171 ( 21%)]  Loss:  2.368723 (2.6906)  Time: 0.583s, 1755.25/s  (2.067s,  495.47/s)  LR: 3.657e-05  Data: 0.019 (1.464)
Train: 188 [ 300/1171 ( 26%)]  Loss:  2.942859 (2.7267)  Time: 1.703s,  601.20/s  (2.061s,  496.81/s)  LR: 3.657e-05  Data: 1.140 (1.457)
Train: 188 [ 350/1171 ( 30%)]  Loss:  2.526462 (2.7016)  Time: 0.583s, 1755.74/s  (2.071s,  494.54/s)  LR: 3.657e-05  Data: 0.021 (1.468)
Train: 188 [ 400/1171 ( 34%)]  Loss:  2.999870 (2.7348)  Time: 2.527s,  405.22/s  (2.077s,  493.05/s)  LR: 3.657e-05  Data: 1.965 (1.475)
Train: 188 [ 450/1171 ( 38%)]  Loss:  2.580977 (2.7194)  Time: 0.584s, 1753.13/s  (2.072s,  494.24/s)  LR: 3.657e-05  Data: 0.019 (1.471)
Train: 188 [ 500/1171 ( 43%)]  Loss:  2.592140 (2.7078)  Time: 6.286s,  162.90/s  (2.078s,  492.83/s)  LR: 3.657e-05  Data: 5.617 (1.477)
Train: 188 [ 550/1171 ( 47%)]  Loss:  2.658777 (2.7037)  Time: 0.585s, 1750.88/s  (2.068s,  495.11/s)  LR: 3.657e-05  Data: 0.018 (1.469)
Train: 188 [ 600/1171 ( 51%)]  Loss:  3.248043 (2.7456)  Time: 6.327s,  161.84/s  (2.067s,  495.44/s)  LR: 3.657e-05  Data: 5.668 (1.469)
Train: 188 [ 650/1171 ( 56%)]  Loss:  2.726938 (2.7443)  Time: 0.582s, 1759.42/s  (2.058s,  497.62/s)  LR: 3.657e-05  Data: 0.020 (1.461)
Train: 188 [ 700/1171 ( 60%)]  Loss:  2.847359 (2.7511)  Time: 6.811s,  150.35/s  (2.088s,  490.52/s)  LR: 3.657e-05  Data: 6.249 (1.491)
Train: 188 [ 750/1171 ( 64%)]  Loss:  3.151624 (2.7762)  Time: 0.583s, 1755.40/s  (2.092s,  489.38/s)  LR: 3.657e-05  Data: 0.020 (1.497)
Train: 188 [ 800/1171 ( 68%)]  Loss:  2.737724 (2.7739)  Time: 7.365s,  139.04/s  (2.109s,  485.64/s)  LR: 3.657e-05  Data: 6.803 (1.514)
Train: 188 [ 850/1171 ( 73%)]  Loss:  2.820691 (2.7765)  Time: 0.585s, 1751.10/s  (2.116s,  483.93/s)  LR: 3.657e-05  Data: 0.021 (1.522)
Train: 188 [ 900/1171 ( 77%)]  Loss:  2.964424 (2.7864)  Time: 6.497s,  157.62/s  (2.122s,  482.66/s)  LR: 3.657e-05  Data: 5.934 (1.529)
Train: 188 [ 950/1171 ( 81%)]  Loss:  2.583149 (2.7762)  Time: 0.585s, 1750.33/s  (2.123s,  482.44/s)  LR: 3.657e-05  Data: 0.019 (1.529)
Train: 188 [1000/1171 ( 85%)]  Loss:  2.322231 (2.7546)  Time: 6.171s,  165.94/s  (2.127s,  481.35/s)  LR: 3.657e-05  Data: 5.472 (1.535)
Train: 188 [1050/1171 ( 90%)]  Loss:  2.803712 (2.7569)  Time: 0.583s, 1755.91/s  (2.124s,  482.04/s)  LR: 3.657e-05  Data: 0.019 (1.531)
Train: 188 [1100/1171 ( 94%)]  Loss:  2.757770 (2.7569)  Time: 6.363s,  160.93/s  (2.145s,  477.35/s)  LR: 3.657e-05  Data: 5.802 (1.551)
Train: 188 [1150/1171 ( 98%)]  Loss:  2.431695 (2.7433)  Time: 0.584s, 1752.72/s  (2.149s,  476.43/s)  LR: 3.657e-05  Data: 0.017 (1.555)
Train: 188 [1170/1171 (100%)]  Loss:  2.807525 (2.7459)  Time: 0.563s, 1817.90/s  (2.152s,  475.90/s)  LR: 3.657e-05  Data: 0.000 (1.558)
Test: [   0/97]  Time: 13.725 (13.725)  Loss:  0.2682 (0.2682)  Acc@1: 97.2656 (97.2656)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.196 (3.095)  Loss:  0.4357 (0.3349)  Acc@1: 91.8945 (95.3393)  Acc@5: 98.6328 (98.9717)
Test: [  97/97]  Time: 0.494 (2.992)  Loss:  0.3099 (0.3490)  Acc@1: 94.7917 (94.8060)  Acc@5: 99.4048 (98.8440)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 189 [   0/1171 (  0%)]  Loss:  2.730507 (2.7305)  Time: 10.378s,   98.67/s  (10.378s,   98.67/s)  LR: 3.423e-05  Data: 9.291 (9.291)
Train: 189 [  50/1171 (  4%)]  Loss:  3.104414 (2.9175)  Time: 0.581s, 1762.78/s  (2.439s,  419.78/s)  LR: 3.423e-05  Data: 0.018 (1.850)
Train: 189 [ 100/1171 (  9%)]  Loss:  2.944406 (2.9264)  Time: 4.212s,  243.14/s  (2.410s,  424.91/s)  LR: 3.423e-05  Data: 3.635 (1.811)
Train: 189 [ 150/1171 ( 13%)]  Loss:  2.753321 (2.8832)  Time: 0.585s, 1751.13/s  (2.274s,  450.34/s)  LR: 3.423e-05  Data: 0.022 (1.675)
Train: 189 [ 200/1171 ( 17%)]  Loss:  2.643484 (2.8352)  Time: 3.088s,  331.55/s  (2.335s,  438.51/s)  LR: 3.423e-05  Data: 2.523 (1.735)
Train: 189 [ 250/1171 ( 21%)]  Loss:  2.557747 (2.7890)  Time: 0.584s, 1754.64/s  (2.328s,  439.87/s)  LR: 3.423e-05  Data: 0.021 (1.729)
Train: 189 [ 300/1171 ( 26%)]  Loss:  2.688493 (2.7746)  Time: 1.686s,  607.50/s  (2.346s,  436.46/s)  LR: 3.423e-05  Data: 1.116 (1.746)
Train: 189 [ 350/1171 ( 30%)]  Loss:  2.665340 (2.7610)  Time: 0.584s, 1754.51/s  (2.331s,  439.39/s)  LR: 3.423e-05  Data: 0.021 (1.731)
Train: 189 [ 400/1171 ( 34%)]  Loss:  3.162632 (2.8056)  Time: 0.586s, 1746.01/s  (2.326s,  440.29/s)  LR: 3.423e-05  Data: 0.017 (1.728)
Train: 189 [ 450/1171 ( 38%)]  Loss:  2.599531 (2.7850)  Time: 0.582s, 1758.66/s  (2.307s,  443.89/s)  LR: 3.423e-05  Data: 0.019 (1.710)
Train: 189 [ 500/1171 ( 43%)]  Loss:  2.946583 (2.7997)  Time: 0.585s, 1751.40/s  (2.307s,  443.96/s)  LR: 3.423e-05  Data: 0.019 (1.710)
Train: 189 [ 550/1171 ( 47%)]  Loss:  2.688604 (2.7904)  Time: 0.581s, 1762.59/s  (2.307s,  443.78/s)  LR: 3.423e-05  Data: 0.018 (1.711)
Train: 189 [ 600/1171 ( 51%)]  Loss:  3.027462 (2.8087)  Time: 0.584s, 1752.45/s  (2.341s,  437.37/s)  LR: 3.423e-05  Data: 0.018 (1.745)
Train: 189 [ 650/1171 ( 56%)]  Loss:  2.845961 (2.8113)  Time: 0.584s, 1752.95/s  (2.350s,  435.71/s)  LR: 3.423e-05  Data: 0.022 (1.756)
Train: 189 [ 700/1171 ( 60%)]  Loss:  2.798045 (2.8104)  Time: 0.584s, 1752.00/s  (2.355s,  434.76/s)  LR: 3.423e-05  Data: 0.019 (1.761)
Train: 189 [ 750/1171 ( 64%)]  Loss:  2.780400 (2.8086)  Time: 0.584s, 1754.17/s  (2.348s,  436.13/s)  LR: 3.423e-05  Data: 0.018 (1.754)
Train: 189 [ 800/1171 ( 68%)]  Loss:  2.743222 (2.8047)  Time: 0.586s, 1747.58/s  (2.341s,  437.40/s)  LR: 3.423e-05  Data: 0.022 (1.747)
Train: 189 [ 850/1171 ( 73%)]  Loss:  2.795224 (2.8042)  Time: 0.585s, 1749.55/s  (2.333s,  438.89/s)  LR: 3.423e-05  Data: 0.019 (1.739)
Train: 189 [ 900/1171 ( 77%)]  Loss:  2.892958 (2.8089)  Time: 0.584s, 1754.21/s  (2.325s,  440.44/s)  LR: 3.423e-05  Data: 0.021 (1.731)
Train: 189 [ 950/1171 ( 81%)]  Loss:  2.950021 (2.8159)  Time: 0.585s, 1751.65/s  (2.332s,  439.10/s)  LR: 3.423e-05  Data: 0.018 (1.738)
Train: 189 [1000/1171 ( 85%)]  Loss:  3.284010 (2.8382)  Time: 2.937s,  348.64/s  (2.334s,  438.75/s)  LR: 3.423e-05  Data: 2.204 (1.738)
Train: 189 [1050/1171 ( 90%)]  Loss:  3.247530 (2.8568)  Time: 2.793s,  366.60/s  (2.334s,  438.68/s)  LR: 3.423e-05  Data: 2.223 (1.738)
Train: 189 [1100/1171 ( 94%)]  Loss:  2.878079 (2.8577)  Time: 1.400s,  731.19/s  (2.333s,  438.98/s)  LR: 3.423e-05  Data: 0.740 (1.734)
Train: 189 [1150/1171 ( 98%)]  Loss:  2.633932 (2.8484)  Time: 1.053s,  972.75/s  (2.330s,  439.43/s)  LR: 3.423e-05  Data: 0.476 (1.731)
Train: 189 [1170/1171 (100%)]  Loss:  2.743854 (2.8442)  Time: 0.565s, 1812.05/s  (2.330s,  439.48/s)  LR: 3.423e-05  Data: 0.000 (1.730)
Test: [   0/97]  Time: 13.206 (13.206)  Loss:  0.2807 (0.2807)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.197 (3.002)  Loss:  0.4075 (0.3365)  Acc@1: 92.9688 (95.3719)  Acc@5: 98.5352 (98.9698)
Test: [  97/97]  Time: 0.120 (2.890)  Loss:  0.3224 (0.3483)  Acc@1: 94.0476 (94.8740)  Acc@5: 99.5536 (98.8480)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 190 [   0/1171 (  0%)]  Loss:  2.669693 (2.6697)  Time: 9.875s,  103.70/s  (9.875s,  103.70/s)  LR: 3.199e-05  Data: 9.253 (9.253)
Train: 190 [  50/1171 (  4%)]  Loss:  2.702344 (2.6860)  Time: 0.589s, 1738.78/s  (2.650s,  386.37/s)  LR: 3.199e-05  Data: 0.020 (2.072)
Train: 190 [ 100/1171 (  9%)]  Loss:  3.222105 (2.8647)  Time: 0.587s, 1745.47/s  (2.550s,  401.51/s)  LR: 3.199e-05  Data: 0.024 (1.970)
Train: 190 [ 150/1171 ( 13%)]  Loss:  2.293488 (2.7219)  Time: 0.586s, 1748.19/s  (2.449s,  418.07/s)  LR: 3.199e-05  Data: 0.024 (1.872)
Train: 190 [ 200/1171 ( 17%)]  Loss:  2.562088 (2.6899)  Time: 0.587s, 1744.01/s  (2.412s,  424.48/s)  LR: 3.199e-05  Data: 0.018 (1.833)
Train: 190 [ 250/1171 ( 21%)]  Loss:  2.669061 (2.6865)  Time: 0.585s, 1751.45/s  (2.354s,  435.00/s)  LR: 3.199e-05  Data: 0.020 (1.773)
Train: 190 [ 300/1171 ( 26%)]  Loss:  2.431005 (2.6500)  Time: 0.584s, 1752.77/s  (2.351s,  435.63/s)  LR: 3.199e-05  Data: 0.017 (1.768)
Train: 190 [ 350/1171 ( 30%)]  Loss:  2.728569 (2.6598)  Time: 0.597s, 1714.95/s  (2.313s,  442.68/s)  LR: 3.199e-05  Data: 0.034 (1.729)
Train: 190 [ 400/1171 ( 34%)]  Loss:  2.787885 (2.6740)  Time: 0.585s, 1749.86/s  (2.298s,  445.63/s)  LR: 3.199e-05  Data: 0.021 (1.714)
Train: 190 [ 450/1171 ( 38%)]  Loss:  2.656718 (2.6723)  Time: 0.582s, 1758.32/s  (2.320s,  441.31/s)  LR: 3.199e-05  Data: 0.020 (1.733)
Train: 190 [ 500/1171 ( 43%)]  Loss:  2.804749 (2.6843)  Time: 2.146s,  477.10/s  (2.360s,  433.83/s)  LR: 3.199e-05  Data: 1.483 (1.772)
Train: 190 [ 550/1171 ( 47%)]  Loss:  2.823846 (2.6960)  Time: 0.585s, 1750.60/s  (2.376s,  430.95/s)  LR: 3.199e-05  Data: 0.018 (1.785)
Train: 190 [ 600/1171 ( 51%)]  Loss:  2.650625 (2.6925)  Time: 7.618s,  134.42/s  (2.395s,  427.58/s)  LR: 3.199e-05  Data: 6.960 (1.803)
Train: 190 [ 650/1171 ( 56%)]  Loss:  2.852142 (2.7039)  Time: 0.583s, 1756.96/s  (2.386s,  429.20/s)  LR: 3.199e-05  Data: 0.020 (1.795)
Train: 190 [ 700/1171 ( 60%)]  Loss:  2.891826 (2.7164)  Time: 6.924s,  147.90/s  (2.387s,  429.02/s)  LR: 3.199e-05  Data: 6.259 (1.796)
Train: 190 [ 750/1171 ( 64%)]  Loss:  2.772033 (2.7199)  Time: 0.585s, 1751.37/s  (2.374s,  431.28/s)  LR: 3.199e-05  Data: 0.018 (1.783)
Train: 190 [ 800/1171 ( 68%)]  Loss:  2.779440 (2.7234)  Time: 7.306s,  140.15/s  (2.392s,  428.15/s)  LR: 3.199e-05  Data: 6.729 (1.800)
Train: 190 [ 850/1171 ( 73%)]  Loss:  2.259297 (2.6976)  Time: 0.588s, 1741.32/s  (2.390s,  428.53/s)  LR: 3.199e-05  Data: 0.021 (1.799)
Train: 190 [ 900/1171 ( 77%)]  Loss:  2.577216 (2.6913)  Time: 7.260s,  141.04/s  (2.396s,  427.37/s)  LR: 3.199e-05  Data: 6.582 (1.806)
Train: 190 [ 950/1171 ( 81%)]  Loss:  3.126130 (2.7130)  Time: 0.585s, 1750.27/s  (2.389s,  428.56/s)  LR: 3.199e-05  Data: 0.020 (1.800)
Train: 190 [1000/1171 ( 85%)]  Loss:  2.653971 (2.7102)  Time: 7.188s,  142.46/s  (2.388s,  428.77/s)  LR: 3.199e-05  Data: 6.597 (1.799)
Train: 190 [1050/1171 ( 90%)]  Loss:  2.854133 (2.7167)  Time: 0.583s, 1756.83/s  (2.380s,  430.25/s)  LR: 3.199e-05  Data: 0.020 (1.791)
Train: 190 [1100/1171 ( 94%)]  Loss:  3.210702 (2.7382)  Time: 6.895s,  148.51/s  (2.377s,  430.88/s)  LR: 3.199e-05  Data: 6.259 (1.788)
Train: 190 [1150/1171 ( 98%)]  Loss:  2.809616 (2.7412)  Time: 0.591s, 1733.33/s  (2.365s,  433.02/s)  LR: 3.199e-05  Data: 0.018 (1.776)
Train: 190 [1170/1171 (100%)]  Loss:  2.990417 (2.7512)  Time: 0.565s, 1810.88/s  (2.373s,  431.50/s)  LR: 3.199e-05  Data: 0.000 (1.785)
Test: [   0/97]  Time: 15.139 (15.139)  Loss:  0.3123 (0.3123)  Acc@1: 96.3867 (96.3867)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (3.239)  Loss:  0.4477 (0.3766)  Acc@1: 92.8711 (95.2742)  Acc@5: 98.5352 (98.9622)
Test: [  97/97]  Time: 0.120 (3.144)  Loss:  0.3539 (0.3830)  Acc@1: 94.1964 (94.8600)  Acc@5: 99.2560 (98.8580)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-190.pth.tar', 94.85999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 191 [   0/1171 (  0%)]  Loss:  3.005894 (3.0059)  Time: 11.201s,   91.42/s  (11.201s,   91.42/s)  LR: 2.986e-05  Data: 10.383 (10.383)
Train: 191 [  50/1171 (  4%)]  Loss:  2.864635 (2.9353)  Time: 0.590s, 1735.56/s  (2.357s,  434.54/s)  LR: 2.986e-05  Data: 0.020 (1.768)
Train: 191 [ 100/1171 (  9%)]  Loss:  2.749014 (2.8732)  Time: 0.732s, 1399.31/s  (2.309s,  443.56/s)  LR: 2.986e-05  Data: 0.050 (1.708)
Train: 191 [ 150/1171 ( 13%)]  Loss:  2.872023 (2.8729)  Time: 0.873s, 1173.45/s  (2.260s,  453.01/s)  LR: 2.986e-05  Data: 0.310 (1.654)
Train: 191 [ 200/1171 ( 17%)]  Loss:  2.435916 (2.7855)  Time: 3.880s,  263.92/s  (2.266s,  452.00/s)  LR: 2.986e-05  Data: 3.195 (1.660)
Train: 191 [ 250/1171 ( 21%)]  Loss:  2.827721 (2.7925)  Time: 0.583s, 1756.00/s  (2.226s,  459.97/s)  LR: 2.986e-05  Data: 0.019 (1.622)
Train: 191 [ 300/1171 ( 26%)]  Loss:  3.084616 (2.8343)  Time: 4.299s,  238.17/s  (2.319s,  441.57/s)  LR: 2.986e-05  Data: 3.696 (1.717)
Train: 191 [ 350/1171 ( 30%)]  Loss:  3.116214 (2.8695)  Time: 0.586s, 1747.64/s  (2.329s,  439.77/s)  LR: 2.986e-05  Data: 0.019 (1.727)
Train: 191 [ 400/1171 ( 34%)]  Loss:  2.716761 (2.8525)  Time: 1.246s,  821.78/s  (2.341s,  437.46/s)  LR: 2.986e-05  Data: 0.648 (1.741)
Train: 191 [ 450/1171 ( 38%)]  Loss:  2.999547 (2.8672)  Time: 0.585s, 1751.08/s  (2.339s,  437.73/s)  LR: 2.986e-05  Data: 0.019 (1.740)
Train: 191 [ 500/1171 ( 43%)]  Loss:  2.743469 (2.8560)  Time: 6.392s,  160.19/s  (2.347s,  436.32/s)  LR: 2.986e-05  Data: 5.812 (1.747)
Train: 191 [ 550/1171 ( 47%)]  Loss:  2.489735 (2.8255)  Time: 0.584s, 1752.72/s  (2.340s,  437.68/s)  LR: 2.986e-05  Data: 0.020 (1.739)
Train: 191 [ 600/1171 ( 51%)]  Loss:  2.338063 (2.7880)  Time: 6.711s,  152.59/s  (2.345s,  436.72/s)  LR: 2.986e-05  Data: 6.149 (1.745)
Train: 191 [ 650/1171 ( 56%)]  Loss:  3.008670 (2.8037)  Time: 0.582s, 1760.32/s  (2.364s,  433.24/s)  LR: 2.986e-05  Data: 0.018 (1.765)
Train: 191 [ 700/1171 ( 60%)]  Loss:  2.714504 (2.7978)  Time: 7.587s,  134.97/s  (2.379s,  430.37/s)  LR: 2.986e-05  Data: 6.856 (1.781)
Train: 191 [ 750/1171 ( 64%)]  Loss:  2.906489 (2.8046)  Time: 0.584s, 1752.46/s  (2.378s,  430.53/s)  LR: 2.986e-05  Data: 0.019 (1.782)
Train: 191 [ 800/1171 ( 68%)]  Loss:  3.028430 (2.8177)  Time: 7.908s,  129.49/s  (2.382s,  429.96/s)  LR: 2.986e-05  Data: 7.229 (1.785)
Train: 191 [ 850/1171 ( 73%)]  Loss:  3.066564 (2.8316)  Time: 0.586s, 1747.40/s  (2.376s,  430.89/s)  LR: 2.986e-05  Data: 0.019 (1.780)
Train: 191 [ 900/1171 ( 77%)]  Loss:  2.893536 (2.8348)  Time: 5.429s,  188.61/s  (2.372s,  431.70/s)  LR: 2.986e-05  Data: 4.759 (1.776)
Train: 191 [ 950/1171 ( 81%)]  Loss:  2.591660 (2.8227)  Time: 0.583s, 1757.01/s  (2.361s,  433.80/s)  LR: 2.986e-05  Data: 0.021 (1.764)
Train: 191 [1000/1171 ( 85%)]  Loss:  2.663649 (2.8151)  Time: 6.528s,  156.86/s  (2.350s,  435.66/s)  LR: 2.986e-05  Data: 5.964 (1.754)
Train: 191 [1050/1171 ( 90%)]  Loss:  2.924868 (2.8201)  Time: 0.586s, 1746.91/s  (2.362s,  433.54/s)  LR: 2.986e-05  Data: 0.021 (1.764)
Train: 191 [1100/1171 ( 94%)]  Loss:  2.827333 (2.8204)  Time: 5.573s,  183.76/s  (2.369s,  432.25/s)  LR: 2.986e-05  Data: 4.806 (1.770)
Train: 191 [1150/1171 ( 98%)]  Loss:  3.037720 (2.8295)  Time: 0.586s, 1748.02/s  (2.368s,  432.45/s)  LR: 2.986e-05  Data: 0.018 (1.769)
Train: 191 [1170/1171 (100%)]  Loss:  3.037030 (2.8378)  Time: 0.566s, 1809.47/s  (2.368s,  432.50/s)  LR: 2.986e-05  Data: 0.000 (1.769)
Test: [   0/97]  Time: 13.342 (13.342)  Loss:  0.2591 (0.2591)  Acc@1: 97.0703 (97.0703)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.197 (3.068)  Loss:  0.4185 (0.3265)  Acc@1: 93.0664 (95.4676)  Acc@5: 98.2422 (98.9928)
Test: [  97/97]  Time: 0.120 (2.960)  Loss:  0.3074 (0.3395)  Acc@1: 94.7917 (94.9800)  Acc@5: 99.4048 (98.8710)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-190.pth.tar', 94.85999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 192 [   0/1171 (  0%)]  Loss:  2.757354 (2.7574)  Time: 10.172s,  100.67/s  (10.172s,  100.67/s)  LR: 2.784e-05  Data: 9.364 (9.364)
Train: 192 [  50/1171 (  4%)]  Loss:  2.674491 (2.7159)  Time: 0.827s, 1238.70/s  (2.253s,  454.57/s)  LR: 2.784e-05  Data: 0.162 (1.641)
Train: 192 [ 100/1171 (  9%)]  Loss:  2.543864 (2.6586)  Time: 2.742s,  373.47/s  (2.387s,  429.02/s)  LR: 2.784e-05  Data: 2.177 (1.769)
Train: 192 [ 150/1171 ( 13%)]  Loss:  2.757117 (2.6832)  Time: 2.841s,  360.47/s  (2.389s,  428.66/s)  LR: 2.784e-05  Data: 2.272 (1.775)
Train: 192 [ 200/1171 ( 17%)]  Loss:  3.214539 (2.7895)  Time: 2.974s,  344.32/s  (2.393s,  427.94/s)  LR: 2.784e-05  Data: 2.409 (1.779)
Train: 192 [ 250/1171 ( 21%)]  Loss:  2.593409 (2.7568)  Time: 2.867s,  357.19/s  (2.351s,  435.51/s)  LR: 2.784e-05  Data: 2.157 (1.741)
Train: 192 [ 300/1171 ( 26%)]  Loss:  2.957979 (2.7855)  Time: 0.589s, 1739.97/s  (2.337s,  438.12/s)  LR: 2.784e-05  Data: 0.019 (1.728)
Train: 192 [ 350/1171 ( 30%)]  Loss:  2.592787 (2.7614)  Time: 4.751s,  215.54/s  (2.332s,  439.11/s)  LR: 2.784e-05  Data: 4.177 (1.721)
Train: 192 [ 400/1171 ( 34%)]  Loss:  3.087431 (2.7977)  Time: 0.588s, 1740.88/s  (2.315s,  442.39/s)  LR: 2.784e-05  Data: 0.018 (1.703)
Train: 192 [ 450/1171 ( 38%)]  Loss:  2.685796 (2.7865)  Time: 6.244s,  164.00/s  (2.310s,  443.37/s)  LR: 2.784e-05  Data: 5.522 (1.698)
Train: 192 [ 500/1171 ( 43%)]  Loss:  2.849681 (2.7922)  Time: 0.586s, 1747.21/s  (2.334s,  438.81/s)  LR: 2.784e-05  Data: 0.019 (1.724)
Train: 192 [ 550/1171 ( 47%)]  Loss:  2.859117 (2.7978)  Time: 5.791s,  176.83/s  (2.373s,  431.44/s)  LR: 2.784e-05  Data: 5.200 (1.764)
Train: 192 [ 600/1171 ( 51%)]  Loss:  3.061148 (2.8181)  Time: 0.584s, 1754.51/s  (2.388s,  428.73/s)  LR: 2.784e-05  Data: 0.017 (1.780)
Train: 192 [ 650/1171 ( 56%)]  Loss:  3.151846 (2.8419)  Time: 0.588s, 1742.48/s  (2.405s,  425.84/s)  LR: 2.784e-05  Data: 0.019 (1.798)
Train: 192 [ 700/1171 ( 60%)]  Loss:  2.943206 (2.8487)  Time: 0.584s, 1752.43/s  (2.399s,  426.76/s)  LR: 2.784e-05  Data: 0.019 (1.794)
Train: 192 [ 750/1171 ( 64%)]  Loss:  2.569348 (2.8312)  Time: 0.585s, 1749.34/s  (2.396s,  427.31/s)  LR: 2.784e-05  Data: 0.017 (1.793)
Train: 192 [ 800/1171 ( 68%)]  Loss:  3.006528 (2.8415)  Time: 0.587s, 1743.78/s  (2.382s,  429.82/s)  LR: 2.784e-05  Data: 0.019 (1.780)
Train: 192 [ 850/1171 ( 73%)]  Loss:  2.695258 (2.8334)  Time: 0.585s, 1750.93/s  (2.394s,  427.80/s)  LR: 2.784e-05  Data: 0.018 (1.792)
Train: 192 [ 900/1171 ( 77%)]  Loss:  2.885871 (2.8361)  Time: 0.583s, 1757.63/s  (2.394s,  427.79/s)  LR: 2.784e-05  Data: 0.021 (1.792)
Train: 192 [ 950/1171 ( 81%)]  Loss:  3.202065 (2.8544)  Time: 0.588s, 1740.28/s  (2.400s,  426.73/s)  LR: 2.784e-05  Data: 0.019 (1.798)
Train: 192 [1000/1171 ( 85%)]  Loss:  2.579258 (2.8413)  Time: 0.583s, 1755.08/s  (2.390s,  428.40/s)  LR: 2.784e-05  Data: 0.021 (1.789)
Train: 192 [1050/1171 ( 90%)]  Loss:  2.252475 (2.8146)  Time: 3.011s,  340.11/s  (2.389s,  428.69/s)  LR: 2.784e-05  Data: 2.449 (1.788)
Train: 192 [1100/1171 ( 94%)]  Loss:  2.923633 (2.8193)  Time: 0.580s, 1764.57/s  (2.378s,  430.55/s)  LR: 2.784e-05  Data: 0.018 (1.777)
Train: 192 [1150/1171 ( 98%)]  Loss:  3.020586 (2.8277)  Time: 0.590s, 1735.01/s  (2.375s,  431.14/s)  LR: 2.784e-05  Data: 0.019 (1.774)
Train: 192 [1170/1171 (100%)]  Loss:  2.564308 (2.8172)  Time: 0.564s, 1814.02/s  (2.372s,  431.73/s)  LR: 2.784e-05  Data: 0.000 (1.771)
Test: [   0/97]  Time: 12.099 (12.099)  Loss:  0.2913 (0.2913)  Acc@1: 96.9727 (96.9727)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.198 (3.147)  Loss:  0.4252 (0.3602)  Acc@1: 93.4570 (95.3316)  Acc@5: 98.5352 (98.9737)
Test: [  97/97]  Time: 0.119 (3.060)  Loss:  0.3268 (0.3690)  Acc@1: 93.8988 (94.9250)  Acc@5: 99.4048 (98.8590)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-192.pth.tar', 94.92500001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-190.pth.tar', 94.85999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 193 [   0/1171 (  0%)]  Loss:  2.607219 (2.6072)  Time: 9.154s,  111.87/s  (9.154s,  111.87/s)  LR: 2.592e-05  Data: 8.475 (8.475)
Train: 193 [  50/1171 (  4%)]  Loss:  2.447003 (2.5271)  Time: 0.587s, 1744.41/s  (2.350s,  435.77/s)  LR: 2.592e-05  Data: 0.019 (1.754)
Train: 193 [ 100/1171 (  9%)]  Loss:  2.805738 (2.6200)  Time: 0.587s, 1743.06/s  (2.332s,  439.07/s)  LR: 2.592e-05  Data: 0.019 (1.742)
Train: 193 [ 150/1171 ( 13%)]  Loss:  3.072420 (2.7331)  Time: 0.585s, 1751.37/s  (2.278s,  449.49/s)  LR: 2.592e-05  Data: 0.021 (1.689)
Train: 193 [ 200/1171 ( 17%)]  Loss:  2.409353 (2.6683)  Time: 1.334s,  767.55/s  (2.268s,  451.46/s)  LR: 2.592e-05  Data: 0.287 (1.675)
Train: 193 [ 250/1171 ( 21%)]  Loss:  2.601033 (2.6571)  Time: 0.584s, 1752.39/s  (2.241s,  457.04/s)  LR: 2.592e-05  Data: 0.021 (1.650)
Train: 193 [ 300/1171 ( 26%)]  Loss:  3.280618 (2.7462)  Time: 1.300s,  787.58/s  (2.238s,  457.54/s)  LR: 2.592e-05  Data: 0.738 (1.645)
Train: 193 [ 350/1171 ( 30%)]  Loss:  3.299001 (2.8153)  Time: 0.581s, 1762.50/s  (2.273s,  450.49/s)  LR: 2.592e-05  Data: 0.019 (1.681)
Train: 193 [ 400/1171 ( 34%)]  Loss:  2.869695 (2.8213)  Time: 0.585s, 1749.89/s  (2.313s,  442.67/s)  LR: 2.592e-05  Data: 0.019 (1.723)
Train: 193 [ 450/1171 ( 38%)]  Loss:  2.945346 (2.8337)  Time: 0.583s, 1756.07/s  (2.322s,  441.00/s)  LR: 2.592e-05  Data: 0.018 (1.730)
Train: 193 [ 500/1171 ( 43%)]  Loss:  2.637925 (2.8159)  Time: 2.677s,  382.55/s  (2.343s,  437.09/s)  LR: 2.592e-05  Data: 2.027 (1.750)
Train: 193 [ 550/1171 ( 47%)]  Loss:  2.813928 (2.8158)  Time: 0.584s, 1754.14/s  (2.346s,  436.45/s)  LR: 2.592e-05  Data: 0.018 (1.753)
Train: 193 [ 600/1171 ( 51%)]  Loss:  2.935879 (2.8250)  Time: 4.832s,  211.93/s  (2.360s,  433.85/s)  LR: 2.592e-05  Data: 4.237 (1.766)
Train: 193 [ 650/1171 ( 56%)]  Loss:  2.919106 (2.8317)  Time: 0.587s, 1744.93/s  (2.352s,  435.43/s)  LR: 2.592e-05  Data: 0.019 (1.755)
Train: 193 [ 700/1171 ( 60%)]  Loss:  2.628658 (2.8182)  Time: 3.378s,  303.15/s  (2.371s,  431.93/s)  LR: 2.592e-05  Data: 2.702 (1.771)
Train: 193 [ 750/1171 ( 64%)]  Loss:  2.722079 (2.8122)  Time: 0.588s, 1740.88/s  (2.377s,  430.75/s)  LR: 2.592e-05  Data: 0.020 (1.777)
Train: 193 [ 800/1171 ( 68%)]  Loss:  2.969154 (2.8214)  Time: 4.063s,  252.04/s  (2.380s,  430.17/s)  LR: 2.592e-05  Data: 3.454 (1.779)
Train: 193 [ 850/1171 ( 73%)]  Loss:  3.007814 (2.8318)  Time: 0.858s, 1193.96/s  (2.382s,  429.90/s)  LR: 2.592e-05  Data: 0.270 (1.780)
Train: 193 [ 900/1171 ( 77%)]  Loss:  2.641653 (2.8218)  Time: 1.136s,  901.75/s  (2.376s,  430.99/s)  LR: 2.592e-05  Data: 0.459 (1.774)
Train: 193 [ 950/1171 ( 81%)]  Loss:  3.038750 (2.8326)  Time: 0.585s, 1751.52/s  (2.375s,  431.20/s)  LR: 2.592e-05  Data: 0.020 (1.773)
Train: 193 [1000/1171 ( 85%)]  Loss:  3.099591 (2.8453)  Time: 0.588s, 1742.17/s  (2.365s,  432.97/s)  LR: 2.592e-05  Data: 0.018 (1.763)
Train: 193 [1050/1171 ( 90%)]  Loss:  3.110873 (2.8574)  Time: 0.586s, 1747.17/s  (2.362s,  433.60/s)  LR: 2.592e-05  Data: 0.017 (1.759)
Train: 193 [1100/1171 ( 94%)]  Loss:  3.067892 (2.8666)  Time: 0.591s, 1733.12/s  (2.370s,  432.14/s)  LR: 2.592e-05  Data: 0.021 (1.767)
Train: 193 [1150/1171 ( 98%)]  Loss:  3.048208 (2.8741)  Time: 0.587s, 1743.92/s  (2.376s,  430.92/s)  LR: 2.592e-05  Data: 0.020 (1.775)
Train: 193 [1170/1171 (100%)]  Loss:  3.278320 (2.8903)  Time: 0.565s, 1813.66/s  (2.381s,  430.11/s)  LR: 2.592e-05  Data: 0.000 (1.779)
Test: [   0/97]  Time: 14.344 (14.344)  Loss:  0.2827 (0.2827)  Acc@1: 97.0703 (97.0703)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.200 (3.171)  Loss:  0.4249 (0.3480)  Acc@1: 93.3594 (95.4944)  Acc@5: 98.3398 (98.9775)
Test: [  97/97]  Time: 0.119 (3.109)  Loss:  0.3077 (0.3592)  Acc@1: 94.9405 (94.9950)  Acc@5: 99.1071 (98.8660)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-192.pth.tar', 94.92500001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-190.pth.tar', 94.85999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 194 [   0/1171 (  0%)]  Loss:  3.276244 (3.2762)  Time: 10.719s,   95.54/s  (10.719s,   95.54/s)  LR: 2.411e-05  Data: 9.695 (9.695)
Train: 194 [  50/1171 (  4%)]  Loss:  2.683336 (2.9798)  Time: 0.703s, 1457.41/s  (2.312s,  443.00/s)  LR: 2.411e-05  Data: 0.134 (1.710)
Train: 194 [ 100/1171 (  9%)]  Loss:  3.043861 (3.0011)  Time: 2.725s,  375.83/s  (2.277s,  449.73/s)  LR: 2.411e-05  Data: 2.080 (1.670)
Train: 194 [ 150/1171 ( 13%)]  Loss:  3.184123 (3.0469)  Time: 0.586s, 1746.67/s  (2.275s,  450.07/s)  LR: 2.411e-05  Data: 0.019 (1.672)
Train: 194 [ 200/1171 ( 17%)]  Loss:  2.917004 (3.0209)  Time: 3.152s,  324.84/s  (2.358s,  434.26/s)  LR: 2.411e-05  Data: 2.591 (1.753)
Train: 194 [ 250/1171 ( 21%)]  Loss:  2.897722 (3.0004)  Time: 0.582s, 1757.98/s  (2.365s,  432.91/s)  LR: 2.411e-05  Data: 0.018 (1.760)
Train: 194 [ 300/1171 ( 26%)]  Loss:  2.263815 (2.8952)  Time: 3.947s,  259.45/s  (2.386s,  429.13/s)  LR: 2.411e-05  Data: 3.373 (1.784)
Train: 194 [ 350/1171 ( 30%)]  Loss:  2.432518 (2.8373)  Time: 0.583s, 1755.39/s  (2.376s,  431.06/s)  LR: 2.411e-05  Data: 0.020 (1.774)
Train: 194 [ 400/1171 ( 34%)]  Loss:  2.804586 (2.8337)  Time: 5.959s,  171.83/s  (2.377s,  430.86/s)  LR: 2.411e-05  Data: 5.378 (1.775)
Train: 194 [ 450/1171 ( 38%)]  Loss:  3.023409 (2.8527)  Time: 0.588s, 1742.07/s  (2.362s,  433.58/s)  LR: 2.411e-05  Data: 0.022 (1.758)
Train: 194 [ 500/1171 ( 43%)]  Loss:  2.816257 (2.8494)  Time: 6.850s,  149.49/s  (2.359s,  434.07/s)  LR: 2.411e-05  Data: 6.194 (1.755)
Train: 194 [ 550/1171 ( 47%)]  Loss:  2.808813 (2.8460)  Time: 0.592s, 1730.91/s  (2.385s,  429.41/s)  LR: 2.411e-05  Data: 0.021 (1.782)
Train: 194 [ 600/1171 ( 51%)]  Loss:  3.136384 (2.8683)  Time: 8.059s,  127.07/s  (2.414s,  424.21/s)  LR: 2.411e-05  Data: 7.465 (1.813)
Train: 194 [ 650/1171 ( 56%)]  Loss:  2.707700 (2.8568)  Time: 0.583s, 1755.50/s  (2.420s,  423.09/s)  LR: 2.411e-05  Data: 0.019 (1.821)
Train: 194 [ 700/1171 ( 60%)]  Loss:  2.705938 (2.8468)  Time: 7.588s,  134.96/s  (2.425s,  422.35/s)  LR: 2.411e-05  Data: 6.997 (1.826)
Train: 194 [ 750/1171 ( 64%)]  Loss:  2.710761 (2.8383)  Time: 0.584s, 1752.42/s  (2.414s,  424.13/s)  LR: 2.411e-05  Data: 0.022 (1.817)
Train: 194 [ 800/1171 ( 68%)]  Loss:  2.867316 (2.8400)  Time: 8.049s,  127.22/s  (2.413s,  424.32/s)  LR: 2.411e-05  Data: 7.441 (1.817)
Train: 194 [ 850/1171 ( 73%)]  Loss:  2.738827 (2.8344)  Time: 0.584s, 1752.74/s  (2.400s,  426.71/s)  LR: 2.411e-05  Data: 0.022 (1.803)
Train: 194 [ 900/1171 ( 77%)]  Loss:  2.999631 (2.8431)  Time: 8.400s,  121.91/s  (2.415s,  423.99/s)  LR: 2.411e-05  Data: 7.820 (1.819)
Train: 194 [ 950/1171 ( 81%)]  Loss:  3.102527 (2.8560)  Time: 0.580s, 1765.54/s  (2.418s,  423.42/s)  LR: 2.411e-05  Data: 0.017 (1.823)
Train: 194 [1000/1171 ( 85%)]  Loss:  2.457056 (2.8370)  Time: 7.150s,  143.21/s  (2.423s,  422.54/s)  LR: 2.411e-05  Data: 6.531 (1.828)
Train: 194 [1050/1171 ( 90%)]  Loss:  2.933705 (2.8414)  Time: 0.670s, 1528.26/s  (2.419s,  423.26/s)  LR: 2.411e-05  Data: 0.018 (1.824)
Train: 194 [1100/1171 ( 94%)]  Loss:  2.713157 (2.8359)  Time: 6.356s,  161.12/s  (2.418s,  423.45/s)  LR: 2.411e-05  Data: 5.770 (1.823)
Train: 194 [1150/1171 ( 98%)]  Loss:  2.662082 (2.8286)  Time: 1.386s,  738.61/s  (2.413s,  424.40/s)  LR: 2.411e-05  Data: 0.820 (1.817)
Train: 194 [1170/1171 (100%)]  Loss:  3.193675 (2.8432)  Time: 0.566s, 1810.50/s  (2.410s,  424.87/s)  LR: 2.411e-05  Data: 0.000 (1.815)
Test: [   0/97]  Time: 13.018 (13.018)  Loss:  0.2981 (0.2981)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (3.075)  Loss:  0.4553 (0.3667)  Acc@1: 92.3828 (95.3278)  Acc@5: 98.3398 (98.9602)
Test: [  97/97]  Time: 0.120 (3.183)  Loss:  0.3314 (0.3779)  Acc@1: 94.9405 (94.8590)  Acc@5: 99.2560 (98.8540)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-192.pth.tar', 94.92500001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-190.pth.tar', 94.85999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-194.pth.tar', 94.85899999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 195 [   0/1171 (  0%)]  Loss:  3.090482 (3.0905)  Time: 12.628s,   81.09/s  (12.628s,   81.09/s)  LR: 2.241e-05  Data: 12.025 (12.025)
Train: 195 [  50/1171 (  4%)]  Loss:  2.759597 (2.9250)  Time: 0.582s, 1760.31/s  (2.599s,  394.00/s)  LR: 2.241e-05  Data: 0.019 (2.004)
Train: 195 [ 100/1171 (  9%)]  Loss:  2.745857 (2.8653)  Time: 0.583s, 1756.54/s  (2.551s,  401.47/s)  LR: 2.241e-05  Data: 0.018 (1.949)
Train: 195 [ 150/1171 ( 13%)]  Loss:  2.438399 (2.7586)  Time: 0.584s, 1753.52/s  (2.465s,  415.44/s)  LR: 2.241e-05  Data: 0.019 (1.868)
Train: 195 [ 200/1171 ( 17%)]  Loss:  3.211680 (2.8492)  Time: 0.588s, 1741.68/s  (2.451s,  417.74/s)  LR: 2.241e-05  Data: 0.017 (1.858)
Train: 195 [ 250/1171 ( 21%)]  Loss:  2.811298 (2.8429)  Time: 0.583s, 1756.75/s  (2.400s,  426.71/s)  LR: 2.241e-05  Data: 0.020 (1.804)
Train: 195 [ 300/1171 ( 26%)]  Loss:  3.123392 (2.8830)  Time: 0.597s, 1715.97/s  (2.385s,  429.39/s)  LR: 2.241e-05  Data: 0.032 (1.786)
Train: 195 [ 350/1171 ( 30%)]  Loss:  3.362775 (2.9429)  Time: 0.588s, 1740.80/s  (2.405s,  425.69/s)  LR: 2.241e-05  Data: 0.020 (1.809)
Train: 195 [ 400/1171 ( 34%)]  Loss:  2.537657 (2.8979)  Time: 0.588s, 1742.32/s  (2.420s,  423.23/s)  LR: 2.241e-05  Data: 0.019 (1.823)
Train: 195 [ 450/1171 ( 38%)]  Loss:  3.096517 (2.9178)  Time: 0.584s, 1754.85/s  (2.422s,  422.85/s)  LR: 2.241e-05  Data: 0.019 (1.827)
Train: 195 [ 500/1171 ( 43%)]  Loss:  2.993855 (2.9247)  Time: 0.587s, 1744.86/s  (2.432s,  421.14/s)  LR: 2.241e-05  Data: 0.018 (1.838)
Train: 195 [ 550/1171 ( 47%)]  Loss:  2.753706 (2.9104)  Time: 0.584s, 1752.67/s  (2.433s,  420.90/s)  LR: 2.241e-05  Data: 0.019 (1.838)
Train: 195 [ 600/1171 ( 51%)]  Loss:  2.967571 (2.9148)  Time: 0.587s, 1743.03/s  (2.445s,  418.77/s)  LR: 2.241e-05  Data: 0.018 (1.851)
Train: 195 [ 650/1171 ( 56%)]  Loss:  3.003865 (2.9212)  Time: 0.586s, 1747.04/s  (2.439s,  419.82/s)  LR: 2.241e-05  Data: 0.021 (1.846)
Train: 195 [ 700/1171 ( 60%)]  Loss:  2.814907 (2.9141)  Time: 0.585s, 1749.02/s  (2.456s,  416.91/s)  LR: 2.241e-05  Data: 0.022 (1.863)
Train: 195 [ 750/1171 ( 64%)]  Loss:  2.850452 (2.9101)  Time: 0.581s, 1763.48/s  (2.457s,  416.77/s)  LR: 2.241e-05  Data: 0.018 (1.864)
Train: 195 [ 800/1171 ( 68%)]  Loss:  2.968298 (2.9135)  Time: 0.585s, 1751.89/s  (2.464s,  415.55/s)  LR: 2.241e-05  Data: 0.021 (1.872)
Train: 195 [ 850/1171 ( 73%)]  Loss:  2.920020 (2.9139)  Time: 0.585s, 1751.32/s  (2.458s,  416.53/s)  LR: 2.241e-05  Data: 0.020 (1.867)
Train: 195 [ 900/1171 ( 77%)]  Loss:  2.696464 (2.9025)  Time: 0.586s, 1748.65/s  (2.467s,  415.15/s)  LR: 2.241e-05  Data: 0.021 (1.875)
Train: 195 [ 950/1171 ( 81%)]  Loss:  3.008875 (2.9078)  Time: 0.582s, 1760.44/s  (2.459s,  416.51/s)  LR: 2.241e-05  Data: 0.019 (1.868)
Train: 195 [1000/1171 ( 85%)]  Loss:  2.864349 (2.9057)  Time: 0.590s, 1736.02/s  (2.452s,  417.60/s)  LR: 2.241e-05  Data: 0.018 (1.862)
Train: 195 [1050/1171 ( 90%)]  Loss:  2.829859 (2.9023)  Time: 0.583s, 1756.26/s  (2.440s,  419.69/s)  LR: 2.241e-05  Data: 0.018 (1.850)
Train: 195 [1100/1171 ( 94%)]  Loss:  3.080004 (2.9100)  Time: 0.587s, 1744.81/s  (2.453s,  417.49/s)  LR: 2.241e-05  Data: 0.020 (1.863)
Train: 195 [1150/1171 ( 98%)]  Loss:  2.653655 (2.8993)  Time: 0.587s, 1744.58/s  (2.453s,  417.49/s)  LR: 2.241e-05  Data: 0.025 (1.863)
Train: 195 [1170/1171 (100%)]  Loss:  3.062114 (2.9058)  Time: 0.565s, 1811.39/s  (2.456s,  416.91/s)  LR: 2.241e-05  Data: 0.000 (1.867)
Test: [   0/97]  Time: 13.990 (13.990)  Loss:  0.2745 (0.2745)  Acc@1: 97.1680 (97.1680)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.335 (3.186)  Loss:  0.4442 (0.3553)  Acc@1: 93.0664 (95.4293)  Acc@5: 98.2422 (98.9871)
Test: [  97/97]  Time: 0.120 (3.126)  Loss:  0.3325 (0.3656)  Acc@1: 94.6429 (94.9610)  Acc@5: 99.4048 (98.8660)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-195.pth.tar', 94.96100002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-192.pth.tar', 94.92500001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-190.pth.tar', 94.85999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-194.pth.tar', 94.85899999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 196 [   0/1171 (  0%)]  Loss:  2.538856 (2.5389)  Time: 10.463s,   97.87/s  (10.463s,   97.87/s)  LR: 2.082e-05  Data: 9.777 (9.777)
Train: 196 [  50/1171 (  4%)]  Loss:  2.704439 (2.6216)  Time: 0.585s, 1751.63/s  (2.344s,  436.84/s)  LR: 2.082e-05  Data: 0.022 (1.749)
Train: 196 [ 100/1171 (  9%)]  Loss:  2.547021 (2.5968)  Time: 0.589s, 1738.36/s  (2.317s,  441.88/s)  LR: 2.082e-05  Data: 0.021 (1.718)
Train: 196 [ 150/1171 ( 13%)]  Loss:  2.795496 (2.6465)  Time: 0.589s, 1737.58/s  (2.401s,  426.57/s)  LR: 2.082e-05  Data: 0.021 (1.802)
Train: 196 [ 200/1171 ( 17%)]  Loss:  2.777804 (2.6727)  Time: 2.373s,  431.51/s  (2.443s,  419.24/s)  LR: 2.082e-05  Data: 1.696 (1.840)
Train: 196 [ 250/1171 ( 21%)]  Loss:  3.045993 (2.7349)  Time: 0.584s, 1754.11/s  (2.444s,  419.00/s)  LR: 2.082e-05  Data: 0.020 (1.843)
Train: 196 [ 300/1171 ( 26%)]  Loss:  2.688592 (2.7283)  Time: 0.582s, 1758.74/s  (2.466s,  415.23/s)  LR: 2.082e-05  Data: 0.019 (1.866)
Train: 196 [ 350/1171 ( 30%)]  Loss:  2.714349 (2.7266)  Time: 0.584s, 1754.04/s  (2.440s,  419.71/s)  LR: 2.082e-05  Data: 0.019 (1.842)
Train: 196 [ 400/1171 ( 34%)]  Loss:  2.351994 (2.6849)  Time: 0.587s, 1744.62/s  (2.431s,  421.22/s)  LR: 2.082e-05  Data: 0.021 (1.835)
Train: 196 [ 450/1171 ( 38%)]  Loss:  2.711137 (2.6876)  Time: 0.581s, 1761.26/s  (2.417s,  423.74/s)  LR: 2.082e-05  Data: 0.019 (1.823)
Train: 196 [ 500/1171 ( 43%)]  Loss:  3.008651 (2.7168)  Time: 0.584s, 1752.34/s  (2.435s,  420.55/s)  LR: 2.082e-05  Data: 0.017 (1.843)
Train: 196 [ 550/1171 ( 47%)]  Loss:  3.050252 (2.7445)  Time: 0.585s, 1750.62/s  (2.458s,  416.64/s)  LR: 2.082e-05  Data: 0.020 (1.867)
Train: 196 [ 600/1171 ( 51%)]  Loss:  2.733640 (2.7437)  Time: 2.975s,  344.24/s  (2.488s,  411.56/s)  LR: 2.082e-05  Data: 2.319 (1.897)
Train: 196 [ 650/1171 ( 56%)]  Loss:  3.104232 (2.7695)  Time: 0.583s, 1757.60/s  (2.481s,  412.66/s)  LR: 2.082e-05  Data: 0.020 (1.890)
Train: 196 [ 700/1171 ( 60%)]  Loss:  2.893685 (2.7777)  Time: 0.586s, 1747.16/s  (2.481s,  412.77/s)  LR: 2.082e-05  Data: 0.018 (1.887)
Train: 196 [ 750/1171 ( 64%)]  Loss:  2.830699 (2.7811)  Time: 0.589s, 1739.30/s  (2.466s,  415.24/s)  LR: 2.082e-05  Data: 0.017 (1.873)
Train: 196 [ 800/1171 ( 68%)]  Loss:  2.905937 (2.7884)  Time: 0.588s, 1740.15/s  (2.461s,  416.08/s)  LR: 2.082e-05  Data: 0.024 (1.869)
Train: 196 [ 850/1171 ( 73%)]  Loss:  3.152717 (2.8086)  Time: 0.585s, 1750.89/s  (2.447s,  418.54/s)  LR: 2.082e-05  Data: 0.018 (1.854)
Train: 196 [ 900/1171 ( 77%)]  Loss:  2.748855 (2.8055)  Time: 1.379s,  742.44/s  (2.463s,  415.73/s)  LR: 2.082e-05  Data: 0.818 (1.869)
Train: 196 [ 950/1171 ( 81%)]  Loss:  3.010503 (2.8157)  Time: 0.583s, 1755.84/s  (2.464s,  415.62/s)  LR: 2.082e-05  Data: 0.018 (1.869)
Train: 196 [1000/1171 ( 85%)]  Loss:  3.190553 (2.8336)  Time: 1.647s,  621.68/s  (2.465s,  415.48/s)  LR: 2.082e-05  Data: 0.994 (1.870)
Train: 196 [1050/1171 ( 90%)]  Loss:  2.439920 (2.8157)  Time: 0.586s, 1747.30/s  (2.460s,  416.26/s)  LR: 2.082e-05  Data: 0.019 (1.866)
Train: 196 [1100/1171 ( 94%)]  Loss:  2.898382 (2.8193)  Time: 3.992s,  256.48/s  (2.460s,  416.20/s)  LR: 2.082e-05  Data: 3.417 (1.864)
Train: 196 [1150/1171 ( 98%)]  Loss:  2.959733 (2.8251)  Time: 0.584s, 1752.35/s  (2.449s,  418.06/s)  LR: 2.082e-05  Data: 0.020 (1.853)
Train: 196 [1170/1171 (100%)]  Loss:  2.683006 (2.8195)  Time: 0.566s, 1810.30/s  (2.446s,  418.61/s)  LR: 2.082e-05  Data: 0.000 (1.850)
Test: [   0/97]  Time: 13.116 (13.116)  Loss:  0.2637 (0.2637)  Acc@1: 96.9727 (96.9727)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (3.259)  Loss:  0.4225 (0.3333)  Acc@1: 93.1641 (95.4389)  Acc@5: 98.4375 (98.9717)
Test: [  97/97]  Time: 0.120 (3.202)  Loss:  0.3084 (0.3442)  Acc@1: 95.0893 (94.9770)  Acc@5: 99.4048 (98.8590)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-196.pth.tar', 94.97700000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-195.pth.tar', 94.96100002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-192.pth.tar', 94.92500001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-190.pth.tar', 94.85999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-194.pth.tar', 94.85899999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 197 [   0/1171 (  0%)]  Loss:  2.592399 (2.5924)  Time: 11.015s,   92.96/s  (11.015s,   92.96/s)  LR: 1.933e-05  Data: 9.918 (9.918)
Train: 197 [  50/1171 (  4%)]  Loss:  2.624455 (2.6084)  Time: 0.585s, 1749.15/s  (2.494s,  410.53/s)  LR: 1.933e-05  Data: 0.019 (1.893)
Train: 197 [ 100/1171 (  9%)]  Loss:  2.969435 (2.7288)  Time: 0.586s, 1746.96/s  (2.442s,  419.37/s)  LR: 1.933e-05  Data: 0.019 (1.848)
Train: 197 [ 150/1171 ( 13%)]  Loss:  2.795016 (2.7453)  Time: 0.586s, 1747.31/s  (2.357s,  434.39/s)  LR: 1.933e-05  Data: 0.018 (1.758)
Train: 197 [ 200/1171 ( 17%)]  Loss:  2.671180 (2.7305)  Time: 1.724s,  594.14/s  (2.337s,  438.22/s)  LR: 1.933e-05  Data: 1.058 (1.738)
Train: 197 [ 250/1171 ( 21%)]  Loss:  2.590897 (2.7072)  Time: 0.590s, 1735.83/s  (2.306s,  444.12/s)  LR: 1.933e-05  Data: 0.018 (1.706)
Train: 197 [ 300/1171 ( 26%)]  Loss:  2.653860 (2.6996)  Time: 2.060s,  497.19/s  (2.299s,  445.44/s)  LR: 1.933e-05  Data: 1.352 (1.699)
Train: 197 [ 350/1171 ( 30%)]  Loss:  2.665323 (2.6953)  Time: 0.583s, 1755.51/s  (2.343s,  436.98/s)  LR: 1.933e-05  Data: 0.018 (1.741)
Train: 197 [ 400/1171 ( 34%)]  Loss:  2.700893 (2.6959)  Time: 1.592s,  643.24/s  (2.359s,  434.16/s)  LR: 1.933e-05  Data: 0.780 (1.755)
Train: 197 [ 450/1171 ( 38%)]  Loss:  2.818030 (2.7081)  Time: 0.584s, 1753.27/s  (2.362s,  433.56/s)  LR: 1.933e-05  Data: 0.019 (1.758)
Train: 197 [ 500/1171 ( 43%)]  Loss:  2.542093 (2.6931)  Time: 3.130s,  327.16/s  (2.381s,  430.14/s)  LR: 1.933e-05  Data: 2.483 (1.777)
Train: 197 [ 550/1171 ( 47%)]  Loss:  2.980069 (2.7170)  Time: 0.583s, 1755.66/s  (2.379s,  430.49/s)  LR: 1.933e-05  Data: 0.019 (1.775)
Train: 197 [ 600/1171 ( 51%)]  Loss:  3.146772 (2.7500)  Time: 3.142s,  325.91/s  (2.383s,  429.74/s)  LR: 1.933e-05  Data: 2.580 (1.780)
Train: 197 [ 650/1171 ( 56%)]  Loss:  2.940062 (2.7636)  Time: 0.584s, 1753.76/s  (2.381s,  430.03/s)  LR: 1.933e-05  Data: 0.022 (1.778)
Train: 197 [ 700/1171 ( 60%)]  Loss:  3.023951 (2.7810)  Time: 1.642s,  623.50/s  (2.407s,  425.44/s)  LR: 1.933e-05  Data: 1.021 (1.804)
Train: 197 [ 750/1171 ( 64%)]  Loss:  3.317405 (2.8145)  Time: 0.590s, 1736.42/s  (2.403s,  426.18/s)  LR: 1.933e-05  Data: 0.025 (1.799)
Train: 197 [ 800/1171 ( 68%)]  Loss:  2.663409 (2.8056)  Time: 3.388s,  302.25/s  (2.409s,  425.08/s)  LR: 1.933e-05  Data: 2.820 (1.806)
Train: 197 [ 850/1171 ( 73%)]  Loss:  2.883797 (2.8099)  Time: 0.589s, 1738.34/s  (2.403s,  426.07/s)  LR: 1.933e-05  Data: 0.025 (1.801)
Train: 197 [ 900/1171 ( 77%)]  Loss:  2.900966 (2.8147)  Time: 3.988s,  256.76/s  (2.407s,  425.39/s)  LR: 1.933e-05  Data: 3.329 (1.803)
Train: 197 [ 950/1171 ( 81%)]  Loss:  2.977452 (2.8229)  Time: 0.588s, 1741.04/s  (2.397s,  427.23/s)  LR: 1.933e-05  Data: 0.020 (1.793)
Train: 197 [1000/1171 ( 85%)]  Loss:  3.123089 (2.8372)  Time: 7.466s,  137.16/s  (2.393s,  427.85/s)  LR: 1.933e-05  Data: 6.887 (1.790)
Train: 197 [1050/1171 ( 90%)]  Loss:  2.735927 (2.8326)  Time: 0.587s, 1744.08/s  (2.384s,  429.55/s)  LR: 1.933e-05  Data: 0.023 (1.782)
Train: 197 [1100/1171 ( 94%)]  Loss:  3.299225 (2.8529)  Time: 6.495s,  157.65/s  (2.396s,  427.38/s)  LR: 1.933e-05  Data: 5.934 (1.794)
Train: 197 [1150/1171 ( 98%)]  Loss:  3.006122 (2.8592)  Time: 0.585s, 1749.42/s  (2.393s,  427.96/s)  LR: 1.933e-05  Data: 0.020 (1.791)
Train: 197 [1170/1171 (100%)]  Loss:  2.571126 (2.8477)  Time: 0.564s, 1815.23/s  (2.392s,  428.11/s)  LR: 1.933e-05  Data: 0.000 (1.790)
Test: [   0/97]  Time: 12.847 (12.847)  Loss:  0.2881 (0.2881)  Acc@1: 96.9727 (96.9727)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (3.074)  Loss:  0.4491 (0.3504)  Acc@1: 92.5781 (95.4561)  Acc@5: 98.4375 (98.9775)
Test: [  97/97]  Time: 0.120 (3.014)  Loss:  0.3492 (0.3622)  Acc@1: 93.7500 (95.0000)  Acc@5: 99.1071 (98.8600)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-197.pth.tar', 95.0)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-196.pth.tar', 94.97700000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-195.pth.tar', 94.96100002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-192.pth.tar', 94.92500001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-190.pth.tar', 94.85999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-194.pth.tar', 94.85899999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 198 [   0/1171 (  0%)]  Loss:  2.611213 (2.6112)  Time: 10.270s,   99.71/s  (10.270s,   99.71/s)  LR: 1.795e-05  Data: 9.405 (9.405)
Train: 198 [  50/1171 (  4%)]  Loss:  2.317712 (2.4645)  Time: 0.584s, 1752.07/s  (2.282s,  448.75/s)  LR: 1.795e-05  Data: 0.019 (1.685)
Train: 198 [ 100/1171 (  9%)]  Loss:  2.377376 (2.4354)  Time: 0.587s, 1744.24/s  (2.217s,  461.92/s)  LR: 1.795e-05  Data: 0.019 (1.620)
Train: 198 [ 150/1171 ( 13%)]  Loss:  2.704504 (2.5027)  Time: 0.583s, 1755.46/s  (2.247s,  455.73/s)  LR: 1.795e-05  Data: 0.021 (1.643)
Train: 198 [ 200/1171 ( 17%)]  Loss:  2.813316 (2.5648)  Time: 0.591s, 1732.78/s  (2.307s,  443.79/s)  LR: 1.795e-05  Data: 0.019 (1.703)
Train: 198 [ 250/1171 ( 21%)]  Loss:  2.995081 (2.6365)  Time: 0.584s, 1753.46/s  (2.329s,  439.60/s)  LR: 1.795e-05  Data: 0.021 (1.724)
Train: 198 [ 300/1171 ( 26%)]  Loss:  2.569782 (2.6270)  Time: 0.585s, 1749.24/s  (2.330s,  439.41/s)  LR: 1.795e-05  Data: 0.018 (1.722)
Train: 198 [ 350/1171 ( 30%)]  Loss:  2.962368 (2.6689)  Time: 0.588s, 1742.73/s  (2.316s,  442.11/s)  LR: 1.795e-05  Data: 0.022 (1.709)
Train: 198 [ 400/1171 ( 34%)]  Loss:  2.885176 (2.6929)  Time: 0.590s, 1735.01/s  (2.313s,  442.77/s)  LR: 1.795e-05  Data: 0.019 (1.706)
Train: 198 [ 450/1171 ( 38%)]  Loss:  2.594854 (2.6831)  Time: 0.583s, 1757.62/s  (2.303s,  444.56/s)  LR: 1.795e-05  Data: 0.021 (1.697)
Train: 198 [ 500/1171 ( 43%)]  Loss:  2.685844 (2.6834)  Time: 0.585s, 1751.10/s  (2.295s,  446.21/s)  LR: 1.795e-05  Data: 0.022 (1.690)
Train: 198 [ 550/1171 ( 47%)]  Loss:  2.472728 (2.6658)  Time: 0.584s, 1753.58/s  (2.341s,  437.39/s)  LR: 1.795e-05  Data: 0.018 (1.738)
Train: 198 [ 600/1171 ( 51%)]  Loss:  2.836690 (2.6790)  Time: 0.582s, 1758.81/s  (2.357s,  434.54/s)  LR: 1.795e-05  Data: 0.019 (1.755)
Train: 198 [ 650/1171 ( 56%)]  Loss:  2.748954 (2.6840)  Time: 0.590s, 1737.04/s  (2.373s,  431.43/s)  LR: 1.795e-05  Data: 0.019 (1.773)
Train: 198 [ 700/1171 ( 60%)]  Loss:  2.868241 (2.6963)  Time: 0.584s, 1753.72/s  (2.367s,  432.64/s)  LR: 1.795e-05  Data: 0.019 (1.768)
Train: 198 [ 750/1171 ( 64%)]  Loss:  2.815927 (2.7037)  Time: 0.588s, 1740.47/s  (2.368s,  432.42/s)  LR: 1.795e-05  Data: 0.020 (1.770)
Train: 198 [ 800/1171 ( 68%)]  Loss:  2.884455 (2.7144)  Time: 0.588s, 1742.68/s  (2.358s,  434.32/s)  LR: 1.795e-05  Data: 0.022 (1.760)
Train: 198 [ 850/1171 ( 73%)]  Loss:  2.868487 (2.7229)  Time: 0.584s, 1754.76/s  (2.355s,  434.81/s)  LR: 1.795e-05  Data: 0.019 (1.758)
Train: 198 [ 900/1171 ( 77%)]  Loss:  2.858419 (2.7301)  Time: 0.586s, 1747.75/s  (2.344s,  436.90/s)  LR: 1.795e-05  Data: 0.021 (1.748)
Train: 198 [ 950/1171 ( 81%)]  Loss:  2.312199 (2.7092)  Time: 0.584s, 1752.30/s  (2.363s,  433.41/s)  LR: 1.795e-05  Data: 0.021 (1.768)
Train: 198 [1000/1171 ( 85%)]  Loss:  3.150717 (2.7302)  Time: 3.033s,  337.57/s  (2.364s,  433.17/s)  LR: 1.795e-05  Data: 2.382 (1.769)
Train: 198 [1050/1171 ( 90%)]  Loss:  2.804540 (2.7336)  Time: 0.585s, 1750.12/s  (2.363s,  433.34/s)  LR: 1.795e-05  Data: 0.018 (1.768)
Train: 198 [1100/1171 ( 94%)]  Loss:  3.180715 (2.7530)  Time: 6.081s,  168.41/s  (2.362s,  433.57/s)  LR: 1.795e-05  Data: 5.424 (1.766)
Train: 198 [1150/1171 ( 98%)]  Loss:  2.414334 (2.7389)  Time: 0.588s, 1741.88/s  (2.372s,  431.73/s)  LR: 1.795e-05  Data: 0.019 (1.776)
Train: 198 [1170/1171 (100%)]  Loss:  2.674191 (2.7363)  Time: 0.565s, 1811.48/s  (2.371s,  431.86/s)  LR: 1.795e-05  Data: 0.000 (1.775)
Test: [   0/97]  Time: 12.375 (12.375)  Loss:  0.2718 (0.2718)  Acc@1: 97.0703 (97.0703)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.197 (2.988)  Loss:  0.4206 (0.3390)  Acc@1: 92.9688 (95.4925)  Acc@5: 98.3398 (98.9756)
Test: [  97/97]  Time: 0.120 (3.072)  Loss:  0.3165 (0.3524)  Acc@1: 94.4940 (94.9930)  Acc@5: 99.1071 (98.8530)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-197.pth.tar', 95.0)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-198.pth.tar', 94.99300000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-196.pth.tar', 94.97700000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-195.pth.tar', 94.96100002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-192.pth.tar', 94.92500001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-190.pth.tar', 94.85999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-194.pth.tar', 94.85899999511719)

Train: 199 [   0/1171 (  0%)]  Loss:  2.673727 (2.6737)  Time: 12.478s,   82.07/s  (12.478s,   82.07/s)  LR: 1.669e-05  Data: 11.623 (11.623)
Train: 199 [  50/1171 (  4%)]  Loss:  2.146018 (2.4099)  Time: 0.584s, 1754.57/s  (2.494s,  410.57/s)  LR: 1.669e-05  Data: 0.020 (1.895)
Train: 199 [ 100/1171 (  9%)]  Loss:  2.726191 (2.5153)  Time: 0.587s, 1744.82/s  (2.417s,  423.63/s)  LR: 1.669e-05  Data: 0.022 (1.823)
Train: 199 [ 150/1171 ( 13%)]  Loss:  2.612074 (2.5395)  Time: 0.583s, 1755.47/s  (2.370s,  432.06/s)  LR: 1.669e-05  Data: 0.022 (1.780)
Train: 199 [ 200/1171 ( 17%)]  Loss:  2.792028 (2.5900)  Time: 0.585s, 1748.98/s  (2.361s,  433.71/s)  LR: 1.669e-05  Data: 0.019 (1.766)
Train: 199 [ 250/1171 ( 21%)]  Loss:  2.397782 (2.5580)  Time: 0.585s, 1751.62/s  (2.323s,  440.79/s)  LR: 1.669e-05  Data: 0.021 (1.728)
Train: 199 [ 300/1171 ( 26%)]  Loss:  2.924757 (2.6104)  Time: 0.586s, 1748.33/s  (2.322s,  440.96/s)  LR: 1.669e-05  Data: 0.021 (1.725)
Train: 199 [ 350/1171 ( 30%)]  Loss:  2.782051 (2.6318)  Time: 2.321s,  441.27/s  (2.300s,  445.17/s)  LR: 1.669e-05  Data: 1.652 (1.698)
Train: 199 [ 400/1171 ( 34%)]  Loss:  2.208042 (2.5847)  Time: 0.589s, 1737.15/s  (2.351s,  435.50/s)  LR: 1.669e-05  Data: 0.026 (1.747)
Train: 199 [ 450/1171 ( 38%)]  Loss:  3.185899 (2.6449)  Time: 1.666s,  614.70/s  (2.353s,  435.20/s)  LR: 1.669e-05  Data: 1.070 (1.748)
Train: 199 [ 500/1171 ( 43%)]  Loss:  3.321429 (2.7064)  Time: 0.590s, 1735.31/s  (2.372s,  431.72/s)  LR: 1.669e-05  Data: 0.021 (1.768)
Train: 199 [ 550/1171 ( 47%)]  Loss:  2.353590 (2.6770)  Time: 0.586s, 1746.20/s  (2.358s,  434.35/s)  LR: 1.669e-05  Data: 0.022 (1.755)
Train: 199 [ 600/1171 ( 51%)]  Loss:  3.124726 (2.7114)  Time: 0.585s, 1751.75/s  (2.346s,  436.40/s)  LR: 1.669e-05  Data: 0.020 (1.744)
Train: 199 [ 650/1171 ( 56%)]  Loss:  2.579110 (2.7020)  Time: 2.816s,  363.68/s  (2.336s,  438.43/s)  LR: 1.669e-05  Data: 2.166 (1.733)
Train: 199 [ 700/1171 ( 60%)]  Loss:  3.291641 (2.7413)  Time: 0.590s, 1734.86/s  (2.335s,  438.56/s)  LR: 1.669e-05  Data: 0.019 (1.733)
Train: 199 [ 750/1171 ( 64%)]  Loss:  2.744449 (2.7415)  Time: 3.901s,  262.53/s  (2.372s,  431.69/s)  LR: 1.669e-05  Data: 3.246 (1.769)
Train: 199 [ 800/1171 ( 68%)]  Loss:  2.348467 (2.7184)  Time: 0.586s, 1748.11/s  (2.374s,  431.42/s)  LR: 1.669e-05  Data: 0.020 (1.770)
Train: 199 [ 850/1171 ( 73%)]  Loss:  2.650506 (2.7146)  Time: 5.222s,  196.10/s  (2.384s,  429.51/s)  LR: 1.669e-05  Data: 4.618 (1.780)
Train: 199 [ 900/1171 ( 77%)]  Loss:  2.803068 (2.7192)  Time: 0.586s, 1747.44/s  (2.385s,  429.33/s)  LR: 1.669e-05  Data: 0.020 (1.781)
Train: 199 [ 950/1171 ( 81%)]  Loss:  2.976591 (2.7321)  Time: 3.175s,  322.55/s  (2.386s,  429.24/s)  LR: 1.669e-05  Data: 2.472 (1.782)
Train: 199 [1000/1171 ( 85%)]  Loss:  2.911507 (2.7406)  Time: 0.582s, 1758.02/s  (2.383s,  429.73/s)  LR: 1.669e-05  Data: 0.019 (1.780)
Train: 199 [1050/1171 ( 90%)]  Loss:  2.643841 (2.7362)  Time: 4.980s,  205.61/s  (2.380s,  430.34/s)  LR: 1.669e-05  Data: 4.356 (1.777)
Train: 199 [1100/1171 ( 94%)]  Loss:  3.240180 (2.7582)  Time: 0.940s, 1089.06/s  (2.373s,  431.54/s)  LR: 1.669e-05  Data: 0.363 (1.770)
Train: 199 [1150/1171 ( 98%)]  Loss:  3.195984 (2.7764)  Time: 6.478s,  158.08/s  (2.391s,  428.23/s)  LR: 1.669e-05  Data: 5.894 (1.790)
Train: 199 [1170/1171 (100%)]  Loss:  2.682112 (2.7726)  Time: 0.563s, 1819.52/s  (2.387s,  428.93/s)  LR: 1.669e-05  Data: 0.000 (1.785)
Test: [   0/97]  Time: 14.757 (14.757)  Loss:  0.2942 (0.2942)  Acc@1: 96.8750 (96.8750)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (3.302)  Loss:  0.4233 (0.3545)  Acc@1: 93.3594 (95.3699)  Acc@5: 98.3398 (98.9602)
Test: [  97/97]  Time: 0.120 (3.258)  Loss:  0.3159 (0.3621)  Acc@1: 94.9405 (94.9860)  Acc@5: 99.4048 (98.8520)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-197.pth.tar', 95.0)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-198.pth.tar', 94.99300000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-199.pth.tar', 94.98600004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-196.pth.tar', 94.97700000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-195.pth.tar', 94.96100002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-192.pth.tar', 94.92500001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-190.pth.tar', 94.85999998535156)

Train: 200 [   0/1171 (  0%)]  Loss:  2.390730 (2.3907)  Time: 11.404s,   89.79/s  (11.404s,   89.79/s)  LR: 1.553e-05  Data: 10.741 (10.741)
Train: 200 [  50/1171 (  4%)]  Loss:  3.050379 (2.7206)  Time: 0.585s, 1751.12/s  (2.493s,  410.80/s)  LR: 1.553e-05  Data: 0.023 (1.911)
Train: 200 [ 100/1171 (  9%)]  Loss:  3.105497 (2.8489)  Time: 0.582s, 1758.95/s  (2.404s,  425.89/s)  LR: 1.553e-05  Data: 0.019 (1.820)
Train: 200 [ 150/1171 ( 13%)]  Loss:  2.509972 (2.7641)  Time: 0.584s, 1753.58/s  (2.331s,  439.21/s)  LR: 1.553e-05  Data: 0.021 (1.743)
Train: 200 [ 200/1171 ( 17%)]  Loss:  2.912623 (2.7938)  Time: 0.589s, 1739.16/s  (2.413s,  424.43/s)  LR: 1.553e-05  Data: 0.025 (1.824)
Train: 200 [ 250/1171 ( 21%)]  Loss:  3.113818 (2.8472)  Time: 0.583s, 1757.31/s  (2.394s,  427.67/s)  LR: 1.553e-05  Data: 0.019 (1.808)
Train: 200 [ 300/1171 ( 26%)]  Loss:  2.991687 (2.8678)  Time: 0.586s, 1746.92/s  (2.477s,  413.42/s)  LR: 1.553e-05  Data: 0.021 (1.889)
Train: 200 [ 350/1171 ( 30%)]  Loss:  2.603539 (2.8348)  Time: 0.583s, 1755.47/s  (2.457s,  416.69/s)  LR: 1.553e-05  Data: 0.018 (1.871)
Train: 200 [ 400/1171 ( 34%)]  Loss:  2.881740 (2.8400)  Time: 3.824s,  267.76/s  (2.551s,  401.33/s)  LR: 1.553e-05  Data: 3.249 (1.964)
Train: 200 [ 450/1171 ( 38%)]  Loss:  2.694100 (2.8254)  Time: 0.582s, 1759.35/s  (2.515s,  407.12/s)  LR: 1.553e-05  Data: 0.019 (1.928)
Train: 200 [ 500/1171 ( 43%)]  Loss:  2.997429 (2.8410)  Time: 4.479s,  228.62/s  (2.506s,  408.66/s)  LR: 1.553e-05  Data: 3.914 (1.917)
Train: 200 [ 550/1171 ( 47%)]  Loss:  2.746532 (2.8332)  Time: 0.581s, 1761.45/s  (2.525s,  405.50/s)  LR: 1.553e-05  Data: 0.018 (1.937)
Train: 200 [ 600/1171 ( 51%)]  Loss:  2.663436 (2.8201)  Time: 4.972s,  205.94/s  (2.539s,  403.37/s)  LR: 1.553e-05  Data: 4.089 (1.948)
Train: 200 [ 650/1171 ( 56%)]  Loss:  2.809142 (2.8193)  Time: 0.584s, 1753.83/s  (2.527s,  405.24/s)  LR: 1.553e-05  Data: 0.022 (1.937)
Train: 200 [ 700/1171 ( 60%)]  Loss:  2.904330 (2.8250)  Time: 4.137s,  247.50/s  (2.526s,  405.45/s)  LR: 1.553e-05  Data: 3.467 (1.935)
Train: 200 [ 750/1171 ( 64%)]  Loss:  3.048836 (2.8390)  Time: 0.586s, 1746.90/s  (2.511s,  407.78/s)  LR: 1.553e-05  Data: 0.020 (1.919)
Train: 200 [ 800/1171 ( 68%)]  Loss:  2.567142 (2.8230)  Time: 4.054s,  252.61/s  (2.503s,  409.06/s)  LR: 1.553e-05  Data: 3.453 (1.911)
Train: 200 [ 850/1171 ( 73%)]  Loss:  2.957050 (2.8304)  Time: 0.586s, 1747.11/s  (2.488s,  411.51/s)  LR: 1.553e-05  Data: 0.020 (1.895)
Train: 200 [ 900/1171 ( 77%)]  Loss:  3.156488 (2.8476)  Time: 4.716s,  217.12/s  (2.487s,  411.77/s)  LR: 1.553e-05  Data: 4.075 (1.893)
Train: 200 [ 950/1171 ( 81%)]  Loss:  3.170898 (2.8638)  Time: 0.584s, 1752.90/s  (2.499s,  409.78/s)  LR: 1.553e-05  Data: 0.019 (1.905)
Train: 200 [1000/1171 ( 85%)]  Loss:  2.731796 (2.8575)  Time: 1.828s,  560.29/s  (2.508s,  408.25/s)  LR: 1.553e-05  Data: 1.084 (1.914)
Train: 200 [1050/1171 ( 90%)]  Loss:  2.615597 (2.8465)  Time: 0.587s, 1745.31/s  (2.503s,  409.08/s)  LR: 1.553e-05  Data: 0.019 (1.909)
Train: 200 [1100/1171 ( 94%)]  Loss:  2.569525 (2.8344)  Time: 4.121s,  248.51/s  (2.503s,  409.16/s)  LR: 1.553e-05  Data: 3.555 (1.909)
Train: 200 [1150/1171 ( 98%)]  Loss:  2.875116 (2.8361)  Time: 0.589s, 1739.70/s  (2.509s,  408.15/s)  LR: 1.553e-05  Data: 0.018 (1.915)
Train: 200 [1170/1171 (100%)]  Loss:  2.287313 (2.8142)  Time: 0.565s, 1811.41/s  (2.507s,  408.46/s)  LR: 1.553e-05  Data: 0.000 (1.912)
Test: [   0/97]  Time: 12.086 (12.086)  Loss:  0.2943 (0.2943)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.197 (3.559)  Loss:  0.4309 (0.3559)  Acc@1: 93.6523 (95.5002)  Acc@5: 98.2422 (98.9775)
Test: [  97/97]  Time: 0.120 (3.542)  Loss:  0.3403 (0.3657)  Acc@1: 93.7500 (95.0250)  Acc@5: 99.2560 (98.8740)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-200.pth.tar', 95.02500005126953)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-197.pth.tar', 95.0)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-198.pth.tar', 94.99300000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-199.pth.tar', 94.98600004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-196.pth.tar', 94.97700000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-195.pth.tar', 94.96100002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-192.pth.tar', 94.92500001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)

Train: 201 [   0/1171 (  0%)]  Loss:  2.287796 (2.2878)  Time: 10.264s,   99.76/s  (10.264s,   99.76/s)  LR: 1.448e-05  Data: 9.513 (9.513)
Train: 201 [  50/1171 (  4%)]  Loss:  2.935076 (2.6114)  Time: 0.582s, 1759.00/s  (2.607s,  392.82/s)  LR: 1.448e-05  Data: 0.016 (2.006)

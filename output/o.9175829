--Start--
Mon May 31 14:03:41 JST 2021
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 3
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
wandb: Currently logged in as: gyama_x (use `wandb login --relogin` to force relogin)
wandb: WARNING Tried to auto resume run with id PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1 but id PreTraining_vit_deit_tiny_patch16_224_1k is set.
wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
wandb: Tracking run with wandb version 0.10.27
wandb: Resuming run PreTraining_vit_deit_tiny_patch16_224_1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gyama_x/pytorch-image-models
wandb: üöÄ View run at https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_1k
wandb: Run data is saved locally in /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210531_140347-PreTraining_vit_deit_tiny_patch16_224_1k
wandb: Run `wandb offline` to turn off syncing.

Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar' (epoch 121)
Using native Torch DistributedDataParallel.
Scheduled epochs: 144
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Train: 122 [   0/1251 (  0%)]  Loss:  4.739893 (4.7399)  Time: 15.920s,   64.32/s  (15.920s,   64.32/s)  LR: 6.593e-05  Data: 15.041 (15.041)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 122 [  50/1251 (  4%)]  Loss:  4.739493 (4.7397)  Time: 0.585s, 1751.79/s  (2.554s,  401.00/s)  LR: 6.593e-05  Data: 0.021 (1.949)
Train: 122 [ 100/1251 (  8%)]  Loss:  4.749485 (4.7430)  Time: 6.854s,  149.39/s  (2.445s,  418.77/s)  LR: 6.593e-05  Data: 6.292 (1.850)
Train: 122 [ 150/1251 ( 12%)]  Loss:  4.376774 (4.6514)  Time: 0.582s, 1759.11/s  (2.348s,  436.13/s)  LR: 6.593e-05  Data: 0.018 (1.748)
Train: 122 [ 200/1251 ( 16%)]  Loss:  4.251768 (4.5715)  Time: 6.088s,  168.21/s  (2.308s,  443.67/s)  LR: 6.593e-05  Data: 5.525 (1.711)
Train: 122 [ 250/1251 ( 20%)]  Loss:  4.156392 (4.5023)  Time: 0.583s, 1755.52/s  (2.246s,  455.97/s)  LR: 6.593e-05  Data: 0.019 (1.649)
Train: 122 [ 300/1251 ( 24%)]  Loss:  4.733438 (4.5353)  Time: 7.942s,  128.94/s  (2.284s,  448.30/s)  LR: 6.593e-05  Data: 7.357 (1.684)
Train: 122 [ 350/1251 ( 28%)]  Loss:  4.206827 (4.4943)  Time: 0.582s, 1760.61/s  (2.242s,  456.78/s)  LR: 6.593e-05  Data: 0.018 (1.644)
Train: 122 [ 400/1251 ( 32%)]  Loss:  4.837791 (4.5324)  Time: 7.596s,  134.80/s  (2.263s,  452.41/s)  LR: 6.593e-05  Data: 7.031 (1.666)
Train: 122 [ 450/1251 ( 36%)]  Loss:  4.438111 (4.5230)  Time: 0.584s, 1752.61/s  (2.256s,  453.85/s)  LR: 6.593e-05  Data: 0.021 (1.659)
Train: 122 [ 500/1251 ( 40%)]  Loss:  4.269399 (4.4999)  Time: 6.219s,  164.67/s  (2.250s,  455.01/s)  LR: 6.593e-05  Data: 5.541 (1.652)
Train: 122 [ 550/1251 ( 44%)]  Loss:  4.537948 (4.5031)  Time: 0.588s, 1741.99/s  (2.240s,  457.23/s)  LR: 6.593e-05  Data: 0.018 (1.641)
Train: 122 [ 600/1251 ( 48%)]  Loss:  5.201955 (4.5569)  Time: 2.325s,  440.37/s  (2.230s,  459.22/s)  LR: 6.593e-05  Data: 1.650 (1.631)
Train: 122 [ 650/1251 ( 52%)]  Loss:  4.366866 (4.5433)  Time: 0.584s, 1753.21/s  (2.221s,  461.15/s)  LR: 6.593e-05  Data: 0.020 (1.621)
Train: 122 [ 700/1251 ( 56%)]  Loss:  4.734009 (4.5560)  Time: 3.580s,  286.03/s  (2.228s,  459.56/s)  LR: 6.593e-05  Data: 3.015 (1.630)
Train: 122 [ 750/1251 ( 60%)]  Loss:  5.027045 (4.5854)  Time: 0.587s, 1744.08/s  (2.246s,  455.85/s)  LR: 6.593e-05  Data: 0.019 (1.647)
Train: 122 [ 800/1251 ( 64%)]  Loss:  4.469237 (4.5786)  Time: 2.267s,  451.64/s  (2.252s,  454.65/s)  LR: 6.593e-05  Data: 1.704 (1.654)
Train: 122 [ 850/1251 ( 68%)]  Loss:  4.628300 (4.5814)  Time: 0.585s, 1749.60/s  (2.267s,  451.66/s)  LR: 6.593e-05  Data: 0.017 (1.669)
Train: 122 [ 900/1251 ( 72%)]  Loss:  4.755136 (4.5905)  Time: 4.214s,  242.99/s  (2.274s,  450.37/s)  LR: 6.593e-05  Data: 3.602 (1.675)
Train: 122 [ 950/1251 ( 76%)]  Loss:  4.391008 (4.5805)  Time: 0.588s, 1742.57/s  (2.273s,  450.58/s)  LR: 6.593e-05  Data: 0.022 (1.674)
Train: 122 [1000/1251 ( 80%)]  Loss:  4.116735 (4.5585)  Time: 6.476s,  158.13/s  (2.277s,  449.70/s)  LR: 6.593e-05  Data: 5.800 (1.677)
Train: 122 [1050/1251 ( 84%)]  Loss:  4.487970 (4.5553)  Time: 0.585s, 1750.62/s  (2.273s,  450.52/s)  LR: 6.593e-05  Data: 0.019 (1.671)
Train: 122 [1100/1251 ( 88%)]  Loss:  4.665288 (4.5600)  Time: 5.101s,  200.74/s  (2.292s,  446.81/s)  LR: 6.593e-05  Data: 4.535 (1.690)
Train: 122 [1150/1251 ( 92%)]  Loss:  3.966352 (4.5353)  Time: 0.586s, 1746.23/s  (2.294s,  446.33/s)  LR: 6.593e-05  Data: 0.020 (1.692)
Train: 122 [1200/1251 ( 96%)]  Loss:  4.366612 (4.5286)  Time: 4.345s,  235.66/s  (2.299s,  445.42/s)  LR: 6.593e-05  Data: 3.778 (1.697)
Train: 122 [1250/1251 (100%)]  Loss:  4.489682 (4.5271)  Time: 0.567s, 1806.04/s  (2.305s,  444.31/s)  LR: 6.593e-05  Data: 0.000 (1.702)
Test: [   0/48]  Time: 15.248 (15.248)  Loss:  1.1315 (1.1315)  Acc@1: 75.6836 (75.6836)  Acc@5: 92.6758 (92.6758)
Test: [  48/48]  Time: 0.359 (3.403)  Loss:  1.1282 (2.0103)  Acc@1: 76.5330 (55.9240)  Acc@5: 90.4481 (79.9580)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-122.pth.tar', 55.92400001953125)

Train: 123 [   0/1251 (  0%)]  Loss:  4.361003 (4.3610)  Time: 8.229s,  124.44/s  (8.229s,  124.44/s)  LR: 6.105e-05  Data: 7.495 (7.495)
Train: 123 [  50/1251 (  4%)]  Loss:  4.295274 (4.3281)  Time: 0.584s, 1753.32/s  (2.268s,  451.47/s)  LR: 6.105e-05  Data: 0.019 (1.665)
Train: 123 [ 100/1251 (  8%)]  Loss:  4.877034 (4.5111)  Time: 0.654s, 1565.75/s  (2.250s,  455.19/s)  LR: 6.105e-05  Data: 0.050 (1.649)
Train: 123 [ 150/1251 ( 12%)]  Loss:  4.350507 (4.4710)  Time: 0.588s, 1741.15/s  (2.345s,  436.71/s)  LR: 6.105e-05  Data: 0.019 (1.744)
Train: 123 [ 200/1251 ( 16%)]  Loss:  4.623302 (4.5014)  Time: 0.588s, 1741.84/s  (2.336s,  438.44/s)  LR: 6.105e-05  Data: 0.017 (1.736)
Train: 123 [ 250/1251 ( 20%)]  Loss:  4.214522 (4.4536)  Time: 0.590s, 1734.21/s  (2.332s,  439.20/s)  LR: 6.105e-05  Data: 0.020 (1.733)
Train: 123 [ 300/1251 ( 24%)]  Loss:  5.282570 (4.5720)  Time: 0.588s, 1742.28/s  (2.353s,  435.21/s)  LR: 6.105e-05  Data: 0.017 (1.754)
Train: 123 [ 350/1251 ( 28%)]  Loss:  4.250812 (4.5319)  Time: 0.585s, 1750.89/s  (2.334s,  438.67/s)  LR: 6.105e-05  Data: 0.020 (1.738)
Train: 123 [ 400/1251 ( 32%)]  Loss:  4.478773 (4.5260)  Time: 0.583s, 1755.01/s  (2.331s,  439.25/s)  LR: 6.105e-05  Data: 0.017 (1.736)
Train: 123 [ 450/1251 ( 36%)]  Loss:  4.935091 (4.5669)  Time: 0.587s, 1743.15/s  (2.311s,  443.16/s)  LR: 6.105e-05  Data: 0.017 (1.717)
Train: 123 [ 500/1251 ( 40%)]  Loss:  4.179578 (4.5317)  Time: 0.589s, 1737.29/s  (2.300s,  445.23/s)  LR: 6.105e-05  Data: 0.017 (1.707)
Train: 123 [ 550/1251 ( 44%)]  Loss:  4.891994 (4.5617)  Time: 0.586s, 1748.55/s  (2.322s,  440.93/s)  LR: 6.105e-05  Data: 0.018 (1.727)
Train: 123 [ 600/1251 ( 48%)]  Loss:  4.805399 (4.5805)  Time: 0.588s, 1741.95/s  (2.339s,  437.71/s)  LR: 6.105e-05  Data: 0.018 (1.745)
Train: 123 [ 650/1251 ( 52%)]  Loss:  4.950036 (4.6068)  Time: 0.584s, 1753.55/s  (2.349s,  435.96/s)  LR: 6.105e-05  Data: 0.018 (1.756)
Train: 123 [ 700/1251 ( 56%)]  Loss:  4.402853 (4.5932)  Time: 0.585s, 1749.96/s  (2.366s,  432.79/s)  LR: 6.105e-05  Data: 0.017 (1.774)
Train: 123 [ 750/1251 ( 60%)]  Loss:  4.864896 (4.6102)  Time: 0.583s, 1755.24/s  (2.361s,  433.66/s)  LR: 6.105e-05  Data: 0.020 (1.770)
Train: 123 [ 800/1251 ( 64%)]  Loss:  5.130664 (4.6408)  Time: 0.586s, 1747.44/s  (2.362s,  433.52/s)  LR: 6.105e-05  Data: 0.023 (1.771)
Train: 123 [ 850/1251 ( 68%)]  Loss:  4.667604 (4.6423)  Time: 0.587s, 1745.51/s  (2.351s,  435.61/s)  LR: 6.105e-05  Data: 0.018 (1.759)
Train: 123 [ 900/1251 ( 72%)]  Loss:  4.712426 (4.6460)  Time: 0.584s, 1752.06/s  (2.368s,  432.35/s)  LR: 6.105e-05  Data: 0.018 (1.777)
Train: 123 [ 950/1251 ( 76%)]  Loss:  4.393351 (4.6334)  Time: 0.585s, 1750.45/s  (2.359s,  434.16/s)  LR: 6.105e-05  Data: 0.020 (1.767)
Train: 123 [1000/1251 ( 80%)]  Loss:  4.674933 (4.6354)  Time: 0.586s, 1748.63/s  (2.367s,  432.66/s)  LR: 6.105e-05  Data: 0.019 (1.776)
Train: 123 [1050/1251 ( 84%)]  Loss:  4.872898 (4.6462)  Time: 0.586s, 1748.20/s  (2.367s,  432.60/s)  LR: 6.105e-05  Data: 0.019 (1.777)
Train: 123 [1100/1251 ( 88%)]  Loss:  5.014751 (4.6622)  Time: 0.792s, 1293.00/s  (2.367s,  432.57/s)  LR: 6.105e-05  Data: 0.155 (1.777)
Train: 123 [1150/1251 ( 92%)]  Loss:  5.088149 (4.6799)  Time: 0.591s, 1732.80/s  (2.359s,  434.09/s)  LR: 6.105e-05  Data: 0.018 (1.769)
Train: 123 [1200/1251 ( 96%)]  Loss:  4.664612 (4.6793)  Time: 0.586s, 1746.33/s  (2.356s,  434.58/s)  LR: 6.105e-05  Data: 0.022 (1.767)
Train: 123 [1250/1251 (100%)]  Loss:  4.810808 (4.6844)  Time: 0.567s, 1807.44/s  (2.351s,  435.60/s)  LR: 6.105e-05  Data: 0.000 (1.761)
Test: [   0/48]  Time: 13.955 (13.955)  Loss:  1.1450 (1.1450)  Acc@1: 76.1719 (76.1719)  Acc@5: 91.9922 (91.9922)
Test: [  48/48]  Time: 0.150 (3.545)  Loss:  1.1159 (2.0045)  Acc@1: 76.4151 (56.0520)  Acc@5: 90.8019 (79.9440)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-123.pth.tar', 56.05199996826172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-122.pth.tar', 55.92400001953125)

Train: 124 [   0/1251 (  0%)]  Loss:  4.753620 (4.7536)  Time: 12.670s,   80.82/s  (12.670s,   80.82/s)  LR: 5.638e-05  Data: 12.077 (12.077)
Train: 124 [  50/1251 (  4%)]  Loss:  4.182178 (4.4679)  Time: 0.585s, 1750.33/s  (2.588s,  395.70/s)  LR: 5.638e-05  Data: 0.020 (2.001)
Train: 124 [ 100/1251 (  8%)]  Loss:  4.328295 (4.4214)  Time: 0.585s, 1749.79/s  (2.495s,  410.35/s)  LR: 5.638e-05  Data: 0.022 (1.890)
Train: 124 [ 150/1251 ( 12%)]  Loss:  5.089052 (4.5883)  Time: 1.629s,  628.73/s  (2.408s,  425.26/s)  LR: 5.638e-05  Data: 0.560 (1.794)
Train: 124 [ 200/1251 ( 16%)]  Loss:  4.890971 (4.6488)  Time: 0.592s, 1728.37/s  (2.346s,  436.48/s)  LR: 5.638e-05  Data: 0.019 (1.733)
Train: 124 [ 250/1251 ( 20%)]  Loss:  4.922596 (4.6945)  Time: 0.589s, 1737.50/s  (2.312s,  442.92/s)  LR: 5.638e-05  Data: 0.026 (1.703)
Train: 124 [ 300/1251 ( 24%)]  Loss:  4.800097 (4.7095)  Time: 0.615s, 1666.39/s  (2.297s,  445.83/s)  LR: 5.638e-05  Data: 0.019 (1.682)
Train: 124 [ 350/1251 ( 28%)]  Loss:  4.598624 (4.6957)  Time: 1.035s,  989.34/s  (2.321s,  441.12/s)  LR: 5.638e-05  Data: 0.414 (1.706)
Train: 124 [ 400/1251 ( 32%)]  Loss:  4.681997 (4.6942)  Time: 0.586s, 1747.19/s  (2.302s,  444.92/s)  LR: 5.638e-05  Data: 0.022 (1.688)
Train: 124 [ 450/1251 ( 36%)]  Loss:  4.701338 (4.6949)  Time: 0.585s, 1751.26/s  (2.320s,  441.33/s)  LR: 5.638e-05  Data: 0.021 (1.707)
Train: 124 [ 500/1251 ( 40%)]  Loss:  4.368492 (4.6652)  Time: 0.585s, 1751.63/s  (2.312s,  442.98/s)  LR: 5.638e-05  Data: 0.020 (1.699)
Train: 124 [ 550/1251 ( 44%)]  Loss:  4.315264 (4.6360)  Time: 0.586s, 1746.12/s  (2.312s,  442.83/s)  LR: 5.638e-05  Data: 0.019 (1.701)
Train: 124 [ 600/1251 ( 48%)]  Loss:  4.309009 (4.6109)  Time: 0.587s, 1743.34/s  (2.305s,  444.31/s)  LR: 5.638e-05  Data: 0.020 (1.696)
Train: 124 [ 650/1251 ( 52%)]  Loss:  3.949909 (4.5637)  Time: 0.585s, 1750.73/s  (2.314s,  442.62/s)  LR: 5.638e-05  Data: 0.020 (1.705)
Train: 124 [ 700/1251 ( 56%)]  Loss:  4.471779 (4.5575)  Time: 0.586s, 1748.11/s  (2.307s,  443.88/s)  LR: 5.638e-05  Data: 0.020 (1.699)
Train: 124 [ 750/1251 ( 60%)]  Loss:  5.046890 (4.5881)  Time: 0.590s, 1736.64/s  (2.331s,  439.26/s)  LR: 5.638e-05  Data: 0.020 (1.725)
Train: 124 [ 800/1251 ( 64%)]  Loss:  4.469758 (4.5812)  Time: 0.586s, 1747.48/s  (2.327s,  440.12/s)  LR: 5.638e-05  Data: 0.022 (1.722)
Train: 124 [ 850/1251 ( 68%)]  Loss:  3.693545 (4.5319)  Time: 0.592s, 1728.76/s  (2.330s,  439.43/s)  LR: 5.638e-05  Data: 0.019 (1.726)
Train: 124 [ 900/1251 ( 72%)]  Loss:  4.627367 (4.5369)  Time: 0.586s, 1747.16/s  (2.333s,  438.89/s)  LR: 5.638e-05  Data: 0.022 (1.729)
Train: 124 [ 950/1251 ( 76%)]  Loss:  4.876671 (4.5539)  Time: 0.586s, 1748.84/s  (2.329s,  439.76/s)  LR: 5.638e-05  Data: 0.019 (1.726)
Train: 124 [1000/1251 ( 80%)]  Loss:  4.663976 (4.5591)  Time: 0.585s, 1749.21/s  (2.323s,  440.89/s)  LR: 5.638e-05  Data: 0.021 (1.720)
Train: 124 [1050/1251 ( 84%)]  Loss:  4.515133 (4.5571)  Time: 0.587s, 1743.75/s  (2.316s,  442.17/s)  LR: 5.638e-05  Data: 0.017 (1.713)
Train: 124 [1100/1251 ( 88%)]  Loss:  4.772827 (4.5665)  Time: 0.839s, 1220.43/s  (2.309s,  443.55/s)  LR: 5.638e-05  Data: 0.167 (1.706)
Train: 124 [1150/1251 ( 92%)]  Loss:  4.238480 (4.5528)  Time: 0.586s, 1747.90/s  (2.322s,  441.03/s)  LR: 5.638e-05  Data: 0.021 (1.718)
Train: 124 [1200/1251 ( 96%)]  Loss:  4.563152 (4.5532)  Time: 2.245s,  456.20/s  (2.332s,  439.08/s)  LR: 5.638e-05  Data: 1.573 (1.728)
Train: 124 [1250/1251 (100%)]  Loss:  4.529181 (4.5523)  Time: 0.568s, 1802.94/s  (2.338s,  438.01/s)  LR: 5.638e-05  Data: 0.000 (1.734)
Test: [   0/48]  Time: 14.735 (14.735)  Loss:  1.1386 (1.1386)  Acc@1: 76.1719 (76.1719)  Acc@5: 91.4062 (91.4062)
Test: [  48/48]  Time: 0.149 (3.295)  Loss:  1.1199 (2.0017)  Acc@1: 75.7076 (56.2400)  Acc@5: 90.5660 (80.1220)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-124.pth.tar', 56.24000004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-123.pth.tar', 56.05199996826172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-122.pth.tar', 55.92400001953125)

Train: 125 [   0/1251 (  0%)]  Loss:  4.558523 (4.5585)  Time: 10.510s,   97.43/s  (10.510s,   97.43/s)  LR: 5.192e-05  Data: 9.634 (9.634)
Train: 125 [  50/1251 (  4%)]  Loss:  4.359264 (4.4589)  Time: 0.582s, 1758.58/s  (2.347s,  436.22/s)  LR: 5.192e-05  Data: 0.019 (1.747)
Train: 125 [ 100/1251 (  8%)]  Loss:  4.777724 (4.5652)  Time: 3.609s,  283.77/s  (2.296s,  446.08/s)  LR: 5.192e-05  Data: 2.957 (1.691)
Train: 125 [ 150/1251 ( 12%)]  Loss:  4.763339 (4.6147)  Time: 0.586s, 1747.08/s  (2.223s,  460.64/s)  LR: 5.192e-05  Data: 0.020 (1.625)
Train: 125 [ 200/1251 ( 16%)]  Loss:  4.377890 (4.5673)  Time: 2.593s,  394.86/s  (2.314s,  442.62/s)  LR: 5.192e-05  Data: 1.950 (1.718)
Train: 125 [ 250/1251 ( 20%)]  Loss:  4.826361 (4.6105)  Time: 0.585s, 1749.78/s  (2.288s,  447.48/s)  LR: 5.192e-05  Data: 0.020 (1.694)
Train: 125 [ 300/1251 ( 24%)]  Loss:  4.800046 (4.6376)  Time: 3.179s,  322.08/s  (2.298s,  445.63/s)  LR: 5.192e-05  Data: 2.496 (1.700)
Train: 125 [ 350/1251 ( 28%)]  Loss:  4.818131 (4.6602)  Time: 0.839s, 1220.48/s  (2.292s,  446.85/s)  LR: 5.192e-05  Data: 0.272 (1.693)
Train: 125 [ 400/1251 ( 32%)]  Loss:  4.371021 (4.6280)  Time: 4.144s,  247.11/s  (2.289s,  447.27/s)  LR: 5.192e-05  Data: 3.581 (1.689)
Train: 125 [ 450/1251 ( 36%)]  Loss:  3.863679 (4.5516)  Time: 1.371s,  747.02/s  (2.273s,  450.45/s)  LR: 5.192e-05  Data: 0.807 (1.674)
Train: 125 [ 500/1251 ( 40%)]  Loss:  5.155270 (4.6065)  Time: 1.693s,  604.69/s  (2.258s,  453.50/s)  LR: 5.192e-05  Data: 1.030 (1.658)
Train: 125 [ 550/1251 ( 44%)]  Loss:  4.849349 (4.6267)  Time: 1.440s,  711.14/s  (2.239s,  457.44/s)  LR: 5.192e-05  Data: 0.877 (1.638)
Train: 125 [ 600/1251 ( 48%)]  Loss:  4.939020 (4.6507)  Time: 1.262s,  811.66/s  (2.280s,  449.04/s)  LR: 5.192e-05  Data: 0.697 (1.679)
Train: 125 [ 650/1251 ( 52%)]  Loss:  4.807232 (4.6619)  Time: 0.586s, 1746.74/s  (2.291s,  446.98/s)  LR: 5.192e-05  Data: 0.020 (1.690)
Train: 125 [ 700/1251 ( 56%)]  Loss:  5.000649 (4.6845)  Time: 1.362s,  752.11/s  (2.312s,  442.93/s)  LR: 5.192e-05  Data: 0.696 (1.709)
Train: 125 [ 750/1251 ( 60%)]  Loss:  4.894030 (4.6976)  Time: 4.260s,  240.37/s  (2.311s,  443.04/s)  LR: 5.192e-05  Data: 3.694 (1.705)
Train: 125 [ 800/1251 ( 64%)]  Loss:  4.755190 (4.7010)  Time: 0.584s, 1754.32/s  (2.308s,  443.71/s)  LR: 5.192e-05  Data: 0.018 (1.701)
Train: 125 [ 850/1251 ( 68%)]  Loss:  4.296710 (4.6785)  Time: 3.818s,  268.19/s  (2.302s,  444.77/s)  LR: 5.192e-05  Data: 3.210 (1.696)
Train: 125 [ 900/1251 ( 72%)]  Loss:  4.117793 (4.6490)  Time: 1.528s,  670.37/s  (2.295s,  446.20/s)  LR: 5.192e-05  Data: 0.843 (1.688)
Train: 125 [ 950/1251 ( 76%)]  Loss:  4.303768 (4.6317)  Time: 8.175s,  125.26/s  (2.292s,  446.73/s)  LR: 5.192e-05  Data: 7.482 (1.685)
Train: 125 [1000/1251 ( 80%)]  Loss:  4.087334 (4.6058)  Time: 2.222s,  460.75/s  (2.304s,  444.53/s)  LR: 5.192e-05  Data: 1.556 (1.696)
Train: 125 [1050/1251 ( 84%)]  Loss:  4.899548 (4.6192)  Time: 5.097s,  200.89/s  (2.311s,  443.06/s)  LR: 5.192e-05  Data: 4.496 (1.704)
Train: 125 [1100/1251 ( 88%)]  Loss:  4.686799 (4.6221)  Time: 2.752s,  372.09/s  (2.310s,  443.37/s)  LR: 5.192e-05  Data: 2.082 (1.703)
Train: 125 [1150/1251 ( 92%)]  Loss:  4.720931 (4.6262)  Time: 7.515s,  136.25/s  (2.315s,  442.24/s)  LR: 5.192e-05  Data: 6.952 (1.710)
Train: 125 [1200/1251 ( 96%)]  Loss:  4.311626 (4.6136)  Time: 0.839s, 1220.54/s  (2.312s,  443.00/s)  LR: 5.192e-05  Data: 0.274 (1.706)
Train: 125 [1250/1251 (100%)]  Loss:  4.463589 (4.6079)  Time: 0.566s, 1807.63/s  (2.308s,  443.67/s)  LR: 5.192e-05  Data: 0.000 (1.702)
Test: [   0/48]  Time: 14.068 (14.068)  Loss:  1.0763 (1.0763)  Acc@1: 77.4414 (77.4414)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.149 (3.167)  Loss:  1.1228 (2.0024)  Acc@1: 78.0660 (56.0700)  Acc@5: 91.2736 (80.2280)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-124.pth.tar', 56.24000004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-125.pth.tar', 56.0700000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-123.pth.tar', 56.05199996826172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-122.pth.tar', 55.92400001953125)

Train: 126 [   0/1251 (  0%)]  Loss:  4.490907 (4.4909)  Time: 11.122s,   92.07/s  (11.122s,   92.07/s)  LR: 4.768e-05  Data: 10.121 (10.121)
Train: 126 [  50/1251 (  4%)]  Loss:  4.249835 (4.3704)  Time: 0.584s, 1753.90/s  (2.685s,  381.37/s)  LR: 4.768e-05  Data: 0.020 (2.088)
Train: 126 [ 100/1251 (  8%)]  Loss:  4.991266 (4.5773)  Time: 2.460s,  416.20/s  (2.662s,  384.71/s)  LR: 4.768e-05  Data: 1.796 (2.038)
Train: 126 [ 150/1251 ( 12%)]  Loss:  3.684719 (4.3542)  Time: 0.584s, 1753.83/s  (2.559s,  400.12/s)  LR: 4.768e-05  Data: 0.020 (1.941)
Train: 126 [ 200/1251 ( 16%)]  Loss:  4.998401 (4.4830)  Time: 0.587s, 1743.11/s  (2.507s,  408.52/s)  LR: 4.768e-05  Data: 0.024 (1.893)
Train: 126 [ 250/1251 ( 20%)]  Loss:  4.996524 (4.5686)  Time: 3.127s,  327.46/s  (2.484s,  412.16/s)  LR: 4.768e-05  Data: 2.337 (1.870)
Train: 126 [ 300/1251 ( 24%)]  Loss:  4.598729 (4.5729)  Time: 1.193s,  858.27/s  (2.441s,  419.47/s)  LR: 4.768e-05  Data: 0.512 (1.826)
Train: 126 [ 350/1251 ( 28%)]  Loss:  4.240010 (4.5313)  Time: 3.980s,  257.29/s  (2.416s,  423.85/s)  LR: 4.768e-05  Data: 3.417 (1.803)
Train: 126 [ 400/1251 ( 32%)]  Loss:  5.300046 (4.6167)  Time: 3.621s,  282.78/s  (2.429s,  421.65/s)  LR: 4.768e-05  Data: 3.055 (1.816)
Train: 126 [ 450/1251 ( 36%)]  Loss:  4.159047 (4.5709)  Time: 2.736s,  374.31/s  (2.407s,  425.34/s)  LR: 4.768e-05  Data: 2.173 (1.794)
Train: 126 [ 500/1251 ( 40%)]  Loss:  4.274642 (4.5440)  Time: 0.588s, 1741.23/s  (2.399s,  426.83/s)  LR: 4.768e-05  Data: 0.023 (1.785)
Train: 126 [ 550/1251 ( 44%)]  Loss:  4.362632 (4.5289)  Time: 3.721s,  275.23/s  (2.405s,  425.75/s)  LR: 4.768e-05  Data: 3.146 (1.790)
Train: 126 [ 600/1251 ( 48%)]  Loss:  4.975059 (4.5632)  Time: 0.590s, 1735.98/s  (2.405s,  425.84/s)  LR: 4.768e-05  Data: 0.025 (1.789)
Train: 126 [ 650/1251 ( 52%)]  Loss:  4.254396 (4.5412)  Time: 1.841s,  556.19/s  (2.408s,  425.18/s)  LR: 4.768e-05  Data: 1.278 (1.791)
Train: 126 [ 700/1251 ( 56%)]  Loss:  4.215915 (4.5195)  Time: 0.594s, 1723.37/s  (2.403s,  426.12/s)  LR: 4.768e-05  Data: 0.026 (1.786)
Train: 126 [ 750/1251 ( 60%)]  Loss:  4.566050 (4.5224)  Time: 5.541s,  184.81/s  (2.403s,  426.19/s)  LR: 4.768e-05  Data: 4.842 (1.786)
Train: 126 [ 800/1251 ( 64%)]  Loss:  4.533169 (4.5230)  Time: 1.122s,  912.88/s  (2.414s,  424.20/s)  LR: 4.768e-05  Data: 0.461 (1.797)
Train: 126 [ 850/1251 ( 68%)]  Loss:  4.513783 (4.5225)  Time: 3.231s,  316.93/s  (2.418s,  423.51/s)  LR: 4.768e-05  Data: 2.549 (1.801)
Train: 126 [ 900/1251 ( 72%)]  Loss:  4.134611 (4.5021)  Time: 2.602s,  393.55/s  (2.415s,  423.98/s)  LR: 4.768e-05  Data: 1.955 (1.797)
Train: 126 [ 950/1251 ( 76%)]  Loss:  4.785152 (4.5162)  Time: 5.752s,  178.02/s  (2.413s,  424.30/s)  LR: 4.768e-05  Data: 5.171 (1.796)
Train: 126 [1000/1251 ( 80%)]  Loss:  4.826290 (4.5310)  Time: 0.591s, 1731.46/s  (2.405s,  425.83/s)  LR: 4.768e-05  Data: 0.021 (1.788)
Train: 126 [1050/1251 ( 84%)]  Loss:  4.246597 (4.5181)  Time: 4.554s,  224.88/s  (2.398s,  426.97/s)  LR: 4.768e-05  Data: 3.877 (1.782)
Train: 126 [1100/1251 ( 88%)]  Loss:  4.446540 (4.5150)  Time: 1.489s,  687.91/s  (2.387s,  429.06/s)  LR: 4.768e-05  Data: 0.925 (1.771)
Train: 126 [1150/1251 ( 92%)]  Loss:  5.073271 (4.5382)  Time: 5.894s,  173.73/s  (2.386s,  429.22/s)  LR: 4.768e-05  Data: 5.205 (1.770)
Train: 126 [1200/1251 ( 96%)]  Loss:  5.193019 (4.5644)  Time: 0.584s, 1752.24/s  (2.389s,  428.67/s)  LR: 4.768e-05  Data: 0.020 (1.774)
Train: 126 [1250/1251 (100%)]  Loss:  4.553581 (4.5640)  Time: 0.564s, 1816.53/s  (2.382s,  429.88/s)  LR: 4.768e-05  Data: 0.000 (1.768)
Test: [   0/48]  Time: 14.745 (14.745)  Loss:  1.1020 (1.1020)  Acc@1: 76.6602 (76.6602)  Acc@5: 92.6758 (92.6758)
Test: [  48/48]  Time: 0.149 (3.402)  Loss:  1.0874 (1.9808)  Acc@1: 77.0047 (56.4980)  Acc@5: 91.2736 (80.4320)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-126.pth.tar', 56.49799996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-124.pth.tar', 56.24000004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-125.pth.tar', 56.0700000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-123.pth.tar', 56.05199996826172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-122.pth.tar', 55.92400001953125)

Train: 127 [   0/1251 (  0%)]  Loss:  4.687881 (4.6879)  Time: 10.783s,   94.97/s  (10.783s,   94.97/s)  LR: 4.366e-05  Data: 9.855 (9.855)
Train: 127 [  50/1251 (  4%)]  Loss:  4.941357 (4.8146)  Time: 0.586s, 1746.04/s  (2.394s,  427.67/s)  LR: 4.366e-05  Data: 0.023 (1.778)
Train: 127 [ 100/1251 (  8%)]  Loss:  4.342409 (4.6572)  Time: 1.439s,  711.55/s  (2.310s,  443.24/s)  LR: 4.366e-05  Data: 0.691 (1.698)
Train: 127 [ 150/1251 ( 12%)]  Loss:  4.548040 (4.6299)  Time: 0.587s, 1745.37/s  (2.382s,  429.89/s)  LR: 4.366e-05  Data: 0.020 (1.769)
Train: 127 [ 200/1251 ( 16%)]  Loss:  4.735384 (4.6510)  Time: 4.375s,  234.08/s  (2.391s,  428.25/s)  LR: 4.366e-05  Data: 3.716 (1.778)
Train: 127 [ 250/1251 ( 20%)]  Loss:  4.794888 (4.6750)  Time: 0.586s, 1746.75/s  (2.464s,  415.65/s)  LR: 4.366e-05  Data: 0.019 (1.847)
Train: 127 [ 300/1251 ( 24%)]  Loss:  4.416216 (4.6380)  Time: 5.922s,  172.90/s  (2.497s,  410.07/s)  LR: 4.366e-05  Data: 5.281 (1.881)
Train: 127 [ 350/1251 ( 28%)]  Loss:  4.525259 (4.6239)  Time: 1.136s,  901.27/s  (2.500s,  409.65/s)  LR: 4.366e-05  Data: 0.456 (1.884)
Train: 127 [ 400/1251 ( 32%)]  Loss:  4.950246 (4.6602)  Time: 5.659s,  180.96/s  (2.509s,  408.20/s)  LR: 4.366e-05  Data: 4.995 (1.895)
Train: 127 [ 450/1251 ( 36%)]  Loss:  4.903300 (4.6845)  Time: 0.589s, 1738.17/s  (2.490s,  411.32/s)  LR: 4.366e-05  Data: 0.021 (1.877)
Train: 127 [ 500/1251 ( 40%)]  Loss:  4.375336 (4.6564)  Time: 6.640s,  154.21/s  (2.461s,  416.17/s)  LR: 4.366e-05  Data: 5.989 (1.851)
Train: 127 [ 550/1251 ( 44%)]  Loss:  4.755386 (4.6646)  Time: 0.590s, 1734.59/s  (2.441s,  419.56/s)  LR: 4.366e-05  Data: 0.025 (1.829)
Train: 127 [ 600/1251 ( 48%)]  Loss:  4.877143 (4.6810)  Time: 7.523s,  136.12/s  (2.477s,  413.46/s)  LR: 4.366e-05  Data: 6.675 (1.865)
Train: 127 [ 650/1251 ( 52%)]  Loss:  5.060491 (4.7081)  Time: 6.002s,  170.62/s  (2.502s,  409.20/s)  LR: 4.366e-05  Data: 5.350 (1.881)
Train: 127 [ 700/1251 ( 56%)]  Loss:  4.879779 (4.7195)  Time: 0.587s, 1745.67/s  (2.535s,  403.89/s)  LR: 4.366e-05  Data: 0.023 (1.908)
Train: 127 [ 750/1251 ( 60%)]  Loss:  4.823997 (4.7261)  Time: 23.722s,   43.17/s  (2.575s,  397.67/s)  LR: 4.366e-05  Data: 16.427 (1.941)
Train: 127 [ 800/1251 ( 64%)]  Loss:  4.817656 (4.7315)  Time: 0.587s, 1743.37/s  (2.565s,  399.15/s)  LR: 4.366e-05  Data: 0.018 (1.933)
Train: 127 [ 850/1251 ( 68%)]  Loss:  4.486630 (4.7179)  Time: 5.962s,  171.76/s  (2.565s,  399.18/s)  LR: 4.366e-05  Data: 5.232 (1.930)
Train: 127 [ 900/1251 ( 72%)]  Loss:  4.652152 (4.7144)  Time: 0.584s, 1753.13/s  (2.555s,  400.85/s)  LR: 4.366e-05  Data: 0.020 (1.922)
Train: 127 [ 950/1251 ( 76%)]  Loss:  4.796513 (4.7185)  Time: 6.235s,  164.23/s  (2.577s,  397.43/s)  LR: 4.366e-05  Data: 5.567 (1.945)
Train: 127 [1000/1251 ( 80%)]  Loss:  4.035972 (4.6860)  Time: 0.585s, 1751.33/s  (2.564s,  399.34/s)  LR: 4.366e-05  Data: 0.017 (1.934)
Train: 127 [1050/1251 ( 84%)]  Loss:  4.126898 (4.6606)  Time: 7.004s,  146.21/s  (2.557s,  400.43/s)  LR: 4.366e-05  Data: 6.325 (1.929)
Train: 127 [1100/1251 ( 88%)]  Loss:  4.974098 (4.6742)  Time: 0.584s, 1754.54/s  (2.546s,  402.27/s)  LR: 4.366e-05  Data: 0.019 (1.919)
Train: 127 [1150/1251 ( 92%)]  Loss:  4.426802 (4.6639)  Time: 7.164s,  142.93/s  (2.536s,  403.86/s)  LR: 4.366e-05  Data: 6.221 (1.910)
Train: 127 [1200/1251 ( 96%)]  Loss:  4.452583 (4.6555)  Time: 0.588s, 1741.85/s  (2.515s,  407.12/s)  LR: 4.366e-05  Data: 0.024 (1.891)
Train: 127 [1250/1251 (100%)]  Loss:  4.351075 (4.6437)  Time: 0.568s, 1802.82/s  (2.495s,  410.49/s)  LR: 4.366e-05  Data: 0.000 (1.871)
Test: [   0/48]  Time: 13.169 (13.169)  Loss:  1.1298 (1.1298)  Acc@1: 77.1484 (77.1484)  Acc@5: 91.9922 (91.9922)
Test: [  48/48]  Time: 0.149 (3.285)  Loss:  1.1021 (1.9889)  Acc@1: 77.5943 (56.5520)  Acc@5: 91.1557 (80.4540)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-127.pth.tar', 56.55200009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-126.pth.tar', 56.49799996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-124.pth.tar', 56.24000004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-125.pth.tar', 56.0700000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-123.pth.tar', 56.05199996826172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-122.pth.tar', 55.92400001953125)

Train: 128 [   0/1251 (  0%)]  Loss:  4.277229 (4.2772)  Time: 9.870s,  103.75/s  (9.870s,  103.75/s)  LR: 3.985e-05  Data: 9.198 (9.198)
Train: 128 [  50/1251 (  4%)]  Loss:  4.541910 (4.4096)  Time: 0.586s, 1748.33/s  (2.405s,  425.73/s)  LR: 3.985e-05  Data: 0.020 (1.813)
Train: 128 [ 100/1251 (  8%)]  Loss:  4.571364 (4.4635)  Time: 0.586s, 1748.40/s  (2.360s,  433.90/s)  LR: 3.985e-05  Data: 0.019 (1.766)
Train: 128 [ 150/1251 ( 12%)]  Loss:  4.744477 (4.5337)  Time: 0.585s, 1749.30/s  (2.264s,  452.36/s)  LR: 3.985e-05  Data: 0.021 (1.674)
Train: 128 [ 200/1251 ( 16%)]  Loss:  4.870774 (4.6012)  Time: 0.585s, 1749.81/s  (2.265s,  452.04/s)  LR: 3.985e-05  Data: 0.019 (1.676)
Train: 128 [ 250/1251 ( 20%)]  Loss:  4.801766 (4.6346)  Time: 0.590s, 1735.53/s  (2.219s,  461.46/s)  LR: 3.985e-05  Data: 0.022 (1.632)
Train: 128 [ 300/1251 ( 24%)]  Loss:  4.975567 (4.6833)  Time: 0.589s, 1738.38/s  (2.213s,  462.70/s)  LR: 3.985e-05  Data: 0.020 (1.625)
Train: 128 [ 350/1251 ( 28%)]  Loss:  4.587829 (4.6714)  Time: 0.586s, 1746.78/s  (2.190s,  467.48/s)  LR: 3.985e-05  Data: 0.021 (1.601)
Train: 128 [ 400/1251 ( 32%)]  Loss:  4.710265 (4.6757)  Time: 0.584s, 1754.66/s  (2.234s,  458.43/s)  LR: 3.985e-05  Data: 0.018 (1.642)
Train: 128 [ 450/1251 ( 36%)]  Loss:  4.983279 (4.7064)  Time: 0.587s, 1745.83/s  (2.232s,  458.79/s)  LR: 3.985e-05  Data: 0.021 (1.639)
Train: 128 [ 500/1251 ( 40%)]  Loss:  4.431762 (4.6815)  Time: 0.586s, 1746.42/s  (2.232s,  458.82/s)  LR: 3.985e-05  Data: 0.018 (1.637)
Train: 128 [ 550/1251 ( 44%)]  Loss:  4.224691 (4.6434)  Time: 0.585s, 1751.15/s  (2.260s,  453.12/s)  LR: 3.985e-05  Data: 0.020 (1.667)
Train: 128 [ 600/1251 ( 48%)]  Loss:  5.001970 (4.6710)  Time: 3.217s,  318.33/s  (2.272s,  450.74/s)  LR: 3.985e-05  Data: 2.536 (1.678)
Train: 128 [ 650/1251 ( 52%)]  Loss:  4.378095 (4.6501)  Time: 0.585s, 1751.86/s  (2.270s,  451.06/s)  LR: 3.985e-05  Data: 0.019 (1.675)
Train: 128 [ 700/1251 ( 56%)]  Loss:  4.837208 (4.6625)  Time: 5.157s,  198.56/s  (2.275s,  450.15/s)  LR: 3.985e-05  Data: 4.455 (1.679)
Train: 128 [ 750/1251 ( 60%)]  Loss:  3.888049 (4.6141)  Time: 0.583s, 1756.35/s  (2.261s,  452.82/s)  LR: 3.985e-05  Data: 0.018 (1.666)
Train: 128 [ 800/1251 ( 64%)]  Loss:  4.242588 (4.5923)  Time: 2.655s,  385.69/s  (2.284s,  448.40/s)  LR: 3.985e-05  Data: 1.990 (1.687)
Train: 128 [ 850/1251 ( 68%)]  Loss:  4.259982 (4.5738)  Time: 0.586s, 1747.52/s  (2.290s,  447.22/s)  LR: 3.985e-05  Data: 0.020 (1.694)
Train: 128 [ 900/1251 ( 72%)]  Loss:  5.133500 (4.6033)  Time: 4.820s,  212.44/s  (2.298s,  445.52/s)  LR: 3.985e-05  Data: 4.151 (1.703)
Train: 128 [ 950/1251 ( 76%)]  Loss:  4.718976 (4.6091)  Time: 0.584s, 1754.04/s  (2.291s,  447.06/s)  LR: 3.985e-05  Data: 0.019 (1.694)
Train: 128 [1000/1251 ( 80%)]  Loss:  4.730604 (4.6149)  Time: 2.570s,  398.38/s  (2.289s,  447.28/s)  LR: 3.985e-05  Data: 1.973 (1.693)
Train: 128 [1050/1251 ( 84%)]  Loss:  4.269409 (4.5991)  Time: 0.588s, 1741.20/s  (2.285s,  448.09/s)  LR: 3.985e-05  Data: 0.021 (1.688)
Train: 128 [1100/1251 ( 88%)]  Loss:  4.294477 (4.5859)  Time: 2.921s,  350.59/s  (2.287s,  447.83/s)  LR: 3.985e-05  Data: 2.352 (1.689)
Train: 128 [1150/1251 ( 92%)]  Loss:  3.733077 (4.5504)  Time: 0.584s, 1752.54/s  (2.292s,  446.74/s)  LR: 3.985e-05  Data: 0.019 (1.696)
Train: 128 [1200/1251 ( 96%)]  Loss:  5.208933 (4.5767)  Time: 0.585s, 1749.34/s  (2.295s,  446.21/s)  LR: 3.985e-05  Data: 0.021 (1.698)
Train: 128 [1250/1251 (100%)]  Loss:  4.943175 (4.5908)  Time: 0.565s, 1812.70/s  (2.298s,  445.53/s)  LR: 3.985e-05  Data: 0.000 (1.702)
Test: [   0/48]  Time: 13.922 (13.922)  Loss:  1.1489 (1.1489)  Acc@1: 75.9766 (75.9766)  Acc@5: 91.7969 (91.7969)
Test: [  48/48]  Time: 0.149 (3.306)  Loss:  1.1223 (1.9921)  Acc@1: 77.0047 (56.6520)  Acc@5: 90.6840 (80.5340)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-128.pth.tar', 56.65199996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-127.pth.tar', 56.55200009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-126.pth.tar', 56.49799996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-124.pth.tar', 56.24000004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-125.pth.tar', 56.0700000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-123.pth.tar', 56.05199996826172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-122.pth.tar', 55.92400001953125)

Train: 129 [   0/1251 (  0%)]  Loss:  4.651832 (4.6518)  Time: 11.375s,   90.02/s  (11.375s,   90.02/s)  LR: 3.627e-05  Data: 10.780 (10.780)
Train: 129 [  50/1251 (  4%)]  Loss:  4.626080 (4.6390)  Time: 0.586s, 1748.28/s  (2.371s,  431.84/s)  LR: 3.627e-05  Data: 0.020 (1.775)
Train: 129 [ 100/1251 (  8%)]  Loss:  5.246179 (4.8414)  Time: 0.584s, 1754.27/s  (2.297s,  445.72/s)  LR: 3.627e-05  Data: 0.019 (1.707)
Train: 129 [ 150/1251 ( 12%)]  Loss:  4.493711 (4.7545)  Time: 0.591s, 1733.29/s  (2.306s,  443.99/s)  LR: 3.627e-05  Data: 0.019 (1.718)
Train: 129 [ 200/1251 ( 16%)]  Loss:  4.849610 (4.7735)  Time: 0.585s, 1750.29/s  (2.258s,  453.53/s)  LR: 3.627e-05  Data: 0.021 (1.669)
Train: 129 [ 250/1251 ( 20%)]  Loss:  4.335308 (4.7005)  Time: 0.585s, 1749.92/s  (2.279s,  449.25/s)  LR: 3.627e-05  Data: 0.021 (1.688)
Train: 129 [ 300/1251 ( 24%)]  Loss:  4.630036 (4.6904)  Time: 2.976s,  344.12/s  (2.304s,  444.42/s)  LR: 3.627e-05  Data: 2.412 (1.714)
Train: 129 [ 350/1251 ( 28%)]  Loss:  4.511358 (4.6680)  Time: 0.587s, 1745.73/s  (2.297s,  445.78/s)  LR: 3.627e-05  Data: 0.019 (1.702)
Train: 129 [ 400/1251 ( 32%)]  Loss:  4.666803 (4.6679)  Time: 2.822s,  362.80/s  (2.283s,  448.59/s)  LR: 3.627e-05  Data: 2.124 (1.687)
Train: 129 [ 450/1251 ( 36%)]  Loss:  5.044631 (4.7056)  Time: 0.587s, 1745.36/s  (2.256s,  453.84/s)  LR: 3.627e-05  Data: 0.020 (1.660)
Train: 129 [ 500/1251 ( 40%)]  Loss:  4.095910 (4.6501)  Time: 4.916s,  208.31/s  (2.245s,  456.08/s)  LR: 3.627e-05  Data: 4.340 (1.649)
Train: 129 [ 550/1251 ( 44%)]  Loss:  4.160539 (4.6093)  Time: 0.587s, 1745.52/s  (2.226s,  460.02/s)  LR: 3.627e-05  Data: 0.019 (1.630)
Train: 129 [ 600/1251 ( 48%)]  Loss:  4.080901 (4.5687)  Time: 4.137s,  247.54/s  (2.223s,  460.66/s)  LR: 3.627e-05  Data: 3.467 (1.626)
Train: 129 [ 650/1251 ( 52%)]  Loss:  4.644971 (4.5741)  Time: 0.587s, 1745.53/s  (2.261s,  452.96/s)  LR: 3.627e-05  Data: 0.021 (1.664)
Train: 129 [ 700/1251 ( 56%)]  Loss:  4.972515 (4.6007)  Time: 5.700s,  179.65/s  (2.276s,  449.95/s)  LR: 3.627e-05  Data: 5.006 (1.679)
Train: 129 [ 750/1251 ( 60%)]  Loss:  4.824177 (4.6147)  Time: 0.585s, 1749.86/s  (2.270s,  451.19/s)  LR: 3.627e-05  Data: 0.021 (1.673)
Train: 129 [ 800/1251 ( 64%)]  Loss:  4.426185 (4.6036)  Time: 4.145s,  247.05/s  (2.271s,  450.93/s)  LR: 3.627e-05  Data: 3.475 (1.674)
Train: 129 [ 850/1251 ( 68%)]  Loss:  4.716165 (4.6098)  Time: 0.586s, 1748.43/s  (2.259s,  453.29/s)  LR: 3.627e-05  Data: 0.022 (1.662)
Train: 129 [ 900/1251 ( 72%)]  Loss:  4.750623 (4.6172)  Time: 6.435s,  159.14/s  (2.251s,  454.91/s)  LR: 3.627e-05  Data: 5.727 (1.654)
Train: 129 [ 950/1251 ( 76%)]  Loss:  4.136931 (4.5932)  Time: 0.586s, 1745.99/s  (2.238s,  457.60/s)  LR: 3.627e-05  Data: 0.021 (1.642)
Train: 129 [1000/1251 ( 80%)]  Loss:  4.435137 (4.5857)  Time: 6.758s,  151.52/s  (2.229s,  459.40/s)  LR: 3.627e-05  Data: 6.192 (1.634)
Train: 129 [1050/1251 ( 84%)]  Loss:  4.762832 (4.5937)  Time: 0.584s, 1753.63/s  (2.234s,  458.43/s)  LR: 3.627e-05  Data: 0.021 (1.639)
Train: 129 [1100/1251 ( 88%)]  Loss:  4.599943 (4.5940)  Time: 6.684s,  153.21/s  (2.254s,  454.25/s)  LR: 3.627e-05  Data: 6.099 (1.660)
Train: 129 [1150/1251 ( 92%)]  Loss:  4.635431 (4.5957)  Time: 0.586s, 1746.70/s  (2.247s,  455.80/s)  LR: 3.627e-05  Data: 0.021 (1.653)
Train: 129 [1200/1251 ( 96%)]  Loss:  4.719934 (4.6007)  Time: 5.896s,  173.67/s  (2.245s,  456.19/s)  LR: 3.627e-05  Data: 5.322 (1.651)
Train: 129 [1250/1251 (100%)]  Loss:  5.089045 (4.6195)  Time: 0.564s, 1816.09/s  (2.240s,  457.07/s)  LR: 3.627e-05  Data: 0.000 (1.648)
Test: [   0/48]  Time: 14.300 (14.300)  Loss:  1.1310 (1.1310)  Acc@1: 77.3438 (77.3438)  Acc@5: 92.4805 (92.4805)
Test: [  48/48]  Time: 0.150 (3.250)  Loss:  1.1698 (2.0035)  Acc@1: 76.1792 (56.6620)  Acc@5: 90.6840 (80.4620)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-129.pth.tar', 56.661999995117185)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-128.pth.tar', 56.65199996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-127.pth.tar', 56.55200009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-126.pth.tar', 56.49799996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-124.pth.tar', 56.24000004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-125.pth.tar', 56.0700000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-123.pth.tar', 56.05199996826172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-122.pth.tar', 55.92400001953125)

Train: 130 [   0/1251 (  0%)]  Loss:  4.744828 (4.7448)  Time: 10.536s,   97.19/s  (10.536s,   97.19/s)  LR: 3.291e-05  Data: 9.585 (9.585)
Train: 130 [  50/1251 (  4%)]  Loss:  4.983042 (4.8639)  Time: 0.586s, 1748.73/s  (2.323s,  440.77/s)  LR: 3.291e-05  Data: 0.018 (1.725)
Train: 130 [ 100/1251 (  8%)]  Loss:  4.918208 (4.8820)  Time: 6.301s,  162.51/s  (2.413s,  424.36/s)  LR: 3.291e-05  Data: 5.642 (1.808)
Train: 130 [ 150/1251 ( 12%)]  Loss:  4.531693 (4.7944)  Time: 0.582s, 1759.09/s  (2.307s,  443.93/s)  LR: 3.291e-05  Data: 0.019 (1.703)
Train: 130 [ 200/1251 ( 16%)]  Loss:  4.067095 (4.6490)  Time: 6.732s,  152.11/s  (2.366s,  432.72/s)  LR: 3.291e-05  Data: 6.129 (1.764)
Train: 130 [ 250/1251 ( 20%)]  Loss:  4.993681 (4.7064)  Time: 0.585s, 1751.09/s  (2.369s,  432.23/s)  LR: 3.291e-05  Data: 0.018 (1.768)
Train: 130 [ 300/1251 ( 24%)]  Loss:  5.014064 (4.7504)  Time: 6.776s,  151.11/s  (2.367s,  432.66/s)  LR: 3.291e-05  Data: 6.189 (1.769)
Train: 130 [ 350/1251 ( 28%)]  Loss:  4.650801 (4.7379)  Time: 0.587s, 1744.02/s  (2.339s,  437.76/s)  LR: 3.291e-05  Data: 0.021 (1.743)
Train: 130 [ 400/1251 ( 32%)]  Loss:  4.289939 (4.6882)  Time: 7.426s,  137.90/s  (2.334s,  438.70/s)  LR: 3.291e-05  Data: 6.707 (1.739)
Train: 130 [ 450/1251 ( 36%)]  Loss:  4.621778 (4.6815)  Time: 0.587s, 1744.30/s  (2.309s,  443.57/s)  LR: 3.291e-05  Data: 0.019 (1.714)
Train: 130 [ 500/1251 ( 40%)]  Loss:  5.229744 (4.7314)  Time: 7.463s,  137.22/s  (2.351s,  435.60/s)  LR: 3.291e-05  Data: 6.890 (1.757)
Train: 130 [ 550/1251 ( 44%)]  Loss:  4.849756 (4.7412)  Time: 0.589s, 1738.48/s  (2.357s,  434.37/s)  LR: 3.291e-05  Data: 0.018 (1.764)
Train: 130 [ 600/1251 ( 48%)]  Loss:  4.184598 (4.6984)  Time: 9.594s,  106.73/s  (2.382s,  429.94/s)  LR: 3.291e-05  Data: 9.001 (1.789)
Train: 130 [ 650/1251 ( 52%)]  Loss:  4.537134 (4.6869)  Time: 0.583s, 1755.00/s  (2.385s,  429.27/s)  LR: 3.291e-05  Data: 0.017 (1.793)
Train: 130 [ 700/1251 ( 56%)]  Loss:  4.346286 (4.6642)  Time: 7.697s,  133.04/s  (2.414s,  424.11/s)  LR: 3.291e-05  Data: 7.107 (1.822)
Train: 130 [ 750/1251 ( 60%)]  Loss:  4.385031 (4.6467)  Time: 1.019s, 1004.86/s  (2.407s,  425.50/s)  LR: 3.291e-05  Data: 0.330 (1.814)
Train: 130 [ 800/1251 ( 64%)]  Loss:  5.070611 (4.6717)  Time: 5.065s,  202.17/s  (2.399s,  426.83/s)  LR: 3.291e-05  Data: 4.488 (1.805)
Train: 130 [ 850/1251 ( 68%)]  Loss:  4.607658 (4.6681)  Time: 6.618s,  154.72/s  (2.411s,  424.71/s)  LR: 3.291e-05  Data: 6.040 (1.816)
Train: 130 [ 900/1251 ( 72%)]  Loss:  4.765173 (4.6732)  Time: 3.530s,  290.07/s  (2.409s,  425.04/s)  LR: 3.291e-05  Data: 2.966 (1.814)
Train: 130 [ 950/1251 ( 76%)]  Loss:  4.125279 (4.6458)  Time: 2.643s,  387.51/s  (2.419s,  423.39/s)  LR: 3.291e-05  Data: 2.079 (1.822)
Train: 130 [1000/1251 ( 80%)]  Loss:  4.514343 (4.6396)  Time: 4.869s,  210.32/s  (2.422s,  422.86/s)  LR: 3.291e-05  Data: 4.193 (1.825)
Train: 130 [1050/1251 ( 84%)]  Loss:  4.324281 (4.6252)  Time: 7.841s,  130.59/s  (2.424s,  422.37/s)  LR: 3.291e-05  Data: 7.253 (1.827)
Train: 130 [1100/1251 ( 88%)]  Loss:  5.160622 (4.6485)  Time: 1.903s,  538.03/s  (2.420s,  423.20/s)  LR: 3.291e-05  Data: 1.340 (1.822)
Train: 130 [1150/1251 ( 92%)]  Loss:  4.604820 (4.6467)  Time: 6.140s,  166.78/s  (2.415s,  423.94/s)  LR: 3.291e-05  Data: 5.536 (1.818)
Train: 130 [1200/1251 ( 96%)]  Loss:  5.021959 (4.6617)  Time: 1.187s,  862.36/s  (2.407s,  425.50/s)  LR: 3.291e-05  Data: 0.535 (1.809)
Train: 130 [1250/1251 (100%)]  Loss:  4.520967 (4.6563)  Time: 0.568s, 1803.98/s  (2.412s,  424.51/s)  LR: 3.291e-05  Data: 0.000 (1.814)
Test: [   0/48]  Time: 13.643 (13.643)  Loss:  1.1105 (1.1105)  Acc@1: 77.2461 (77.2461)  Acc@5: 92.3828 (92.3828)
Test: [  48/48]  Time: 0.149 (3.432)  Loss:  1.1110 (1.9754)  Acc@1: 77.9481 (56.8820)  Acc@5: 91.2736 (80.6440)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-130.pth.tar', 56.881999987792966)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-129.pth.tar', 56.661999995117185)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-128.pth.tar', 56.65199996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-127.pth.tar', 56.55200009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-126.pth.tar', 56.49799996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-124.pth.tar', 56.24000004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-125.pth.tar', 56.0700000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-123.pth.tar', 56.05199996826172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-122.pth.tar', 55.92400001953125)

Train: 131 [   0/1251 (  0%)]  Loss:  4.527118 (4.5271)  Time: 12.163s,   84.19/s  (12.163s,   84.19/s)  LR: 2.978e-05  Data: 11.356 (11.356)
Train: 131 [  50/1251 (  4%)]  Loss:  4.459368 (4.4932)  Time: 0.583s, 1756.75/s  (2.543s,  402.63/s)  LR: 2.978e-05  Data: 0.019 (1.952)
Train: 131 [ 100/1251 (  8%)]  Loss:  4.142917 (4.3765)  Time: 0.587s, 1743.64/s  (2.469s,  414.78/s)  LR: 2.978e-05  Data: 0.020 (1.880)
Train: 131 [ 150/1251 ( 12%)]  Loss:  4.608705 (4.4345)  Time: 0.584s, 1753.35/s  (2.376s,  430.97/s)  LR: 2.978e-05  Data: 0.019 (1.785)
Train: 131 [ 200/1251 ( 16%)]  Loss:  4.360984 (4.4198)  Time: 0.586s, 1746.56/s  (2.360s,  433.92/s)  LR: 2.978e-05  Data: 0.023 (1.772)
Train: 131 [ 250/1251 ( 20%)]  Loss:  4.864353 (4.4939)  Time: 0.587s, 1743.48/s  (2.380s,  430.24/s)  LR: 2.978e-05  Data: 0.018 (1.792)
Train: 131 [ 300/1251 ( 24%)]  Loss:  4.298105 (4.4659)  Time: 0.586s, 1746.87/s  (2.434s,  420.63/s)  LR: 2.978e-05  Data: 0.019 (1.845)
Train: 131 [ 350/1251 ( 28%)]  Loss:  5.020704 (4.5353)  Time: 0.585s, 1749.11/s  (2.425s,  422.19/s)  LR: 2.978e-05  Data: 0.020 (1.837)
Train: 131 [ 400/1251 ( 32%)]  Loss:  5.035988 (4.5909)  Time: 0.584s, 1754.45/s  (2.418s,  423.56/s)  LR: 2.978e-05  Data: 0.020 (1.830)
Train: 131 [ 450/1251 ( 36%)]  Loss:  4.207956 (4.5526)  Time: 0.586s, 1746.39/s  (2.398s,  427.02/s)  LR: 2.978e-05  Data: 0.023 (1.810)
Train: 131 [ 500/1251 ( 40%)]  Loss:  4.854466 (4.5801)  Time: 0.586s, 1748.85/s  (2.397s,  427.12/s)  LR: 2.978e-05  Data: 0.019 (1.810)
Train: 131 [ 550/1251 ( 44%)]  Loss:  4.069853 (4.5375)  Time: 0.585s, 1751.36/s  (2.379s,  430.43/s)  LR: 2.978e-05  Data: 0.019 (1.791)
Train: 131 [ 600/1251 ( 48%)]  Loss:  5.061934 (4.5779)  Time: 0.980s, 1045.40/s  (2.384s,  429.55/s)  LR: 2.978e-05  Data: 0.310 (1.793)
Train: 131 [ 650/1251 ( 52%)]  Loss:  4.805078 (4.5941)  Time: 0.588s, 1741.27/s  (2.409s,  425.04/s)  LR: 2.978e-05  Data: 0.022 (1.817)
Train: 131 [ 700/1251 ( 56%)]  Loss:  4.894641 (4.6141)  Time: 0.589s, 1738.12/s  (2.421s,  423.00/s)  LR: 2.978e-05  Data: 0.018 (1.828)
Train: 131 [ 750/1251 ( 60%)]  Loss:  4.375365 (4.5992)  Time: 0.584s, 1753.87/s  (2.418s,  423.48/s)  LR: 2.978e-05  Data: 0.020 (1.825)
Train: 131 [ 800/1251 ( 64%)]  Loss:  4.717571 (4.6062)  Time: 0.586s, 1748.35/s  (2.423s,  422.61/s)  LR: 2.978e-05  Data: 0.018 (1.831)
Train: 131 [ 850/1251 ( 68%)]  Loss:  4.716101 (4.6123)  Time: 0.582s, 1758.28/s  (2.416s,  423.86/s)  LR: 2.978e-05  Data: 0.017 (1.824)
Train: 131 [ 900/1251 ( 72%)]  Loss:  4.073980 (4.5840)  Time: 0.584s, 1754.57/s  (2.412s,  424.60/s)  LR: 2.978e-05  Data: 0.020 (1.821)
Train: 131 [ 950/1251 ( 76%)]  Loss:  4.598244 (4.5847)  Time: 0.591s, 1731.28/s  (2.404s,  425.92/s)  LR: 2.978e-05  Data: 0.025 (1.812)
Train: 131 [1000/1251 ( 80%)]  Loss:  4.329306 (4.5725)  Time: 0.588s, 1742.98/s  (2.399s,  426.91/s)  LR: 2.978e-05  Data: 0.022 (1.805)
Train: 131 [1050/1251 ( 84%)]  Loss:  4.640882 (4.5756)  Time: 0.585s, 1749.40/s  (2.408s,  425.30/s)  LR: 2.978e-05  Data: 0.022 (1.813)
Train: 131 [1100/1251 ( 88%)]  Loss:  4.645323 (4.5786)  Time: 3.103s,  330.03/s  (2.414s,  424.12/s)  LR: 2.978e-05  Data: 2.540 (1.818)
Train: 131 [1150/1251 ( 92%)]  Loss:  4.770815 (4.5867)  Time: 0.585s, 1749.34/s  (2.431s,  421.26/s)  LR: 2.978e-05  Data: 0.022 (1.833)
Train: 131 [1200/1251 ( 96%)]  Loss:  4.426362 (4.5802)  Time: 4.072s,  251.48/s  (2.430s,  421.33/s)  LR: 2.978e-05  Data: 3.497 (1.831)
Train: 131 [1250/1251 (100%)]  Loss:  4.719074 (4.5856)  Time: 0.568s, 1804.32/s  (2.423s,  422.54/s)  LR: 2.978e-05  Data: 0.000 (1.824)
Test: [   0/48]  Time: 14.821 (14.821)  Loss:  1.0631 (1.0631)  Acc@1: 76.8555 (76.8555)  Acc@5: 92.7734 (92.7734)
Test: [  48/48]  Time: 0.148 (3.303)  Loss:  1.0833 (1.9617)  Acc@1: 77.3585 (57.0540)  Acc@5: 91.3915 (80.8160)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-131.pth.tar', 57.05400011962891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-130.pth.tar', 56.881999987792966)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-129.pth.tar', 56.661999995117185)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-128.pth.tar', 56.65199996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-127.pth.tar', 56.55200009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-126.pth.tar', 56.49799996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-124.pth.tar', 56.24000004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-125.pth.tar', 56.0700000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-123.pth.tar', 56.05199996826172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-122.pth.tar', 55.92400001953125)

Train: 132 [   0/1251 (  0%)]  Loss:  4.649384 (4.6494)  Time: 11.178s,   91.61/s  (11.178s,   91.61/s)  LR: 2.687e-05  Data: 10.430 (10.430)
Train: 132 [  50/1251 (  4%)]  Loss:  4.635217 (4.6423)  Time: 0.588s, 1741.06/s  (2.417s,  423.58/s)  LR: 2.687e-05  Data: 0.019 (1.827)
Train: 132 [ 100/1251 (  8%)]  Loss:  4.697079 (4.6606)  Time: 2.161s,  473.92/s  (2.535s,  403.96/s)  LR: 2.687e-05  Data: 1.571 (1.939)
Train: 132 [ 150/1251 ( 12%)]  Loss:  4.082093 (4.5159)  Time: 0.587s, 1743.89/s  (2.496s,  410.28/s)  LR: 2.687e-05  Data: 0.023 (1.894)
Train: 132 [ 200/1251 ( 16%)]  Loss:  4.924403 (4.5976)  Time: 5.471s,  187.18/s  (2.477s,  413.41/s)  LR: 2.687e-05  Data: 4.907 (1.880)
Train: 132 [ 250/1251 ( 20%)]  Loss:  4.518534 (4.5845)  Time: 0.586s, 1748.68/s  (2.424s,  422.37/s)  LR: 2.687e-05  Data: 0.022 (1.829)
Train: 132 [ 300/1251 ( 24%)]  Loss:  4.970268 (4.6396)  Time: 6.315s,  162.15/s  (2.415s,  423.94/s)  LR: 2.687e-05  Data: 5.657 (1.817)
Train: 132 [ 350/1251 ( 28%)]  Loss:  4.066563 (4.5679)  Time: 0.593s, 1727.97/s  (2.386s,  429.08/s)  LR: 2.687e-05  Data: 0.021 (1.787)
Train: 132 [ 400/1251 ( 32%)]  Loss:  4.614718 (4.5731)  Time: 6.218s,  164.68/s  (2.375s,  431.16/s)  LR: 2.687e-05  Data: 5.523 (1.776)
Train: 132 [ 450/1251 ( 36%)]  Loss:  4.024045 (4.5182)  Time: 0.585s, 1749.74/s  (2.396s,  427.30/s)  LR: 2.687e-05  Data: 0.018 (1.800)
Train: 132 [ 500/1251 ( 40%)]  Loss:  4.787274 (4.5427)  Time: 8.725s,  117.37/s  (2.397s,  427.13/s)  LR: 2.687e-05  Data: 8.064 (1.801)
Train: 132 [ 550/1251 ( 44%)]  Loss:  4.524117 (4.5411)  Time: 0.589s, 1739.31/s  (2.394s,  427.71/s)  LR: 2.687e-05  Data: 0.022 (1.800)
Train: 132 [ 600/1251 ( 48%)]  Loss:  4.965699 (4.5738)  Time: 8.805s,  116.30/s  (2.416s,  423.90/s)  LR: 2.687e-05  Data: 8.162 (1.822)
Train: 132 [ 650/1251 ( 52%)]  Loss:  4.704446 (4.5831)  Time: 0.587s, 1744.14/s  (2.417s,  423.65/s)  LR: 2.687e-05  Data: 0.020 (1.824)
Train: 132 [ 700/1251 ( 56%)]  Loss:  4.478917 (4.5762)  Time: 7.469s,  137.10/s  (2.443s,  419.18/s)  LR: 2.687e-05  Data: 6.894 (1.850)
Train: 132 [ 750/1251 ( 60%)]  Loss:  4.716567 (4.5850)  Time: 0.585s, 1751.26/s  (2.428s,  421.67/s)  LR: 2.687e-05  Data: 0.019 (1.836)
Train: 132 [ 800/1251 ( 64%)]  Loss:  4.592837 (4.5854)  Time: 10.282s,   99.60/s  (2.442s,  419.25/s)  LR: 2.687e-05  Data: 9.691 (1.850)
Train: 132 [ 850/1251 ( 68%)]  Loss:  4.995702 (4.6082)  Time: 0.585s, 1751.02/s  (2.429s,  421.66/s)  LR: 2.687e-05  Data: 0.020 (1.837)
Train: 132 [ 900/1251 ( 72%)]  Loss:  4.774632 (4.6170)  Time: 6.780s,  151.03/s  (2.433s,  420.94/s)  LR: 2.687e-05  Data: 6.138 (1.842)
Train: 132 [ 950/1251 ( 76%)]  Loss:  4.188857 (4.5956)  Time: 0.588s, 1741.60/s  (2.426s,  422.17/s)  LR: 2.687e-05  Data: 0.020 (1.835)
Train: 132 [1000/1251 ( 80%)]  Loss:  4.580267 (4.5948)  Time: 8.080s,  126.74/s  (2.429s,  421.62/s)  LR: 2.687e-05  Data: 7.501 (1.838)
Train: 132 [1050/1251 ( 84%)]  Loss:  4.535865 (4.5922)  Time: 0.583s, 1757.66/s  (2.419s,  423.24/s)  LR: 2.687e-05  Data: 0.019 (1.829)
Train: 132 [1100/1251 ( 88%)]  Loss:  4.284211 (4.5788)  Time: 8.573s,  119.44/s  (2.416s,  423.83/s)  LR: 2.687e-05  Data: 7.934 (1.826)
Train: 132 [1150/1251 ( 92%)]  Loss:  4.262654 (4.5656)  Time: 0.588s, 1742.49/s  (2.405s,  425.77/s)  LR: 2.687e-05  Data: 0.024 (1.815)
Train: 132 [1200/1251 ( 96%)]  Loss:  4.577981 (4.5661)  Time: 7.551s,  135.62/s  (2.419s,  423.25/s)  LR: 2.687e-05  Data: 6.943 (1.830)
Train: 132 [1250/1251 (100%)]  Loss:  4.709489 (4.5716)  Time: 0.565s, 1812.38/s  (2.420s,  423.16/s)  LR: 2.687e-05  Data: 0.000 (1.831)
Test: [   0/48]  Time: 14.174 (14.174)  Loss:  1.0810 (1.0810)  Acc@1: 77.7344 (77.7344)  Acc@5: 92.7734 (92.7734)
Test: [  48/48]  Time: 0.149 (3.392)  Loss:  1.1192 (1.9683)  Acc@1: 77.1226 (57.2800)  Acc@5: 90.6840 (80.7780)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-132.pth.tar', 57.28000001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-131.pth.tar', 57.05400011962891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-130.pth.tar', 56.881999987792966)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-129.pth.tar', 56.661999995117185)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-128.pth.tar', 56.65199996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-127.pth.tar', 56.55200009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-126.pth.tar', 56.49799996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-124.pth.tar', 56.24000004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-125.pth.tar', 56.0700000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-123.pth.tar', 56.05199996826172)

Train: 133 [   0/1251 (  0%)]  Loss:  4.711806 (4.7118)  Time: 11.956s,   85.65/s  (11.956s,   85.65/s)  LR: 2.419e-05  Data: 10.918 (10.918)
Train: 133 [  50/1251 (  4%)]  Loss:  4.522641 (4.6172)  Time: 0.585s, 1750.66/s  (2.475s,  413.65/s)  LR: 2.419e-05  Data: 0.021 (1.850)
Train: 133 [ 100/1251 (  8%)]  Loss:  4.633364 (4.6226)  Time: 1.048s,  977.49/s  (2.391s,  428.30/s)  LR: 2.419e-05  Data: 0.444 (1.777)
Train: 133 [ 150/1251 ( 12%)]  Loss:  4.961895 (4.7074)  Time: 0.586s, 1746.24/s  (2.332s,  439.13/s)  LR: 2.419e-05  Data: 0.018 (1.721)
Train: 133 [ 200/1251 ( 16%)]  Loss:  4.354081 (4.6368)  Time: 2.955s,  346.53/s  (2.322s,  441.03/s)  LR: 2.419e-05  Data: 2.353 (1.714)
Train: 133 [ 250/1251 ( 20%)]  Loss:  4.313569 (4.5829)  Time: 0.587s, 1744.24/s  (2.437s,  420.11/s)  LR: 2.419e-05  Data: 0.019 (1.834)
Train: 133 [ 300/1251 ( 24%)]  Loss:  4.018613 (4.5023)  Time: 5.157s,  198.56/s  (2.419s,  423.26/s)  LR: 2.419e-05  Data: 4.574 (1.814)
Train: 133 [ 350/1251 ( 28%)]  Loss:  4.815518 (4.5414)  Time: 0.588s, 1741.33/s  (2.416s,  423.80/s)  LR: 2.419e-05  Data: 0.022 (1.812)
Train: 133 [ 400/1251 ( 32%)]  Loss:  4.712003 (4.5604)  Time: 7.681s,  133.31/s  (2.422s,  422.73/s)  LR: 2.419e-05  Data: 7.103 (1.821)
Train: 133 [ 450/1251 ( 36%)]  Loss:  4.683381 (4.5727)  Time: 0.585s, 1750.31/s  (2.405s,  425.84/s)  LR: 2.419e-05  Data: 0.021 (1.806)
Train: 133 [ 500/1251 ( 40%)]  Loss:  4.683216 (4.5827)  Time: 6.797s,  150.66/s  (2.393s,  427.87/s)  LR: 2.419e-05  Data: 6.215 (1.796)
Train: 133 [ 550/1251 ( 44%)]  Loss:  4.515263 (4.5771)  Time: 0.588s, 1742.38/s  (2.375s,  431.25/s)  LR: 2.419e-05  Data: 0.023 (1.778)
Train: 133 [ 600/1251 ( 48%)]  Loss:  4.415011 (4.5646)  Time: 14.931s,   68.58/s  (2.392s,  428.11/s)  LR: 2.419e-05  Data: 14.281 (1.796)
Train: 133 [ 650/1251 ( 52%)]  Loss:  4.811612 (4.5823)  Time: 0.591s, 1733.53/s  (2.419s,  423.23/s)  LR: 2.419e-05  Data: 0.022 (1.824)
Train: 133 [ 700/1251 ( 56%)]  Loss:  4.282380 (4.5623)  Time: 4.548s,  225.14/s  (2.438s,  419.98/s)  LR: 2.419e-05  Data: 3.939 (1.842)
Train: 133 [ 750/1251 ( 60%)]  Loss:  4.335764 (4.5481)  Time: 0.586s, 1747.62/s  (2.435s,  420.56/s)  LR: 2.419e-05  Data: 0.020 (1.839)
Train: 133 [ 800/1251 ( 64%)]  Loss:  4.535896 (4.5474)  Time: 8.362s,  122.46/s  (2.434s,  420.64/s)  LR: 2.419e-05  Data: 7.798 (1.839)
Train: 133 [ 850/1251 ( 68%)]  Loss:  4.200864 (4.5282)  Time: 0.590s, 1735.00/s  (2.426s,  422.01/s)  LR: 2.419e-05  Data: 0.021 (1.832)
Train: 133 [ 900/1251 ( 72%)]  Loss:  3.868353 (4.4934)  Time: 7.589s,  134.93/s  (2.425s,  422.24/s)  LR: 2.419e-05  Data: 6.998 (1.831)
Train: 133 [ 950/1251 ( 76%)]  Loss:  4.567363 (4.4971)  Time: 0.587s, 1744.41/s  (2.417s,  423.64/s)  LR: 2.419e-05  Data: 0.019 (1.823)
Train: 133 [1000/1251 ( 80%)]  Loss:  4.176667 (4.4819)  Time: 7.309s,  140.10/s  (2.432s,  421.06/s)  LR: 2.419e-05  Data: 6.717 (1.838)
Train: 133 [1050/1251 ( 84%)]  Loss:  4.693843 (4.4915)  Time: 0.588s, 1742.86/s  (2.432s,  421.04/s)  LR: 2.419e-05  Data: 0.020 (1.838)
Train: 133 [1100/1251 ( 88%)]  Loss:  4.661500 (4.4989)  Time: 14.813s,   69.13/s  (2.442s,  419.36/s)  LR: 2.419e-05  Data: 14.226 (1.849)
Train: 133 [1150/1251 ( 92%)]  Loss:  4.722672 (4.5082)  Time: 0.584s, 1753.72/s  (2.448s,  418.31/s)  LR: 2.419e-05  Data: 0.020 (1.855)
Train: 133 [1200/1251 ( 96%)]  Loss:  4.603400 (4.5120)  Time: 8.247s,  124.17/s  (2.450s,  417.97/s)  LR: 2.419e-05  Data: 7.606 (1.856)
Train: 133 [1250/1251 (100%)]  Loss:  4.285964 (4.5033)  Time: 0.565s, 1813.64/s  (2.440s,  419.69/s)  LR: 2.419e-05  Data: 0.000 (1.846)
Test: [   0/48]  Time: 14.111 (14.111)  Loss:  1.1191 (1.1191)  Acc@1: 77.2461 (77.2461)  Acc@5: 92.2852 (92.2852)
Test: [  48/48]  Time: 0.151 (3.263)  Loss:  1.1256 (1.9674)  Acc@1: 77.4764 (57.1240)  Acc@5: 90.9198 (80.9000)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-132.pth.tar', 57.28000001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-133.pth.tar', 57.124000041503905)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-131.pth.tar', 57.05400011962891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-130.pth.tar', 56.881999987792966)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-129.pth.tar', 56.661999995117185)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-128.pth.tar', 56.65199996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-127.pth.tar', 56.55200009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-126.pth.tar', 56.49799996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-124.pth.tar', 56.24000004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-125.pth.tar', 56.0700000390625)

Train: 134 [   0/1251 (  0%)]  Loss:  4.734421 (4.7344)  Time: 11.768s,   87.02/s  (11.768s,   87.02/s)  LR: 2.173e-05  Data: 11.205 (11.205)
Train: 134 [  50/1251 (  4%)]  Loss:  4.355491 (4.5450)  Time: 0.584s, 1752.42/s  (2.706s,  378.44/s)  LR: 2.173e-05  Data: 0.021 (2.114)
Train: 134 [ 100/1251 (  8%)]  Loss:  4.667973 (4.5860)  Time: 3.464s,  295.64/s  (2.618s,  391.18/s)  LR: 2.173e-05  Data: 2.520 (2.021)
Train: 134 [ 150/1251 ( 12%)]  Loss:  4.364902 (4.5307)  Time: 0.587s, 1744.52/s  (2.527s,  405.25/s)  LR: 2.173e-05  Data: 0.019 (1.930)
Train: 134 [ 200/1251 ( 16%)]  Loss:  4.881021 (4.6008)  Time: 6.187s,  165.50/s  (2.464s,  415.56/s)  LR: 2.173e-05  Data: 5.543 (1.865)
Train: 134 [ 250/1251 ( 20%)]  Loss:  4.599802 (4.6006)  Time: 0.588s, 1741.47/s  (2.422s,  422.85/s)  LR: 2.173e-05  Data: 0.021 (1.823)
Train: 134 [ 300/1251 ( 24%)]  Loss:  5.066853 (4.6672)  Time: 7.283s,  140.60/s  (2.399s,  426.81/s)  LR: 2.173e-05  Data: 6.675 (1.804)
Train: 134 [ 350/1251 ( 28%)]  Loss:  4.782516 (4.6816)  Time: 0.588s, 1741.55/s  (2.361s,  433.65/s)  LR: 2.173e-05  Data: 0.022 (1.766)
Train: 134 [ 400/1251 ( 32%)]  Loss:  4.756017 (4.6899)  Time: 7.778s,  131.65/s  (2.392s,  428.15/s)  LR: 2.173e-05  Data: 7.103 (1.796)
Train: 134 [ 450/1251 ( 36%)]  Loss:  4.700705 (4.6910)  Time: 0.586s, 1747.28/s  (2.369s,  432.31/s)  LR: 2.173e-05  Data: 0.022 (1.773)
Train: 134 [ 500/1251 ( 40%)]  Loss:  4.526515 (4.6760)  Time: 6.908s,  148.24/s  (2.386s,  429.18/s)  LR: 2.173e-05  Data: 6.228 (1.791)
Train: 134 [ 550/1251 ( 44%)]  Loss:  4.231025 (4.6389)  Time: 0.587s, 1743.88/s  (2.387s,  429.01/s)  LR: 2.173e-05  Data: 0.019 (1.793)
Train: 134 [ 600/1251 ( 48%)]  Loss:  4.558642 (4.6328)  Time: 7.759s,  131.97/s  (2.402s,  426.37/s)  LR: 2.173e-05  Data: 7.182 (1.808)
Train: 134 [ 650/1251 ( 52%)]  Loss:  4.517872 (4.6246)  Time: 0.585s, 1749.44/s  (2.395s,  427.64/s)  LR: 2.173e-05  Data: 0.021 (1.801)
Train: 134 [ 700/1251 ( 56%)]  Loss:  4.479613 (4.6149)  Time: 7.263s,  140.98/s  (2.422s,  422.84/s)  LR: 2.173e-05  Data: 6.574 (1.829)
Train: 134 [ 750/1251 ( 60%)]  Loss:  4.803308 (4.6267)  Time: 0.591s, 1732.17/s  (2.406s,  425.64/s)  LR: 2.173e-05  Data: 0.021 (1.814)
Train: 134 [ 800/1251 ( 64%)]  Loss:  4.135325 (4.5978)  Time: 4.215s,  242.92/s  (2.422s,  422.75/s)  LR: 2.173e-05  Data: 3.532 (1.830)
Train: 134 [ 850/1251 ( 68%)]  Loss:  4.622134 (4.5991)  Time: 0.587s, 1744.96/s  (2.422s,  422.75/s)  LR: 2.173e-05  Data: 0.021 (1.830)
Train: 134 [ 900/1251 ( 72%)]  Loss:  3.991885 (4.5672)  Time: 4.693s,  218.22/s  (2.424s,  422.43/s)  LR: 2.173e-05  Data: 3.989 (1.830)
Train: 134 [ 950/1251 ( 76%)]  Loss:  4.634936 (4.5705)  Time: 0.586s, 1747.53/s  (2.420s,  423.12/s)  LR: 2.173e-05  Data: 0.020 (1.826)
Train: 134 [1000/1251 ( 80%)]  Loss:  4.670857 (4.5753)  Time: 5.692s,  179.92/s  (2.418s,  423.49/s)  LR: 2.173e-05  Data: 4.995 (1.823)
Train: 134 [1050/1251 ( 84%)]  Loss:  4.621129 (4.5774)  Time: 0.586s, 1746.61/s  (2.406s,  425.52/s)  LR: 2.173e-05  Data: 0.021 (1.812)
Train: 134 [1100/1251 ( 88%)]  Loss:  4.851304 (4.5893)  Time: 5.581s,  183.47/s  (2.404s,  425.93/s)  LR: 2.173e-05  Data: 4.992 (1.810)
Train: 134 [1150/1251 ( 92%)]  Loss:  4.636115 (4.5913)  Time: 0.589s, 1739.77/s  (2.414s,  424.24/s)  LR: 2.173e-05  Data: 0.022 (1.820)
Train: 134 [1200/1251 ( 96%)]  Loss:  4.459415 (4.5860)  Time: 4.769s,  214.72/s  (2.412s,  424.58/s)  LR: 2.173e-05  Data: 4.098 (1.817)
Train: 134 [1250/1251 (100%)]  Loss:  4.865923 (4.5968)  Time: 0.566s, 1808.63/s  (2.408s,  425.34/s)  LR: 2.173e-05  Data: 0.000 (1.811)
Test: [   0/48]  Time: 15.547 (15.547)  Loss:  1.0831 (1.0831)  Acc@1: 77.3438 (77.3438)  Acc@5: 92.7734 (92.7734)
Test: [  48/48]  Time: 0.152 (3.360)  Loss:  1.1203 (1.9508)  Acc@1: 77.4764 (57.3640)  Acc@5: 90.9198 (80.9780)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-134.pth.tar', 57.36400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-132.pth.tar', 57.28000001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-133.pth.tar', 57.124000041503905)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-131.pth.tar', 57.05400011962891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-130.pth.tar', 56.881999987792966)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-129.pth.tar', 56.661999995117185)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-128.pth.tar', 56.65199996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-127.pth.tar', 56.55200009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-126.pth.tar', 56.49799996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-124.pth.tar', 56.24000004882812)

Train: 135 [   0/1251 (  0%)]  Loss:  4.950304 (4.9503)  Time: 11.407s,   89.77/s  (11.407s,   89.77/s)  LR: 1.951e-05  Data: 10.309 (10.309)
Train: 135 [  50/1251 (  4%)]  Loss:  4.935780 (4.9430)  Time: 0.590s, 1734.91/s  (2.449s,  418.15/s)  LR: 1.951e-05  Data: 0.021 (1.861)
Train: 135 [ 100/1251 (  8%)]  Loss:  4.408938 (4.7650)  Time: 2.644s,  387.34/s  (2.451s,  417.80/s)  LR: 1.951e-05  Data: 1.979 (1.857)
Train: 135 [ 150/1251 ( 12%)]  Loss:  4.415863 (4.6777)  Time: 0.586s, 1748.89/s  (2.358s,  434.22/s)  LR: 1.951e-05  Data: 0.019 (1.771)
Train: 135 [ 200/1251 ( 16%)]  Loss:  4.288464 (4.5999)  Time: 0.590s, 1736.48/s  (2.422s,  422.73/s)  LR: 1.951e-05  Data: 0.023 (1.835)
Train: 135 [ 250/1251 ( 20%)]  Loss:  4.148438 (4.5246)  Time: 0.585s, 1749.19/s  (2.486s,  411.94/s)  LR: 1.951e-05  Data: 0.019 (1.896)
Train: 135 [ 300/1251 ( 24%)]  Loss:  4.067891 (4.4594)  Time: 0.589s, 1739.31/s  (2.477s,  413.33/s)  LR: 1.951e-05  Data: 0.020 (1.890)
Train: 135 [ 350/1251 ( 28%)]  Loss:  4.507073 (4.4653)  Time: 0.583s, 1755.92/s  (2.446s,  418.57/s)  LR: 1.951e-05  Data: 0.020 (1.859)
Train: 135 [ 400/1251 ( 32%)]  Loss:  5.067669 (4.5323)  Time: 0.588s, 1740.85/s  (2.446s,  418.60/s)  LR: 1.951e-05  Data: 0.022 (1.859)
Train: 135 [ 450/1251 ( 36%)]  Loss:  5.061030 (4.5851)  Time: 0.582s, 1758.34/s  (2.423s,  422.68/s)  LR: 1.951e-05  Data: 0.018 (1.834)
Train: 135 [ 500/1251 ( 40%)]  Loss:  4.555761 (4.5825)  Time: 0.587s, 1745.39/s  (2.408s,  425.32/s)  LR: 1.951e-05  Data: 0.019 (1.818)
Train: 135 [ 550/1251 ( 44%)]  Loss:  4.525725 (4.5777)  Time: 0.587s, 1745.73/s  (2.386s,  429.25/s)  LR: 1.951e-05  Data: 0.020 (1.797)
Train: 135 [ 600/1251 ( 48%)]  Loss:  4.086562 (4.5400)  Time: 0.869s, 1179.03/s  (2.422s,  422.76/s)  LR: 1.951e-05  Data: 0.218 (1.833)
Train: 135 [ 650/1251 ( 52%)]  Loss:  4.507957 (4.5377)  Time: 2.654s,  385.77/s  (2.435s,  420.52/s)  LR: 1.951e-05  Data: 2.091 (1.846)
Train: 135 [ 700/1251 ( 56%)]  Loss:  4.228035 (4.5170)  Time: 0.591s, 1733.33/s  (2.446s,  418.62/s)  LR: 1.951e-05  Data: 0.023 (1.855)
Train: 135 [ 750/1251 ( 60%)]  Loss:  4.953356 (4.5443)  Time: 0.591s, 1732.81/s  (2.442s,  419.35/s)  LR: 1.951e-05  Data: 0.023 (1.851)
Train: 135 [ 800/1251 ( 64%)]  Loss:  4.601174 (4.5476)  Time: 0.586s, 1748.46/s  (2.443s,  419.17/s)  LR: 1.951e-05  Data: 0.022 (1.851)
Train: 135 [ 850/1251 ( 68%)]  Loss:  4.647490 (4.5532)  Time: 0.769s, 1330.94/s  (2.430s,  421.39/s)  LR: 1.951e-05  Data: 0.142 (1.837)
Train: 135 [ 900/1251 ( 72%)]  Loss:  4.597100 (4.5555)  Time: 0.586s, 1746.35/s  (2.424s,  422.41/s)  LR: 1.951e-05  Data: 0.018 (1.830)
Train: 135 [ 950/1251 ( 76%)]  Loss:  4.998604 (4.5777)  Time: 3.837s,  266.90/s  (2.437s,  420.18/s)  LR: 1.951e-05  Data: 3.261 (1.841)
Train: 135 [1000/1251 ( 80%)]  Loss:  4.759953 (4.5863)  Time: 0.586s, 1748.79/s  (2.439s,  419.82/s)  LR: 1.951e-05  Data: 0.018 (1.842)
Train: 135 [1050/1251 ( 84%)]  Loss:  4.787757 (4.5955)  Time: 4.298s,  238.26/s  (2.438s,  419.93/s)  LR: 1.951e-05  Data: 3.724 (1.840)
Train: 135 [1100/1251 ( 88%)]  Loss:  4.951125 (4.6110)  Time: 0.584s, 1753.08/s  (2.446s,  418.72/s)  LR: 1.951e-05  Data: 0.021 (1.847)
Train: 135 [1150/1251 ( 92%)]  Loss:  4.686124 (4.6141)  Time: 3.937s,  260.11/s  (2.446s,  418.56/s)  LR: 1.951e-05  Data: 3.283 (1.848)
Train: 135 [1200/1251 ( 96%)]  Loss:  4.977615 (4.6286)  Time: 0.585s, 1749.87/s  (2.439s,  419.89/s)  LR: 1.951e-05  Data: 0.021 (1.840)
Train: 135 [1250/1251 (100%)]  Loss:  4.875451 (4.6381)  Time: 0.565s, 1813.32/s  (2.435s,  420.56/s)  LR: 1.951e-05  Data: 0.000 (1.836)
Test: [   0/48]  Time: 14.423 (14.423)  Loss:  1.0865 (1.0865)  Acc@1: 77.2461 (77.2461)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.149 (3.505)  Loss:  1.1159 (1.9477)  Acc@1: 77.2406 (57.3720)  Acc@5: 91.0377 (81.0440)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-135.pth.tar', 57.37200006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-134.pth.tar', 57.36400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-132.pth.tar', 57.28000001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-133.pth.tar', 57.124000041503905)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-131.pth.tar', 57.05400011962891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-130.pth.tar', 56.881999987792966)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-129.pth.tar', 56.661999995117185)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-128.pth.tar', 56.65199996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-127.pth.tar', 56.55200009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-126.pth.tar', 56.49799996582031)

Train: 136 [   0/1251 (  0%)]  Loss:  4.825718 (4.8257)  Time: 11.527s,   88.83/s  (11.527s,   88.83/s)  LR: 1.752e-05  Data: 10.769 (10.769)
Train: 136 [  50/1251 (  4%)]  Loss:  4.340086 (4.5829)  Time: 0.587s, 1744.03/s  (2.540s,  403.14/s)  LR: 1.752e-05  Data: 0.019 (1.942)
Train: 136 [ 100/1251 (  8%)]  Loss:  4.841833 (4.6692)  Time: 0.746s, 1371.89/s  (2.457s,  416.74/s)  LR: 1.752e-05  Data: 0.020 (1.853)
Train: 136 [ 150/1251 ( 12%)]  Loss:  4.652211 (4.6650)  Time: 0.589s, 1738.98/s  (2.388s,  428.83/s)  LR: 1.752e-05  Data: 0.022 (1.790)
Train: 136 [ 200/1251 ( 16%)]  Loss:  4.512921 (4.6346)  Time: 0.588s, 1741.97/s  (2.394s,  427.65/s)  LR: 1.752e-05  Data: 0.024 (1.794)
Train: 136 [ 250/1251 ( 20%)]  Loss:  4.744023 (4.6528)  Time: 0.584s, 1753.73/s  (2.354s,  435.02/s)  LR: 1.752e-05  Data: 0.018 (1.757)
Train: 136 [ 300/1251 ( 24%)]  Loss:  4.925678 (4.6918)  Time: 0.588s, 1742.91/s  (2.348s,  436.10/s)  LR: 1.752e-05  Data: 0.019 (1.751)
Train: 136 [ 350/1251 ( 28%)]  Loss:  4.518521 (4.6701)  Time: 0.585s, 1751.86/s  (2.316s,  442.17/s)  LR: 1.752e-05  Data: 0.018 (1.720)
Train: 136 [ 400/1251 ( 32%)]  Loss:  4.882419 (4.6937)  Time: 0.592s, 1730.27/s  (2.365s,  432.91/s)  LR: 1.752e-05  Data: 0.019 (1.771)
Train: 136 [ 450/1251 ( 36%)]  Loss:  5.065393 (4.7309)  Time: 0.584s, 1753.01/s  (2.366s,  432.79/s)  LR: 1.752e-05  Data: 0.018 (1.770)
Train: 136 [ 500/1251 ( 40%)]  Loss:  4.863134 (4.7429)  Time: 0.585s, 1751.79/s  (2.370s,  431.98/s)  LR: 1.752e-05  Data: 0.019 (1.776)
Train: 136 [ 550/1251 ( 44%)]  Loss:  5.076435 (4.7707)  Time: 0.589s, 1737.36/s  (2.362s,  433.62/s)  LR: 1.752e-05  Data: 0.021 (1.767)
Train: 136 [ 600/1251 ( 48%)]  Loss:  4.604564 (4.7579)  Time: 0.588s, 1740.14/s  (2.372s,  431.71/s)  LR: 1.752e-05  Data: 0.021 (1.777)
Train: 136 [ 650/1251 ( 52%)]  Loss:  4.685541 (4.7527)  Time: 0.584s, 1754.17/s  (2.371s,  431.89/s)  LR: 1.752e-05  Data: 0.019 (1.777)
Train: 136 [ 700/1251 ( 56%)]  Loss:  4.877380 (4.7611)  Time: 0.587s, 1745.56/s  (2.399s,  426.80/s)  LR: 1.752e-05  Data: 0.019 (1.805)
Train: 136 [ 750/1251 ( 60%)]  Loss:  4.507895 (4.7452)  Time: 1.776s,  576.44/s  (2.420s,  423.14/s)  LR: 1.752e-05  Data: 1.188 (1.826)
Train: 136 [ 800/1251 ( 64%)]  Loss:  4.731812 (4.7444)  Time: 0.585s, 1749.61/s  (2.424s,  422.40/s)  LR: 1.752e-05  Data: 0.022 (1.829)
Train: 136 [ 850/1251 ( 68%)]  Loss:  4.237984 (4.7163)  Time: 6.534s,  156.72/s  (2.427s,  421.94/s)  LR: 1.752e-05  Data: 5.870 (1.831)
Train: 136 [ 900/1251 ( 72%)]  Loss:  4.140868 (4.6860)  Time: 0.589s, 1738.96/s  (2.427s,  421.95/s)  LR: 1.752e-05  Data: 0.017 (1.830)
Train: 136 [ 950/1251 ( 76%)]  Loss:  5.022772 (4.7029)  Time: 6.978s,  146.75/s  (2.428s,  421.80/s)  LR: 1.752e-05  Data: 6.399 (1.831)
Train: 136 [1000/1251 ( 80%)]  Loss:  4.611521 (4.6985)  Time: 0.595s, 1719.62/s  (2.419s,  423.32/s)  LR: 1.752e-05  Data: 0.020 (1.823)
Train: 136 [1050/1251 ( 84%)]  Loss:  4.669679 (4.6972)  Time: 7.192s,  142.39/s  (2.416s,  423.92/s)  LR: 1.752e-05  Data: 6.510 (1.820)
Train: 136 [1100/1251 ( 88%)]  Loss:  4.860250 (4.7043)  Time: 0.587s, 1744.64/s  (2.424s,  422.52/s)  LR: 1.752e-05  Data: 0.018 (1.828)
Train: 136 [1150/1251 ( 92%)]  Loss:  4.947898 (4.7144)  Time: 4.235s,  241.82/s  (2.423s,  422.67/s)  LR: 1.752e-05  Data: 3.546 (1.827)
Train: 136 [1200/1251 ( 96%)]  Loss:  4.600739 (4.7099)  Time: 0.588s, 1740.07/s  (2.421s,  422.89/s)  LR: 1.752e-05  Data: 0.020 (1.825)
Train: 136 [1250/1251 (100%)]  Loss:  4.753920 (4.7116)  Time: 0.564s, 1815.10/s  (2.419s,  423.36/s)  LR: 1.752e-05  Data: 0.000 (1.822)
Test: [   0/48]  Time: 13.747 (13.747)  Loss:  1.0729 (1.0729)  Acc@1: 78.0273 (78.0273)  Acc@5: 92.4805 (92.4805)
Test: [  48/48]  Time: 0.149 (3.338)  Loss:  1.0953 (1.9319)  Acc@1: 77.9481 (57.5300)  Acc@5: 91.5094 (81.1780)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-136.pth.tar', 57.52999998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-135.pth.tar', 57.37200006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-134.pth.tar', 57.36400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-132.pth.tar', 57.28000001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-133.pth.tar', 57.124000041503905)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-131.pth.tar', 57.05400011962891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-130.pth.tar', 56.881999987792966)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-129.pth.tar', 56.661999995117185)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-128.pth.tar', 56.65199996582031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-127.pth.tar', 56.55200009277344)

Train: 137 [   0/1251 (  0%)]  Loss:  4.689838 (4.6898)  Time: 12.027s,   85.14/s  (12.027s,   85.14/s)  LR: 1.576e-05  Data: 10.652 (10.652)
Train: 137 [  50/1251 (  4%)]  Loss:  4.875957 (4.7829)  Time: 0.586s, 1746.11/s  (2.404s,  425.93/s)  LR: 1.576e-05  Data: 0.021 (1.781)
Train: 137 [ 100/1251 (  8%)]  Loss:  4.602608 (4.7228)  Time: 1.819s,  563.08/s  (2.328s,  439.87/s)  LR: 1.576e-05  Data: 1.078 (1.715)
Train: 137 [ 150/1251 ( 12%)]  Loss:  5.044286 (4.8032)  Time: 0.590s, 1735.73/s  (2.392s,  428.03/s)  LR: 1.576e-05  Data: 0.018 (1.776)
Train: 137 [ 200/1251 ( 16%)]  Loss:  5.006004 (4.8437)  Time: 3.486s,  293.71/s  (2.358s,  434.26/s)  LR: 1.576e-05  Data: 2.793 (1.737)
Train: 137 [ 250/1251 ( 20%)]  Loss:  4.561097 (4.7966)  Time: 0.584s, 1752.23/s  (2.476s,  413.60/s)  LR: 1.576e-05  Data: 0.021 (1.857)
Train: 137 [ 300/1251 ( 24%)]  Loss:  4.855136 (4.8050)  Time: 0.583s, 1755.43/s  (2.436s,  420.36/s)  LR: 1.576e-05  Data: 0.020 (1.824)
Train: 137 [ 350/1251 ( 28%)]  Loss:  4.145473 (4.7225)  Time: 0.583s, 1755.58/s  (2.430s,  421.36/s)  LR: 1.576e-05  Data: 0.020 (1.822)
Train: 137 [ 400/1251 ( 32%)]  Loss:  4.978713 (4.7510)  Time: 0.584s, 1752.57/s  (2.405s,  425.82/s)  LR: 1.576e-05  Data: 0.021 (1.799)
Train: 137 [ 450/1251 ( 36%)]  Loss:  3.824188 (4.6583)  Time: 0.595s, 1722.01/s  (2.396s,  427.30/s)  LR: 1.576e-05  Data: 0.019 (1.793)
Train: 137 [ 500/1251 ( 40%)]  Loss:  4.212573 (4.6178)  Time: 0.583s, 1756.39/s  (2.373s,  431.54/s)  LR: 1.576e-05  Data: 0.018 (1.771)
Train: 137 [ 550/1251 ( 44%)]  Loss:  4.288233 (4.5903)  Time: 0.586s, 1747.02/s  (2.405s,  425.80/s)  LR: 1.576e-05  Data: 0.020 (1.804)
Train: 137 [ 600/1251 ( 48%)]  Loss:  4.861730 (4.6112)  Time: 0.583s, 1757.22/s  (2.412s,  424.49/s)  LR: 1.576e-05  Data: 0.019 (1.812)
Train: 137 [ 650/1251 ( 52%)]  Loss:  4.585420 (4.6094)  Time: 0.585s, 1749.41/s  (2.428s,  421.67/s)  LR: 1.576e-05  Data: 0.017 (1.830)
Train: 137 [ 700/1251 ( 56%)]  Loss:  4.424037 (4.5970)  Time: 0.585s, 1750.48/s  (2.430s,  421.46/s)  LR: 1.576e-05  Data: 0.022 (1.832)
Train: 137 [ 750/1251 ( 60%)]  Loss:  3.890198 (4.5528)  Time: 0.590s, 1734.24/s  (2.433s,  420.91/s)  LR: 1.576e-05  Data: 0.025 (1.835)
Train: 137 [ 800/1251 ( 64%)]  Loss:  4.515868 (4.5507)  Time: 0.591s, 1734.08/s  (2.424s,  422.48/s)  LR: 1.576e-05  Data: 0.024 (1.826)
Train: 137 [ 850/1251 ( 68%)]  Loss:  4.908333 (4.5705)  Time: 0.587s, 1743.78/s  (2.419s,  423.26/s)  LR: 1.576e-05  Data: 0.023 (1.821)
Train: 137 [ 900/1251 ( 72%)]  Loss:  4.605686 (4.5724)  Time: 0.585s, 1749.05/s  (2.428s,  421.77/s)  LR: 1.576e-05  Data: 0.018 (1.829)
Train: 137 [ 950/1251 ( 76%)]  Loss:  4.880724 (4.5878)  Time: 5.646s,  181.37/s  (2.432s,  421.10/s)  LR: 1.576e-05  Data: 5.073 (1.833)
Train: 137 [1000/1251 ( 80%)]  Loss:  4.778674 (4.5969)  Time: 0.587s, 1744.12/s  (2.424s,  422.39/s)  LR: 1.576e-05  Data: 0.019 (1.825)
Train: 137 [1050/1251 ( 84%)]  Loss:  4.238872 (4.5806)  Time: 1.875s,  546.17/s  (2.425s,  422.33/s)  LR: 1.576e-05  Data: 1.312 (1.824)
Train: 137 [1100/1251 ( 88%)]  Loss:  4.377087 (4.5718)  Time: 0.590s, 1735.36/s  (2.430s,  421.37/s)  LR: 1.576e-05  Data: 0.019 (1.829)
Train: 137 [1150/1251 ( 92%)]  Loss:  4.711728 (4.5776)  Time: 2.344s,  436.84/s  (2.431s,  421.31/s)  LR: 1.576e-05  Data: 1.768 (1.830)
Train: 137 [1200/1251 ( 96%)]  Loss:  4.383465 (4.5698)  Time: 0.811s, 1262.48/s  (2.419s,  423.24/s)  LR: 1.576e-05  Data: 0.237 (1.818)
Train: 137 [1250/1251 (100%)]  Loss:  4.347265 (4.5613)  Time: 0.567s, 1804.73/s  (2.417s,  423.74/s)  LR: 1.576e-05  Data: 0.000 (1.815)
Test: [   0/48]  Time: 21.798 (21.798)  Loss:  1.0887 (1.0887)  Acc@1: 77.5391 (77.5391)  Acc@5: 92.4805 (92.4805)
Test: [  48/48]  Time: 0.149 (3.677)  Loss:  1.0915 (1.9471)  Acc@1: 77.8302 (57.4500)  Acc@5: 91.7453 (81.0660)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-136.pth.tar', 57.52999998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-137.pth.tar', 57.45000006591797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-135.pth.tar', 57.37200006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-134.pth.tar', 57.36400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-132.pth.tar', 57.28000001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-133.pth.tar', 57.124000041503905)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-131.pth.tar', 57.05400011962891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-130.pth.tar', 56.881999987792966)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-129.pth.tar', 56.661999995117185)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-128.pth.tar', 56.65199996582031)

Train: 138 [   0/1251 (  0%)]  Loss:  3.618341 (3.6183)  Time: 12.543s,   81.64/s  (12.543s,   81.64/s)  LR: 1.423e-05  Data: 11.179 (11.179)
Train: 138 [  50/1251 (  4%)]  Loss:  4.143025 (3.8807)  Time: 0.584s, 1752.74/s  (2.513s,  407.53/s)  LR: 1.423e-05  Data: 0.020 (1.911)
Train: 138 [ 100/1251 (  8%)]  Loss:  4.665418 (4.1423)  Time: 0.630s, 1625.82/s  (2.409s,  425.07/s)  LR: 1.423e-05  Data: 0.018 (1.811)
Train: 138 [ 150/1251 ( 12%)]  Loss:  4.696940 (4.2809)  Time: 0.587s, 1744.07/s  (2.369s,  432.32/s)  LR: 1.423e-05  Data: 0.021 (1.772)
Train: 138 [ 200/1251 ( 16%)]  Loss:  4.294457 (4.2836)  Time: 2.448s,  418.30/s  (2.374s,  431.36/s)  LR: 1.423e-05  Data: 1.672 (1.774)
Train: 138 [ 250/1251 ( 20%)]  Loss:  4.374123 (4.2987)  Time: 0.588s, 1742.32/s  (2.325s,  440.51/s)  LR: 1.423e-05  Data: 0.021 (1.726)
Train: 138 [ 300/1251 ( 24%)]  Loss:  4.874684 (4.3810)  Time: 0.589s, 1739.06/s  (2.300s,  445.17/s)  LR: 1.423e-05  Data: 0.019 (1.699)
Train: 138 [ 350/1251 ( 28%)]  Loss:  4.901545 (4.4461)  Time: 0.769s, 1331.97/s  (2.346s,  436.53/s)  LR: 1.423e-05  Data: 0.098 (1.739)
Train: 138 [ 400/1251 ( 32%)]  Loss:  4.161207 (4.4144)  Time: 4.340s,  235.96/s  (2.337s,  438.12/s)  LR: 1.423e-05  Data: 3.774 (1.731)
Train: 138 [ 450/1251 ( 36%)]  Loss:  4.445639 (4.4175)  Time: 0.585s, 1749.88/s  (2.337s,  438.22/s)  LR: 1.423e-05  Data: 0.022 (1.730)
Train: 138 [ 500/1251 ( 40%)]  Loss:  4.277811 (4.4048)  Time: 4.151s,  246.67/s  (2.340s,  437.62/s)  LR: 1.423e-05  Data: 3.486 (1.733)
Train: 138 [ 550/1251 ( 44%)]  Loss:  4.865031 (4.4432)  Time: 0.587s, 1743.54/s  (2.331s,  439.35/s)  LR: 1.423e-05  Data: 0.024 (1.724)
Train: 138 [ 600/1251 ( 48%)]  Loss:  4.634707 (4.4579)  Time: 5.129s,  199.67/s  (2.334s,  438.69/s)  LR: 1.423e-05  Data: 4.453 (1.727)
Train: 138 [ 650/1251 ( 52%)]  Loss:  4.637411 (4.4707)  Time: 0.589s, 1738.97/s  (2.334s,  438.73/s)  LR: 1.423e-05  Data: 0.022 (1.727)
Train: 138 [ 700/1251 ( 56%)]  Loss:  4.638155 (4.4819)  Time: 2.976s,  344.14/s  (2.381s,  430.06/s)  LR: 1.423e-05  Data: 2.325 (1.774)
Train: 138 [ 750/1251 ( 60%)]  Loss:  4.443766 (4.4795)  Time: 0.586s, 1747.17/s  (2.375s,  431.15/s)  LR: 1.423e-05  Data: 0.023 (1.769)
Train: 138 [ 800/1251 ( 64%)]  Loss:  4.433522 (4.4768)  Time: 4.681s,  218.75/s  (2.382s,  429.81/s)  LR: 1.423e-05  Data: 4.083 (1.776)
Train: 138 [ 850/1251 ( 68%)]  Loss:  4.706457 (4.4896)  Time: 0.587s, 1745.00/s  (2.379s,  430.37/s)  LR: 1.423e-05  Data: 0.019 (1.774)
Train: 138 [ 900/1251 ( 72%)]  Loss:  4.381179 (4.4839)  Time: 5.705s,  179.49/s  (2.381s,  430.02/s)  LR: 1.423e-05  Data: 5.142 (1.776)
Train: 138 [ 950/1251 ( 76%)]  Loss:  4.527247 (4.4860)  Time: 0.589s, 1738.68/s  (2.374s,  431.25/s)  LR: 1.423e-05  Data: 0.019 (1.769)
Train: 138 [1000/1251 ( 80%)]  Loss:  4.379951 (4.4810)  Time: 7.533s,  135.93/s  (2.372s,  431.72/s)  LR: 1.423e-05  Data: 6.882 (1.767)
Train: 138 [1050/1251 ( 84%)]  Loss:  4.788711 (4.4950)  Time: 0.587s, 1745.67/s  (2.361s,  433.79/s)  LR: 1.423e-05  Data: 0.023 (1.757)
Train: 138 [1100/1251 ( 88%)]  Loss:  4.866034 (4.5111)  Time: 6.549s,  156.35/s  (2.379s,  430.40/s)  LR: 1.423e-05  Data: 5.821 (1.775)
Train: 138 [1150/1251 ( 92%)]  Loss:  4.197374 (4.4980)  Time: 0.584s, 1754.10/s  (2.367s,  432.67/s)  LR: 1.423e-05  Data: 0.019 (1.764)
Train: 138 [1200/1251 ( 96%)]  Loss:  4.635960 (4.5035)  Time: 7.209s,  142.05/s  (2.375s,  431.13/s)  LR: 1.423e-05  Data: 6.549 (1.772)
Train: 138 [1250/1251 (100%)]  Loss:  4.603753 (4.5074)  Time: 0.564s, 1814.67/s  (2.366s,  432.79/s)  LR: 1.423e-05  Data: 0.000 (1.763)
Test: [   0/48]  Time: 15.295 (15.295)  Loss:  1.0655 (1.0655)  Acc@1: 77.9297 (77.9297)  Acc@5: 92.7734 (92.7734)
Test: [  48/48]  Time: 0.148 (3.267)  Loss:  1.1013 (1.9343)  Acc@1: 77.3585 (57.5000)  Acc@5: 91.0377 (81.1420)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-136.pth.tar', 57.52999998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-138.pth.tar', 57.49999999023437)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-137.pth.tar', 57.45000006591797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-135.pth.tar', 57.37200006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-134.pth.tar', 57.36400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-132.pth.tar', 57.28000001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-133.pth.tar', 57.124000041503905)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-131.pth.tar', 57.05400011962891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-130.pth.tar', 56.881999987792966)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-129.pth.tar', 56.661999995117185)

Train: 139 [   0/1251 (  0%)]  Loss:  4.663043 (4.6630)  Time: 11.067s,   92.53/s  (11.067s,   92.53/s)  LR: 1.294e-05  Data: 10.505 (10.505)
Train: 139 [  50/1251 (  4%)]  Loss:  4.442865 (4.5530)  Time: 0.586s, 1747.15/s  (2.306s,  444.06/s)  LR: 1.294e-05  Data: 0.022 (1.705)
Train: 139 [ 100/1251 (  8%)]  Loss:  4.839760 (4.6486)  Time: 0.584s, 1752.60/s  (2.260s,  453.16/s)  LR: 1.294e-05  Data: 0.022 (1.665)
Train: 139 [ 150/1251 ( 12%)]  Loss:  4.448297 (4.5985)  Time: 0.583s, 1756.86/s  (2.339s,  437.73/s)  LR: 1.294e-05  Data: 0.018 (1.743)
Train: 139 [ 200/1251 ( 16%)]  Loss:  4.993424 (4.6775)  Time: 0.585s, 1749.35/s  (2.303s,  444.59/s)  LR: 1.294e-05  Data: 0.018 (1.709)
Train: 139 [ 250/1251 ( 20%)]  Loss:  4.130723 (4.5864)  Time: 0.584s, 1753.19/s  (2.368s,  432.41/s)  LR: 1.294e-05  Data: 0.018 (1.775)
Train: 139 [ 300/1251 ( 24%)]  Loss:  4.738826 (4.6081)  Time: 0.757s, 1353.60/s  (2.373s,  431.56/s)  LR: 1.294e-05  Data: 0.096 (1.779)
Train: 139 [ 350/1251 ( 28%)]  Loss:  4.398651 (4.5819)  Time: 0.591s, 1733.54/s  (2.341s,  437.39/s)  LR: 1.294e-05  Data: 0.020 (1.745)
Train: 139 [ 400/1251 ( 32%)]  Loss:  4.962939 (4.6243)  Time: 4.396s,  232.96/s  (2.346s,  436.55/s)  LR: 1.294e-05  Data: 3.706 (1.750)
Train: 139 [ 450/1251 ( 36%)]  Loss:  4.066209 (4.5685)  Time: 0.590s, 1736.17/s  (2.326s,  440.27/s)  LR: 1.294e-05  Data: 0.020 (1.730)
Train: 139 [ 500/1251 ( 40%)]  Loss:  4.326513 (4.5465)  Time: 4.988s,  205.28/s  (2.322s,  440.96/s)  LR: 1.294e-05  Data: 4.391 (1.726)
Train: 139 [ 550/1251 ( 44%)]  Loss:  4.530682 (4.5452)  Time: 0.586s, 1746.07/s  (2.346s,  436.49/s)  LR: 1.294e-05  Data: 0.019 (1.750)
Train: 139 [ 600/1251 ( 48%)]  Loss:  4.685771 (4.5560)  Time: 7.001s,  146.26/s  (2.344s,  436.79/s)  LR: 1.294e-05  Data: 6.395 (1.748)
Train: 139 [ 650/1251 ( 52%)]  Loss:  4.182027 (4.5293)  Time: 0.585s, 1751.44/s  (2.361s,  433.79/s)  LR: 1.294e-05  Data: 0.019 (1.763)
Train: 139 [ 700/1251 ( 56%)]  Loss:  4.241252 (4.5101)  Time: 3.719s,  275.34/s  (2.360s,  433.90/s)  LR: 1.294e-05  Data: 3.121 (1.762)
Train: 139 [ 750/1251 ( 60%)]  Loss:  4.530474 (4.5113)  Time: 0.584s, 1752.34/s  (2.357s,  434.54/s)  LR: 1.294e-05  Data: 0.020 (1.758)
Train: 139 [ 800/1251 ( 64%)]  Loss:  5.018543 (4.5412)  Time: 3.437s,  297.97/s  (2.358s,  434.34/s)  LR: 1.294e-05  Data: 2.862 (1.758)
Train: 139 [ 850/1251 ( 68%)]  Loss:  4.460922 (4.5367)  Time: 0.586s, 1747.81/s  (2.357s,  434.46/s)  LR: 1.294e-05  Data: 0.019 (1.755)
Train: 139 [ 900/1251 ( 72%)]  Loss:  5.053886 (4.5639)  Time: 1.510s,  678.33/s  (2.369s,  432.26/s)  LR: 1.294e-05  Data: 0.832 (1.766)
Train: 139 [ 950/1251 ( 76%)]  Loss:  4.420603 (4.5568)  Time: 0.587s, 1743.22/s  (2.369s,  432.27/s)  LR: 1.294e-05  Data: 0.022 (1.764)
Train: 139 [1000/1251 ( 80%)]  Loss:  5.015009 (4.5786)  Time: 0.585s, 1749.12/s  (2.368s,  432.50/s)  LR: 1.294e-05  Data: 0.019 (1.763)
Train: 139 [1050/1251 ( 84%)]  Loss:  4.231028 (4.5628)  Time: 1.469s,  696.99/s  (2.363s,  433.29/s)  LR: 1.294e-05  Data: 0.888 (1.758)
Train: 139 [1100/1251 ( 88%)]  Loss:  4.091546 (4.5423)  Time: 1.201s,  852.69/s  (2.363s,  433.42/s)  LR: 1.294e-05  Data: 0.507 (1.756)
Train: 139 [1150/1251 ( 92%)]  Loss:  4.529305 (4.5418)  Time: 1.236s,  828.33/s  (2.373s,  431.57/s)  LR: 1.294e-05  Data: 0.617 (1.766)
Train: 139 [1200/1251 ( 96%)]  Loss:  4.459617 (4.5385)  Time: 0.590s, 1734.61/s  (2.370s,  432.07/s)  LR: 1.294e-05  Data: 0.021 (1.762)
Train: 139 [1250/1251 (100%)]  Loss:  5.036660 (4.5576)  Time: 0.564s, 1814.37/s  (2.362s,  433.61/s)  LR: 1.294e-05  Data: 0.000 (1.754)
Test: [   0/48]  Time: 14.081 (14.081)  Loss:  1.0791 (1.0791)  Acc@1: 77.3438 (77.3438)  Acc@5: 92.4805 (92.4805)
Test: [  48/48]  Time: 0.149 (3.510)  Loss:  1.0934 (1.9358)  Acc@1: 77.4764 (57.5400)  Acc@5: 91.1557 (81.1680)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-139.pth.tar', 57.54000004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-136.pth.tar', 57.52999998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-138.pth.tar', 57.49999999023437)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-137.pth.tar', 57.45000006591797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-135.pth.tar', 57.37200006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-134.pth.tar', 57.36400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-132.pth.tar', 57.28000001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-133.pth.tar', 57.124000041503905)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-131.pth.tar', 57.05400011962891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-130.pth.tar', 56.881999987792966)

Train: 140 [   0/1251 (  0%)]  Loss:  5.021399 (5.0214)  Time: 9.498s,  107.81/s  (9.498s,  107.81/s)  LR: 1.188e-05  Data: 8.806 (8.806)
Train: 140 [  50/1251 (  4%)]  Loss:  4.863395 (4.9424)  Time: 0.588s, 1741.41/s  (2.479s,  413.15/s)  LR: 1.188e-05  Data: 0.022 (1.858)
Train: 140 [ 100/1251 (  8%)]  Loss:  4.956487 (4.9471)  Time: 0.586s, 1747.71/s  (2.341s,  437.39/s)  LR: 1.188e-05  Data: 0.022 (1.723)
Train: 140 [ 150/1251 ( 12%)]  Loss:  4.837861 (4.9198)  Time: 0.583s, 1756.02/s  (2.323s,  440.73/s)  LR: 1.188e-05  Data: 0.020 (1.720)
Train: 140 [ 200/1251 ( 16%)]  Loss:  4.979368 (4.9317)  Time: 0.584s, 1754.64/s  (2.306s,  444.04/s)  LR: 1.188e-05  Data: 0.020 (1.707)
Train: 140 [ 250/1251 ( 20%)]  Loss:  4.452002 (4.8518)  Time: 0.588s, 1742.76/s  (2.280s,  449.13/s)  LR: 1.188e-05  Data: 0.023 (1.684)
Train: 140 [ 300/1251 ( 24%)]  Loss:  4.747777 (4.8369)  Time: 1.911s,  535.78/s  (2.279s,  449.34/s)  LR: 1.188e-05  Data: 1.346 (1.681)
Train: 140 [ 350/1251 ( 28%)]  Loss:  4.945775 (4.8505)  Time: 0.585s, 1749.16/s  (2.304s,  444.41/s)  LR: 1.188e-05  Data: 0.020 (1.704)
Train: 140 [ 400/1251 ( 32%)]  Loss:  4.598065 (4.8225)  Time: 0.585s, 1751.39/s  (2.312s,  442.95/s)  LR: 1.188e-05  Data: 0.021 (1.714)
Train: 140 [ 450/1251 ( 36%)]  Loss:  4.841907 (4.8244)  Time: 1.253s,  817.56/s  (2.303s,  444.63/s)  LR: 1.188e-05  Data: 0.684 (1.707)
Train: 140 [ 500/1251 ( 40%)]  Loss:  4.855748 (4.8273)  Time: 0.588s, 1740.14/s  (2.308s,  443.58/s)  LR: 1.188e-05  Data: 0.022 (1.712)
Train: 140 [ 550/1251 ( 44%)]  Loss:  4.343714 (4.7870)  Time: 0.585s, 1751.90/s  (2.298s,  445.61/s)  LR: 1.188e-05  Data: 0.021 (1.702)
Train: 140 [ 600/1251 ( 48%)]  Loss:  4.571672 (4.7704)  Time: 0.588s, 1741.64/s  (2.310s,  443.24/s)  LR: 1.188e-05  Data: 0.022 (1.714)
Train: 140 [ 650/1251 ( 52%)]  Loss:  3.718411 (4.6953)  Time: 0.583s, 1755.56/s  (2.311s,  443.10/s)  LR: 1.188e-05  Data: 0.018 (1.714)
Train: 140 [ 700/1251 ( 56%)]  Loss:  4.888156 (4.7081)  Time: 0.974s, 1050.83/s  (2.317s,  442.02/s)  LR: 1.188e-05  Data: 0.411 (1.720)
Train: 140 [ 750/1251 ( 60%)]  Loss:  4.366830 (4.6868)  Time: 3.876s,  264.18/s  (2.362s,  433.50/s)  LR: 1.188e-05  Data: 3.223 (1.765)
Train: 140 [ 800/1251 ( 64%)]  Loss:  4.738018 (4.6898)  Time: 1.706s,  600.33/s  (2.368s,  432.48/s)  LR: 1.188e-05  Data: 1.124 (1.769)
Train: 140 [ 850/1251 ( 68%)]  Loss:  5.044603 (4.7095)  Time: 3.096s,  330.79/s  (2.368s,  432.39/s)  LR: 1.188e-05  Data: 2.417 (1.769)
Train: 140 [ 900/1251 ( 72%)]  Loss:  4.768629 (4.7126)  Time: 2.899s,  353.22/s  (2.367s,  432.57/s)  LR: 1.188e-05  Data: 2.336 (1.767)
Train: 140 [ 950/1251 ( 76%)]  Loss:  4.386175 (4.6963)  Time: 3.931s,  260.47/s  (2.362s,  433.59/s)  LR: 1.188e-05  Data: 3.277 (1.759)
Train: 140 [1000/1251 ( 80%)]  Loss:  4.241038 (4.6746)  Time: 1.333s,  768.27/s  (2.358s,  434.21/s)  LR: 1.188e-05  Data: 0.669 (1.756)
Train: 140 [1050/1251 ( 84%)]  Loss:  4.477717 (4.6657)  Time: 1.210s,  846.24/s  (2.351s,  435.61/s)  LR: 1.188e-05  Data: 0.334 (1.747)
Train: 140 [1100/1251 ( 88%)]  Loss:  4.983530 (4.6795)  Time: 3.694s,  277.19/s  (2.364s,  433.25/s)  LR: 1.188e-05  Data: 3.128 (1.760)
Train: 140 [1150/1251 ( 92%)]  Loss:  4.785072 (4.6839)  Time: 0.586s, 1748.74/s  (2.360s,  433.88/s)  LR: 1.188e-05  Data: 0.020 (1.757)
Train: 140 [1200/1251 ( 96%)]  Loss:  4.742464 (4.6862)  Time: 3.850s,  265.95/s  (2.361s,  433.68/s)  LR: 1.188e-05  Data: 3.267 (1.759)
Train: 140 [1250/1251 (100%)]  Loss:  4.894212 (4.6942)  Time: 0.565s, 1811.38/s  (2.357s,  434.43/s)  LR: 1.188e-05  Data: 0.000 (1.755)
Test: [   0/48]  Time: 13.622 (13.622)  Loss:  1.0781 (1.0781)  Acc@1: 76.9531 (76.9531)  Acc@5: 92.5781 (92.5781)
Test: [  48/48]  Time: 0.149 (3.258)  Loss:  1.0935 (1.9325)  Acc@1: 77.4764 (57.6140)  Acc@5: 90.8019 (81.2060)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-140.pth.tar', 57.61400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-139.pth.tar', 57.54000004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-136.pth.tar', 57.52999998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-138.pth.tar', 57.49999999023437)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-137.pth.tar', 57.45000006591797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-135.pth.tar', 57.37200006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-134.pth.tar', 57.36400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-132.pth.tar', 57.28000001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-133.pth.tar', 57.124000041503905)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-131.pth.tar', 57.05400011962891)

Train: 141 [   0/1251 (  0%)]  Loss:  4.926378 (4.9264)  Time: 11.652s,   87.88/s  (11.652s,   87.88/s)  LR: 1.106e-05  Data: 10.337 (10.337)
Train: 141 [  50/1251 (  4%)]  Loss:  4.717269 (4.8218)  Time: 0.587s, 1745.40/s  (2.345s,  436.73/s)  LR: 1.106e-05  Data: 0.020 (1.747)
Train: 141 [ 100/1251 (  8%)]  Loss:  4.647322 (4.7637)  Time: 0.583s, 1754.96/s  (2.295s,  446.21/s)  LR: 1.106e-05  Data: 0.019 (1.700)
Train: 141 [ 150/1251 ( 12%)]  Loss:  4.530975 (4.7055)  Time: 0.585s, 1751.45/s  (2.232s,  458.71/s)  LR: 1.106e-05  Data: 0.021 (1.639)
Train: 141 [ 200/1251 ( 16%)]  Loss:  4.127823 (4.5900)  Time: 0.588s, 1741.11/s  (2.337s,  438.08/s)  LR: 1.106e-05  Data: 0.018 (1.745)
Train: 141 [ 250/1251 ( 20%)]  Loss:  4.585008 (4.5891)  Time: 0.588s, 1741.88/s  (2.278s,  449.61/s)  LR: 1.106e-05  Data: 0.021 (1.688)
Train: 141 [ 300/1251 ( 24%)]  Loss:  3.822214 (4.4796)  Time: 0.585s, 1749.04/s  (2.378s,  430.53/s)  LR: 1.106e-05  Data: 0.018 (1.791)
Train: 141 [ 350/1251 ( 28%)]  Loss:  4.882372 (4.5299)  Time: 0.589s, 1738.75/s  (2.359s,  434.17/s)  LR: 1.106e-05  Data: 0.024 (1.771)
Train: 141 [ 400/1251 ( 32%)]  Loss:  4.991223 (4.5812)  Time: 0.583s, 1755.78/s  (2.345s,  436.61/s)  LR: 1.106e-05  Data: 0.020 (1.756)
Train: 141 [ 450/1251 ( 36%)]  Loss:  4.368590 (4.5599)  Time: 1.450s,  706.24/s  (2.333s,  438.98/s)  LR: 1.106e-05  Data: 0.745 (1.742)
Train: 141 [ 500/1251 ( 40%)]  Loss:  4.072163 (4.5156)  Time: 0.585s, 1750.71/s  (2.320s,  441.41/s)  LR: 1.106e-05  Data: 0.021 (1.727)
Train: 141 [ 550/1251 ( 44%)]  Loss:  4.740876 (4.5344)  Time: 1.474s,  694.86/s  (2.335s,  438.64/s)  LR: 1.106e-05  Data: 0.850 (1.742)
Train: 141 [ 600/1251 ( 48%)]  Loss:  5.027618 (4.5723)  Time: 1.317s,  777.63/s  (2.353s,  435.14/s)  LR: 1.106e-05  Data: 0.701 (1.760)
Train: 141 [ 650/1251 ( 52%)]  Loss:  4.713190 (4.5824)  Time: 6.361s,  160.98/s  (2.362s,  433.49/s)  LR: 1.106e-05  Data: 5.695 (1.767)
Train: 141 [ 700/1251 ( 56%)]  Loss:  5.154773 (4.6205)  Time: 0.585s, 1749.09/s  (2.363s,  433.29/s)  LR: 1.106e-05  Data: 0.022 (1.767)
Train: 141 [ 750/1251 ( 60%)]  Loss:  4.390273 (4.6061)  Time: 7.382s,  138.71/s  (2.367s,  432.61/s)  LR: 1.106e-05  Data: 6.782 (1.769)
Train: 141 [ 800/1251 ( 64%)]  Loss:  4.877875 (4.6221)  Time: 1.834s,  558.42/s  (2.360s,  433.82/s)  LR: 1.106e-05  Data: 1.256 (1.763)
Train: 141 [ 850/1251 ( 68%)]  Loss:  4.597548 (4.6207)  Time: 2.859s,  358.15/s  (2.358s,  434.20/s)  LR: 1.106e-05  Data: 2.296 (1.760)
Train: 141 [ 900/1251 ( 72%)]  Loss:  4.236651 (4.6005)  Time: 0.587s, 1744.32/s  (2.354s,  435.07/s)  LR: 1.106e-05  Data: 0.024 (1.756)
Train: 141 [ 950/1251 ( 76%)]  Loss:  4.609746 (4.6010)  Time: 0.587s, 1744.00/s  (2.366s,  432.82/s)  LR: 1.106e-05  Data: 0.019 (1.769)
Train: 141 [1000/1251 ( 80%)]  Loss:  5.066029 (4.6231)  Time: 0.586s, 1748.75/s  (2.359s,  434.02/s)  LR: 1.106e-05  Data: 0.022 (1.763)
Train: 141 [1050/1251 ( 84%)]  Loss:  4.043834 (4.5968)  Time: 0.586s, 1747.47/s  (2.364s,  433.11/s)  LR: 1.106e-05  Data: 0.018 (1.768)
Train: 141 [1100/1251 ( 88%)]  Loss:  4.758938 (4.6039)  Time: 0.586s, 1748.12/s  (2.360s,  433.86/s)  LR: 1.106e-05  Data: 0.021 (1.763)
Train: 141 [1150/1251 ( 92%)]  Loss:  3.803539 (4.5705)  Time: 0.584s, 1751.97/s  (2.356s,  434.68/s)  LR: 1.106e-05  Data: 0.020 (1.759)
Train: 141 [1200/1251 ( 96%)]  Loss:  4.434108 (4.5651)  Time: 0.585s, 1749.45/s  (2.366s,  432.73/s)  LR: 1.106e-05  Data: 0.022 (1.770)
Train: 141 [1250/1251 (100%)]  Loss:  5.010122 (4.5822)  Time: 0.564s, 1814.20/s  (2.362s,  433.51/s)  LR: 1.106e-05  Data: 0.000 (1.766)
Test: [   0/48]  Time: 13.819 (13.819)  Loss:  1.0805 (1.0805)  Acc@1: 77.9297 (77.9297)  Acc@5: 92.2852 (92.2852)
Test: [  48/48]  Time: 0.149 (3.442)  Loss:  1.0908 (1.9324)  Acc@1: 78.1840 (57.6120)  Acc@5: 91.6274 (81.2600)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-140.pth.tar', 57.61400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-141.pth.tar', 57.61200009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-139.pth.tar', 57.54000004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-136.pth.tar', 57.52999998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-138.pth.tar', 57.49999999023437)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-137.pth.tar', 57.45000006591797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-135.pth.tar', 57.37200006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-134.pth.tar', 57.36400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-132.pth.tar', 57.28000001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-133.pth.tar', 57.124000041503905)

Train: 142 [   0/1251 (  0%)]  Loss:  4.330537 (4.3305)  Time: 12.701s,   80.62/s  (12.701s,   80.62/s)  LR: 1.047e-05  Data: 10.489 (10.489)
Train: 142 [  50/1251 (  4%)]  Loss:  4.690356 (4.5104)  Time: 0.587s, 1744.09/s  (2.302s,  444.86/s)  LR: 1.047e-05  Data: 0.020 (1.680)
Train: 142 [ 100/1251 (  8%)]  Loss:  4.571731 (4.5309)  Time: 0.588s, 1741.39/s  (2.327s,  440.05/s)  LR: 1.047e-05  Data: 0.022 (1.715)
Train: 142 [ 150/1251 ( 12%)]  Loss:  4.939929 (4.6331)  Time: 0.590s, 1736.69/s  (2.275s,  450.14/s)  LR: 1.047e-05  Data: 0.024 (1.674)
Train: 142 [ 200/1251 ( 16%)]  Loss:  5.032508 (4.7130)  Time: 0.585s, 1748.95/s  (2.285s,  448.22/s)  LR: 1.047e-05  Data: 0.020 (1.682)
Train: 142 [ 250/1251 ( 20%)]  Loss:  4.512571 (4.6796)  Time: 0.586s, 1747.16/s  (2.249s,  455.40/s)  LR: 1.047e-05  Data: 0.022 (1.650)
Train: 142 [ 300/1251 ( 24%)]  Loss:  4.290943 (4.6241)  Time: 0.586s, 1748.83/s  (2.255s,  454.06/s)  LR: 1.047e-05  Data: 0.021 (1.659)
Train: 142 [ 350/1251 ( 28%)]  Loss:  5.000754 (4.6712)  Time: 0.591s, 1733.29/s  (2.230s,  459.19/s)  LR: 1.047e-05  Data: 0.023 (1.637)
Train: 142 [ 400/1251 ( 32%)]  Loss:  4.564601 (4.6593)  Time: 0.582s, 1758.83/s  (2.286s,  448.04/s)  LR: 1.047e-05  Data: 0.019 (1.688)
Train: 142 [ 450/1251 ( 36%)]  Loss:  3.915688 (4.5850)  Time: 0.599s, 1710.19/s  (2.265s,  452.19/s)  LR: 1.047e-05  Data: 0.023 (1.667)
Train: 142 [ 500/1251 ( 40%)]  Loss:  4.357867 (4.5643)  Time: 0.585s, 1749.31/s  (2.281s,  448.94/s)  LR: 1.047e-05  Data: 0.017 (1.683)
Train: 142 [ 550/1251 ( 44%)]  Loss:  4.697456 (4.5754)  Time: 0.584s, 1752.74/s  (2.270s,  451.16/s)  LR: 1.047e-05  Data: 0.018 (1.672)
Train: 142 [ 600/1251 ( 48%)]  Loss:  4.727599 (4.5871)  Time: 0.585s, 1749.25/s  (2.278s,  449.43/s)  LR: 1.047e-05  Data: 0.018 (1.681)
Train: 142 [ 650/1251 ( 52%)]  Loss:  4.288394 (4.5658)  Time: 0.584s, 1753.85/s  (2.286s,  448.02/s)  LR: 1.047e-05  Data: 0.020 (1.687)
Train: 142 [ 700/1251 ( 56%)]  Loss:  4.629877 (4.5701)  Time: 0.584s, 1754.14/s  (2.284s,  448.29/s)  LR: 1.047e-05  Data: 0.020 (1.686)
Train: 142 [ 750/1251 ( 60%)]  Loss:  4.541398 (4.5683)  Time: 0.586s, 1746.50/s  (2.288s,  447.54/s)  LR: 1.047e-05  Data: 0.021 (1.690)
Train: 142 [ 800/1251 ( 64%)]  Loss:  5.070157 (4.5978)  Time: 0.583s, 1756.29/s  (2.330s,  439.53/s)  LR: 1.047e-05  Data: 0.018 (1.731)
Train: 142 [ 850/1251 ( 68%)]  Loss:  4.436157 (4.5888)  Time: 0.583s, 1755.69/s  (2.339s,  437.73/s)  LR: 1.047e-05  Data: 0.018 (1.742)
Train: 142 [ 900/1251 ( 72%)]  Loss:  3.934273 (4.5544)  Time: 0.586s, 1748.60/s  (2.329s,  439.63/s)  LR: 1.047e-05  Data: 0.022 (1.732)
Train: 142 [ 950/1251 ( 76%)]  Loss:  4.240854 (4.5387)  Time: 0.587s, 1745.12/s  (2.335s,  438.52/s)  LR: 1.047e-05  Data: 0.021 (1.738)
Train: 142 [1000/1251 ( 80%)]  Loss:  4.686230 (4.5457)  Time: 0.586s, 1748.22/s  (2.327s,  440.14/s)  LR: 1.047e-05  Data: 0.020 (1.729)
Train: 142 [1050/1251 ( 84%)]  Loss:  4.225077 (4.5311)  Time: 0.588s, 1742.14/s  (2.329s,  439.72/s)  LR: 1.047e-05  Data: 0.019 (1.732)
Train: 142 [1100/1251 ( 88%)]  Loss:  4.454959 (4.5278)  Time: 0.585s, 1750.12/s  (2.320s,  441.44/s)  LR: 1.047e-05  Data: 0.018 (1.723)
Train: 142 [1150/1251 ( 92%)]  Loss:  4.355011 (4.5206)  Time: 0.587s, 1745.68/s  (2.333s,  438.96/s)  LR: 1.047e-05  Data: 0.018 (1.736)
Train: 142 [1200/1251 ( 96%)]  Loss:  4.750280 (4.5298)  Time: 0.588s, 1740.59/s  (2.327s,  440.13/s)  LR: 1.047e-05  Data: 0.022 (1.730)
Train: 142 [1250/1251 (100%)]  Loss:  4.840356 (4.5418)  Time: 0.565s, 1812.75/s  (2.329s,  439.73/s)  LR: 1.047e-05  Data: 0.000 (1.731)
Test: [   0/48]  Time: 14.938 (14.938)  Loss:  1.0565 (1.0565)  Acc@1: 77.6367 (77.6367)  Acc@5: 92.9688 (92.9688)
Test: [  48/48]  Time: 0.149 (3.236)  Loss:  1.0963 (1.9316)  Acc@1: 78.0660 (57.7400)  Acc@5: 91.2736 (81.2480)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-142.pth.tar', 57.7400000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-140.pth.tar', 57.61400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-141.pth.tar', 57.61200009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-139.pth.tar', 57.54000004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-136.pth.tar', 57.52999998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-138.pth.tar', 57.49999999023437)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-137.pth.tar', 57.45000006591797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-135.pth.tar', 57.37200006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-134.pth.tar', 57.36400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-132.pth.tar', 57.28000001708985)

Train: 143 [   0/1251 (  0%)]  Loss:  4.984154 (4.9842)  Time: 10.991s,   93.17/s  (10.991s,   93.17/s)  LR: 1.012e-05  Data: 10.253 (10.253)
Train: 143 [  50/1251 (  4%)]  Loss:  4.739211 (4.8617)  Time: 0.584s, 1754.19/s  (2.347s,  436.29/s)  LR: 1.012e-05  Data: 0.019 (1.768)
Train: 143 [ 100/1251 (  8%)]  Loss:  4.676265 (4.7999)  Time: 0.586s, 1748.14/s  (2.267s,  451.62/s)  LR: 1.012e-05  Data: 0.020 (1.687)
Train: 143 [ 150/1251 ( 12%)]  Loss:  4.693035 (4.7732)  Time: 0.587s, 1743.83/s  (2.220s,  461.36/s)  LR: 1.012e-05  Data: 0.020 (1.633)
Train: 143 [ 200/1251 ( 16%)]  Loss:  4.407889 (4.7001)  Time: 0.879s, 1165.31/s  (2.217s,  461.92/s)  LR: 1.012e-05  Data: 0.233 (1.626)
Train: 143 [ 250/1251 ( 20%)]  Loss:  4.615830 (4.6861)  Time: 1.612s,  635.39/s  (2.294s,  446.45/s)  LR: 1.012e-05  Data: 0.597 (1.700)
Train: 143 [ 300/1251 ( 24%)]  Loss:  4.670924 (4.6839)  Time: 0.584s, 1753.15/s  (2.269s,  451.25/s)  LR: 1.012e-05  Data: 0.019 (1.676)
Train: 143 [ 350/1251 ( 28%)]  Loss:  4.528920 (4.6645)  Time: 0.589s, 1738.04/s  (2.282s,  448.71/s)  LR: 1.012e-05  Data: 0.020 (1.687)
Train: 143 [ 400/1251 ( 32%)]  Loss:  4.726544 (4.6714)  Time: 0.585s, 1750.35/s  (2.327s,  440.07/s)  LR: 1.012e-05  Data: 0.020 (1.733)
Train: 143 [ 450/1251 ( 36%)]  Loss:  4.955712 (4.6998)  Time: 0.585s, 1749.34/s  (2.309s,  443.50/s)  LR: 1.012e-05  Data: 0.021 (1.715)
Train: 143 [ 500/1251 ( 40%)]  Loss:  4.274076 (4.6611)  Time: 0.584s, 1752.50/s  (2.302s,  444.81/s)  LR: 1.012e-05  Data: 0.019 (1.709)
Train: 143 [ 550/1251 ( 44%)]  Loss:  4.682225 (4.6629)  Time: 0.586s, 1748.06/s  (2.293s,  446.50/s)  LR: 1.012e-05  Data: 0.020 (1.700)
Train: 143 [ 600/1251 ( 48%)]  Loss:  5.027582 (4.6910)  Time: 0.587s, 1744.12/s  (2.320s,  441.38/s)  LR: 1.012e-05  Data: 0.018 (1.726)
Train: 143 [ 650/1251 ( 52%)]  Loss:  4.317345 (4.6643)  Time: 0.587s, 1745.69/s  (2.334s,  438.71/s)  LR: 1.012e-05  Data: 0.021 (1.738)
Train: 143 [ 700/1251 ( 56%)]  Loss:  4.355971 (4.6437)  Time: 3.835s,  267.02/s  (2.355s,  434.87/s)  LR: 1.012e-05  Data: 3.272 (1.759)
Train: 143 [ 750/1251 ( 60%)]  Loss:  4.672121 (4.6455)  Time: 0.592s, 1729.65/s  (2.353s,  435.14/s)  LR: 1.012e-05  Data: 0.022 (1.758)
Train: 143 [ 800/1251 ( 64%)]  Loss:  4.745207 (4.6514)  Time: 5.585s,  183.34/s  (2.351s,  435.62/s)  LR: 1.012e-05  Data: 5.013 (1.756)
Train: 143 [ 850/1251 ( 68%)]  Loss:  3.987436 (4.6145)  Time: 0.586s, 1748.78/s  (2.338s,  438.07/s)  LR: 1.012e-05  Data: 0.022 (1.740)
Train: 143 [ 900/1251 ( 72%)]  Loss:  3.799955 (4.5716)  Time: 4.703s,  217.71/s  (2.333s,  438.96/s)  LR: 1.012e-05  Data: 4.119 (1.734)
Train: 143 [ 950/1251 ( 76%)]  Loss:  4.454390 (4.5657)  Time: 0.588s, 1740.59/s  (2.318s,  441.76/s)  LR: 1.012e-05  Data: 0.022 (1.718)
Train: 143 [1000/1251 ( 80%)]  Loss:  4.682702 (4.5713)  Time: 2.837s,  360.91/s  (2.337s,  438.11/s)  LR: 1.012e-05  Data: 1.799 (1.736)
Train: 143 [1050/1251 ( 84%)]  Loss:  4.792801 (4.5814)  Time: 0.586s, 1746.14/s  (2.327s,  439.96/s)  LR: 1.012e-05  Data: 0.022 (1.726)
Train: 143 [1100/1251 ( 88%)]  Loss:  4.667126 (4.5851)  Time: 1.901s,  538.62/s  (2.319s,  441.65/s)  LR: 1.012e-05  Data: 1.245 (1.716)
Train: 143 [1150/1251 ( 92%)]  Loss:  4.772783 (4.5929)  Time: 0.589s, 1739.30/s  (2.317s,  441.87/s)  LR: 1.012e-05  Data: 0.020 (1.715)
Train: 143 [1200/1251 ( 96%)]  Loss:  4.736481 (4.5987)  Time: 0.676s, 1514.87/s  (2.316s,  442.15/s)  LR: 1.012e-05  Data: 0.019 (1.713)
Train: 143 [1250/1251 (100%)]  Loss:  4.716268 (4.6032)  Time: 0.568s, 1802.60/s  (2.306s,  443.97/s)  LR: 1.012e-05  Data: 0.000 (1.703)
Test: [   0/48]  Time: 13.072 (13.072)  Loss:  1.0696 (1.0696)  Acc@1: 77.7344 (77.7344)  Acc@5: 92.5781 (92.5781)
Test: [  48/48]  Time: 0.149 (3.443)  Loss:  1.0844 (1.9291)  Acc@1: 78.0660 (57.7280)  Acc@5: 91.6274 (81.3460)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-142.pth.tar', 57.7400000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-143.pth.tar', 57.7280000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-140.pth.tar', 57.61400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-141.pth.tar', 57.61200009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-139.pth.tar', 57.54000004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-136.pth.tar', 57.52999998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-138.pth.tar', 57.49999999023437)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-137.pth.tar', 57.45000006591797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-135.pth.tar', 57.37200006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-134.pth.tar', 57.36400004150391)

*** Best metric: 57.7400000390625 (epoch 142)

wandb: Waiting for W&B process to finish, PID 37540
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210531_140347-PreTraining_vit_deit_tiny_patch16_224_1k/logs/debug.log
wandb: Find internal logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210531_140347-PreTraining_vit_deit_tiny_patch16_224_1k/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 143
wandb:     _runtime 457496
wandb:    eval_loss 1.92909
wandb:    eval_top1 57.728
wandb:    eval_top5 81.346
wandb:   _timestamp 1622506404
wandb:   train_loss 4.60319
wandb:        _step 143
wandb: Run history:
wandb:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   train_loss ‚ñÇ‚ñá‚ñÉ‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñÉ‚ñÅ‚ñÉ‚ñá‚ñÑ‚ñÇ‚ñÑ
wandb:    eval_loss ‚ñà‚ñá‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    eval_top1 ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:    eval_top5 ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced PreTraining_vit_deit_tiny_patch16_224_1k: https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_1k
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
--End--
Tue Jun 1 09:13:36 JST 2021

--Start--
Wed May 26 00:17:34 JST 2021
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 1
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
wandb: Currently logged in as: gyama_x (use `wandb login --relogin` to force relogin)
wandb: WARNING Tried to auto resume run with id PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k but id PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1 is set.
wandb: wandb version 0.10.30 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
wandb: Tracking run with wandb version 0.10.27
wandb: Resuming run PreTraining_vit_deit_tiny_patch16_224_fake_1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gyama_x/pytorch-image-models
wandb: üöÄ View run at https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run data is saved locally in /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210526_001853-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run `wandb offline` to turn off syncing.

Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar' (epoch 43)
Using native Torch DistributedDataParallel.
Scheduled epochs: 66
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Train: 44 [   0/1171 (  0%)]  Loss:  3.256801 (3.2568)  Time: 14.855s,   68.93/s  (14.855s,   68.93/s)  LR: 2.575e-04  Data: 13.412 (13.412)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 44 [  50/1171 (  4%)]  Loss:  3.436406 (3.3466)  Time: 0.584s, 1752.51/s  (2.529s,  404.89/s)  LR: 2.575e-04  Data: 0.020 (1.929)
Train: 44 [ 100/1171 (  9%)]  Loss:  3.440997 (3.3781)  Time: 3.699s,  276.81/s  (2.519s,  406.49/s)  LR: 2.575e-04  Data: 3.136 (1.912)
Train: 44 [ 150/1171 ( 13%)]  Loss:  2.860327 (3.2486)  Time: 5.547s,  184.61/s  (2.476s,  413.65/s)  LR: 2.575e-04  Data: 4.955 (1.868)
Train: 44 [ 200/1171 ( 17%)]  Loss:  2.954824 (3.1899)  Time: 2.928s,  349.73/s  (2.427s,  421.86/s)  LR: 2.575e-04  Data: 2.362 (1.821)
Train: 44 [ 250/1171 ( 21%)]  Loss:  2.536299 (3.0809)  Time: 7.972s,  128.45/s  (2.441s,  419.46/s)  LR: 2.575e-04  Data: 7.267 (1.831)
Train: 44 [ 300/1171 ( 26%)]  Loss:  3.325264 (3.1158)  Time: 4.190s,  244.39/s  (2.449s,  418.15/s)  LR: 2.575e-04  Data: 3.625 (1.837)
Train: 44 [ 350/1171 ( 30%)]  Loss:  2.887337 (3.0873)  Time: 4.738s,  216.14/s  (2.456s,  416.92/s)  LR: 2.575e-04  Data: 4.104 (1.845)
Train: 44 [ 400/1171 ( 34%)]  Loss:  3.366727 (3.1183)  Time: 0.589s, 1738.20/s  (2.450s,  417.95/s)  LR: 2.575e-04  Data: 0.020 (1.839)
Train: 44 [ 450/1171 ( 38%)]  Loss:  2.935148 (3.1000)  Time: 7.958s,  128.68/s  (2.455s,  417.07/s)  LR: 2.575e-04  Data: 7.394 (1.847)
Train: 44 [ 500/1171 ( 43%)]  Loss:  2.925084 (3.0841)  Time: 0.589s, 1739.63/s  (2.433s,  420.80/s)  LR: 2.575e-04  Data: 0.024 (1.827)
Train: 44 [ 550/1171 ( 47%)]  Loss:  3.083501 (3.0841)  Time: 7.646s,  133.93/s  (2.428s,  421.67/s)  LR: 2.575e-04  Data: 6.980 (1.824)
Train: 44 [ 600/1171 ( 51%)]  Loss:  3.821321 (3.1408)  Time: 0.588s, 1741.32/s  (2.406s,  425.54/s)  LR: 2.575e-04  Data: 0.023 (1.802)
Train: 44 [ 650/1171 ( 56%)]  Loss:  3.103699 (3.1381)  Time: 4.185s,  244.70/s  (2.432s,  420.99/s)  LR: 2.575e-04  Data: 3.601 (1.826)
Train: 44 [ 700/1171 ( 60%)]  Loss:  3.336395 (3.1513)  Time: 3.479s,  294.35/s  (2.450s,  417.94/s)  LR: 2.575e-04  Data: 2.914 (1.842)
Train: 44 [ 750/1171 ( 64%)]  Loss:  3.693199 (3.1852)  Time: 2.089s,  490.30/s  (2.470s,  414.64/s)  LR: 2.575e-04  Data: 1.515 (1.860)
Train: 44 [ 800/1171 ( 68%)]  Loss:  3.132675 (3.1821)  Time: 8.145s,  125.72/s  (2.490s,  411.29/s)  LR: 2.575e-04  Data: 7.568 (1.880)
Train: 44 [ 850/1171 ( 73%)]  Loss:  3.198188 (3.1830)  Time: 1.985s,  515.85/s  (2.494s,  410.51/s)  LR: 2.575e-04  Data: 1.405 (1.887)
Train: 44 [ 900/1171 ( 77%)]  Loss:  3.535705 (3.2016)  Time: 8.159s,  125.50/s  (2.507s,  408.52/s)  LR: 2.575e-04  Data: 7.585 (1.898)
Train: 44 [ 950/1171 ( 81%)]  Loss:  3.001042 (3.1915)  Time: 0.586s, 1748.45/s  (2.501s,  409.44/s)  LR: 2.575e-04  Data: 0.020 (1.893)
Train: 44 [1000/1171 ( 85%)]  Loss:  2.702040 (3.1682)  Time: 9.000s,  113.77/s  (2.537s,  403.55/s)  LR: 2.575e-04  Data: 8.266 (1.929)
Train: 44 [1050/1171 ( 90%)]  Loss:  3.150518 (3.1674)  Time: 0.583s, 1756.49/s  (2.550s,  401.62/s)  LR: 2.575e-04  Data: 0.017 (1.940)
Train: 44 [1100/1171 ( 94%)]  Loss:  3.267881 (3.1718)  Time: 9.004s,  113.73/s  (2.561s,  399.78/s)  LR: 2.575e-04  Data: 8.425 (1.953)
Train: 44 [1150/1171 ( 98%)]  Loss:  2.754278 (3.1544)  Time: 4.041s,  253.39/s  (2.562s,  399.63/s)  LR: 2.575e-04  Data: 3.354 (1.954)
Train: 44 [1170/1171 (100%)]  Loss:  3.146435 (3.1541)  Time: 0.565s, 1811.34/s  (2.560s,  400.07/s)  LR: 2.575e-04  Data: 0.000 (1.952)
Test: [   0/97]  Time: 15.683 (15.683)  Loss:  0.3649 (0.3649)  Acc@1: 94.1406 (94.1406)  Acc@5: 99.1211 (99.1211)
Test: [  50/97]  Time: 0.428 (3.411)  Loss:  0.5733 (0.4502)  Acc@1: 88.7695 (92.4977)  Acc@5: 97.7539 (98.5888)
Test: [  97/97]  Time: 0.489 (3.446)  Loss:  0.3596 (0.4647)  Acc@1: 94.7917 (92.1360)  Acc@5: 98.6607 (98.2420)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-44.pth.tar', 92.13600003417969)

Train: 45 [   0/1171 (  0%)]  Loss:  3.150790 (3.1508)  Time: 14.996s,   68.28/s  (14.996s,   68.28/s)  LR: 2.374e-04  Data: 13.720 (13.720)
Train: 45 [  50/1171 (  4%)]  Loss:  3.621542 (3.3862)  Time: 0.588s, 1742.43/s  (2.954s,  346.62/s)  LR: 2.374e-04  Data: 0.023 (2.346)
Train: 45 [ 100/1171 (  9%)]  Loss:  3.320547 (3.3643)  Time: 0.585s, 1750.72/s  (2.877s,  355.90/s)  LR: 2.374e-04  Data: 0.020 (2.276)
Train: 45 [ 150/1171 ( 13%)]  Loss:  3.125518 (3.3046)  Time: 1.831s,  559.17/s  (2.781s,  368.17/s)  LR: 2.374e-04  Data: 1.268 (2.173)
Train: 45 [ 200/1171 ( 17%)]  Loss:  3.050973 (3.2539)  Time: 0.585s, 1750.99/s  (2.745s,  373.04/s)  LR: 2.374e-04  Data: 0.020 (2.140)
Train: 45 [ 250/1171 ( 21%)]  Loss:  2.810898 (3.1800)  Time: 8.091s,  126.57/s  (2.715s,  377.19/s)  LR: 2.374e-04  Data: 7.367 (2.110)
Train: 45 [ 300/1171 ( 26%)]  Loss:  3.082314 (3.1661)  Time: 0.587s, 1744.65/s  (2.664s,  384.34/s)  LR: 2.374e-04  Data: 0.022 (2.054)
Train: 45 [ 350/1171 ( 30%)]  Loss:  3.001756 (3.1455)  Time: 1.093s,  937.28/s  (2.699s,  379.41/s)  LR: 2.374e-04  Data: 0.528 (2.090)
Train: 45 [ 400/1171 ( 34%)]  Loss:  3.648392 (3.2014)  Time: 0.586s, 1747.27/s  (2.706s,  378.42/s)  LR: 2.374e-04  Data: 0.021 (2.098)
Train: 45 [ 450/1171 ( 38%)]  Loss:  2.957220 (3.1770)  Time: 2.326s,  440.18/s  (2.705s,  378.56/s)  LR: 2.374e-04  Data: 1.667 (2.099)
Train: 45 [ 500/1171 ( 43%)]  Loss:  3.277361 (3.1861)  Time: 0.589s, 1738.04/s  (2.713s,  377.42/s)  LR: 2.374e-04  Data: 0.019 (2.106)
Train: 45 [ 550/1171 ( 47%)]  Loss:  3.039190 (3.1739)  Time: 10.168s,  100.70/s  (2.721s,  376.28/s)  LR: 2.374e-04  Data: 8.836 (2.114)
Train: 45 [ 600/1171 ( 51%)]  Loss:  3.454443 (3.1955)  Time: 0.585s, 1750.96/s  (2.705s,  378.58/s)  LR: 2.374e-04  Data: 0.020 (2.098)
Train: 45 [ 650/1171 ( 56%)]  Loss:  3.330714 (3.2051)  Time: 8.487s,  120.65/s  (2.696s,  379.88/s)  LR: 2.374e-04  Data: 7.921 (2.088)
Train: 45 [ 700/1171 ( 60%)]  Loss:  3.057719 (3.1953)  Time: 0.589s, 1738.41/s  (2.716s,  377.00/s)  LR: 2.374e-04  Data: 0.021 (2.110)
Train: 45 [ 750/1171 ( 64%)]  Loss:  3.074699 (3.1878)  Time: 9.069s,  112.91/s  (2.726s,  375.66/s)  LR: 2.374e-04  Data: 8.374 (2.121)
Train: 45 [ 800/1171 ( 68%)]  Loss:  2.900218 (3.1708)  Time: 0.588s, 1741.16/s  (2.716s,  376.96/s)  LR: 2.374e-04  Data: 0.024 (2.113)
Train: 45 [ 850/1171 ( 73%)]  Loss:  3.195515 (3.1722)  Time: 8.739s,  117.18/s  (2.718s,  376.76/s)  LR: 2.374e-04  Data: 8.168 (2.114)
Train: 45 [ 900/1171 ( 77%)]  Loss:  3.323713 (3.1802)  Time: 0.588s, 1741.46/s  (2.706s,  378.36/s)  LR: 2.374e-04  Data: 0.023 (2.102)
Train: 45 [ 950/1171 ( 81%)]  Loss:  3.169654 (3.1797)  Time: 3.417s,  299.66/s  (2.697s,  379.72/s)  LR: 2.374e-04  Data: 2.834 (2.092)
Train: 45 [1000/1171 ( 85%)]  Loss:  3.792926 (3.2089)  Time: 0.588s, 1742.38/s  (2.702s,  379.05/s)  LR: 2.374e-04  Data: 0.022 (2.097)
Train: 45 [1050/1171 ( 90%)]  Loss:  3.572855 (3.2254)  Time: 4.501s,  227.52/s  (2.705s,  378.54/s)  LR: 2.374e-04  Data: 3.839 (2.100)
Train: 45 [1100/1171 ( 94%)]  Loss:  3.247523 (3.2264)  Time: 3.665s,  279.41/s  (2.705s,  378.54/s)  LR: 2.374e-04  Data: 3.052 (2.099)
Train: 45 [1150/1171 ( 98%)]  Loss:  3.241278 (3.2270)  Time: 3.833s,  267.13/s  (2.700s,  379.22/s)  LR: 2.374e-04  Data: 3.269 (2.094)
Train: 45 [1170/1171 (100%)]  Loss:  3.104009 (3.2221)  Time: 0.566s, 1808.97/s  (2.696s,  379.89/s)  LR: 2.374e-04  Data: 0.000 (2.089)
Test: [   0/97]  Time: 16.258 (16.258)  Loss:  0.3562 (0.3562)  Acc@1: 95.3125 (95.3125)  Acc@5: 99.3164 (99.3164)
Test: [  50/97]  Time: 0.200 (3.396)  Loss:  0.5896 (0.4678)  Acc@1: 88.5742 (92.6720)  Acc@5: 97.8516 (98.6060)
Test: [  97/97]  Time: 0.120 (3.297)  Loss:  0.3853 (0.4845)  Acc@1: 94.1964 (92.1760)  Acc@5: 98.5119 (98.2300)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-45.pth.tar', 92.17599998535157)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-44.pth.tar', 92.13600003417969)

Train: 46 [   0/1171 (  0%)]  Loss:  3.122646 (3.1226)  Time: 11.710s,   87.45/s  (11.710s,   87.45/s)  LR: 2.179e-04  Data: 10.536 (10.536)
Train: 46 [  50/1171 (  4%)]  Loss:  3.082195 (3.1024)  Time: 0.687s, 1490.41/s  (2.854s,  358.81/s)  LR: 2.179e-04  Data: 0.019 (2.254)
Train: 46 [ 100/1171 (  9%)]  Loss:  3.821314 (3.3421)  Time: 0.593s, 1727.25/s  (2.837s,  361.00/s)  LR: 2.179e-04  Data: 0.025 (2.226)
Train: 46 [ 150/1171 ( 13%)]  Loss:  2.543061 (3.1423)  Time: 0.925s, 1107.34/s  (2.737s,  374.07/s)  LR: 2.179e-04  Data: 0.358 (2.135)
Train: 46 [ 200/1171 ( 17%)]  Loss:  2.874118 (3.0887)  Time: 0.590s, 1736.12/s  (2.689s,  380.82/s)  LR: 2.179e-04  Data: 0.020 (2.080)
Train: 46 [ 250/1171 ( 21%)]  Loss:  3.012426 (3.0760)  Time: 3.229s,  317.17/s  (2.660s,  384.96/s)  LR: 2.179e-04  Data: 2.530 (2.043)
Train: 46 [ 300/1171 ( 26%)]  Loss:  2.750073 (3.0294)  Time: 0.588s, 1740.84/s  (2.624s,  390.23/s)  LR: 2.179e-04  Data: 0.019 (2.007)
Train: 46 [ 350/1171 ( 30%)]  Loss:  3.152706 (3.0448)  Time: 0.586s, 1746.88/s  (2.585s,  396.07/s)  LR: 2.179e-04  Data: 0.021 (1.970)
Train: 46 [ 400/1171 ( 34%)]  Loss:  3.195946 (3.0616)  Time: 0.588s, 1740.57/s  (2.618s,  391.14/s)  LR: 2.179e-04  Data: 0.020 (2.005)
Train: 46 [ 450/1171 ( 38%)]  Loss:  3.174337 (3.0729)  Time: 0.588s, 1741.89/s  (2.631s,  389.22/s)  LR: 2.179e-04  Data: 0.020 (2.017)
Train: 46 [ 500/1171 ( 43%)]  Loss:  3.322701 (3.0956)  Time: 0.588s, 1741.06/s  (2.636s,  388.53/s)  LR: 2.179e-04  Data: 0.023 (2.024)
Train: 46 [ 550/1171 ( 47%)]  Loss:  3.065372 (3.0931)  Time: 0.589s, 1739.29/s  (2.651s,  386.30/s)  LR: 2.179e-04  Data: 0.021 (2.041)
Train: 46 [ 600/1171 ( 51%)]  Loss:  3.063825 (3.0908)  Time: 0.590s, 1736.27/s  (2.650s,  386.38/s)  LR: 2.179e-04  Data: 0.021 (2.039)
Train: 46 [ 650/1171 ( 56%)]  Loss:  3.377279 (3.1113)  Time: 0.588s, 1740.07/s  (2.638s,  388.18/s)  LR: 2.179e-04  Data: 0.024 (2.027)
Train: 46 [ 700/1171 ( 60%)]  Loss:  3.400874 (3.1306)  Time: 0.588s, 1741.97/s  (2.625s,  390.14/s)  LR: 2.179e-04  Data: 0.019 (2.014)
Train: 46 [ 750/1171 ( 64%)]  Loss:  3.383191 (3.1464)  Time: 0.590s, 1736.86/s  (2.646s,  386.96/s)  LR: 2.179e-04  Data: 0.024 (2.035)
Train: 46 [ 800/1171 ( 68%)]  Loss:  3.266383 (3.1534)  Time: 0.585s, 1749.11/s  (2.645s,  387.20/s)  LR: 2.179e-04  Data: 0.020 (2.034)
Train: 46 [ 850/1171 ( 73%)]  Loss:  2.521000 (3.1183)  Time: 0.589s, 1739.81/s  (2.647s,  386.89/s)  LR: 2.179e-04  Data: 0.023 (2.036)
Train: 46 [ 900/1171 ( 77%)]  Loss:  2.993623 (3.1117)  Time: 2.616s,  391.42/s  (2.644s,  387.35/s)  LR: 2.179e-04  Data: 1.995 (2.033)
Train: 46 [ 950/1171 ( 81%)]  Loss:  3.582611 (3.1353)  Time: 0.591s, 1731.35/s  (2.649s,  386.56/s)  LR: 2.179e-04  Data: 0.024 (2.039)
Train: 46 [1000/1171 ( 85%)]  Loss:  2.969701 (3.1274)  Time: 2.465s,  415.44/s  (2.649s,  386.61/s)  LR: 2.179e-04  Data: 1.899 (2.038)
Train: 46 [1050/1171 ( 90%)]  Loss:  3.169622 (3.1293)  Time: 0.587s, 1745.54/s  (2.650s,  386.36/s)  LR: 2.179e-04  Data: 0.020 (2.040)
Train: 46 [1100/1171 ( 94%)]  Loss:  3.714117 (3.1547)  Time: 2.867s,  357.20/s  (2.677s,  382.51/s)  LR: 2.179e-04  Data: 2.217 (2.066)
Train: 46 [1150/1171 ( 98%)]  Loss:  3.166626 (3.1552)  Time: 0.586s, 1746.24/s  (2.691s,  380.56/s)  LR: 2.179e-04  Data: 0.021 (2.080)
Train: 46 [1170/1171 (100%)]  Loss:  3.482611 (3.1683)  Time: 0.568s, 1801.86/s  (2.693s,  380.23/s)  LR: 2.179e-04  Data: 0.000 (2.083)
Test: [   0/97]  Time: 16.869 (16.869)  Loss:  0.4083 (0.4083)  Acc@1: 95.8008 (95.8008)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.198 (3.481)  Loss:  0.7192 (0.5108)  Acc@1: 85.3516 (92.8960)  Acc@5: 97.2656 (98.5811)
Test: [  97/97]  Time: 0.120 (3.311)  Loss:  0.4939 (0.5287)  Acc@1: 92.8571 (92.2820)  Acc@5: 98.5119 (98.2030)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-46.pth.tar', 92.28200002929688)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-45.pth.tar', 92.17599998535157)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-44.pth.tar', 92.13600003417969)

Train: 47 [   0/1171 (  0%)]  Loss:  3.386115 (3.3861)  Time: 10.464s,   97.86/s  (10.464s,   97.86/s)  LR: 1.990e-04  Data: 9.844 (9.844)
Train: 47 [  50/1171 (  4%)]  Loss:  3.182747 (3.2844)  Time: 1.061s,  965.57/s  (2.468s,  414.90/s)  LR: 1.990e-04  Data: 0.496 (1.871)
Train: 47 [ 100/1171 (  9%)]  Loss:  3.011244 (3.1934)  Time: 0.588s, 1740.40/s  (2.627s,  389.75/s)  LR: 1.990e-04  Data: 0.022 (2.035)
Train: 47 [ 150/1171 ( 13%)]  Loss:  3.226980 (3.2018)  Time: 1.345s,  761.10/s  (2.691s,  380.48/s)  LR: 1.990e-04  Data: 0.780 (2.093)
Train: 47 [ 200/1171 ( 17%)]  Loss:  2.628875 (3.0872)  Time: 0.586s, 1746.01/s  (2.758s,  371.27/s)  LR: 1.990e-04  Data: 0.021 (2.158)
Train: 47 [ 250/1171 ( 21%)]  Loss:  3.061155 (3.0829)  Time: 0.587s, 1744.50/s  (2.756s,  371.61/s)  LR: 1.990e-04  Data: 0.020 (2.159)
Train: 47 [ 300/1171 ( 26%)]  Loss:  3.557567 (3.1507)  Time: 0.587s, 1744.17/s  (2.766s,  370.15/s)  LR: 1.990e-04  Data: 0.021 (2.172)
Train: 47 [ 350/1171 ( 30%)]  Loss:  3.511751 (3.1958)  Time: 0.586s, 1748.86/s  (2.734s,  374.61/s)  LR: 1.990e-04  Data: 0.021 (2.141)
Train: 47 [ 400/1171 ( 34%)]  Loss:  3.058496 (3.1805)  Time: 0.588s, 1740.64/s  (2.719s,  376.59/s)  LR: 1.990e-04  Data: 0.020 (2.127)
Train: 47 [ 450/1171 ( 38%)]  Loss:  3.424172 (3.2049)  Time: 0.590s, 1734.41/s  (2.740s,  373.69/s)  LR: 1.990e-04  Data: 0.025 (2.149)
Train: 47 [ 500/1171 ( 43%)]  Loss:  3.074686 (3.1931)  Time: 0.585s, 1749.15/s  (2.751s,  372.22/s)  LR: 1.990e-04  Data: 0.020 (2.158)
Train: 47 [ 550/1171 ( 47%)]  Loss:  2.788472 (3.1594)  Time: 0.590s, 1735.06/s  (2.750s,  372.39/s)  LR: 1.990e-04  Data: 0.025 (2.156)
Train: 47 [ 600/1171 ( 51%)]  Loss:  2.702964 (3.1242)  Time: 0.590s, 1736.13/s  (2.748s,  372.63/s)  LR: 1.990e-04  Data: 0.022 (2.153)
Train: 47 [ 650/1171 ( 56%)]  Loss:  3.451779 (3.1476)  Time: 1.479s,  692.36/s  (2.731s,  374.89/s)  LR: 1.990e-04  Data: 0.845 (2.135)
Train: 47 [ 700/1171 ( 60%)]  Loss:  3.054997 (3.1415)  Time: 0.587s, 1743.57/s  (2.711s,  377.71/s)  LR: 1.990e-04  Data: 0.019 (2.114)
Train: 47 [ 750/1171 ( 64%)]  Loss:  3.283275 (3.1503)  Time: 2.875s,  356.23/s  (2.705s,  378.52/s)  LR: 1.990e-04  Data: 2.194 (2.107)
Train: 47 [ 800/1171 ( 68%)]  Loss:  3.416712 (3.1660)  Time: 0.588s, 1741.00/s  (2.710s,  377.84/s)  LR: 1.990e-04  Data: 0.020 (2.111)
Train: 47 [ 850/1171 ( 73%)]  Loss:  3.389584 (3.1784)  Time: 0.586s, 1748.02/s  (2.701s,  379.11/s)  LR: 1.990e-04  Data: 0.021 (2.103)
Train: 47 [ 900/1171 ( 77%)]  Loss:  3.264359 (3.1829)  Time: 0.587s, 1745.47/s  (2.700s,  379.20/s)  LR: 1.990e-04  Data: 0.020 (2.103)
Train: 47 [ 950/1171 ( 81%)]  Loss:  2.920617 (3.1698)  Time: 0.590s, 1736.05/s  (2.688s,  380.89/s)  LR: 1.990e-04  Data: 0.025 (2.091)
Train: 47 [1000/1171 ( 85%)]  Loss:  3.021596 (3.1628)  Time: 0.585s, 1750.08/s  (2.683s,  381.59/s)  LR: 1.990e-04  Data: 0.020 (2.086)
Train: 47 [1050/1171 ( 90%)]  Loss:  3.329232 (3.1703)  Time: 0.588s, 1741.95/s  (2.669s,  383.66/s)  LR: 1.990e-04  Data: 0.022 (2.070)
Train: 47 [1100/1171 ( 94%)]  Loss:  3.216584 (3.1723)  Time: 0.588s, 1741.32/s  (2.675s,  382.80/s)  LR: 1.990e-04  Data: 0.021 (2.075)
Train: 47 [1150/1171 ( 98%)]  Loss:  3.387184 (3.1813)  Time: 1.854s,  552.24/s  (2.673s,  383.07/s)  LR: 1.990e-04  Data: 1.239 (2.073)
Train: 47 [1170/1171 (100%)]  Loss:  3.404137 (3.1902)  Time: 0.567s, 1805.83/s  (2.672s,  383.27/s)  LR: 1.990e-04  Data: 0.000 (2.071)
Test: [   0/97]  Time: 15.867 (15.867)  Loss:  0.3615 (0.3615)  Acc@1: 95.7031 (95.7031)  Acc@5: 99.3164 (99.3164)
Test: [  50/97]  Time: 0.202 (3.401)  Loss:  0.6044 (0.4696)  Acc@1: 88.2812 (92.8232)  Acc@5: 97.8516 (98.6252)
Test: [  97/97]  Time: 0.120 (3.349)  Loss:  0.3994 (0.4786)  Acc@1: 94.0476 (92.3790)  Acc@5: 98.6607 (98.3210)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-47.pth.tar', 92.37900002441407)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-46.pth.tar', 92.28200002929688)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-45.pth.tar', 92.17599998535157)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-44.pth.tar', 92.13600003417969)

Train: 48 [   0/1171 (  0%)]  Loss:  3.084952 (3.0850)  Time: 10.998s,   93.11/s  (10.998s,   93.11/s)  LR: 1.808e-04  Data: 10.066 (10.066)
Train: 48 [  50/1171 (  4%)]  Loss:  3.016679 (3.0508)  Time: 0.588s, 1742.86/s  (2.527s,  405.24/s)  LR: 1.808e-04  Data: 0.020 (1.883)
Train: 48 [ 100/1171 (  9%)]  Loss:  2.938919 (3.0135)  Time: 0.589s, 1739.31/s  (2.450s,  417.93/s)  LR: 1.808e-04  Data: 0.023 (1.826)
Train: 48 [ 150/1171 ( 13%)]  Loss:  3.049657 (3.0226)  Time: 0.587s, 1744.52/s  (2.538s,  403.41/s)  LR: 1.808e-04  Data: 0.020 (1.926)
Train: 48 [ 200/1171 ( 17%)]  Loss:  3.661049 (3.1503)  Time: 0.591s, 1732.73/s  (2.556s,  400.69/s)  LR: 1.808e-04  Data: 0.019 (1.944)
Train: 48 [ 250/1171 ( 21%)]  Loss:  2.911431 (3.1104)  Time: 0.587s, 1745.76/s  (2.581s,  396.76/s)  LR: 1.808e-04  Data: 0.019 (1.969)
Train: 48 [ 300/1171 ( 26%)]  Loss:  3.337449 (3.1429)  Time: 0.587s, 1743.98/s  (2.608s,  392.59/s)  LR: 1.808e-04  Data: 0.021 (1.995)
Train: 48 [ 350/1171 ( 30%)]  Loss:  2.793665 (3.0992)  Time: 0.589s, 1737.88/s  (2.584s,  396.22/s)  LR: 1.808e-04  Data: 0.019 (1.972)
Train: 48 [ 400/1171 ( 34%)]  Loss:  3.494432 (3.1431)  Time: 0.587s, 1744.53/s  (2.572s,  398.19/s)  LR: 1.808e-04  Data: 0.021 (1.960)
Train: 48 [ 450/1171 ( 38%)]  Loss:  3.043678 (3.1332)  Time: 0.588s, 1742.46/s  (2.551s,  401.41/s)  LR: 1.808e-04  Data: 0.020 (1.940)
Train: 48 [ 500/1171 ( 43%)]  Loss:  3.151265 (3.1348)  Time: 0.590s, 1736.32/s  (2.582s,  396.60/s)  LR: 1.808e-04  Data: 0.024 (1.973)
Train: 48 [ 550/1171 ( 47%)]  Loss:  3.273386 (3.1464)  Time: 0.588s, 1741.76/s  (2.614s,  391.73/s)  LR: 1.808e-04  Data: 0.020 (2.005)
Train: 48 [ 600/1171 ( 51%)]  Loss:  3.426544 (3.1679)  Time: 1.153s,  888.22/s  (2.637s,  388.34/s)  LR: 1.808e-04  Data: 0.588 (2.029)
Train: 48 [ 650/1171 ( 56%)]  Loss:  3.516598 (3.1928)  Time: 0.588s, 1742.71/s  (2.635s,  388.59/s)  LR: 1.808e-04  Data: 0.020 (2.026)
Train: 48 [ 700/1171 ( 60%)]  Loss:  3.360193 (3.2040)  Time: 0.586s, 1746.21/s  (2.635s,  388.58/s)  LR: 1.808e-04  Data: 0.020 (2.027)
Train: 48 [ 750/1171 ( 64%)]  Loss:  2.829931 (3.1806)  Time: 0.592s, 1730.27/s  (2.624s,  390.28/s)  LR: 1.808e-04  Data: 0.026 (2.015)
Train: 48 [ 800/1171 ( 68%)]  Loss:  3.434218 (3.1955)  Time: 0.587s, 1744.12/s  (2.614s,  391.70/s)  LR: 1.808e-04  Data: 0.023 (2.006)
Train: 48 [ 850/1171 ( 73%)]  Loss:  3.032670 (3.1865)  Time: 3.121s,  328.08/s  (2.629s,  389.54/s)  LR: 1.808e-04  Data: 2.148 (2.019)
Train: 48 [ 900/1171 ( 77%)]  Loss:  3.134145 (3.1837)  Time: 0.591s, 1731.55/s  (2.628s,  389.66/s)  LR: 1.808e-04  Data: 0.020 (2.018)
Train: 48 [ 950/1171 ( 81%)]  Loss:  3.510463 (3.2001)  Time: 1.765s,  580.13/s  (2.612s,  391.96/s)  LR: 1.808e-04  Data: 1.080 (2.003)
Train: 48 [1000/1171 ( 85%)]  Loss:  2.792658 (3.1807)  Time: 0.589s, 1738.68/s  (2.604s,  393.18/s)  LR: 1.808e-04  Data: 0.022 (1.996)
Train: 48 [1050/1171 ( 90%)]  Loss:  2.472023 (3.1485)  Time: 0.595s, 1722.05/s  (2.593s,  394.99/s)  LR: 1.808e-04  Data: 0.020 (1.984)
Train: 48 [1100/1171 ( 94%)]  Loss:  3.420519 (3.1603)  Time: 0.588s, 1741.15/s  (2.581s,  396.67/s)  LR: 1.808e-04  Data: 0.022 (1.974)
Train: 48 [1150/1171 ( 98%)]  Loss:  3.521107 (3.1753)  Time: 0.590s, 1736.26/s  (2.565s,  399.28/s)  LR: 1.808e-04  Data: 0.019 (1.958)
Train: 48 [1170/1171 (100%)]  Loss:  2.934767 (3.1657)  Time: 0.564s, 1816.06/s  (2.558s,  400.27/s)  LR: 1.808e-04  Data: 0.000 (1.952)
Test: [   0/97]  Time: 12.963 (12.963)  Loss:  0.3369 (0.3369)  Acc@1: 95.8984 (95.8984)  Acc@5: 99.6094 (99.6094)
Test: [  50/97]  Time: 0.198 (3.321)  Loss:  0.5881 (0.4484)  Acc@1: 90.4297 (93.2234)  Acc@5: 97.9492 (98.6405)
Test: [  97/97]  Time: 0.120 (3.246)  Loss:  0.4027 (0.4657)  Acc@1: 93.8988 (92.7340)  Acc@5: 98.8095 (98.3170)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-48.pth.tar', 92.73400001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-47.pth.tar', 92.37900002441407)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-46.pth.tar', 92.28200002929688)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-45.pth.tar', 92.17599998535157)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-44.pth.tar', 92.13600003417969)

Train: 49 [   0/1171 (  0%)]  Loss:  3.065086 (3.0651)  Time: 11.667s,   87.77/s  (11.667s,   87.77/s)  LR: 1.634e-04  Data: 11.040 (11.040)
Train: 49 [  50/1171 (  4%)]  Loss:  2.691669 (2.8784)  Time: 0.586s, 1748.13/s  (2.474s,  413.97/s)  LR: 1.634e-04  Data: 0.021 (1.873)
Train: 49 [ 100/1171 (  9%)]  Loss:  3.268332 (3.0084)  Time: 0.590s, 1735.96/s  (2.367s,  432.71/s)  LR: 1.634e-04  Data: 0.025 (1.780)
Train: 49 [ 150/1171 ( 13%)]  Loss:  3.406325 (3.1079)  Time: 0.587s, 1743.93/s  (2.285s,  448.10/s)  LR: 1.634e-04  Data: 0.021 (1.696)
Train: 49 [ 200/1171 ( 17%)]  Loss:  2.666867 (3.0197)  Time: 0.587s, 1743.81/s  (2.277s,  449.66/s)  LR: 1.634e-04  Data: 0.018 (1.689)
Train: 49 [ 250/1171 ( 21%)]  Loss:  2.914387 (3.0021)  Time: 0.586s, 1746.32/s  (2.230s,  459.11/s)  LR: 1.634e-04  Data: 0.022 (1.644)
Train: 49 [ 300/1171 ( 26%)]  Loss:  3.659109 (3.0960)  Time: 0.588s, 1740.81/s  (2.299s,  445.40/s)  LR: 1.634e-04  Data: 0.019 (1.704)
Train: 49 [ 350/1171 ( 30%)]  Loss:  3.762886 (3.1793)  Time: 0.587s, 1743.63/s  (2.299s,  445.38/s)  LR: 1.634e-04  Data: 0.021 (1.705)
Train: 49 [ 400/1171 ( 34%)]  Loss:  3.146427 (3.1757)  Time: 0.584s, 1752.56/s  (2.331s,  439.23/s)  LR: 1.634e-04  Data: 0.020 (1.736)
Train: 49 [ 450/1171 ( 38%)]  Loss:  3.415941 (3.1997)  Time: 0.587s, 1743.74/s  (2.328s,  439.90/s)  LR: 1.634e-04  Data: 0.018 (1.733)
Train: 49 [ 500/1171 ( 43%)]  Loss:  2.874678 (3.1702)  Time: 0.585s, 1750.96/s  (2.338s,  438.06/s)  LR: 1.634e-04  Data: 0.019 (1.743)
Train: 49 [ 550/1171 ( 47%)]  Loss:  3.158059 (3.1691)  Time: 0.586s, 1746.95/s  (2.336s,  438.40/s)  LR: 1.634e-04  Data: 0.018 (1.742)
Train: 49 [ 600/1171 ( 51%)]  Loss:  3.212407 (3.1725)  Time: 0.586s, 1748.29/s  (2.361s,  433.68/s)  LR: 1.634e-04  Data: 0.019 (1.767)
Train: 49 [ 650/1171 ( 56%)]  Loss:  3.278727 (3.1801)  Time: 0.588s, 1740.30/s  (2.379s,  430.35/s)  LR: 1.634e-04  Data: 0.021 (1.786)
Train: 49 [ 700/1171 ( 60%)]  Loss:  2.889673 (3.1607)  Time: 0.811s, 1262.67/s  (2.420s,  423.17/s)  LR: 1.634e-04  Data: 0.211 (1.826)
Train: 49 [ 750/1171 ( 64%)]  Loss:  3.022948 (3.1521)  Time: 0.588s, 1742.45/s  (2.434s,  420.71/s)  LR: 1.634e-04  Data: 0.021 (1.840)
Train: 49 [ 800/1171 ( 68%)]  Loss:  3.297667 (3.1607)  Time: 0.590s, 1736.51/s  (2.448s,  418.22/s)  LR: 1.634e-04  Data: 0.020 (1.855)
Train: 49 [ 850/1171 ( 73%)]  Loss:  3.430594 (3.1757)  Time: 1.575s,  650.16/s  (2.459s,  416.51/s)  LR: 1.634e-04  Data: 0.976 (1.864)
Train: 49 [ 900/1171 ( 77%)]  Loss:  2.869497 (3.1595)  Time: 1.332s,  768.67/s  (2.462s,  415.96/s)  LR: 1.634e-04  Data: 0.674 (1.865)
Train: 49 [ 950/1171 ( 81%)]  Loss:  3.450350 (3.1741)  Time: 0.587s, 1745.25/s  (2.460s,  416.23/s)  LR: 1.634e-04  Data: 0.020 (1.863)
Train: 49 [1000/1171 ( 85%)]  Loss:  3.466683 (3.1880)  Time: 2.814s,  363.93/s  (2.494s,  410.56/s)  LR: 1.634e-04  Data: 2.239 (1.896)
Train: 49 [1050/1171 ( 90%)]  Loss:  3.605457 (3.2070)  Time: 0.586s, 1747.15/s  (2.561s,  399.80/s)  LR: 1.634e-04  Data: 0.020 (1.963)
Train: 49 [1100/1171 ( 94%)]  Loss:  3.393239 (3.2151)  Time: 0.586s, 1747.32/s  (2.614s,  391.73/s)  LR: 1.634e-04  Data: 0.021 (2.015)
Train: 49 [1150/1171 ( 98%)]  Loss:  3.437610 (3.2244)  Time: 0.586s, 1747.50/s  (2.623s,  390.37/s)  LR: 1.634e-04  Data: 0.021 (2.025)
Train: 49 [1170/1171 (100%)]  Loss:  3.572222 (3.2383)  Time: 0.564s, 1815.63/s  (2.620s,  390.83/s)  LR: 1.634e-04  Data: 0.000 (2.022)
Test: [   0/97]  Time: 15.755 (15.755)  Loss:  0.3632 (0.3632)  Acc@1: 95.1172 (95.1172)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.202 (3.308)  Loss:  0.6098 (0.4568)  Acc@1: 89.6484 (93.2847)  Acc@5: 97.6562 (98.6979)
Test: [  97/97]  Time: 0.120 (3.388)  Loss:  0.3855 (0.4695)  Acc@1: 94.6429 (92.7580)  Acc@5: 98.5119 (98.3590)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-49.pth.tar', 92.75800002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-48.pth.tar', 92.73400001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-47.pth.tar', 92.37900002441407)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-46.pth.tar', 92.28200002929688)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-45.pth.tar', 92.17599998535157)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-44.pth.tar', 92.13600003417969)

Train: 50 [   0/1171 (  0%)]  Loss:  3.697558 (3.6976)  Time: 12.060s,   84.91/s  (12.060s,   84.91/s)  LR: 1.468e-04  Data: 11.374 (11.374)
Train: 50 [  50/1171 (  4%)]  Loss:  3.114287 (3.4059)  Time: 0.588s, 1741.05/s  (2.826s,  362.31/s)  LR: 1.468e-04  Data: 0.018 (2.226)
Train: 50 [ 100/1171 (  9%)]  Loss:  3.400629 (3.4042)  Time: 0.589s, 1739.46/s  (2.760s,  371.03/s)  LR: 1.468e-04  Data: 0.023 (2.165)
Train: 50 [ 150/1171 ( 13%)]  Loss:  3.578606 (3.4478)  Time: 0.587s, 1744.05/s  (2.674s,  382.92/s)  LR: 1.468e-04  Data: 0.020 (2.076)
Train: 50 [ 200/1171 ( 17%)]  Loss:  3.196163 (3.3974)  Time: 1.955s,  523.78/s  (2.663s,  384.56/s)  LR: 1.468e-04  Data: 1.368 (2.056)
Train: 50 [ 250/1171 ( 21%)]  Loss:  3.264245 (3.3752)  Time: 2.567s,  398.96/s  (2.622s,  390.52/s)  LR: 1.468e-04  Data: 1.992 (2.010)
Train: 50 [ 300/1171 ( 26%)]  Loss:  2.572111 (3.2605)  Time: 0.586s, 1746.52/s  (2.599s,  394.06/s)  LR: 1.468e-04  Data: 0.021 (1.985)
Train: 50 [ 350/1171 ( 30%)]  Loss:  2.618534 (3.1803)  Time: 0.623s, 1644.30/s  (2.624s,  390.29/s)  LR: 1.468e-04  Data: 0.019 (2.010)
Train: 50 [ 400/1171 ( 34%)]  Loss:  3.113010 (3.1728)  Time: 0.588s, 1741.87/s  (2.643s,  387.40/s)  LR: 1.468e-04  Data: 0.017 (2.028)
Train: 50 [ 450/1171 ( 38%)]  Loss:  3.351296 (3.1906)  Time: 2.220s,  461.35/s  (2.641s,  387.68/s)  LR: 1.468e-04  Data: 1.643 (2.022)
Train: 50 [ 500/1171 ( 43%)]  Loss:  3.143501 (3.1864)  Time: 0.587s, 1744.50/s  (2.650s,  386.37/s)  LR: 1.468e-04  Data: 0.022 (2.029)
Train: 50 [ 550/1171 ( 47%)]  Loss:  3.154149 (3.1837)  Time: 0.587s, 1745.33/s  (2.646s,  387.06/s)  LR: 1.468e-04  Data: 0.019 (2.027)
Train: 50 [ 600/1171 ( 51%)]  Loss:  3.427692 (3.2024)  Time: 0.589s, 1739.98/s  (2.655s,  385.69/s)  LR: 1.468e-04  Data: 0.023 (2.039)
Train: 50 [ 650/1171 ( 56%)]  Loss:  3.085727 (3.1941)  Time: 0.587s, 1743.52/s  (2.635s,  388.57/s)  LR: 1.468e-04  Data: 0.020 (2.021)
Train: 50 [ 700/1171 ( 60%)]  Loss:  3.005492 (3.1815)  Time: 0.588s, 1742.58/s  (2.652s,  386.20/s)  LR: 1.468e-04  Data: 0.023 (2.039)
Train: 50 [ 750/1171 ( 64%)]  Loss:  3.165027 (3.1805)  Time: 0.587s, 1745.63/s  (2.642s,  387.53/s)  LR: 1.468e-04  Data: 0.021 (2.032)
Train: 50 [ 800/1171 ( 68%)]  Loss:  3.176007 (3.1802)  Time: 0.590s, 1735.55/s  (2.632s,  388.99/s)  LR: 1.468e-04  Data: 0.020 (2.023)
Train: 50 [ 850/1171 ( 73%)]  Loss:  3.097046 (3.1756)  Time: 0.589s, 1739.83/s  (2.608s,  392.63/s)  LR: 1.468e-04  Data: 0.024 (2.000)
Train: 50 [ 900/1171 ( 77%)]  Loss:  3.296823 (3.1820)  Time: 0.597s, 1713.99/s  (2.588s,  395.74/s)  LR: 1.468e-04  Data: 0.022 (1.980)
Train: 50 [ 950/1171 ( 81%)]  Loss:  3.456006 (3.1957)  Time: 2.387s,  429.07/s  (2.561s,  399.77/s)  LR: 1.468e-04  Data: 1.789 (1.954)
Train: 50 [1000/1171 ( 85%)]  Loss:  2.721802 (3.1731)  Time: 1.177s,  869.82/s  (2.541s,  403.04/s)  LR: 1.468e-04  Data: 0.566 (1.934)
Train: 50 [1050/1171 ( 90%)]  Loss:  3.258894 (3.1770)  Time: 6.317s,  162.11/s  (2.533s,  404.32/s)  LR: 1.468e-04  Data: 5.682 (1.926)
Train: 50 [1100/1171 ( 94%)]  Loss:  3.056487 (3.1718)  Time: 2.405s,  425.79/s  (2.522s,  405.97/s)  LR: 1.468e-04  Data: 1.820 (1.917)
Train: 50 [1150/1171 ( 98%)]  Loss:  3.002525 (3.1647)  Time: 6.140s,  166.78/s  (2.518s,  406.74/s)  LR: 1.468e-04  Data: 5.507 (1.912)
Train: 50 [1170/1171 (100%)]  Loss:  3.487608 (3.1776)  Time: 0.568s, 1803.57/s  (2.512s,  407.59/s)  LR: 1.468e-04  Data: 0.000 (1.907)
Test: [   0/97]  Time: 14.314 (14.314)  Loss:  0.3380 (0.3380)  Acc@1: 95.6055 (95.6055)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.197 (3.046)  Loss:  0.5702 (0.4524)  Acc@1: 90.5273 (93.4149)  Acc@5: 98.2422 (98.6922)
Test: [  97/97]  Time: 0.120 (2.916)  Loss:  0.3772 (0.4657)  Acc@1: 94.9405 (92.9280)  Acc@5: 98.6607 (98.3970)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-50.pth.tar', 92.92799999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-49.pth.tar', 92.75800002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-48.pth.tar', 92.73400001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-47.pth.tar', 92.37900002441407)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-46.pth.tar', 92.28200002929688)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-45.pth.tar', 92.17599998535157)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-44.pth.tar', 92.13600003417969)

Train: 51 [   0/1171 (  0%)]  Loss:  3.547326 (3.5473)  Time: 11.180s,   91.59/s  (11.180s,   91.59/s)  LR: 1.309e-04  Data: 9.978 (9.978)
Train: 51 [  50/1171 (  4%)]  Loss:  3.030213 (3.2888)  Time: 0.587s, 1744.02/s  (2.256s,  453.85/s)  LR: 1.309e-04  Data: 0.022 (1.655)
Train: 51 [ 100/1171 (  9%)]  Loss:  2.946874 (3.1748)  Time: 0.586s, 1746.79/s  (2.193s,  466.92/s)  LR: 1.309e-04  Data: 0.020 (1.594)
Train: 51 [ 150/1171 ( 13%)]  Loss:  2.669010 (3.0484)  Time: 4.163s,  245.95/s  (2.244s,  456.28/s)  LR: 1.309e-04  Data: 3.588 (1.648)
Train: 51 [ 200/1171 ( 17%)]  Loss:  3.608134 (3.1603)  Time: 0.586s, 1747.45/s  (2.244s,  456.36/s)  LR: 1.309e-04  Data: 0.021 (1.646)
Train: 51 [ 250/1171 ( 21%)]  Loss:  3.109698 (3.1519)  Time: 3.762s,  272.23/s  (2.264s,  452.28/s)  LR: 1.309e-04  Data: 3.183 (1.661)
Train: 51 [ 300/1171 ( 26%)]  Loss:  3.596150 (3.2153)  Time: 0.586s, 1747.23/s  (2.267s,  451.76/s)  LR: 1.309e-04  Data: 0.021 (1.658)
Train: 51 [ 350/1171 ( 30%)]  Loss:  3.724446 (3.2790)  Time: 0.585s, 1750.77/s  (2.258s,  453.56/s)  LR: 1.309e-04  Data: 0.019 (1.650)
Train: 51 [ 400/1171 ( 34%)]  Loss:  2.868149 (3.2333)  Time: 0.586s, 1747.43/s  (2.248s,  455.59/s)  LR: 1.309e-04  Data: 0.020 (1.639)
Train: 51 [ 450/1171 ( 38%)]  Loss:  3.406392 (3.2506)  Time: 0.590s, 1736.02/s  (2.233s,  458.48/s)  LR: 1.309e-04  Data: 0.020 (1.627)
Train: 51 [ 500/1171 ( 43%)]  Loss:  3.375167 (3.2620)  Time: 3.297s,  310.55/s  (2.220s,  461.16/s)  LR: 1.309e-04  Data: 2.731 (1.614)
Train: 51 [ 550/1171 ( 47%)]  Loss:  3.112553 (3.2495)  Time: 0.586s, 1748.13/s  (2.233s,  458.51/s)  LR: 1.309e-04  Data: 0.019 (1.626)
Train: 51 [ 600/1171 ( 51%)]  Loss:  3.343114 (3.2567)  Time: 0.587s, 1745.78/s  (2.252s,  454.78/s)  LR: 1.309e-04  Data: 0.020 (1.641)
Train: 51 [ 650/1171 ( 56%)]  Loss:  3.396953 (3.2667)  Time: 2.454s,  417.21/s  (2.265s,  452.19/s)  LR: 1.309e-04  Data: 1.888 (1.656)
Train: 51 [ 700/1171 ( 60%)]  Loss:  3.055539 (3.2526)  Time: 0.587s, 1744.06/s  (2.270s,  451.03/s)  LR: 1.309e-04  Data: 0.021 (1.661)
Train: 51 [ 750/1171 ( 64%)]  Loss:  3.214611 (3.2503)  Time: 0.587s, 1744.77/s  (2.266s,  451.96/s)  LR: 1.309e-04  Data: 0.022 (1.656)
Train: 51 [ 800/1171 ( 68%)]  Loss:  3.200432 (3.2473)  Time: 0.587s, 1743.65/s  (2.265s,  452.00/s)  LR: 1.309e-04  Data: 0.019 (1.658)
Train: 51 [ 850/1171 ( 73%)]  Loss:  3.292360 (3.2498)  Time: 0.588s, 1742.66/s  (2.256s,  453.89/s)  LR: 1.309e-04  Data: 0.021 (1.650)
Train: 51 [ 900/1171 ( 77%)]  Loss:  2.934023 (3.2332)  Time: 0.588s, 1741.46/s  (2.251s,  454.96/s)  LR: 1.309e-04  Data: 0.022 (1.645)
Train: 51 [ 950/1171 ( 81%)]  Loss:  3.327264 (3.2379)  Time: 1.492s,  686.36/s  (2.257s,  453.79/s)  LR: 1.309e-04  Data: 0.927 (1.652)
Train: 51 [1000/1171 ( 85%)]  Loss:  3.347285 (3.2431)  Time: 0.587s, 1744.84/s  (2.260s,  453.07/s)  LR: 1.309e-04  Data: 0.020 (1.656)
Train: 51 [1050/1171 ( 90%)]  Loss:  3.171079 (3.2399)  Time: 0.593s, 1727.41/s  (2.262s,  452.60/s)  LR: 1.309e-04  Data: 0.022 (1.658)
Train: 51 [1100/1171 ( 94%)]  Loss:  3.413504 (3.2474)  Time: 0.589s, 1738.73/s  (2.269s,  451.26/s)  LR: 1.309e-04  Data: 0.020 (1.666)
Train: 51 [1150/1171 ( 98%)]  Loss:  2.834205 (3.2302)  Time: 0.587s, 1744.41/s  (2.269s,  451.35/s)  LR: 1.309e-04  Data: 0.023 (1.666)
Train: 51 [1170/1171 (100%)]  Loss:  3.394430 (3.2368)  Time: 0.566s, 1810.16/s  (2.269s,  451.28/s)  LR: 1.309e-04  Data: 0.000 (1.667)
Test: [   0/97]  Time: 13.115 (13.115)  Loss:  0.3418 (0.3418)  Acc@1: 96.1914 (96.1914)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (2.984)  Loss:  0.5554 (0.4356)  Acc@1: 90.4297 (93.7155)  Acc@5: 97.9492 (98.7209)
Test: [  97/97]  Time: 0.120 (2.906)  Loss:  0.4098 (0.4521)  Acc@1: 93.3036 (93.1010)  Acc@5: 98.8095 (98.4480)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-51.pth.tar', 93.10100001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-50.pth.tar', 92.92799999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-49.pth.tar', 92.75800002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-48.pth.tar', 92.73400001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-47.pth.tar', 92.37900002441407)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-46.pth.tar', 92.28200002929688)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-45.pth.tar', 92.17599998535157)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-44.pth.tar', 92.13600003417969)

Train: 52 [   0/1171 (  0%)]  Loss:  2.828522 (2.8285)  Time: 10.341s,   99.03/s  (10.341s,   99.03/s)  LR: 1.159e-04  Data: 9.441 (9.441)
Train: 52 [  50/1171 (  4%)]  Loss:  2.997060 (2.9128)  Time: 0.590s, 1736.86/s  (2.527s,  405.23/s)  LR: 1.159e-04  Data: 0.019 (1.932)
Train: 52 [ 100/1171 (  9%)]  Loss:  2.829903 (2.8852)  Time: 0.587s, 1743.04/s  (2.511s,  407.87/s)  LR: 1.159e-04  Data: 0.022 (1.914)
Train: 52 [ 150/1171 ( 13%)]  Loss:  3.071226 (2.9317)  Time: 0.591s, 1733.70/s  (2.423s,  422.68/s)  LR: 1.159e-04  Data: 0.024 (1.831)
Train: 52 [ 200/1171 ( 17%)]  Loss:  3.130562 (2.9715)  Time: 3.830s,  267.33/s  (2.410s,  424.92/s)  LR: 1.159e-04  Data: 3.160 (1.813)
Train: 52 [ 250/1171 ( 21%)]  Loss:  3.286927 (3.0240)  Time: 0.591s, 1731.38/s  (2.357s,  434.49/s)  LR: 1.159e-04  Data: 0.026 (1.755)
Train: 52 [ 300/1171 ( 26%)]  Loss:  2.890493 (3.0050)  Time: 4.503s,  227.43/s  (2.340s,  437.68/s)  LR: 1.159e-04  Data: 3.938 (1.740)
Train: 52 [ 350/1171 ( 30%)]  Loss:  2.994061 (3.0036)  Time: 0.591s, 1732.91/s  (2.310s,  443.24/s)  LR: 1.159e-04  Data: 0.021 (1.713)
Train: 52 [ 400/1171 ( 34%)]  Loss:  2.568240 (2.9552)  Time: 3.309s,  309.44/s  (2.292s,  446.85/s)  LR: 1.159e-04  Data: 2.739 (1.694)
Train: 52 [ 450/1171 ( 38%)]  Loss:  2.938402 (2.9535)  Time: 0.585s, 1751.10/s  (2.310s,  443.28/s)  LR: 1.159e-04  Data: 0.020 (1.710)
Train: 52 [ 500/1171 ( 43%)]  Loss:  3.250704 (2.9806)  Time: 3.484s,  293.95/s  (2.328s,  439.85/s)  LR: 1.159e-04  Data: 2.820 (1.727)
Train: 52 [ 550/1171 ( 47%)]  Loss:  3.315633 (3.0085)  Time: 0.586s, 1745.98/s  (2.339s,  437.87/s)  LR: 1.159e-04  Data: 0.021 (1.738)
Train: 52 [ 600/1171 ( 51%)]  Loss:  3.144614 (3.0189)  Time: 2.934s,  348.98/s  (2.351s,  435.60/s)  LR: 1.159e-04  Data: 2.287 (1.750)
Train: 52 [ 650/1171 ( 56%)]  Loss:  3.422347 (3.0478)  Time: 0.585s, 1749.19/s  (2.343s,  437.12/s)  LR: 1.159e-04  Data: 0.021 (1.742)
Train: 52 [ 700/1171 ( 60%)]  Loss:  3.209648 (3.0586)  Time: 0.587s, 1744.24/s  (2.341s,  437.42/s)  LR: 1.159e-04  Data: 0.022 (1.739)
Train: 52 [ 750/1171 ( 64%)]  Loss:  3.112861 (3.0620)  Time: 0.585s, 1750.44/s  (2.327s,  440.13/s)  LR: 1.159e-04  Data: 0.019 (1.725)
Train: 52 [ 800/1171 ( 68%)]  Loss:  3.166353 (3.0681)  Time: 0.592s, 1729.92/s  (2.329s,  439.63/s)  LR: 1.159e-04  Data: 0.024 (1.726)
Train: 52 [ 850/1171 ( 73%)]  Loss:  3.559757 (3.0954)  Time: 0.587s, 1744.81/s  (2.329s,  439.74/s)  LR: 1.159e-04  Data: 0.020 (1.726)
Train: 52 [ 900/1171 ( 77%)]  Loss:  3.130921 (3.0973)  Time: 0.589s, 1739.72/s  (2.332s,  439.11/s)  LR: 1.159e-04  Data: 0.023 (1.729)
Train: 52 [ 950/1171 ( 81%)]  Loss:  3.305991 (3.1077)  Time: 0.587s, 1744.33/s  (2.332s,  439.12/s)  LR: 1.159e-04  Data: 0.022 (1.729)
Train: 52 [1000/1171 ( 85%)]  Loss:  3.573069 (3.1299)  Time: 0.592s, 1728.57/s  (2.330s,  439.46/s)  LR: 1.159e-04  Data: 0.023 (1.728)
Train: 52 [1050/1171 ( 90%)]  Loss:  2.696982 (3.1102)  Time: 0.584s, 1754.69/s  (2.330s,  439.43/s)  LR: 1.159e-04  Data: 0.017 (1.728)
Train: 52 [1100/1171 ( 94%)]  Loss:  3.168159 (3.1127)  Time: 0.586s, 1747.58/s  (2.321s,  441.23/s)  LR: 1.159e-04  Data: 0.021 (1.720)
Train: 52 [1150/1171 ( 98%)]  Loss:  3.204830 (3.1166)  Time: 0.586s, 1747.06/s  (2.316s,  442.14/s)  LR: 1.159e-04  Data: 0.020 (1.715)
Train: 52 [1170/1171 (100%)]  Loss:  2.938323 (3.1094)  Time: 0.565s, 1811.12/s  (2.313s,  442.71/s)  LR: 1.159e-04  Data: 0.000 (1.712)
Test: [   0/97]  Time: 12.961 (12.961)  Loss:  0.3445 (0.3445)  Acc@1: 95.8008 (95.8008)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (3.236)  Loss:  0.5276 (0.4112)  Acc@1: 91.0156 (93.8055)  Acc@5: 98.2422 (98.7630)
Test: [  97/97]  Time: 0.121 (3.117)  Loss:  0.3511 (0.4274)  Acc@1: 93.8988 (93.2660)  Acc@5: 98.9583 (98.4680)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-52.pth.tar', 93.26600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-51.pth.tar', 93.10100001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-50.pth.tar', 92.92799999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-49.pth.tar', 92.75800002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-48.pth.tar', 92.73400001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-47.pth.tar', 92.37900002441407)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-46.pth.tar', 92.28200002929688)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-45.pth.tar', 92.17599998535157)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-44.pth.tar', 92.13600003417969)

Train: 53 [   0/1171 (  0%)]  Loss:  3.004812 (3.0048)  Time: 11.324s,   90.43/s  (11.324s,   90.43/s)  LR: 1.018e-04  Data: 10.738 (10.738)
Train: 53 [  50/1171 (  4%)]  Loss:  2.869043 (2.9369)  Time: 0.588s, 1741.99/s  (2.380s,  430.19/s)  LR: 1.018e-04  Data: 0.021 (1.781)
Train: 53 [ 100/1171 (  9%)]  Loss:  3.316092 (3.0633)  Time: 0.588s, 1740.37/s  (2.309s,  443.40/s)  LR: 1.018e-04  Data: 0.017 (1.711)
Train: 53 [ 150/1171 ( 13%)]  Loss:  3.127809 (3.0794)  Time: 0.588s, 1742.72/s  (2.241s,  457.02/s)  LR: 1.018e-04  Data: 0.023 (1.644)
Train: 53 [ 200/1171 ( 17%)]  Loss:  2.924831 (3.0485)  Time: 0.587s, 1744.12/s  (2.233s,  458.67/s)  LR: 1.018e-04  Data: 0.018 (1.640)
Train: 53 [ 250/1171 ( 21%)]  Loss:  2.892198 (3.0225)  Time: 0.588s, 1741.54/s  (2.195s,  466.61/s)  LR: 1.018e-04  Data: 0.022 (1.604)
Train: 53 [ 300/1171 ( 26%)]  Loss:  3.033469 (3.0240)  Time: 0.586s, 1748.17/s  (2.223s,  460.56/s)  LR: 1.018e-04  Data: 0.021 (1.634)
Train: 53 [ 350/1171 ( 30%)]  Loss:  2.953080 (3.0152)  Time: 0.587s, 1743.95/s  (2.229s,  459.41/s)  LR: 1.018e-04  Data: 0.023 (1.640)
Train: 53 [ 400/1171 ( 34%)]  Loss:  2.973366 (3.0105)  Time: 0.586s, 1747.74/s  (2.251s,  454.81/s)  LR: 1.018e-04  Data: 0.021 (1.661)
Train: 53 [ 450/1171 ( 38%)]  Loss:  3.171076 (3.0266)  Time: 0.584s, 1752.94/s  (2.245s,  456.06/s)  LR: 1.018e-04  Data: 0.019 (1.655)
Train: 53 [ 500/1171 ( 43%)]  Loss:  2.748777 (3.0013)  Time: 0.589s, 1737.84/s  (2.253s,  454.52/s)  LR: 1.018e-04  Data: 0.023 (1.663)
Train: 53 [ 550/1171 ( 47%)]  Loss:  3.256752 (3.0226)  Time: 0.589s, 1739.12/s  (2.251s,  454.93/s)  LR: 1.018e-04  Data: 0.020 (1.661)
Train: 53 [ 600/1171 ( 51%)]  Loss:  3.485585 (3.0582)  Time: 0.592s, 1729.36/s  (2.253s,  454.48/s)  LR: 1.018e-04  Data: 0.024 (1.662)
Train: 53 [ 650/1171 ( 56%)]  Loss:  3.295393 (3.0752)  Time: 0.590s, 1734.87/s  (2.249s,  455.28/s)  LR: 1.018e-04  Data: 0.022 (1.659)
Train: 53 [ 700/1171 ( 60%)]  Loss:  3.265747 (3.0879)  Time: 0.588s, 1742.03/s  (2.268s,  451.45/s)  LR: 1.018e-04  Data: 0.022 (1.677)
Train: 53 [ 750/1171 ( 64%)]  Loss:  3.670665 (3.1243)  Time: 0.588s, 1741.95/s  (2.268s,  451.41/s)  LR: 1.018e-04  Data: 0.019 (1.678)
Train: 53 [ 800/1171 ( 68%)]  Loss:  3.016883 (3.1180)  Time: 0.587s, 1744.96/s  (2.281s,  448.87/s)  LR: 1.018e-04  Data: 0.022 (1.691)
Train: 53 [ 850/1171 ( 73%)]  Loss:  3.075893 (3.1156)  Time: 0.585s, 1749.09/s  (2.283s,  448.54/s)  LR: 1.018e-04  Data: 0.017 (1.692)
Train: 53 [ 900/1171 ( 77%)]  Loss:  3.215826 (3.1209)  Time: 0.589s, 1737.62/s  (2.286s,  447.99/s)  LR: 1.018e-04  Data: 0.024 (1.694)
Train: 53 [ 950/1171 ( 81%)]  Loss:  3.250565 (3.1274)  Time: 0.585s, 1749.05/s  (2.283s,  448.50/s)  LR: 1.018e-04  Data: 0.020 (1.692)
Train: 53 [1000/1171 ( 85%)]  Loss:  3.344418 (3.1377)  Time: 0.588s, 1741.75/s  (2.278s,  449.54/s)  LR: 1.018e-04  Data: 0.023 (1.686)
Train: 53 [1050/1171 ( 90%)]  Loss:  2.980317 (3.1306)  Time: 0.588s, 1742.89/s  (2.274s,  450.22/s)  LR: 1.018e-04  Data: 0.019 (1.683)
Train: 53 [1100/1171 ( 94%)]  Loss:  3.558604 (3.1492)  Time: 0.590s, 1735.36/s  (2.282s,  448.77/s)  LR: 1.018e-04  Data: 0.025 (1.690)
Train: 53 [1150/1171 ( 98%)]  Loss:  3.222957 (3.1523)  Time: 0.588s, 1741.00/s  (2.293s,  446.49/s)  LR: 1.018e-04  Data: 0.019 (1.701)
Train: 53 [1170/1171 (100%)]  Loss:  2.845931 (3.1400)  Time: 0.568s, 1804.27/s  (2.295s,  446.17/s)  LR: 1.018e-04  Data: 0.000 (1.703)
Test: [   0/97]  Time: 13.988 (13.988)  Loss:  0.3549 (0.3549)  Acc@1: 95.9961 (95.9961)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (3.163)  Loss:  0.5365 (0.4354)  Acc@1: 91.0156 (93.8553)  Acc@5: 98.1445 (98.7400)
Test: [  97/97]  Time: 0.120 (3.041)  Loss:  0.3942 (0.4470)  Acc@1: 94.3452 (93.3100)  Acc@5: 98.6607 (98.4650)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-53.pth.tar', 93.31000004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-52.pth.tar', 93.26600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-51.pth.tar', 93.10100001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-50.pth.tar', 92.92799999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-49.pth.tar', 92.75800002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-48.pth.tar', 92.73400001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-47.pth.tar', 92.37900002441407)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-46.pth.tar', 92.28200002929688)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-45.pth.tar', 92.17599998535157)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-44.pth.tar', 92.13600003417969)

Train: 54 [   0/1171 (  0%)]  Loss:  2.819059 (2.8191)  Time: 10.703s,   95.68/s  (10.703s,   95.68/s)  LR: 8.858e-05  Data: 10.134 (10.134)
Train: 54 [  50/1171 (  4%)]  Loss:  2.600064 (2.7096)  Time: 0.586s, 1747.34/s  (2.304s,  444.54/s)  LR: 8.858e-05  Data: 0.021 (1.716)
Train: 54 [ 100/1171 (  9%)]  Loss:  2.606214 (2.6751)  Time: 0.595s, 1721.67/s  (2.306s,  444.04/s)  LR: 8.858e-05  Data: 0.019 (1.711)
Train: 54 [ 150/1171 ( 13%)]  Loss:  2.973309 (2.7497)  Time: 0.584s, 1752.52/s  (2.235s,  458.11/s)  LR: 8.858e-05  Data: 0.020 (1.644)
Train: 54 [ 200/1171 ( 17%)]  Loss:  3.142007 (2.8281)  Time: 1.078s,  949.90/s  (2.329s,  439.65/s)  LR: 8.858e-05  Data: 0.416 (1.735)
Train: 54 [ 250/1171 ( 21%)]  Loss:  3.343833 (2.9141)  Time: 0.587s, 1745.83/s  (2.333s,  438.95/s)  LR: 8.858e-05  Data: 0.021 (1.738)
Train: 54 [ 300/1171 ( 26%)]  Loss:  2.829757 (2.9020)  Time: 3.248s,  315.28/s  (2.343s,  437.11/s)  LR: 8.858e-05  Data: 2.681 (1.747)
Train: 54 [ 350/1171 ( 30%)]  Loss:  3.254629 (2.9461)  Time: 0.586s, 1746.75/s  (2.320s,  441.43/s)  LR: 8.858e-05  Data: 0.022 (1.723)
Train: 54 [ 400/1171 ( 34%)]  Loss:  3.240527 (2.9788)  Time: 2.824s,  362.58/s  (2.317s,  441.92/s)  LR: 8.858e-05  Data: 2.235 (1.720)
Train: 54 [ 450/1171 ( 38%)]  Loss:  2.792894 (2.9602)  Time: 0.586s, 1747.41/s  (2.299s,  445.32/s)  LR: 8.858e-05  Data: 0.020 (1.703)
Train: 54 [ 500/1171 ( 43%)]  Loss:  2.912138 (2.9559)  Time: 0.589s, 1737.07/s  (2.282s,  448.73/s)  LR: 8.858e-05  Data: 0.021 (1.685)
Train: 54 [ 550/1171 ( 47%)]  Loss:  2.639226 (2.9295)  Time: 0.588s, 1742.43/s  (2.273s,  450.55/s)  LR: 8.858e-05  Data: 0.019 (1.672)
Train: 54 [ 600/1171 ( 51%)]  Loss:  3.023519 (2.9367)  Time: 1.082s,  946.25/s  (2.300s,  445.29/s)  LR: 8.858e-05  Data: 0.518 (1.697)
Train: 54 [ 650/1171 ( 56%)]  Loss:  2.953667 (2.9379)  Time: 0.590s, 1734.22/s  (2.307s,  443.89/s)  LR: 8.858e-05  Data: 0.021 (1.705)
Train: 54 [ 700/1171 ( 60%)]  Loss:  3.067548 (2.9466)  Time: 0.588s, 1742.91/s  (2.311s,  443.09/s)  LR: 8.858e-05  Data: 0.021 (1.709)
Train: 54 [ 750/1171 ( 64%)]  Loss:  3.009844 (2.9505)  Time: 0.587s, 1743.09/s  (2.308s,  443.63/s)  LR: 8.858e-05  Data: 0.021 (1.706)
Train: 54 [ 800/1171 ( 68%)]  Loss:  3.172166 (2.9636)  Time: 0.713s, 1436.85/s  (2.300s,  445.29/s)  LR: 8.858e-05  Data: 0.119 (1.697)
Train: 54 [ 850/1171 ( 73%)]  Loss:  3.267782 (2.9805)  Time: 0.588s, 1740.11/s  (2.297s,  445.80/s)  LR: 8.858e-05  Data: 0.021 (1.695)
Train: 54 [ 900/1171 ( 77%)]  Loss:  3.153762 (2.9896)  Time: 1.680s,  609.61/s  (2.287s,  447.76/s)  LR: 8.858e-05  Data: 1.109 (1.686)
Train: 54 [ 950/1171 ( 81%)]  Loss:  2.529467 (2.9666)  Time: 0.587s, 1744.36/s  (2.291s,  447.00/s)  LR: 8.858e-05  Data: 0.021 (1.689)
Train: 54 [1000/1171 ( 85%)]  Loss:  3.287477 (2.9819)  Time: 5.700s,  179.64/s  (2.293s,  446.50/s)  LR: 8.858e-05  Data: 5.124 (1.693)
Train: 54 [1050/1171 ( 90%)]  Loss:  3.027018 (2.9839)  Time: 1.673s,  611.90/s  (2.297s,  445.79/s)  LR: 8.858e-05  Data: 0.964 (1.696)
Train: 54 [1100/1171 ( 94%)]  Loss:  3.616998 (3.0114)  Time: 6.214s,  164.78/s  (2.299s,  445.34/s)  LR: 8.858e-05  Data: 5.527 (1.698)
Train: 54 [1150/1171 ( 98%)]  Loss:  2.699096 (2.9984)  Time: 0.589s, 1737.64/s  (2.295s,  446.15/s)  LR: 8.858e-05  Data: 0.021 (1.693)
Train: 54 [1170/1171 (100%)]  Loss:  2.877089 (2.9936)  Time: 0.567s, 1804.64/s  (2.294s,  446.46/s)  LR: 8.858e-05  Data: 0.000 (1.692)
Test: [   0/97]  Time: 13.325 (13.325)  Loss:  0.3399 (0.3399)  Acc@1: 96.3867 (96.3867)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.196 (2.933)  Loss:  0.5378 (0.4310)  Acc@1: 90.4297 (93.8974)  Acc@5: 98.1445 (98.7477)
Test: [  97/97]  Time: 0.120 (2.922)  Loss:  0.3732 (0.4434)  Acc@1: 94.6429 (93.4150)  Acc@5: 98.6607 (98.4730)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-54.pth.tar', 93.41500002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-53.pth.tar', 93.31000004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-52.pth.tar', 93.26600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-51.pth.tar', 93.10100001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-50.pth.tar', 92.92799999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-49.pth.tar', 92.75800002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-48.pth.tar', 92.73400001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-47.pth.tar', 92.37900002441407)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-46.pth.tar', 92.28200002929688)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-45.pth.tar', 92.17599998535157)

Train: 55 [   0/1171 (  0%)]  Loss:  2.933073 (2.9331)  Time: 10.449s,   98.00/s  (10.449s,   98.00/s)  LR: 7.632e-05  Data: 9.340 (9.340)
Train: 55 [  50/1171 (  4%)]  Loss:  2.207500 (2.5703)  Time: 0.589s, 1738.33/s  (2.472s,  414.20/s)  LR: 7.632e-05  Data: 0.023 (1.872)
Train: 55 [ 100/1171 (  9%)]  Loss:  3.019629 (2.7201)  Time: 1.326s,  772.02/s  (2.406s,  425.57/s)  LR: 7.632e-05  Data: 0.751 (1.814)
Train: 55 [ 150/1171 ( 13%)]  Loss:  2.944266 (2.7761)  Time: 0.591s, 1734.03/s  (2.366s,  432.71/s)  LR: 7.632e-05  Data: 0.023 (1.772)
Train: 55 [ 200/1171 ( 17%)]  Loss:  3.031040 (2.8271)  Time: 1.792s,  571.44/s  (2.370s,  432.07/s)  LR: 7.632e-05  Data: 1.228 (1.774)
Train: 55 [ 250/1171 ( 21%)]  Loss:  2.543240 (2.7798)  Time: 0.588s, 1742.81/s  (2.336s,  438.30/s)  LR: 7.632e-05  Data: 0.020 (1.739)
Train: 55 [ 300/1171 ( 26%)]  Loss:  3.156391 (2.8336)  Time: 0.587s, 1743.72/s  (2.333s,  438.94/s)  LR: 7.632e-05  Data: 0.021 (1.734)
Train: 55 [ 350/1171 ( 30%)]  Loss:  3.162103 (2.8747)  Time: 0.586s, 1747.54/s  (2.302s,  444.79/s)  LR: 7.632e-05  Data: 0.020 (1.704)
Train: 55 [ 400/1171 ( 34%)]  Loss:  2.411357 (2.8232)  Time: 1.243s,  823.74/s  (2.294s,  446.47/s)  LR: 7.632e-05  Data: 0.532 (1.693)
Train: 55 [ 450/1171 ( 38%)]  Loss:  3.471922 (2.8881)  Time: 2.112s,  484.74/s  (2.332s,  439.19/s)  LR: 7.632e-05  Data: 1.540 (1.730)
Train: 55 [ 500/1171 ( 43%)]  Loss:  3.736964 (2.9652)  Time: 0.587s, 1743.38/s  (2.362s,  433.53/s)  LR: 7.632e-05  Data: 0.021 (1.762)
Train: 55 [ 550/1171 ( 47%)]  Loss:  2.638473 (2.9380)  Time: 7.990s,  128.17/s  (2.400s,  426.71/s)  LR: 7.632e-05  Data: 7.383 (1.798)
Train: 55 [ 600/1171 ( 51%)]  Loss:  3.539960 (2.9843)  Time: 0.589s, 1738.26/s  (2.413s,  424.35/s)  LR: 7.632e-05  Data: 0.022 (1.813)
Train: 55 [ 650/1171 ( 56%)]  Loss:  2.909460 (2.9790)  Time: 8.539s,  119.92/s  (2.427s,  421.92/s)  LR: 7.632e-05  Data: 7.873 (1.827)
Train: 55 [ 700/1171 ( 60%)]  Loss:  3.682441 (3.0259)  Time: 0.588s, 1740.18/s  (2.421s,  422.97/s)  LR: 7.632e-05  Data: 0.020 (1.822)
Train: 55 [ 750/1171 ( 64%)]  Loss:  3.119427 (3.0317)  Time: 6.745s,  151.82/s  (2.425s,  422.26/s)  LR: 7.632e-05  Data: 6.074 (1.825)
Train: 55 [ 800/1171 ( 68%)]  Loss:  2.582617 (3.0053)  Time: 1.193s,  858.23/s  (2.445s,  418.87/s)  LR: 7.632e-05  Data: 0.602 (1.845)
Train: 55 [ 850/1171 ( 73%)]  Loss:  2.978157 (3.0038)  Time: 5.930s,  172.69/s  (2.454s,  417.27/s)  LR: 7.632e-05  Data: 5.318 (1.851)
Train: 55 [ 900/1171 ( 77%)]  Loss:  2.964757 (3.0017)  Time: 0.587s, 1744.78/s  (2.457s,  416.78/s)  LR: 7.632e-05  Data: 0.019 (1.854)
Train: 55 [ 950/1171 ( 81%)]  Loss:  3.240083 (3.0136)  Time: 2.341s,  437.42/s  (2.456s,  416.87/s)  LR: 7.632e-05  Data: 1.396 (1.853)
Train: 55 [1000/1171 ( 85%)]  Loss:  3.323129 (3.0284)  Time: 0.593s, 1726.30/s  (2.456s,  416.88/s)  LR: 7.632e-05  Data: 0.021 (1.853)
Train: 55 [1050/1171 ( 90%)]  Loss:  2.910446 (3.0230)  Time: 2.441s,  419.55/s  (2.450s,  417.94/s)  LR: 7.632e-05  Data: 1.876 (1.848)
Train: 55 [1100/1171 ( 94%)]  Loss:  3.541737 (3.0456)  Time: 0.586s, 1748.01/s  (2.446s,  418.70/s)  LR: 7.632e-05  Data: 0.019 (1.843)
Train: 55 [1150/1171 ( 98%)]  Loss:  3.447430 (3.0623)  Time: 4.447s,  230.27/s  (2.455s,  417.15/s)  LR: 7.632e-05  Data: 3.852 (1.853)
Train: 55 [1170/1171 (100%)]  Loss:  3.054326 (3.0620)  Time: 0.566s, 1808.92/s  (2.454s,  417.35/s)  LR: 7.632e-05  Data: 0.000 (1.850)
Test: [   0/97]  Time: 14.841 (14.841)  Loss:  0.3405 (0.3405)  Acc@1: 95.6055 (95.6055)  Acc@5: 99.6094 (99.6094)
Test: [  50/97]  Time: 0.198 (3.299)  Loss:  0.5659 (0.4275)  Acc@1: 89.4531 (93.9779)  Acc@5: 98.1445 (98.7803)
Test: [  97/97]  Time: 0.120 (3.200)  Loss:  0.3902 (0.4385)  Acc@1: 93.3036 (93.4250)  Acc@5: 98.6607 (98.5100)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-55.pth.tar', 93.42500001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-54.pth.tar', 93.41500002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-53.pth.tar', 93.31000004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-52.pth.tar', 93.26600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-51.pth.tar', 93.10100001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-50.pth.tar', 92.92799999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-49.pth.tar', 92.75800002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-48.pth.tar', 92.73400001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-47.pth.tar', 92.37900002441407)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-46.pth.tar', 92.28200002929688)

Train: 56 [   0/1171 (  0%)]  Loss:  2.583688 (2.5837)  Time: 11.114s,   92.13/s  (11.114s,   92.13/s)  LR: 6.503e-05  Data: 10.239 (10.239)
Train: 56 [  50/1171 (  4%)]  Loss:  3.330784 (2.9572)  Time: 0.890s, 1150.55/s  (2.454s,  417.31/s)  LR: 6.503e-05  Data: 0.228 (1.823)
Train: 56 [ 100/1171 (  9%)]  Loss:  3.412371 (3.1089)  Time: 0.587s, 1744.36/s  (2.328s,  439.86/s)  LR: 6.503e-05  Data: 0.022 (1.711)
Train: 56 [ 150/1171 ( 13%)]  Loss:  2.856769 (3.0459)  Time: 0.590s, 1734.32/s  (2.263s,  452.43/s)  LR: 6.503e-05  Data: 0.018 (1.645)
Train: 56 [ 200/1171 ( 17%)]  Loss:  3.160988 (3.0689)  Time: 3.370s,  303.86/s  (2.257s,  453.65/s)  LR: 6.503e-05  Data: 2.714 (1.644)
Train: 56 [ 250/1171 ( 21%)]  Loss:  3.329809 (3.1124)  Time: 0.586s, 1745.97/s  (2.362s,  433.56/s)  LR: 6.503e-05  Data: 0.021 (1.742)
Train: 56 [ 300/1171 ( 26%)]  Loss:  3.269449 (3.1348)  Time: 4.467s,  229.23/s  (2.433s,  420.81/s)  LR: 6.503e-05  Data: 3.807 (1.815)
Train: 56 [ 350/1171 ( 30%)]  Loss:  2.858094 (3.1002)  Time: 0.591s, 1733.22/s  (2.463s,  415.83/s)  LR: 6.503e-05  Data: 0.020 (1.842)
Train: 56 [ 400/1171 ( 34%)]  Loss:  3.138467 (3.1045)  Time: 3.891s,  263.20/s  (2.507s,  408.45/s)  LR: 6.503e-05  Data: 3.314 (1.888)
Train: 56 [ 450/1171 ( 38%)]  Loss:  2.963650 (3.0904)  Time: 0.585s, 1748.99/s  (2.518s,  406.60/s)  LR: 6.503e-05  Data: 0.020 (1.901)
Train: 56 [ 500/1171 ( 43%)]  Loss:  3.399649 (3.1185)  Time: 6.303s,  162.46/s  (2.538s,  403.51/s)  LR: 6.503e-05  Data: 5.739 (1.920)
Train: 56 [ 550/1171 ( 47%)]  Loss:  3.083147 (3.1156)  Time: 0.590s, 1734.91/s  (2.557s,  400.54/s)  LR: 6.503e-05  Data: 0.017 (1.939)
Train: 56 [ 600/1171 ( 51%)]  Loss:  2.966310 (3.1041)  Time: 2.057s,  497.76/s  (2.620s,  390.84/s)  LR: 6.503e-05  Data: 1.370 (2.001)
Train: 56 [ 650/1171 ( 56%)]  Loss:  3.149481 (3.1073)  Time: 0.590s, 1736.41/s  (2.651s,  386.30/s)  LR: 6.503e-05  Data: 0.020 (2.031)
Train: 56 [ 700/1171 ( 60%)]  Loss:  3.093227 (3.1064)  Time: 6.412s,  159.71/s  (2.653s,  385.94/s)  LR: 6.503e-05  Data: 5.789 (2.035)
Train: 56 [ 750/1171 ( 64%)]  Loss:  3.250077 (3.1154)  Time: 0.587s, 1743.59/s  (2.656s,  385.48/s)  LR: 6.503e-05  Data: 0.019 (2.037)
Train: 56 [ 800/1171 ( 68%)]  Loss:  2.834638 (3.0989)  Time: 9.044s,  113.23/s  (2.665s,  384.20/s)  LR: 6.503e-05  Data: 8.461 (2.048)
Train: 56 [ 850/1171 ( 73%)]  Loss:  3.301860 (3.1101)  Time: 0.592s, 1730.35/s  (2.661s,  384.82/s)  LR: 6.503e-05  Data: 0.019 (2.045)
Train: 56 [ 900/1171 ( 77%)]  Loss:  3.482067 (3.1297)  Time: 12.894s,   79.42/s  (2.717s,  376.90/s)  LR: 6.503e-05  Data: 12.252 (2.102)
Train: 56 [ 950/1171 ( 81%)]  Loss:  3.411597 (3.1438)  Time: 0.586s, 1746.09/s  (2.741s,  373.53/s)  LR: 6.503e-05  Data: 0.019 (2.127)
Train: 56 [1000/1171 ( 85%)]  Loss:  3.016372 (3.1377)  Time: 10.516s,   97.37/s  (2.754s,  371.79/s)  LR: 6.503e-05  Data: 9.824 (2.141)
Train: 56 [1050/1171 ( 90%)]  Loss:  2.966693 (3.1300)  Time: 0.586s, 1747.67/s  (2.758s,  371.35/s)  LR: 6.503e-05  Data: 0.020 (2.146)
Train: 56 [1100/1171 ( 94%)]  Loss:  2.659129 (3.1095)  Time: 7.459s,  137.29/s  (2.756s,  371.56/s)  LR: 6.503e-05  Data: 6.891 (2.145)
Train: 56 [1150/1171 ( 98%)]  Loss:  3.150472 (3.1112)  Time: 5.310s,  192.84/s  (2.747s,  372.82/s)  LR: 6.503e-05  Data: 4.732 (2.135)
Train: 56 [1170/1171 (100%)]  Loss:  2.521933 (3.0876)  Time: 0.568s, 1803.91/s  (2.752s,  372.03/s)  LR: 6.503e-05  Data: 0.000 (2.141)
Test: [   0/97]  Time: 18.172 (18.172)  Loss:  0.3513 (0.3513)  Acc@1: 95.1172 (95.1172)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (3.760)  Loss:  0.5422 (0.4253)  Acc@1: 90.8203 (94.1521)  Acc@5: 98.3398 (98.7956)
Test: [  97/97]  Time: 0.120 (3.558)  Loss:  0.3852 (0.4368)  Acc@1: 93.7500 (93.5330)  Acc@5: 99.1071 (98.5270)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-56.pth.tar', 93.533)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-55.pth.tar', 93.42500001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-54.pth.tar', 93.41500002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-53.pth.tar', 93.31000004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-52.pth.tar', 93.26600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-51.pth.tar', 93.10100001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-50.pth.tar', 92.92799999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-49.pth.tar', 92.75800002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-48.pth.tar', 92.73400001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-47.pth.tar', 92.37900002441407)

Train: 57 [   0/1171 (  0%)]  Loss:  2.496458 (2.4965)  Time: 13.907s,   73.63/s  (13.907s,   73.63/s)  LR: 5.473e-05  Data: 12.545 (12.545)
Train: 57 [  50/1171 (  4%)]  Loss:  3.249756 (2.8731)  Time: 0.941s, 1088.43/s  (2.758s,  371.26/s)  LR: 5.473e-05  Data: 0.281 (2.125)
Train: 57 [ 100/1171 (  9%)]  Loss:  2.914649 (2.8870)  Time: 0.587s, 1744.73/s  (2.704s,  378.66/s)  LR: 5.473e-05  Data: 0.021 (2.087)
Train: 57 [ 150/1171 ( 13%)]  Loss:  2.498531 (2.7898)  Time: 0.585s, 1750.39/s  (2.598s,  394.07/s)  LR: 5.473e-05  Data: 0.019 (1.992)
Train: 57 [ 200/1171 ( 17%)]  Loss:  2.820974 (2.7961)  Time: 0.586s, 1747.88/s  (2.641s,  387.74/s)  LR: 5.473e-05  Data: 0.021 (2.036)
Train: 57 [ 250/1171 ( 21%)]  Loss:  2.925146 (2.8176)  Time: 0.587s, 1745.01/s  (2.717s,  376.83/s)  LR: 5.473e-05  Data: 0.020 (2.116)
Train: 57 [ 300/1171 ( 26%)]  Loss:  3.185638 (2.8702)  Time: 6.883s,  148.77/s  (2.731s,  374.94/s)  LR: 5.473e-05  Data: 6.315 (2.127)
Train: 57 [ 350/1171 ( 30%)]  Loss:  3.377340 (2.9336)  Time: 0.588s, 1742.09/s  (2.716s,  377.02/s)  LR: 5.473e-05  Data: 0.018 (2.109)
Train: 57 [ 400/1171 ( 34%)]  Loss:  3.371694 (2.9822)  Time: 0.590s, 1734.13/s  (2.733s,  374.72/s)  LR: 5.473e-05  Data: 0.026 (2.125)
Train: 57 [ 450/1171 ( 38%)]  Loss:  3.053455 (2.9894)  Time: 0.586s, 1746.89/s  (2.702s,  379.05/s)  LR: 5.473e-05  Data: 0.020 (2.096)
Train: 57 [ 500/1171 ( 43%)]  Loss:  2.643881 (2.9580)  Time: 0.593s, 1727.06/s  (2.687s,  381.06/s)  LR: 5.473e-05  Data: 0.024 (2.083)
Train: 57 [ 550/1171 ( 47%)]  Loss:  3.194757 (2.9777)  Time: 0.589s, 1737.23/s  (2.703s,  378.91/s)  LR: 5.473e-05  Data: 0.019 (2.097)
Train: 57 [ 600/1171 ( 51%)]  Loss:  2.158406 (2.9147)  Time: 0.587s, 1744.02/s  (2.735s,  374.40/s)  LR: 5.473e-05  Data: 0.020 (2.129)
Train: 57 [ 650/1171 ( 56%)]  Loss:  2.774580 (2.9047)  Time: 0.587s, 1744.67/s  (2.737s,  374.09/s)  LR: 5.473e-05  Data: 0.022 (2.130)
Train: 57 [ 700/1171 ( 60%)]  Loss:  2.536697 (2.8801)  Time: 0.585s, 1749.69/s  (2.739s,  373.92/s)  LR: 5.473e-05  Data: 0.020 (2.131)
Train: 57 [ 750/1171 ( 64%)]  Loss:  2.565378 (2.8605)  Time: 0.587s, 1744.14/s  (2.709s,  377.97/s)  LR: 5.473e-05  Data: 0.018 (2.103)
Train: 57 [ 800/1171 ( 68%)]  Loss:  3.207703 (2.8809)  Time: 0.587s, 1745.34/s  (2.688s,  381.00/s)  LR: 5.473e-05  Data: 0.021 (2.082)
Train: 57 [ 850/1171 ( 73%)]  Loss:  2.915679 (2.8828)  Time: 0.586s, 1747.06/s  (2.656s,  385.61/s)  LR: 5.473e-05  Data: 0.018 (2.050)
Train: 57 [ 900/1171 ( 77%)]  Loss:  3.638740 (2.9226)  Time: 0.590s, 1736.61/s  (2.667s,  383.89/s)  LR: 5.473e-05  Data: 0.020 (2.062)
Train: 57 [ 950/1171 ( 81%)]  Loss:  3.217188 (2.9373)  Time: 0.587s, 1745.61/s  (2.678s,  382.40/s)  LR: 5.473e-05  Data: 0.018 (2.074)
Train: 57 [1000/1171 ( 85%)]  Loss:  3.018816 (2.9412)  Time: 0.586s, 1748.06/s  (2.691s,  380.50/s)  LR: 5.473e-05  Data: 0.020 (2.087)
Train: 57 [1050/1171 ( 90%)]  Loss:  3.140639 (2.9503)  Time: 0.586s, 1746.26/s  (2.671s,  383.37/s)  LR: 5.473e-05  Data: 0.022 (2.067)
Train: 57 [1100/1171 ( 94%)]  Loss:  2.914235 (2.9487)  Time: 0.587s, 1745.22/s  (2.653s,  385.96/s)  LR: 5.473e-05  Data: 0.018 (2.051)
Train: 57 [1150/1171 ( 98%)]  Loss:  2.777790 (2.9416)  Time: 0.589s, 1737.39/s  (2.629s,  389.43/s)  LR: 5.473e-05  Data: 0.019 (2.027)
Train: 57 [1170/1171 (100%)]  Loss:  3.463705 (2.9625)  Time: 0.567s, 1806.09/s  (2.621s,  390.75/s)  LR: 5.473e-05  Data: 0.000 (2.018)
Test: [   0/97]  Time: 12.903 (12.903)  Loss:  0.3387 (0.3387)  Acc@1: 96.0938 (96.0938)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.201 (2.874)  Loss:  0.5227 (0.4083)  Acc@1: 91.6016 (94.3034)  Acc@5: 98.1445 (98.7898)
Test: [  97/97]  Time: 0.120 (3.165)  Loss:  0.3920 (0.4233)  Acc@1: 93.7500 (93.6960)  Acc@5: 98.6607 (98.5210)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-57.pth.tar', 93.696)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-56.pth.tar', 93.533)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-55.pth.tar', 93.42500001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-54.pth.tar', 93.41500002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-53.pth.tar', 93.31000004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-52.pth.tar', 93.26600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-51.pth.tar', 93.10100001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-50.pth.tar', 92.92799999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-49.pth.tar', 92.75800002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-48.pth.tar', 92.73400001220703)

Train: 58 [   0/1171 (  0%)]  Loss:  3.428844 (3.4288)  Time: 13.653s,   75.00/s  (13.653s,   75.00/s)  LR: 4.546e-05  Data: 11.948 (11.948)
Train: 58 [  50/1171 (  4%)]  Loss:  2.479742 (2.9543)  Time: 0.583s, 1755.92/s  (2.651s,  386.31/s)  LR: 4.546e-05  Data: 0.019 (2.037)
Train: 58 [ 100/1171 (  9%)]  Loss:  2.754698 (2.8878)  Time: 1.309s,  782.29/s  (2.569s,  398.56/s)  LR: 4.546e-05  Data: 0.744 (1.960)
Train: 58 [ 150/1171 ( 13%)]  Loss:  2.938587 (2.9005)  Time: 0.588s, 1740.52/s  (2.449s,  418.11/s)  LR: 4.546e-05  Data: 0.020 (1.832)
Train: 58 [ 200/1171 ( 17%)]  Loss:  2.790397 (2.8785)  Time: 0.604s, 1694.55/s  (2.396s,  427.33/s)  LR: 4.546e-05  Data: 0.020 (1.777)
Train: 58 [ 250/1171 ( 21%)]  Loss:  2.666716 (2.8432)  Time: 0.590s, 1736.64/s  (2.332s,  439.04/s)  LR: 4.546e-05  Data: 0.019 (1.713)
Train: 58 [ 300/1171 ( 26%)]  Loss:  2.772273 (2.8330)  Time: 5.435s,  188.41/s  (2.328s,  439.87/s)  LR: 4.546e-05  Data: 4.778 (1.704)
Train: 58 [ 350/1171 ( 30%)]  Loss:  2.803595 (2.8294)  Time: 0.586s, 1746.82/s  (2.397s,  427.27/s)  LR: 4.546e-05  Data: 0.019 (1.775)
Train: 58 [ 400/1171 ( 34%)]  Loss:  2.622315 (2.8064)  Time: 4.722s,  216.86/s  (2.478s,  413.22/s)  LR: 4.546e-05  Data: 4.050 (1.858)
Train: 58 [ 450/1171 ( 38%)]  Loss:  3.068255 (2.8325)  Time: 0.584s, 1752.50/s  (2.510s,  408.00/s)  LR: 4.546e-05  Data: 0.020 (1.889)
Train: 58 [ 500/1171 ( 43%)]  Loss:  2.878368 (2.8367)  Time: 7.908s,  129.50/s  (2.536s,  403.73/s)  LR: 4.546e-05  Data: 7.340 (1.917)
Train: 58 [ 550/1171 ( 47%)]  Loss:  3.174821 (2.8649)  Time: 0.586s, 1746.76/s  (2.543s,  402.61/s)  LR: 4.546e-05  Data: 0.021 (1.927)
Train: 58 [ 600/1171 ( 51%)]  Loss:  2.833044 (2.8624)  Time: 7.462s,  137.22/s  (2.552s,  401.26/s)  LR: 4.546e-05  Data: 6.843 (1.937)
Train: 58 [ 650/1171 ( 56%)]  Loss:  2.848998 (2.8615)  Time: 0.590s, 1736.64/s  (2.533s,  404.28/s)  LR: 4.546e-05  Data: 0.021 (1.919)
Train: 58 [ 700/1171 ( 60%)]  Loss:  3.210289 (2.8847)  Time: 9.392s,  109.03/s  (2.550s,  401.59/s)  LR: 4.546e-05  Data: 8.727 (1.937)
Train: 58 [ 750/1171 ( 64%)]  Loss:  2.610681 (2.8676)  Time: 0.587s, 1743.17/s  (2.542s,  402.76/s)  LR: 4.546e-05  Data: 0.017 (1.931)
Train: 58 [ 800/1171 ( 68%)]  Loss:  2.687475 (2.8570)  Time: 7.460s,  137.27/s  (2.538s,  403.44/s)  LR: 4.546e-05  Data: 6.895 (1.927)
Train: 58 [ 850/1171 ( 73%)]  Loss:  2.813335 (2.8546)  Time: 0.585s, 1749.02/s  (2.526s,  405.40/s)  LR: 4.546e-05  Data: 0.021 (1.914)
Train: 58 [ 900/1171 ( 77%)]  Loss:  2.693526 (2.8461)  Time: 8.481s,  120.74/s  (2.529s,  404.92/s)  LR: 4.546e-05  Data: 7.902 (1.919)
Train: 58 [ 950/1171 ( 81%)]  Loss:  2.757212 (2.8417)  Time: 0.587s, 1745.49/s  (2.520s,  406.41/s)  LR: 4.546e-05  Data: 0.021 (1.910)
Train: 58 [1000/1171 ( 85%)]  Loss:  2.604514 (2.8304)  Time: 4.771s,  214.64/s  (2.514s,  407.32/s)  LR: 4.546e-05  Data: 4.056 (1.905)
Train: 58 [1050/1171 ( 90%)]  Loss:  2.965041 (2.8365)  Time: 0.584s, 1753.24/s  (2.547s,  402.05/s)  LR: 4.546e-05  Data: 0.018 (1.937)
Train: 58 [1100/1171 ( 94%)]  Loss:  3.357802 (2.8592)  Time: 9.751s,  105.02/s  (2.560s,  400.04/s)  LR: 4.546e-05  Data: 9.182 (1.951)
Train: 58 [1150/1171 ( 98%)]  Loss:  2.937712 (2.8624)  Time: 0.586s, 1748.33/s  (2.561s,  399.80/s)  LR: 4.546e-05  Data: 0.020 (1.953)
Train: 58 [1170/1171 (100%)]  Loss:  2.833382 (2.8613)  Time: 0.565s, 1812.06/s  (2.563s,  399.49/s)  LR: 4.546e-05  Data: 0.000 (1.956)
Test: [   0/97]  Time: 14.423 (14.423)  Loss:  0.3485 (0.3485)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.3164 (99.3164)
Test: [  50/97]  Time: 0.198 (3.487)  Loss:  0.5357 (0.4334)  Acc@1: 91.4062 (94.2057)  Acc@5: 98.2422 (98.7764)
Test: [  97/97]  Time: 0.120 (3.312)  Loss:  0.3862 (0.4441)  Acc@1: 93.4524 (93.7110)  Acc@5: 99.1071 (98.5470)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-58.pth.tar', 93.71100002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-57.pth.tar', 93.696)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-56.pth.tar', 93.533)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-55.pth.tar', 93.42500001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-54.pth.tar', 93.41500002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-53.pth.tar', 93.31000004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-52.pth.tar', 93.26600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-51.pth.tar', 93.10100001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-50.pth.tar', 92.92799999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-49.pth.tar', 92.75800002197266)

Train: 59 [   0/1171 (  0%)]  Loss:  2.860640 (2.8606)  Time: 11.635s,   88.01/s  (11.635s,   88.01/s)  LR: 3.722e-05  Data: 10.428 (10.428)
Train: 59 [  50/1171 (  4%)]  Loss:  3.234288 (3.0475)  Time: 2.143s,  477.78/s  (2.831s,  361.72/s)  LR: 3.722e-05  Data: 1.579 (2.206)
Train: 59 [ 100/1171 (  9%)]  Loss:  2.886140 (2.9937)  Time: 0.858s, 1193.23/s  (2.847s,  359.70/s)  LR: 3.722e-05  Data: 0.201 (2.221)
Train: 59 [ 150/1171 ( 13%)]  Loss:  2.894392 (2.9689)  Time: 0.585s, 1751.36/s  (2.812s,  364.19/s)  LR: 3.722e-05  Data: 0.020 (2.192)
Train: 59 [ 200/1171 ( 17%)]  Loss:  2.991376 (2.9734)  Time: 6.826s,  150.01/s  (2.821s,  363.02/s)  LR: 3.722e-05  Data: 6.168 (2.204)
Train: 59 [ 250/1171 ( 21%)]  Loss:  3.252964 (3.0200)  Time: 0.589s, 1737.92/s  (2.771s,  369.55/s)  LR: 3.722e-05  Data: 0.023 (2.153)
Train: 59 [ 300/1171 ( 26%)]  Loss:  3.082467 (3.0289)  Time: 6.310s,  162.28/s  (2.729s,  375.19/s)  LR: 3.722e-05  Data: 5.731 (2.114)
Train: 59 [ 350/1171 ( 30%)]  Loss:  3.284467 (3.0608)  Time: 0.586s, 1747.03/s  (2.681s,  381.95/s)  LR: 3.722e-05  Data: 0.021 (2.067)
Train: 59 [ 400/1171 ( 34%)]  Loss:  3.456893 (3.1048)  Time: 9.482s,  108.00/s  (2.719s,  376.67/s)  LR: 3.722e-05  Data: 8.542 (2.108)
Train: 59 [ 450/1171 ( 38%)]  Loss:  3.327521 (3.1271)  Time: 0.585s, 1749.53/s  (2.717s,  376.95/s)  LR: 3.722e-05  Data: 0.019 (2.107)
Train: 59 [ 500/1171 ( 43%)]  Loss:  3.453841 (3.1568)  Time: 9.174s,  111.62/s  (2.750s,  372.38/s)  LR: 3.722e-05  Data: 8.612 (2.136)
Train: 59 [ 550/1171 ( 47%)]  Loss:  2.962574 (3.1406)  Time: 0.588s, 1741.64/s  (2.757s,  371.43/s)  LR: 3.722e-05  Data: 0.023 (2.142)
Train: 59 [ 600/1171 ( 51%)]  Loss:  3.244786 (3.1486)  Time: 8.014s,  127.77/s  (2.755s,  371.62/s)  LR: 3.722e-05  Data: 7.435 (2.141)
Train: 59 [ 650/1171 ( 56%)]  Loss:  3.158701 (3.1494)  Time: 0.588s, 1740.21/s  (2.744s,  373.22/s)  LR: 3.722e-05  Data: 0.023 (2.130)
Train: 59 [ 700/1171 ( 60%)]  Loss:  2.989375 (3.1387)  Time: 14.911s,   68.67/s  (2.747s,  372.75/s)  LR: 3.722e-05  Data: 14.290 (2.134)
Train: 59 [ 750/1171 ( 64%)]  Loss:  3.144118 (3.1390)  Time: 0.588s, 1741.15/s  (2.756s,  371.54/s)  LR: 3.722e-05  Data: 0.022 (2.143)
Train: 59 [ 800/1171 ( 68%)]  Loss:  2.650195 (3.1103)  Time: 1.390s,  736.89/s  (2.754s,  371.80/s)  LR: 3.722e-05  Data: 0.701 (2.142)
Train: 59 [ 850/1171 ( 73%)]  Loss:  2.646065 (3.0845)  Time: 0.586s, 1747.67/s  (2.755s,  371.63/s)  LR: 3.722e-05  Data: 0.021 (2.144)
Train: 59 [ 900/1171 ( 77%)]  Loss:  3.368963 (3.0995)  Time: 0.589s, 1737.88/s  (2.734s,  374.61/s)  LR: 3.722e-05  Data: 0.021 (2.124)
Train: 59 [ 950/1171 ( 81%)]  Loss:  2.953161 (3.0921)  Time: 0.587s, 1745.11/s  (2.723s,  376.08/s)  LR: 3.722e-05  Data: 0.020 (2.113)
Train: 59 [1000/1171 ( 85%)]  Loss:  3.150285 (3.0949)  Time: 0.589s, 1738.65/s  (2.700s,  379.25/s)  LR: 3.722e-05  Data: 0.024 (2.092)
Train: 59 [1050/1171 ( 90%)]  Loss:  3.330781 (3.1056)  Time: 0.592s, 1730.65/s  (2.692s,  380.45/s)  LR: 3.722e-05  Data: 0.021 (2.085)
Train: 59 [1100/1171 ( 94%)]  Loss:  3.349552 (3.1162)  Time: 0.587s, 1743.56/s  (2.684s,  381.46/s)  LR: 3.722e-05  Data: 0.023 (2.078)
Train: 59 [1150/1171 ( 98%)]  Loss:  3.103133 (3.1157)  Time: 0.584s, 1752.43/s  (2.684s,  381.47/s)  LR: 3.722e-05  Data: 0.019 (2.079)
Train: 59 [1170/1171 (100%)]  Loss:  3.346895 (3.1249)  Time: 0.565s, 1811.33/s  (2.684s,  381.57/s)  LR: 3.722e-05  Data: 0.000 (2.079)
Test: [   0/97]  Time: 15.985 (15.985)  Loss:  0.3299 (0.3299)  Acc@1: 96.3867 (96.3867)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.201 (3.271)  Loss:  0.4943 (0.4012)  Acc@1: 91.6016 (94.3340)  Acc@5: 98.1445 (98.7975)
Test: [  97/97]  Time: 0.120 (3.189)  Loss:  0.3656 (0.4158)  Acc@1: 94.1964 (93.7630)  Acc@5: 98.8095 (98.5770)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-59.pth.tar', 93.7630000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-58.pth.tar', 93.71100002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-57.pth.tar', 93.696)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-56.pth.tar', 93.533)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-55.pth.tar', 93.42500001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-54.pth.tar', 93.41500002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-53.pth.tar', 93.31000004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-52.pth.tar', 93.26600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-51.pth.tar', 93.10100001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-50.pth.tar', 92.92799999511719)

Train: 60 [   0/1171 (  0%)]  Loss:  3.185310 (3.1853)  Time: 10.883s,   94.09/s  (10.883s,   94.09/s)  LR: 3.005e-05  Data: 10.185 (10.185)
Train: 60 [  50/1171 (  4%)]  Loss:  3.214624 (3.2000)  Time: 0.588s, 1740.25/s  (2.408s,  425.24/s)  LR: 3.005e-05  Data: 0.024 (1.804)
Train: 60 [ 100/1171 (  9%)]  Loss:  2.973326 (3.1244)  Time: 0.592s, 1729.73/s  (2.341s,  437.37/s)  LR: 3.005e-05  Data: 0.019 (1.734)
Train: 60 [ 150/1171 ( 13%)]  Loss:  2.706665 (3.0200)  Time: 0.593s, 1727.01/s  (2.571s,  398.21/s)  LR: 3.005e-05  Data: 0.025 (1.959)
Train: 60 [ 200/1171 ( 17%)]  Loss:  2.782371 (2.9725)  Time: 3.253s,  314.78/s  (2.656s,  385.60/s)  LR: 3.005e-05  Data: 2.689 (2.044)
Train: 60 [ 250/1171 ( 21%)]  Loss:  2.989790 (2.9753)  Time: 0.589s, 1738.39/s  (2.662s,  384.60/s)  LR: 3.005e-05  Data: 0.024 (2.049)
Train: 60 [ 300/1171 ( 26%)]  Loss:  3.052570 (2.9864)  Time: 8.496s,  120.53/s  (2.694s,  380.05/s)  LR: 3.005e-05  Data: 7.901 (2.084)
Train: 60 [ 350/1171 ( 30%)]  Loss:  3.032845 (2.9922)  Time: 0.591s, 1733.00/s  (2.675s,  382.75/s)  LR: 3.005e-05  Data: 0.025 (2.070)
Train: 60 [ 400/1171 ( 34%)]  Loss:  3.160106 (3.0108)  Time: 8.040s,  127.36/s  (2.669s,  383.72/s)  LR: 3.005e-05  Data: 7.368 (2.066)
Train: 60 [ 450/1171 ( 38%)]  Loss:  3.425564 (3.0523)  Time: 0.591s, 1733.04/s  (2.695s,  379.98/s)  LR: 3.005e-05  Data: 0.025 (2.095)
Train: 60 [ 500/1171 ( 43%)]  Loss:  2.940068 (3.0421)  Time: 11.579s,   88.43/s  (2.748s,  372.66/s)  LR: 3.005e-05  Data: 10.996 (2.149)
Train: 60 [ 550/1171 ( 47%)]  Loss:  2.757479 (3.0184)  Time: 0.589s, 1737.75/s  (2.760s,  371.07/s)  LR: 3.005e-05  Data: 0.024 (2.162)
Train: 60 [ 600/1171 ( 51%)]  Loss:  2.983920 (3.0157)  Time: 7.595s,  134.83/s  (2.780s,  368.28/s)  LR: 3.005e-05  Data: 6.931 (2.181)
Train: 60 [ 650/1171 ( 56%)]  Loss:  2.944696 (3.0107)  Time: 0.589s, 1737.38/s  (2.772s,  369.35/s)  LR: 3.005e-05  Data: 0.024 (2.173)
Train: 60 [ 700/1171 ( 60%)]  Loss:  3.113543 (3.0175)  Time: 5.456s,  187.67/s  (2.755s,  371.64/s)  LR: 3.005e-05  Data: 4.725 (2.156)
Train: 60 [ 750/1171 ( 64%)]  Loss:  3.060901 (3.0202)  Time: 0.589s, 1738.39/s  (2.753s,  371.95/s)  LR: 3.005e-05  Data: 0.024 (2.152)
Train: 60 [ 800/1171 ( 68%)]  Loss:  3.337737 (3.0389)  Time: 5.630s,  181.88/s  (2.775s,  369.06/s)  LR: 3.005e-05  Data: 4.962 (2.172)
Train: 60 [ 850/1171 ( 73%)]  Loss:  3.486204 (3.0638)  Time: 0.591s, 1733.28/s  (2.783s,  367.89/s)  LR: 3.005e-05  Data: 0.025 (2.179)
Train: 60 [ 900/1171 ( 77%)]  Loss:  3.017154 (3.0613)  Time: 0.587s, 1745.32/s  (2.786s,  367.58/s)  LR: 3.005e-05  Data: 0.021 (2.180)
Train: 60 [ 950/1171 ( 81%)]  Loss:  2.780023 (3.0472)  Time: 0.588s, 1740.70/s  (2.788s,  367.23/s)  LR: 3.005e-05  Data: 0.023 (2.183)
Train: 60 [1000/1171 ( 85%)]  Loss:  3.255515 (3.0572)  Time: 0.586s, 1746.33/s  (2.779s,  368.49/s)  LR: 3.005e-05  Data: 0.020 (2.174)
Train: 60 [1050/1171 ( 90%)]  Loss:  2.912802 (3.0506)  Time: 0.590s, 1736.57/s  (2.773s,  369.23/s)  LR: 3.005e-05  Data: 0.025 (2.169)
Train: 60 [1100/1171 ( 94%)]  Loss:  2.719102 (3.0362)  Time: 0.588s, 1741.34/s  (2.788s,  367.23/s)  LR: 3.005e-05  Data: 0.020 (2.183)
Train: 60 [1150/1171 ( 98%)]  Loss:  3.440153 (3.0530)  Time: 0.585s, 1748.98/s  (2.797s,  366.06/s)  LR: 3.005e-05  Data: 0.021 (2.192)
Train: 60 [1170/1171 (100%)]  Loss:  3.047910 (3.0528)  Time: 0.565s, 1811.35/s  (2.799s,  365.86/s)  LR: 3.005e-05  Data: 0.000 (2.194)
Test: [   0/97]  Time: 16.143 (16.143)  Loss:  0.3084 (0.3084)  Acc@1: 95.8984 (95.8984)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.197 (3.622)  Loss:  0.4940 (0.3908)  Acc@1: 91.6992 (94.3532)  Acc@5: 98.2422 (98.8090)
Test: [  97/97]  Time: 0.120 (3.504)  Loss:  0.3539 (0.4054)  Acc@1: 94.0476 (93.7520)  Acc@5: 99.1071 (98.5670)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-59.pth.tar', 93.7630000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-60.pth.tar', 93.75200002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-58.pth.tar', 93.71100002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-57.pth.tar', 93.696)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-56.pth.tar', 93.533)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-55.pth.tar', 93.42500001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-54.pth.tar', 93.41500002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-53.pth.tar', 93.31000004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-52.pth.tar', 93.26600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-51.pth.tar', 93.10100001464843)

Train: 61 [   0/1171 (  0%)]  Loss:  3.066418 (3.0664)  Time: 9.073s,  112.86/s  (9.073s,  112.86/s)  LR: 2.395e-05  Data: 8.265 (8.265)
Train: 61 [  50/1171 (  4%)]  Loss:  3.015349 (3.0409)  Time: 0.586s, 1747.80/s  (2.623s,  390.42/s)  LR: 2.395e-05  Data: 0.021 (2.037)
Train: 61 [ 100/1171 (  9%)]  Loss:  3.275878 (3.1192)  Time: 1.270s,  806.04/s  (2.751s,  372.22/s)  LR: 2.395e-05  Data: 0.589 (2.162)
Train: 61 [ 150/1171 ( 13%)]  Loss:  2.887086 (3.0612)  Time: 0.586s, 1747.73/s  (2.745s,  373.02/s)  LR: 2.395e-05  Data: 0.020 (2.155)
Train: 61 [ 200/1171 ( 17%)]  Loss:  2.539043 (2.9568)  Time: 0.590s, 1736.97/s  (2.842s,  360.33/s)  LR: 2.395e-05  Data: 0.025 (2.249)
Train: 61 [ 250/1171 ( 21%)]  Loss:  3.164203 (2.9913)  Time: 0.586s, 1747.85/s  (2.852s,  359.08/s)  LR: 2.395e-05  Data: 0.021 (2.257)
Train: 61 [ 300/1171 ( 26%)]  Loss:  2.721891 (2.9528)  Time: 0.588s, 1742.11/s  (2.869s,  356.96/s)  LR: 2.395e-05  Data: 0.023 (2.273)
Train: 61 [ 350/1171 ( 30%)]  Loss:  2.667495 (2.9172)  Time: 0.586s, 1748.05/s  (2.849s,  359.39/s)  LR: 2.395e-05  Data: 0.020 (2.256)
Train: 61 [ 400/1171 ( 34%)]  Loss:  3.298297 (2.9595)  Time: 0.588s, 1741.01/s  (2.874s,  356.29/s)  LR: 2.395e-05  Data: 0.023 (2.281)
Train: 61 [ 450/1171 ( 38%)]  Loss:  3.323639 (2.9959)  Time: 0.589s, 1738.78/s  (2.883s,  355.23/s)  LR: 2.395e-05  Data: 0.020 (2.291)
Train: 61 [ 500/1171 ( 43%)]  Loss:  2.532811 (2.9538)  Time: 0.587s, 1745.22/s  (2.902s,  352.91/s)  LR: 2.395e-05  Data: 0.022 (2.310)
Train: 61 [ 550/1171 ( 47%)]  Loss:  2.945384 (2.9531)  Time: 4.579s,  223.63/s  (2.906s,  352.33/s)  LR: 2.395e-05  Data: 3.752 (2.312)
Train: 61 [ 600/1171 ( 51%)]  Loss:  3.185843 (2.9710)  Time: 0.599s, 1708.62/s  (2.909s,  351.99/s)  LR: 2.395e-05  Data: 0.021 (2.313)
Train: 61 [ 650/1171 ( 56%)]  Loss:  3.075171 (2.9785)  Time: 0.590s, 1736.11/s  (2.883s,  355.13/s)  LR: 2.395e-05  Data: 0.025 (2.288)
Train: 61 [ 700/1171 ( 60%)]  Loss:  3.319153 (3.0012)  Time: 0.590s, 1734.31/s  (2.867s,  357.18/s)  LR: 2.395e-05  Data: 0.025 (2.272)
Train: 61 [ 750/1171 ( 64%)]  Loss:  2.943909 (2.9976)  Time: 0.592s, 1729.16/s  (2.880s,  355.58/s)  LR: 2.395e-05  Data: 0.020 (2.285)
Train: 61 [ 800/1171 ( 68%)]  Loss:  2.722183 (2.9814)  Time: 0.591s, 1733.60/s  (2.887s,  354.71/s)  LR: 2.395e-05  Data: 0.025 (2.293)
Train: 61 [ 850/1171 ( 73%)]  Loss:  2.814019 (2.9721)  Time: 0.590s, 1736.46/s  (2.870s,  356.84/s)  LR: 2.395e-05  Data: 0.022 (2.275)
Train: 61 [ 900/1171 ( 77%)]  Loss:  3.274440 (2.9880)  Time: 0.590s, 1735.66/s  (2.861s,  357.94/s)  LR: 2.395e-05  Data: 0.022 (2.267)
Train: 61 [ 950/1171 ( 81%)]  Loss:  2.833513 (2.9803)  Time: 3.295s,  310.80/s  (2.845s,  359.98/s)  LR: 2.395e-05  Data: 2.730 (2.249)
Train: 61 [1000/1171 ( 85%)]  Loss:  2.880391 (2.9755)  Time: 0.587s, 1743.71/s  (2.830s,  361.78/s)  LR: 2.395e-05  Data: 0.020 (2.234)
Train: 61 [1050/1171 ( 90%)]  Loss:  2.586429 (2.9578)  Time: 2.626s,  390.02/s  (2.827s,  362.22/s)  LR: 2.395e-05  Data: 1.957 (2.230)
Train: 61 [1100/1171 ( 94%)]  Loss:  2.933780 (2.9568)  Time: 0.587s, 1744.30/s  (2.835s,  361.22/s)  LR: 2.395e-05  Data: 0.019 (2.238)
Train: 61 [1150/1171 ( 98%)]  Loss:  2.902051 (2.9545)  Time: 0.782s, 1309.49/s  (2.829s,  361.97/s)  LR: 2.395e-05  Data: 0.165 (2.232)
Train: 61 [1170/1171 (100%)]  Loss:  2.873387 (2.9513)  Time: 0.566s, 1810.58/s  (2.825s,  362.43/s)  LR: 2.395e-05  Data: 0.000 (2.229)
Test: [   0/97]  Time: 15.148 (15.148)  Loss:  0.3201 (0.3201)  Acc@1: 96.0938 (96.0938)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.197 (3.537)  Loss:  0.5047 (0.3995)  Acc@1: 91.3086 (94.3627)  Acc@5: 98.2422 (98.7994)
Test: [  97/97]  Time: 0.120 (3.404)  Loss:  0.3692 (0.4125)  Acc@1: 93.8988 (93.8200)  Acc@5: 98.8095 (98.5550)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-61.pth.tar', 93.82000006347656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-59.pth.tar', 93.7630000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-60.pth.tar', 93.75200002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-58.pth.tar', 93.71100002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-57.pth.tar', 93.696)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-56.pth.tar', 93.533)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-55.pth.tar', 93.42500001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-54.pth.tar', 93.41500002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-53.pth.tar', 93.31000004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-52.pth.tar', 93.26600001220703)

Train: 62 [   0/1171 (  0%)]  Loss:  2.832236 (2.8322)  Time: 11.956s,   85.65/s  (11.956s,   85.65/s)  LR: 1.895e-05  Data: 11.249 (11.249)
Train: 62 [  50/1171 (  4%)]  Loss:  3.344717 (3.0885)  Time: 0.585s, 1751.92/s  (2.496s,  410.23/s)  LR: 1.895e-05  Data: 0.018 (1.912)
Train: 62 [ 100/1171 (  9%)]  Loss:  3.132925 (3.1033)  Time: 0.586s, 1747.00/s  (2.758s,  371.35/s)  LR: 1.895e-05  Data: 0.021 (2.175)
Train: 62 [ 150/1171 ( 13%)]  Loss:  2.642060 (2.9880)  Time: 0.590s, 1735.91/s  (2.761s,  370.90/s)  LR: 1.895e-05  Data: 0.022 (2.172)
Train: 62 [ 200/1171 ( 17%)]  Loss:  2.694617 (2.9293)  Time: 0.586s, 1746.21/s  (2.748s,  372.66/s)  LR: 1.895e-05  Data: 0.020 (2.151)
Train: 62 [ 250/1171 ( 21%)]  Loss:  2.752190 (2.8998)  Time: 2.502s,  409.20/s  (2.741s,  373.58/s)  LR: 1.895e-05  Data: 1.935 (2.142)
Train: 62 [ 300/1171 ( 26%)]  Loss:  2.770581 (2.8813)  Time: 0.588s, 1740.56/s  (2.722s,  376.22/s)  LR: 1.895e-05  Data: 0.021 (2.121)
Train: 62 [ 350/1171 ( 30%)]  Loss:  3.432578 (2.9502)  Time: 2.298s,  445.68/s  (2.695s,  379.96/s)  LR: 1.895e-05  Data: 1.733 (2.093)
Train: 62 [ 400/1171 ( 34%)]  Loss:  2.885216 (2.9430)  Time: 0.585s, 1750.15/s  (2.705s,  378.59/s)  LR: 1.895e-05  Data: 0.019 (2.099)
Train: 62 [ 450/1171 ( 38%)]  Loss:  3.092834 (2.9580)  Time: 0.586s, 1746.66/s  (2.762s,  370.73/s)  LR: 1.895e-05  Data: 0.020 (2.157)
Train: 62 [ 500/1171 ( 43%)]  Loss:  2.936760 (2.9561)  Time: 0.587s, 1744.87/s  (2.804s,  365.15/s)  LR: 1.895e-05  Data: 0.021 (2.201)
Train: 62 [ 550/1171 ( 47%)]  Loss:  3.104745 (2.9685)  Time: 0.590s, 1735.79/s  (2.819s,  363.30/s)  LR: 1.895e-05  Data: 0.019 (2.216)
Train: 62 [ 600/1171 ( 51%)]  Loss:  3.481447 (3.0079)  Time: 0.586s, 1746.91/s  (2.830s,  361.78/s)  LR: 1.895e-05  Data: 0.018 (2.229)
Train: 62 [ 650/1171 ( 56%)]  Loss:  3.230431 (3.0238)  Time: 0.587s, 1743.02/s  (2.806s,  364.93/s)  LR: 1.895e-05  Data: 0.021 (2.206)
Train: 62 [ 700/1171 ( 60%)]  Loss:  3.102537 (3.0291)  Time: 0.589s, 1737.51/s  (2.787s,  367.45/s)  LR: 1.895e-05  Data: 0.020 (2.186)
Train: 62 [ 750/1171 ( 64%)]  Loss:  3.233382 (3.0418)  Time: 0.586s, 1748.69/s  (2.791s,  366.92/s)  LR: 1.895e-05  Data: 0.021 (2.191)
Train: 62 [ 800/1171 ( 68%)]  Loss:  2.735256 (3.0238)  Time: 0.588s, 1742.35/s  (2.793s,  366.60/s)  LR: 1.895e-05  Data: 0.021 (2.195)
Train: 62 [ 850/1171 ( 73%)]  Loss:  2.419155 (2.9902)  Time: 0.589s, 1738.68/s  (2.786s,  367.51/s)  LR: 1.895e-05  Data: 0.021 (2.187)
Train: 62 [ 900/1171 ( 77%)]  Loss:  2.718521 (2.9759)  Time: 0.596s, 1718.34/s  (2.782s,  368.09/s)  LR: 1.895e-05  Data: 0.028 (2.183)
Train: 62 [ 950/1171 ( 81%)]  Loss:  3.435489 (2.9989)  Time: 0.588s, 1741.92/s  (2.764s,  370.42/s)  LR: 1.895e-05  Data: 0.022 (2.166)
Train: 62 [1000/1171 ( 85%)]  Loss:  2.835927 (2.9911)  Time: 0.591s, 1732.47/s  (2.753s,  371.91/s)  LR: 1.895e-05  Data: 0.023 (2.154)
Train: 62 [1050/1171 ( 90%)]  Loss:  2.526555 (2.9700)  Time: 3.492s,  293.25/s  (2.735s,  374.44/s)  LR: 1.895e-05  Data: 2.825 (2.135)
Train: 62 [1100/1171 ( 94%)]  Loss:  3.094549 (2.9754)  Time: 3.478s,  294.44/s  (2.746s,  372.93/s)  LR: 1.895e-05  Data: 2.859 (2.146)
Train: 62 [1150/1171 ( 98%)]  Loss:  2.821432 (2.9690)  Time: 0.939s, 1090.30/s  (2.745s,  373.10/s)  LR: 1.895e-05  Data: 0.274 (2.143)
Train: 62 [1170/1171 (100%)]  Loss:  3.389823 (2.9858)  Time: 0.565s, 1812.07/s  (2.745s,  373.02/s)  LR: 1.895e-05  Data: 0.000 (2.144)
Test: [   0/97]  Time: 15.031 (15.031)  Loss:  0.3197 (0.3197)  Acc@1: 96.4844 (96.4844)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (3.375)  Loss:  0.5178 (0.4041)  Acc@1: 90.5273 (94.4010)  Acc@5: 98.1445 (98.7937)
Test: [  97/97]  Time: 0.120 (3.289)  Loss:  0.3623 (0.4157)  Acc@1: 94.6429 (93.8650)  Acc@5: 98.6607 (98.5700)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-62.pth.tar', 93.86500002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-61.pth.tar', 93.82000006347656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-59.pth.tar', 93.7630000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-60.pth.tar', 93.75200002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-58.pth.tar', 93.71100002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-57.pth.tar', 93.696)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-56.pth.tar', 93.533)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-55.pth.tar', 93.42500001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-54.pth.tar', 93.41500002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-53.pth.tar', 93.31000004882813)

Train: 63 [   0/1171 (  0%)]  Loss:  3.467482 (3.4675)  Time: 11.728s,   87.31/s  (11.728s,   87.31/s)  LR: 1.504e-05  Data: 10.663 (10.663)
Train: 63 [  50/1171 (  4%)]  Loss:  2.207072 (2.8373)  Time: 0.586s, 1748.50/s  (2.496s,  410.18/s)  LR: 1.504e-05  Data: 0.019 (1.885)
Train: 63 [ 100/1171 (  9%)]  Loss:  2.647073 (2.7739)  Time: 3.490s,  293.44/s  (2.551s,  401.43/s)  LR: 1.504e-05  Data: 2.897 (1.948)
Train: 63 [ 150/1171 ( 13%)]  Loss:  3.041473 (2.8408)  Time: 0.589s, 1739.91/s  (2.611s,  392.25/s)  LR: 1.504e-05  Data: 0.021 (1.998)
Train: 63 [ 200/1171 ( 17%)]  Loss:  3.261814 (2.9250)  Time: 5.109s,  200.41/s  (2.670s,  383.52/s)  LR: 1.504e-05  Data: 4.545 (2.064)
Train: 63 [ 250/1171 ( 21%)]  Loss:  3.149584 (2.9624)  Time: 0.593s, 1726.95/s  (2.663s,  384.53/s)  LR: 1.504e-05  Data: 0.022 (2.055)
Train: 63 [ 300/1171 ( 26%)]  Loss:  2.910995 (2.9551)  Time: 5.287s,  193.67/s  (2.651s,  386.21/s)  LR: 1.504e-05  Data: 4.707 (2.045)
Train: 63 [ 350/1171 ( 30%)]  Loss:  3.271657 (2.9946)  Time: 0.589s, 1738.05/s  (2.619s,  390.95/s)  LR: 1.504e-05  Data: 0.021 (2.012)
Train: 63 [ 400/1171 ( 34%)]  Loss:  2.700280 (2.9619)  Time: 8.859s,  115.59/s  (2.617s,  391.22/s)  LR: 1.504e-05  Data: 8.262 (2.010)
Train: 63 [ 450/1171 ( 38%)]  Loss:  2.988763 (2.9646)  Time: 0.586s, 1746.75/s  (2.609s,  392.48/s)  LR: 1.504e-05  Data: 0.018 (2.004)
Train: 63 [ 500/1171 ( 43%)]  Loss:  2.533555 (2.9254)  Time: 9.287s,  110.26/s  (2.645s,  387.21/s)  LR: 1.504e-05  Data: 8.616 (2.041)
Train: 63 [ 550/1171 ( 47%)]  Loss:  3.362496 (2.9619)  Time: 0.592s, 1729.04/s  (2.660s,  384.90/s)  LR: 1.504e-05  Data: 0.023 (2.056)
Train: 63 [ 600/1171 ( 51%)]  Loss:  3.119769 (2.9740)  Time: 8.737s,  117.20/s  (2.671s,  383.33/s)  LR: 1.504e-05  Data: 7.962 (2.066)
Train: 63 [ 650/1171 ( 56%)]  Loss:  3.447669 (3.0078)  Time: 0.588s, 1740.93/s  (2.662s,  384.74/s)  LR: 1.504e-05  Data: 0.023 (2.056)
Train: 63 [ 700/1171 ( 60%)]  Loss:  2.905661 (3.0010)  Time: 5.534s,  185.03/s  (2.656s,  385.57/s)  LR: 1.504e-05  Data: 4.956 (2.051)
Train: 63 [ 750/1171 ( 64%)]  Loss:  2.305773 (2.9576)  Time: 0.594s, 1724.91/s  (2.640s,  387.85/s)  LR: 1.504e-05  Data: 0.024 (2.036)
Train: 63 [ 800/1171 ( 68%)]  Loss:  3.446241 (2.9863)  Time: 11.072s,   92.49/s  (2.662s,  384.68/s)  LR: 1.504e-05  Data: 10.503 (2.059)
Train: 63 [ 850/1171 ( 73%)]  Loss:  3.100057 (2.9926)  Time: 0.593s, 1726.77/s  (2.670s,  383.56/s)  LR: 1.504e-05  Data: 0.026 (2.068)
Train: 63 [ 900/1171 ( 77%)]  Loss:  3.133514 (3.0000)  Time: 9.458s,  108.27/s  (2.680s,  382.06/s)  LR: 1.504e-05  Data: 8.800 (2.079)
Train: 63 [ 950/1171 ( 81%)]  Loss:  3.311749 (3.0156)  Time: 0.593s, 1726.59/s  (2.682s,  381.83/s)  LR: 1.504e-05  Data: 0.027 (2.081)
Train: 63 [1000/1171 ( 85%)]  Loss:  3.276512 (3.0281)  Time: 8.107s,  126.31/s  (2.683s,  381.59/s)  LR: 1.504e-05  Data: 7.502 (2.084)
Train: 63 [1050/1171 ( 90%)]  Loss:  2.682284 (3.0123)  Time: 0.589s, 1739.54/s  (2.672s,  383.25/s)  LR: 1.504e-05  Data: 0.024 (2.072)
Train: 63 [1100/1171 ( 94%)]  Loss:  3.238744 (3.0222)  Time: 12.055s,   84.94/s  (2.674s,  382.94/s)  LR: 1.504e-05  Data: 11.472 (2.075)
Train: 63 [1150/1171 ( 98%)]  Loss:  3.211977 (3.0301)  Time: 0.591s, 1732.16/s  (2.688s,  380.99/s)  LR: 1.504e-05  Data: 0.025 (2.089)
Train: 63 [1170/1171 (100%)]  Loss:  3.221594 (3.0378)  Time: 0.565s, 1811.29/s  (2.691s,  380.55/s)  LR: 1.504e-05  Data: 0.000 (2.093)
Test: [   0/97]  Time: 16.024 (16.024)  Loss:  0.3201 (0.3201)  Acc@1: 96.2891 (96.2891)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.200 (3.526)  Loss:  0.5049 (0.4017)  Acc@1: 91.2109 (94.4183)  Acc@5: 98.1445 (98.8032)
Test: [  97/97]  Time: 0.120 (3.443)  Loss:  0.3717 (0.4140)  Acc@1: 94.3452 (93.8650)  Acc@5: 98.8095 (98.5710)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-63.pth.tar', 93.86500004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-62.pth.tar', 93.86500002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-61.pth.tar', 93.82000006347656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-59.pth.tar', 93.7630000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-60.pth.tar', 93.75200002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-58.pth.tar', 93.71100002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-57.pth.tar', 93.696)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-56.pth.tar', 93.533)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-55.pth.tar', 93.42500001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-54.pth.tar', 93.41500002197266)

Train: 64 [   0/1171 (  0%)]  Loss:  3.031639 (3.0316)  Time: 11.575s,   88.47/s  (11.575s,   88.47/s)  LR: 1.224e-05  Data: 10.896 (10.896)
Train: 64 [  50/1171 (  4%)]  Loss:  2.857527 (2.9446)  Time: 0.586s, 1747.09/s  (2.571s,  398.23/s)  LR: 1.224e-05  Data: 0.022 (1.974)
Train: 64 [ 100/1171 (  9%)]  Loss:  3.127341 (3.0055)  Time: 0.770s, 1330.34/s  (2.536s,  403.77/s)  LR: 1.224e-05  Data: 0.190 (1.928)
Train: 64 [ 150/1171 ( 13%)]  Loss:  2.828326 (2.9612)  Time: 0.586s, 1748.20/s  (2.566s,  399.04/s)  LR: 1.224e-05  Data: 0.021 (1.959)
Train: 64 [ 200/1171 ( 17%)]  Loss:  2.562534 (2.8815)  Time: 0.586s, 1748.91/s  (2.673s,  383.11/s)  LR: 1.224e-05  Data: 0.020 (2.069)
Train: 64 [ 250/1171 ( 21%)]  Loss:  3.268417 (2.9460)  Time: 0.585s, 1751.18/s  (2.670s,  383.51/s)  LR: 1.224e-05  Data: 0.019 (2.073)
Train: 64 [ 300/1171 ( 26%)]  Loss:  3.384974 (3.0087)  Time: 0.590s, 1736.41/s  (2.702s,  379.03/s)  LR: 1.224e-05  Data: 0.020 (2.105)
Train: 64 [ 350/1171 ( 30%)]  Loss:  3.504744 (3.0707)  Time: 0.590s, 1735.81/s  (2.681s,  381.99/s)  LR: 1.224e-05  Data: 0.019 (2.086)
Train: 64 [ 400/1171 ( 34%)]  Loss:  3.180039 (3.0828)  Time: 0.586s, 1746.94/s  (2.673s,  383.10/s)  LR: 1.224e-05  Data: 0.020 (2.079)
Train: 64 [ 450/1171 ( 38%)]  Loss:  3.158452 (3.0904)  Time: 0.587s, 1745.90/s  (2.650s,  386.40/s)  LR: 1.224e-05  Data: 0.020 (2.057)
Train: 64 [ 500/1171 ( 43%)]  Loss:  3.293530 (3.1089)  Time: 0.587s, 1745.14/s  (2.700s,  379.29/s)  LR: 1.224e-05  Data: 0.020 (2.105)
Train: 64 [ 550/1171 ( 47%)]  Loss:  2.603861 (3.0668)  Time: 0.587s, 1744.48/s  (2.728s,  375.38/s)  LR: 1.224e-05  Data: 0.020 (2.132)
Train: 64 [ 600/1171 ( 51%)]  Loss:  2.751006 (3.0425)  Time: 0.589s, 1737.51/s  (2.748s,  372.67/s)  LR: 1.224e-05  Data: 0.021 (2.152)
Train: 64 [ 650/1171 ( 56%)]  Loss:  2.374885 (2.9948)  Time: 0.588s, 1741.07/s  (2.747s,  372.76/s)  LR: 1.224e-05  Data: 0.022 (2.151)
Train: 64 [ 700/1171 ( 60%)]  Loss:  3.050282 (2.9985)  Time: 0.588s, 1741.69/s  (2.735s,  374.41/s)  LR: 1.224e-05  Data: 0.021 (2.138)
Train: 64 [ 750/1171 ( 64%)]  Loss:  3.340916 (3.0199)  Time: 0.589s, 1739.59/s  (2.722s,  376.20/s)  LR: 1.224e-05  Data: 0.020 (2.124)
Train: 64 [ 800/1171 ( 68%)]  Loss:  3.484130 (3.0472)  Time: 0.589s, 1737.12/s  (2.720s,  376.50/s)  LR: 1.224e-05  Data: 0.021 (2.120)
Train: 64 [ 850/1171 ( 73%)]  Loss:  2.465428 (3.0149)  Time: 0.587s, 1745.20/s  (2.729s,  375.26/s)  LR: 1.224e-05  Data: 0.020 (2.126)
Train: 64 [ 900/1171 ( 77%)]  Loss:  2.688224 (2.9977)  Time: 0.586s, 1748.46/s  (2.736s,  374.26/s)  LR: 1.224e-05  Data: 0.018 (2.134)
Train: 64 [ 950/1171 ( 81%)]  Loss:  3.418350 (3.0187)  Time: 0.584s, 1752.52/s  (2.736s,  374.28/s)  LR: 1.224e-05  Data: 0.019 (2.134)
Train: 64 [1000/1171 ( 85%)]  Loss:  3.247825 (3.0296)  Time: 0.588s, 1742.60/s  (2.732s,  374.81/s)  LR: 1.224e-05  Data: 0.019 (2.129)
Train: 64 [1050/1171 ( 90%)]  Loss:  3.442522 (3.0484)  Time: 0.586s, 1747.28/s  (2.723s,  376.10/s)  LR: 1.224e-05  Data: 0.019 (2.119)
Train: 64 [1100/1171 ( 94%)]  Loss:  2.626157 (3.0300)  Time: 0.587s, 1745.48/s  (2.713s,  377.47/s)  LR: 1.224e-05  Data: 0.021 (2.109)
Train: 64 [1150/1171 ( 98%)]  Loss:  3.340006 (3.0430)  Time: 0.591s, 1731.56/s  (2.718s,  376.70/s)  LR: 1.224e-05  Data: 0.026 (2.115)
Train: 64 [1170/1171 (100%)]  Loss:  3.142614 (3.0469)  Time: 0.564s, 1815.28/s  (2.726s,  375.65/s)  LR: 1.224e-05  Data: 0.000 (2.122)
Test: [   0/97]  Time: 16.528 (16.528)  Loss:  0.3161 (0.3161)  Acc@1: 95.9961 (95.9961)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (3.690)  Loss:  0.5148 (0.3955)  Acc@1: 90.9180 (94.5217)  Acc@5: 98.0469 (98.8090)
Test: [  97/97]  Time: 0.120 (3.500)  Loss:  0.3554 (0.4087)  Acc@1: 94.3452 (93.9510)  Acc@5: 98.9583 (98.5730)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-64.pth.tar', 93.9509999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-63.pth.tar', 93.86500004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-62.pth.tar', 93.86500002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-61.pth.tar', 93.82000006347656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-59.pth.tar', 93.7630000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-60.pth.tar', 93.75200002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-58.pth.tar', 93.71100002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-57.pth.tar', 93.696)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-56.pth.tar', 93.533)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-55.pth.tar', 93.42500001464843)

Train: 65 [   0/1171 (  0%)]  Loss:  3.221427 (3.2214)  Time: 11.886s,   86.15/s  (11.886s,   86.15/s)  LR: 1.056e-05  Data: 11.051 (11.051)
Train: 65 [  50/1171 (  4%)]  Loss:  2.746870 (2.9841)  Time: 0.587s, 1743.11/s  (2.707s,  378.34/s)  LR: 1.056e-05  Data: 0.019 (2.106)
Train: 65 [ 100/1171 (  9%)]  Loss:  2.836188 (2.9348)  Time: 4.756s,  215.32/s  (2.612s,  392.06/s)  LR: 1.056e-05  Data: 4.082 (2.013)
Train: 65 [ 150/1171 ( 13%)]  Loss:  3.431652 (3.0590)  Time: 0.707s, 1447.89/s  (2.523s,  405.92/s)  LR: 1.056e-05  Data: 0.142 (1.918)
Train: 65 [ 200/1171 ( 17%)]  Loss:  2.882809 (3.0238)  Time: 2.569s,  398.54/s  (2.663s,  384.50/s)  LR: 1.056e-05  Data: 2.005 (2.052)
Train: 65 [ 250/1171 ( 21%)]  Loss:  3.057564 (3.0294)  Time: 3.521s,  290.81/s  (2.706s,  378.36/s)  LR: 1.056e-05  Data: 2.877 (2.089)
Train: 65 [ 300/1171 ( 26%)]  Loss:  2.808162 (2.9978)  Time: 3.598s,  284.61/s  (2.709s,  377.98/s)  LR: 1.056e-05  Data: 3.013 (2.090)
Train: 65 [ 350/1171 ( 30%)]  Loss:  2.506004 (2.9363)  Time: 2.196s,  466.41/s  (2.702s,  378.98/s)  LR: 1.056e-05  Data: 1.553 (2.082)
Train: 65 [ 400/1171 ( 34%)]  Loss:  2.912595 (2.9337)  Time: 5.945s,  172.24/s  (2.715s,  377.20/s)  LR: 1.056e-05  Data: 5.366 (2.094)
Train: 65 [ 450/1171 ( 38%)]  Loss:  3.187640 (2.9591)  Time: 0.588s, 1741.43/s  (2.688s,  380.91/s)  LR: 1.056e-05  Data: 0.023 (2.070)
Train: 65 [ 500/1171 ( 43%)]  Loss:  3.112542 (2.9730)  Time: 3.939s,  259.93/s  (2.724s,  375.89/s)  LR: 1.056e-05  Data: 3.292 (2.107)
Train: 65 [ 550/1171 ( 47%)]  Loss:  2.707049 (2.9509)  Time: 0.590s, 1735.44/s  (2.752s,  372.10/s)  LR: 1.056e-05  Data: 0.021 (2.135)
Train: 65 [ 600/1171 ( 51%)]  Loss:  3.258793 (2.9746)  Time: 0.590s, 1734.38/s  (2.779s,  368.45/s)  LR: 1.056e-05  Data: 0.024 (2.163)
Train: 65 [ 650/1171 ( 56%)]  Loss:  3.157187 (2.9876)  Time: 0.591s, 1732.58/s  (2.783s,  367.91/s)  LR: 1.056e-05  Data: 0.022 (2.168)
Train: 65 [ 700/1171 ( 60%)]  Loss:  3.005822 (2.9888)  Time: 0.589s, 1738.63/s  (2.785s,  367.73/s)  LR: 1.056e-05  Data: 0.022 (2.171)
Train: 65 [ 750/1171 ( 64%)]  Loss:  2.988689 (2.9888)  Time: 0.586s, 1746.39/s  (2.770s,  369.67/s)  LR: 1.056e-05  Data: 0.019 (2.157)
Train: 65 [ 800/1171 ( 68%)]  Loss:  3.369487 (3.0112)  Time: 0.588s, 1741.39/s  (2.759s,  371.13/s)  LR: 1.056e-05  Data: 0.023 (2.148)
Train: 65 [ 850/1171 ( 73%)]  Loss:  3.011080 (3.0112)  Time: 3.901s,  262.48/s  (2.782s,  368.04/s)  LR: 1.056e-05  Data: 3.337 (2.171)
Train: 65 [ 900/1171 ( 77%)]  Loss:  3.077817 (3.0147)  Time: 0.589s, 1739.08/s  (2.789s,  367.13/s)  LR: 1.056e-05  Data: 0.023 (2.179)
Train: 65 [ 950/1171 ( 81%)]  Loss:  2.728064 (3.0004)  Time: 4.349s,  235.46/s  (2.790s,  367.08/s)  LR: 1.056e-05  Data: 3.693 (2.180)
Train: 65 [1000/1171 ( 85%)]  Loss:  3.237172 (3.0116)  Time: 0.588s, 1740.22/s  (2.787s,  367.42/s)  LR: 1.056e-05  Data: 0.019 (2.177)
Train: 65 [1050/1171 ( 90%)]  Loss:  3.133205 (3.0172)  Time: 1.591s,  643.50/s  (2.778s,  368.55/s)  LR: 1.056e-05  Data: 1.023 (2.168)
Train: 65 [1100/1171 ( 94%)]  Loss:  2.925641 (3.0132)  Time: 2.877s,  355.90/s  (2.771s,  369.52/s)  LR: 1.056e-05  Data: 2.313 (2.161)
Train: 65 [1150/1171 ( 98%)]  Loss:  2.859027 (3.0068)  Time: 0.588s, 1741.74/s  (2.778s,  368.59/s)  LR: 1.056e-05  Data: 0.021 (2.167)
Train: 65 [1170/1171 (100%)]  Loss:  3.317123 (3.0192)  Time: 0.564s, 1814.80/s  (2.782s,  368.08/s)  LR: 1.056e-05  Data: 0.000 (2.171)
Test: [   0/97]  Time: 16.005 (16.005)  Loss:  0.3147 (0.3147)  Acc@1: 96.5820 (96.5820)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.198 (3.642)  Loss:  0.5026 (0.3966)  Acc@1: 91.5039 (94.4968)  Acc@5: 98.1445 (98.8186)
Test: [  97/97]  Time: 0.120 (3.551)  Loss:  0.3511 (0.4095)  Acc@1: 94.7917 (93.9330)  Acc@5: 98.8095 (98.5820)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-64.pth.tar', 93.9509999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-65.pth.tar', 93.93300003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-63.pth.tar', 93.86500004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-62.pth.tar', 93.86500002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-61.pth.tar', 93.82000006347656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-59.pth.tar', 93.7630000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-60.pth.tar', 93.75200002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-58.pth.tar', 93.71100002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-57.pth.tar', 93.696)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-56.pth.tar', 93.533)

*** Best metric: 93.9509999975586 (epoch 64)

wandb: Waiting for W&B process to finish, PID 53344
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210526_001853-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1/logs/debug.log
wandb: Find internal logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210526_001853-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1/logs/debug-internal.log
wandb: Run summary:
wandb:    eval_loss 0.40954
wandb:    eval_top1 93.933
wandb:    eval_top5 98.582
wandb:   _timestamp 1622029999
wandb:   train_loss 3.01918
wandb:        _step 65
wandb:        epoch 65
wandb:     _runtime 220455
wandb: Run history:
wandb:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   train_loss ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñÜ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÅ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ
wandb:    eval_loss ‚ñÑ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ
wandb:    eval_top1 ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:    eval_top5 ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced PreTraining_vit_deit_tiny_patch16_224_fake_1k: https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
--End--
Wed May 26 20:53:31 JST 2021

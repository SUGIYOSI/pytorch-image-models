--Start--
Sun May 23 20:17:08 JST 2021
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 1
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
wandb: Currently logged in as: gyama_x (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.10.30 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.10.27
wandb: Syncing run PreTraining_vit_deit_tiny_patch16_224_fake_1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gyama_x/pytorch-image-models
wandb: üöÄ View run at https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run data is saved locally in /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210523_201758-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run `wandb offline` to turn off syncing.

Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Using native Torch DistributedDataParallel.
Scheduled epochs: 22
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Train: 0 [   0/1171 (  0%)]  Loss:  6.963746 (6.9637)  Time: 15.962s,   64.15/s  (15.962s,   64.15/s)  LR: 1.000e-04  Data: 14.485 (14.485)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 0 [  50/1171 (  4%)]  Loss:  6.871742 (6.9177)  Time: 0.584s, 1754.90/s  (2.519s,  406.50/s)  LR: 1.000e-04  Data: 0.018 (1.912)
Train: 0 [ 100/1171 (  9%)]  Loss:  6.833007 (6.8895)  Time: 2.702s,  378.97/s  (2.403s,  426.12/s)  LR: 1.000e-04  Data: 2.104 (1.800)
Train: 0 [ 150/1171 ( 13%)]  Loss:  6.778726 (6.8618)  Time: 0.589s, 1738.17/s  (2.287s,  447.81/s)  LR: 1.000e-04  Data: 0.019 (1.688)
Train: 0 [ 200/1171 ( 17%)]  Loss:  6.752467 (6.8399)  Time: 0.583s, 1755.28/s  (2.243s,  456.46/s)  LR: 1.000e-04  Data: 0.018 (1.645)
Train: 0 [ 250/1171 ( 21%)]  Loss:  6.714851 (6.8191)  Time: 0.581s, 1761.79/s  (2.190s,  467.64/s)  LR: 1.000e-04  Data: 0.017 (1.591)
Train: 0 [ 300/1171 ( 26%)]  Loss:  6.719290 (6.8048)  Time: 0.587s, 1744.69/s  (2.201s,  465.30/s)  LR: 1.000e-04  Data: 0.021 (1.606)
Train: 0 [ 350/1171 ( 30%)]  Loss:  6.635778 (6.7837)  Time: 0.587s, 1744.14/s  (2.223s,  460.69/s)  LR: 1.000e-04  Data: 0.022 (1.629)
Train: 0 [ 400/1171 ( 34%)]  Loss:  6.666462 (6.7707)  Time: 0.587s, 1743.76/s  (2.238s,  457.55/s)  LR: 1.000e-04  Data: 0.019 (1.646)
Train: 0 [ 450/1171 ( 38%)]  Loss:  6.538551 (6.7475)  Time: 0.584s, 1752.21/s  (2.233s,  458.55/s)  LR: 1.000e-04  Data: 0.021 (1.642)
Train: 0 [ 500/1171 ( 43%)]  Loss:  6.558603 (6.7303)  Time: 0.584s, 1752.46/s  (2.238s,  457.61/s)  LR: 1.000e-04  Data: 0.019 (1.647)
Train: 0 [ 550/1171 ( 47%)]  Loss:  6.536902 (6.7142)  Time: 0.586s, 1746.63/s  (2.218s,  461.76/s)  LR: 1.000e-04  Data: 0.020 (1.627)
Train: 0 [ 600/1171 ( 51%)]  Loss:  6.561938 (6.7025)  Time: 0.583s, 1757.55/s  (2.218s,  461.58/s)  LR: 1.000e-04  Data: 0.018 (1.628)
Train: 0 [ 650/1171 ( 56%)]  Loss:  6.422079 (6.6824)  Time: 0.586s, 1747.05/s  (2.209s,  463.62/s)  LR: 1.000e-04  Data: 0.021 (1.619)
Train: 0 [ 700/1171 ( 60%)]  Loss:  6.440293 (6.6663)  Time: 0.584s, 1753.39/s  (2.246s,  455.99/s)  LR: 1.000e-04  Data: 0.019 (1.655)
Train: 0 [ 750/1171 ( 64%)]  Loss:  6.499833 (6.6559)  Time: 0.584s, 1754.63/s  (2.258s,  453.51/s)  LR: 1.000e-04  Data: 0.020 (1.668)
Train: 0 [ 800/1171 ( 68%)]  Loss:  6.328465 (6.6366)  Time: 0.586s, 1746.76/s  (2.284s,  448.30/s)  LR: 1.000e-04  Data: 0.024 (1.695)
Train: 0 [ 850/1171 ( 73%)]  Loss:  6.408848 (6.6240)  Time: 0.584s, 1753.02/s  (2.300s,  445.25/s)  LR: 1.000e-04  Data: 0.021 (1.711)
Train: 0 [ 900/1171 ( 77%)]  Loss:  6.419518 (6.6132)  Time: 0.584s, 1754.07/s  (2.311s,  443.05/s)  LR: 1.000e-04  Data: 0.021 (1.722)
Train: 0 [ 950/1171 ( 81%)]  Loss:  6.253762 (6.5952)  Time: 0.585s, 1750.97/s  (2.308s,  443.70/s)  LR: 1.000e-04  Data: 0.020 (1.719)
Train: 0 [1000/1171 ( 85%)]  Loss:  6.162705 (6.5746)  Time: 0.702s, 1459.60/s  (2.309s,  443.47/s)  LR: 1.000e-04  Data: 0.020 (1.719)
Train: 0 [1050/1171 ( 90%)]  Loss:  6.258851 (6.5603)  Time: 0.582s, 1758.47/s  (2.303s,  444.61/s)  LR: 1.000e-04  Data: 0.018 (1.712)
Train: 0 [1100/1171 ( 94%)]  Loss:  6.224406 (6.5457)  Time: 2.145s,  477.40/s  (2.330s,  439.50/s)  LR: 1.000e-04  Data: 1.578 (1.738)
Train: 0 [1150/1171 ( 98%)]  Loss:  6.196023 (6.5311)  Time: 0.585s, 1750.34/s  (2.341s,  437.40/s)  LR: 1.000e-04  Data: 0.022 (1.748)
Train: 0 [1170/1171 (100%)]  Loss:  6.236282 (6.5193)  Time: 0.565s, 1812.24/s  (2.344s,  436.92/s)  LR: 1.000e-04  Data: 0.000 (1.751)
Test: [   0/97]  Time: 16.495 (16.495)  Loss:  4.1687 (4.1687)  Acc@1: 12.2070 (12.2070)  Acc@5: 65.7227 (65.7227)
Test: [  50/97]  Time: 0.195 (3.274)  Loss:  5.7508 (5.4484)  Acc@1:  3.0273 ( 7.6019)  Acc@5: 10.9375 (23.2460)
Test: [  97/97]  Time: 0.468 (3.105)  Loss:  4.9866 (5.3996)  Acc@1: 17.2619 (10.0330)  Acc@5: 41.8155 (26.7530)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-0.pth.tar', 10.033000006103515)

Train: 1 [   0/1171 (  0%)]  Loss:  6.239352 (6.2394)  Time: 10.828s,   94.57/s  (10.828s,   94.57/s)  LR: 2.800e-04  Data: 9.749 (9.749)
Train: 1 [  50/1171 (  4%)]  Loss:  6.364963 (6.3022)  Time: 0.584s, 1753.18/s  (2.260s,  453.14/s)  LR: 2.800e-04  Data: 0.017 (1.669)
Train: 1 [ 100/1171 (  9%)]  Loss:  6.325386 (6.3099)  Time: 0.631s, 1623.97/s  (2.216s,  462.19/s)  LR: 2.800e-04  Data: 0.045 (1.631)
Train: 1 [ 150/1171 ( 13%)]  Loss:  6.140028 (6.2674)  Time: 0.584s, 1752.02/s  (2.283s,  448.59/s)  LR: 2.800e-04  Data: 0.018 (1.688)
Train: 1 [ 200/1171 ( 17%)]  Loss:  6.101653 (6.2343)  Time: 4.662s,  219.64/s  (2.353s,  435.17/s)  LR: 2.800e-04  Data: 4.094 (1.756)
Train: 1 [ 250/1171 ( 21%)]  Loss:  6.010446 (6.1970)  Time: 0.585s, 1749.50/s  (2.374s,  431.34/s)  LR: 2.800e-04  Data: 0.020 (1.776)
Train: 1 [ 300/1171 ( 26%)]  Loss:  6.005828 (6.1697)  Time: 6.317s,  162.10/s  (2.395s,  427.51/s)  LR: 2.800e-04  Data: 5.747 (1.794)
Train: 1 [ 350/1171 ( 30%)]  Loss:  5.982582 (6.1463)  Time: 0.587s, 1744.93/s  (2.388s,  428.83/s)  LR: 2.800e-04  Data: 0.020 (1.788)
Train: 1 [ 400/1171 ( 34%)]  Loss:  6.182183 (6.1503)  Time: 7.837s,  130.66/s  (2.405s,  425.71/s)  LR: 2.800e-04  Data: 7.273 (1.805)
Train: 1 [ 450/1171 ( 38%)]  Loss:  5.841907 (6.1194)  Time: 0.585s, 1750.04/s  (2.387s,  429.07/s)  LR: 2.800e-04  Data: 0.021 (1.789)
Train: 1 [ 500/1171 ( 43%)]  Loss:  5.980012 (6.1068)  Time: 6.536s,  156.68/s  (2.390s,  428.46/s)  LR: 2.800e-04  Data: 5.867 (1.790)
Train: 1 [ 550/1171 ( 47%)]  Loss:  5.848008 (6.0852)  Time: 0.583s, 1755.98/s  (2.443s,  419.20/s)  LR: 2.800e-04  Data: 0.017 (1.841)
Train: 1 [ 600/1171 ( 51%)]  Loss:  5.985502 (6.0775)  Time: 9.059s,  113.03/s  (2.473s,  414.07/s)  LR: 2.800e-04  Data: 8.498 (1.872)
Train: 1 [ 650/1171 ( 56%)]  Loss:  5.843410 (6.0608)  Time: 0.587s, 1744.50/s  (2.472s,  414.17/s)  LR: 2.800e-04  Data: 0.022 (1.873)
Train: 1 [ 700/1171 ( 60%)]  Loss:  5.834895 (6.0457)  Time: 7.746s,  132.20/s  (2.478s,  413.29/s)  LR: 2.800e-04  Data: 7.165 (1.878)
Train: 1 [ 750/1171 ( 64%)]  Loss:  5.732470 (6.0262)  Time: 0.587s, 1745.21/s  (2.469s,  414.68/s)  LR: 2.800e-04  Data: 0.020 (1.869)
Train: 1 [ 800/1171 ( 68%)]  Loss:  5.714422 (6.0078)  Time: 6.908s,  148.23/s  (2.465s,  415.34/s)  LR: 2.800e-04  Data: 6.142 (1.866)
Train: 1 [ 850/1171 ( 73%)]  Loss:  5.721843 (5.9919)  Time: 0.585s, 1751.07/s  (2.454s,  417.33/s)  LR: 2.800e-04  Data: 0.021 (1.854)
Train: 1 [ 900/1171 ( 77%)]  Loss:  5.809649 (5.9823)  Time: 2.920s,  350.63/s  (2.473s,  414.00/s)  LR: 2.800e-04  Data: 2.357 (1.872)
Train: 1 [ 950/1171 ( 81%)]  Loss:  5.822188 (5.9743)  Time: 0.583s, 1756.38/s  (2.483s,  412.46/s)  LR: 2.800e-04  Data: 0.020 (1.882)
Train: 1 [1000/1171 ( 85%)]  Loss:  6.105293 (5.9806)  Time: 2.596s,  394.40/s  (2.486s,  411.90/s)  LR: 2.800e-04  Data: 1.923 (1.884)
Train: 1 [1050/1171 ( 90%)]  Loss:  5.837328 (5.9741)  Time: 0.585s, 1750.75/s  (2.482s,  412.59/s)  LR: 2.800e-04  Data: 0.021 (1.880)
Train: 1 [1100/1171 ( 94%)]  Loss:  5.765502 (5.9650)  Time: 0.584s, 1752.01/s  (2.484s,  412.23/s)  LR: 2.800e-04  Data: 0.021 (1.883)
Train: 1 [1150/1171 ( 98%)]  Loss:  5.653022 (5.9520)  Time: 0.580s, 1764.95/s  (2.476s,  413.49/s)  LR: 2.800e-04  Data: 0.018 (1.876)
Train: 1 [1170/1171 (100%)]  Loss:  5.709773 (5.9423)  Time: 0.564s, 1814.31/s  (2.476s,  413.58/s)  LR: 2.800e-04  Data: 0.000 (1.876)
Test: [   0/97]  Time: 13.735 (13.735)  Loss:  2.6732 (2.6732)  Acc@1: 39.0625 (39.0625)  Acc@5: 86.7188 (86.7188)
Test: [  50/97]  Time: 0.196 (3.474)  Loss:  4.3309 (3.8566)  Acc@1: 22.1680 (28.9962)  Acc@5: 44.6289 (55.7100)
Test: [  97/97]  Time: 0.120 (3.366)  Loss:  3.0890 (3.8014)  Acc@1: 52.5298 (31.1200)  Acc@5: 72.6190 (57.0970)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-1.pth.tar', 31.120000002441405)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-0.pth.tar', 10.033000006103515)

Train: 2 [   0/1171 (  0%)]  Loss:  5.701925 (5.7019)  Time: 11.815s,   86.67/s  (11.815s,   86.67/s)  LR: 4.600e-04  Data: 10.754 (10.754)
Train: 2 [  50/1171 (  4%)]  Loss:  5.730434 (5.7162)  Time: 0.585s, 1750.91/s  (2.569s,  398.54/s)  LR: 4.600e-04  Data: 0.020 (1.970)
Train: 2 [ 100/1171 (  9%)]  Loss:  6.054895 (5.8291)  Time: 0.584s, 1753.60/s  (2.487s,  411.69/s)  LR: 4.600e-04  Data: 0.019 (1.893)
Train: 2 [ 150/1171 ( 13%)]  Loss:  5.309005 (5.6991)  Time: 2.690s,  380.67/s  (2.407s,  425.47/s)  LR: 4.600e-04  Data: 2.121 (1.816)
Train: 2 [ 200/1171 ( 17%)]  Loss:  5.585111 (5.6763)  Time: 0.660s, 1551.37/s  (2.372s,  431.67/s)  LR: 4.600e-04  Data: 0.018 (1.777)
Train: 2 [ 250/1171 ( 21%)]  Loss:  5.494192 (5.6459)  Time: 1.648s,  621.25/s  (2.338s,  438.04/s)  LR: 4.600e-04  Data: 0.931 (1.738)
Train: 2 [ 300/1171 ( 26%)]  Loss:  5.222729 (5.5855)  Time: 6.020s,  170.10/s  (2.421s,  422.95/s)  LR: 4.600e-04  Data: 5.451 (1.818)
Train: 2 [ 350/1171 ( 30%)]  Loss:  5.521922 (5.5775)  Time: 0.585s, 1749.65/s  (2.432s,  421.01/s)  LR: 4.600e-04  Data: 0.021 (1.829)
Train: 2 [ 400/1171 ( 34%)]  Loss:  5.672206 (5.5880)  Time: 4.900s,  208.98/s  (2.467s,  415.15/s)  LR: 4.600e-04  Data: 4.338 (1.866)
Train: 2 [ 450/1171 ( 38%)]  Loss:  5.361904 (5.5654)  Time: 0.590s, 1735.89/s  (2.465s,  415.50/s)  LR: 4.600e-04  Data: 0.021 (1.863)
Train: 2 [ 500/1171 ( 43%)]  Loss:  5.720389 (5.5795)  Time: 6.156s,  166.33/s  (2.472s,  414.20/s)  LR: 4.600e-04  Data: 5.450 (1.870)
Train: 2 [ 550/1171 ( 47%)]  Loss:  5.348999 (5.5603)  Time: 0.585s, 1749.75/s  (2.462s,  415.94/s)  LR: 4.600e-04  Data: 0.016 (1.859)
Train: 2 [ 600/1171 ( 51%)]  Loss:  5.316636 (5.5416)  Time: 6.167s,  166.05/s  (2.469s,  414.67/s)  LR: 4.600e-04  Data: 5.482 (1.867)
Train: 2 [ 650/1171 ( 56%)]  Loss:  5.531568 (5.5409)  Time: 0.588s, 1742.46/s  (2.481s,  412.71/s)  LR: 4.600e-04  Data: 0.021 (1.878)
Train: 2 [ 700/1171 ( 60%)]  Loss:  5.691730 (5.5509)  Time: 8.665s,  118.18/s  (2.501s,  409.48/s)  LR: 4.600e-04  Data: 7.952 (1.897)
Train: 2 [ 750/1171 ( 64%)]  Loss:  5.439198 (5.5439)  Time: 1.279s,  800.88/s  (2.497s,  410.10/s)  LR: 4.600e-04  Data: 0.707 (1.893)
Train: 2 [ 800/1171 ( 68%)]  Loss:  5.471221 (5.5397)  Time: 7.888s,  129.82/s  (2.501s,  409.42/s)  LR: 4.600e-04  Data: 7.284 (1.898)
Train: 2 [ 850/1171 ( 73%)]  Loss:  4.793970 (5.4982)  Time: 1.424s,  719.21/s  (2.487s,  411.70/s)  LR: 4.600e-04  Data: 0.796 (1.884)
Train: 2 [ 900/1171 ( 77%)]  Loss:  5.090129 (5.4767)  Time: 7.878s,  129.98/s  (2.486s,  411.92/s)  LR: 4.600e-04  Data: 7.039 (1.883)
Train: 2 [ 950/1171 ( 81%)]  Loss:  5.745605 (5.4902)  Time: 0.585s, 1749.75/s  (2.476s,  413.61/s)  LR: 4.600e-04  Data: 0.020 (1.872)
Train: 2 [1000/1171 ( 85%)]  Loss:  5.195324 (5.4761)  Time: 10.375s,   98.70/s  (2.474s,  413.89/s)  LR: 4.600e-04  Data: 9.676 (1.870)
Train: 2 [1050/1171 ( 90%)]  Loss:  5.356312 (5.4707)  Time: 0.588s, 1740.96/s  (2.484s,  412.19/s)  LR: 4.600e-04  Data: 0.022 (1.880)
Train: 2 [1100/1171 ( 94%)]  Loss:  5.769392 (5.4837)  Time: 7.339s,  139.54/s  (2.496s,  410.22/s)  LR: 4.600e-04  Data: 6.705 (1.892)
Train: 2 [1150/1171 ( 98%)]  Loss:  5.291459 (5.4757)  Time: 0.587s, 1743.23/s  (2.489s,  411.45/s)  LR: 4.600e-04  Data: 0.022 (1.885)
Train: 2 [1170/1171 (100%)]  Loss:  5.555055 (5.4789)  Time: 0.564s, 1814.21/s  (2.484s,  412.28/s)  LR: 4.600e-04  Data: 0.000 (1.880)
Test: [   0/97]  Time: 12.878 (12.878)  Loss:  1.7825 (1.7825)  Acc@1: 57.2266 (57.2266)  Acc@5: 92.5781 (92.5781)
Test: [  50/97]  Time: 0.197 (3.029)  Loss:  3.5097 (2.8501)  Acc@1: 32.6172 (44.0142)  Acc@5: 55.6641 (71.4461)
Test: [  97/97]  Time: 0.119 (2.961)  Loss:  1.7106 (2.8266)  Acc@1: 73.8095 (45.8670)  Acc@5: 86.4583 (71.6680)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-2.pth.tar', 45.86700000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-1.pth.tar', 31.120000002441405)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-0.pth.tar', 10.033000006103515)

Train: 3 [   0/1171 (  0%)]  Loss:  5.561818 (5.5618)  Time: 10.303s,   99.39/s  (10.303s,   99.39/s)  LR: 6.400e-04  Data: 9.400 (9.400)
Train: 3 [  50/1171 (  4%)]  Loss:  5.484517 (5.5232)  Time: 0.958s, 1069.29/s  (2.268s,  451.57/s)  LR: 6.400e-04  Data: 0.395 (1.652)
Train: 3 [ 100/1171 (  9%)]  Loss:  5.274949 (5.4404)  Time: 0.588s, 1742.36/s  (2.462s,  415.94/s)  LR: 6.400e-04  Data: 0.019 (1.850)
Train: 3 [ 150/1171 ( 13%)]  Loss:  5.238850 (5.3900)  Time: 0.865s, 1183.80/s  (2.435s,  420.53/s)  LR: 6.400e-04  Data: 0.300 (1.829)
Train: 3 [ 200/1171 ( 17%)]  Loss:  4.765035 (5.2650)  Time: 0.583s, 1755.96/s  (2.459s,  416.38/s)  LR: 6.400e-04  Data: 0.020 (1.855)
Train: 3 [ 250/1171 ( 21%)]  Loss:  5.150106 (5.2459)  Time: 0.586s, 1746.94/s  (2.414s,  424.15/s)  LR: 6.400e-04  Data: 0.020 (1.814)
Train: 3 [ 300/1171 ( 26%)]  Loss:  5.433914 (5.2727)  Time: 0.586s, 1748.84/s  (2.413s,  424.32/s)  LR: 6.400e-04  Data: 0.022 (1.814)
Train: 3 [ 350/1171 ( 30%)]  Loss:  5.627487 (5.3171)  Time: 3.489s,  293.52/s  (2.394s,  427.70/s)  LR: 6.400e-04  Data: 2.927 (1.794)
Train: 3 [ 400/1171 ( 34%)]  Loss:  5.181347 (5.3020)  Time: 5.078s,  201.64/s  (2.381s,  430.08/s)  LR: 6.400e-04  Data: 4.468 (1.781)
Train: 3 [ 450/1171 ( 38%)]  Loss:  5.516494 (5.3235)  Time: 2.232s,  458.73/s  (2.359s,  434.01/s)  LR: 6.400e-04  Data: 1.643 (1.760)
Train: 3 [ 500/1171 ( 43%)]  Loss:  4.924669 (5.2872)  Time: 6.050s,  169.26/s  (2.419s,  423.32/s)  LR: 6.400e-04  Data: 5.478 (1.819)
Train: 3 [ 550/1171 ( 47%)]  Loss:  4.700575 (5.2383)  Time: 0.590s, 1735.25/s  (2.440s,  419.76/s)  LR: 6.400e-04  Data: 0.020 (1.839)
Train: 3 [ 600/1171 ( 51%)]  Loss:  4.697307 (5.1967)  Time: 8.683s,  117.94/s  (2.458s,  416.55/s)  LR: 6.400e-04  Data: 8.109 (1.860)
Train: 3 [ 650/1171 ( 56%)]  Loss:  5.479001 (5.2169)  Time: 0.587s, 1744.26/s  (2.460s,  416.27/s)  LR: 6.400e-04  Data: 0.018 (1.862)
Train: 3 [ 700/1171 ( 60%)]  Loss:  4.948977 (5.1990)  Time: 4.126s,  248.18/s  (2.450s,  417.94/s)  LR: 6.400e-04  Data: 3.430 (1.853)
Train: 3 [ 750/1171 ( 64%)]  Loss:  5.198629 (5.1990)  Time: 0.589s, 1737.93/s  (2.432s,  421.02/s)  LR: 6.400e-04  Data: 0.025 (1.836)
Train: 3 [ 800/1171 ( 68%)]  Loss:  5.380104 (5.2096)  Time: 1.101s,  929.95/s  (2.420s,  423.22/s)  LR: 6.400e-04  Data: 0.454 (1.821)
Train: 3 [ 850/1171 ( 73%)]  Loss:  5.324304 (5.2160)  Time: 0.586s, 1747.34/s  (2.440s,  419.66/s)  LR: 6.400e-04  Data: 0.023 (1.839)
Train: 3 [ 900/1171 ( 77%)]  Loss:  5.033876 (5.2064)  Time: 5.738s,  178.45/s  (2.450s,  417.90/s)  LR: 6.400e-04  Data: 5.006 (1.849)
Train: 3 [ 950/1171 ( 81%)]  Loss:  4.841404 (5.1882)  Time: 1.970s,  519.72/s  (2.452s,  417.62/s)  LR: 6.400e-04  Data: 1.317 (1.850)
Train: 3 [1000/1171 ( 85%)]  Loss:  4.914242 (5.1751)  Time: 7.725s,  132.56/s  (2.461s,  416.17/s)  LR: 6.400e-04  Data: 7.158 (1.856)
Train: 3 [1050/1171 ( 90%)]  Loss:  5.233346 (5.1778)  Time: 3.634s,  281.76/s  (2.461s,  416.09/s)  LR: 6.400e-04  Data: 3.070 (1.855)
Train: 3 [1100/1171 ( 94%)]  Loss:  4.961378 (5.1684)  Time: 4.390s,  233.25/s  (2.460s,  416.31/s)  LR: 6.400e-04  Data: 3.730 (1.855)
Train: 3 [1150/1171 ( 98%)]  Loss:  5.105594 (5.1657)  Time: 4.319s,  237.10/s  (2.456s,  416.91/s)  LR: 6.400e-04  Data: 3.712 (1.851)
Train: 3 [1170/1171 (100%)]  Loss:  5.145111 (5.1649)  Time: 0.564s, 1814.34/s  (2.455s,  417.06/s)  LR: 6.400e-04  Data: 0.000 (1.849)
Test: [   0/97]  Time: 22.044 (22.044)  Loss:  1.3009 (1.3009)  Acc@1: 71.3867 (71.3867)  Acc@5: 94.5312 (94.5312)
Test: [  50/97]  Time: 0.198 (3.636)  Loss:  2.7547 (2.0577)  Acc@1: 44.1406 (58.1438)  Acc@5: 66.4062 (83.2491)
Test: [  97/97]  Time: 0.119 (3.426)  Loss:  1.2168 (2.0742)  Acc@1: 76.0417 (58.8630)  Acc@5: 88.5417 (82.1250)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-3.pth.tar', 58.862999982910154)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-2.pth.tar', 45.86700000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-1.pth.tar', 31.120000002441405)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-0.pth.tar', 10.033000006103515)

Train: 4 [   0/1171 (  0%)]  Loss:  4.755404 (4.7554)  Time: 12.756s,   80.28/s  (12.756s,   80.28/s)  LR: 8.200e-04  Data: 11.824 (11.824)
Train: 4 [  50/1171 (  4%)]  Loss:  4.993052 (4.8742)  Time: 0.585s, 1749.78/s  (2.546s,  402.17/s)  LR: 8.200e-04  Data: 0.020 (1.954)
Train: 4 [ 100/1171 (  9%)]  Loss:  4.895602 (4.8814)  Time: 0.583s, 1755.25/s  (2.456s,  416.95/s)  LR: 8.200e-04  Data: 0.018 (1.869)
Train: 4 [ 150/1171 ( 13%)]  Loss:  4.630402 (4.8186)  Time: 0.584s, 1754.84/s  (2.380s,  430.25/s)  LR: 8.200e-04  Data: 0.019 (1.794)
Train: 4 [ 200/1171 ( 17%)]  Loss:  5.410411 (4.9370)  Time: 0.584s, 1753.04/s  (2.353s,  435.21/s)  LR: 8.200e-04  Data: 0.018 (1.767)
Train: 4 [ 250/1171 ( 21%)]  Loss:  4.998595 (4.9472)  Time: 2.295s,  446.11/s  (2.373s,  431.55/s)  LR: 8.200e-04  Data: 1.649 (1.786)
Train: 4 [ 300/1171 ( 26%)]  Loss:  5.137310 (4.9744)  Time: 0.584s, 1753.33/s  (2.401s,  426.57/s)  LR: 8.200e-04  Data: 0.020 (1.812)
Train: 4 [ 350/1171 ( 30%)]  Loss:  4.622805 (4.9304)  Time: 0.585s, 1750.97/s  (2.413s,  424.36/s)  LR: 8.200e-04  Data: 0.022 (1.815)
Train: 4 [ 400/1171 ( 34%)]  Loss:  5.443271 (4.9874)  Time: 0.584s, 1752.19/s  (2.443s,  419.14/s)  LR: 8.200e-04  Data: 0.018 (1.846)
Train: 4 [ 450/1171 ( 38%)]  Loss:  5.050932 (4.9938)  Time: 0.592s, 1729.43/s  (2.431s,  421.29/s)  LR: 8.200e-04  Data: 0.021 (1.833)
Train: 4 [ 500/1171 ( 43%)]  Loss:  4.795456 (4.9757)  Time: 0.585s, 1751.81/s  (2.436s,  420.30/s)  LR: 8.200e-04  Data: 0.022 (1.841)
Train: 4 [ 550/1171 ( 47%)]  Loss:  4.952599 (4.9738)  Time: 0.588s, 1742.11/s  (2.433s,  420.89/s)  LR: 8.200e-04  Data: 0.021 (1.839)
Train: 4 [ 600/1171 ( 51%)]  Loss:  5.564761 (5.0193)  Time: 0.584s, 1752.52/s  (2.446s,  418.71/s)  LR: 8.200e-04  Data: 0.019 (1.850)
Train: 4 [ 650/1171 ( 56%)]  Loss:  5.281929 (5.0380)  Time: 0.585s, 1751.16/s  (2.460s,  416.19/s)  LR: 8.200e-04  Data: 0.020 (1.866)
Train: 4 [ 700/1171 ( 60%)]  Loss:  4.905364 (5.0292)  Time: 0.585s, 1751.33/s  (2.465s,  415.44/s)  LR: 8.200e-04  Data: 0.022 (1.870)
Train: 4 [ 750/1171 ( 64%)]  Loss:  4.635870 (5.0046)  Time: 0.586s, 1748.22/s  (2.459s,  416.39/s)  LR: 8.200e-04  Data: 0.017 (1.864)
Train: 4 [ 800/1171 ( 68%)]  Loss:  4.957640 (5.0018)  Time: 0.585s, 1750.00/s  (2.457s,  416.69/s)  LR: 8.200e-04  Data: 0.020 (1.863)
Train: 4 [ 850/1171 ( 73%)]  Loss:  4.751638 (4.9879)  Time: 0.585s, 1750.39/s  (2.444s,  418.97/s)  LR: 8.200e-04  Data: 0.018 (1.849)
Train: 4 [ 900/1171 ( 77%)]  Loss:  4.954713 (4.9862)  Time: 0.583s, 1756.78/s  (2.438s,  419.99/s)  LR: 8.200e-04  Data: 0.021 (1.844)
Train: 4 [ 950/1171 ( 81%)]  Loss:  5.168650 (4.9953)  Time: 0.586s, 1746.08/s  (2.436s,  420.41/s)  LR: 8.200e-04  Data: 0.018 (1.842)
Train: 4 [1000/1171 ( 85%)]  Loss:  4.514307 (4.9724)  Time: 0.585s, 1749.93/s  (2.468s,  414.83/s)  LR: 8.200e-04  Data: 0.022 (1.875)
Train: 4 [1050/1171 ( 90%)]  Loss:  3.869763 (4.9223)  Time: 0.587s, 1745.14/s  (2.495s,  410.47/s)  LR: 8.200e-04  Data: 0.020 (1.900)
Train: 4 [1100/1171 ( 94%)]  Loss:  5.016183 (4.9264)  Time: 0.589s, 1739.93/s  (2.517s,  406.82/s)  LR: 8.200e-04  Data: 0.022 (1.922)
Train: 4 [1150/1171 ( 98%)]  Loss:  4.967010 (4.9281)  Time: 0.586s, 1747.69/s  (2.524s,  405.74/s)  LR: 8.200e-04  Data: 0.017 (1.930)
Train: 4 [1170/1171 (100%)]  Loss:  4.589259 (4.9145)  Time: 0.564s, 1814.02/s  (2.527s,  405.25/s)  LR: 8.200e-04  Data: 0.000 (1.933)
Test: [   0/97]  Time: 16.025 (16.025)  Loss:  0.9691 (0.9691)  Acc@1: 75.0977 (75.0977)  Acc@5: 97.4609 (97.4609)
Test: [  50/97]  Time: 0.197 (3.331)  Loss:  2.1738 (1.5938)  Acc@1: 54.7852 (67.1262)  Acc@5: 80.7617 (88.8997)
Test: [  97/97]  Time: 0.120 (3.347)  Loss:  0.9091 (1.6237)  Acc@1: 82.2917 (67.5730)  Acc@5: 93.6012 (87.8100)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-4.pth.tar', 67.57300003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-3.pth.tar', 58.862999982910154)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-2.pth.tar', 45.86700000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-1.pth.tar', 31.120000002441405)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-0.pth.tar', 10.033000006103515)

Train: 5 [   0/1171 (  0%)]  Loss:  4.590608 (4.5906)  Time: 16.343s,   62.66/s  (16.343s,   62.66/s)  LR: 8.791e-04  Data: 15.615 (15.615)
Train: 5 [  50/1171 (  4%)]  Loss:  4.356974 (4.4738)  Time: 0.580s, 1765.63/s  (2.823s,  362.74/s)  LR: 8.791e-04  Data: 0.017 (2.234)
Train: 5 [ 100/1171 (  9%)]  Loss:  4.927031 (4.6249)  Time: 0.587s, 1745.18/s  (2.731s,  374.94/s)  LR: 8.791e-04  Data: 0.019 (2.145)
Train: 5 [ 150/1171 ( 13%)]  Loss:  4.848847 (4.6809)  Time: 0.593s, 1726.57/s  (2.675s,  382.80/s)  LR: 8.791e-04  Data: 0.020 (2.086)
Train: 5 [ 200/1171 ( 17%)]  Loss:  4.209504 (4.5866)  Time: 0.585s, 1751.59/s  (2.644s,  387.29/s)  LR: 8.791e-04  Data: 0.018 (2.058)
Train: 5 [ 250/1171 ( 21%)]  Loss:  4.377076 (4.5517)  Time: 0.586s, 1746.54/s  (2.572s,  398.15/s)  LR: 8.791e-04  Data: 0.021 (1.986)
Train: 5 [ 300/1171 ( 26%)]  Loss:  5.303564 (4.6591)  Time: 0.586s, 1748.61/s  (2.529s,  404.95/s)  LR: 8.791e-04  Data: 0.018 (1.943)
Train: 5 [ 350/1171 ( 30%)]  Loss:  5.388626 (4.7503)  Time: 0.585s, 1749.47/s  (2.495s,  410.47/s)  LR: 8.791e-04  Data: 0.018 (1.907)
Train: 5 [ 400/1171 ( 34%)]  Loss:  4.690987 (4.7437)  Time: 0.586s, 1748.41/s  (2.559s,  400.19/s)  LR: 8.791e-04  Data: 0.018 (1.971)
Train: 5 [ 450/1171 ( 38%)]  Loss:  5.081579 (4.7775)  Time: 0.589s, 1738.55/s  (2.554s,  401.01/s)  LR: 8.791e-04  Data: 0.017 (1.965)
Train: 5 [ 500/1171 ( 43%)]  Loss:  4.437886 (4.7466)  Time: 2.548s,  401.93/s  (2.575s,  397.72/s)  LR: 8.791e-04  Data: 1.885 (1.985)
Train: 5 [ 550/1171 ( 47%)]  Loss:  4.466957 (4.7233)  Time: 0.585s, 1751.20/s  (2.574s,  397.90/s)  LR: 8.791e-04  Data: 0.017 (1.982)
Train: 5 [ 600/1171 ( 51%)]  Loss:  4.704763 (4.7219)  Time: 2.485s,  412.00/s  (2.613s,  391.82/s)  LR: 8.791e-04  Data: 1.908 (2.014)
Train: 5 [ 650/1171 ( 56%)]  Loss:  4.730429 (4.7225)  Time: 0.586s, 1746.02/s  (2.621s,  390.73/s)  LR: 8.791e-04  Data: 0.017 (2.021)
Train: 5 [ 700/1171 ( 60%)]  Loss:  4.253255 (4.6912)  Time: 5.005s,  204.59/s  (2.670s,  383.48/s)  LR: 8.791e-04  Data: 4.310 (2.069)
Train: 5 [ 750/1171 ( 64%)]  Loss:  4.366389 (4.6709)  Time: 0.587s, 1744.98/s  (2.715s,  377.17/s)  LR: 8.791e-04  Data: 0.022 (2.099)
Train: 5 [ 800/1171 ( 68%)]  Loss:  4.887103 (4.6836)  Time: 0.584s, 1754.10/s  (2.723s,  376.08/s)  LR: 8.791e-04  Data: 0.019 (2.107)
Train: 5 [ 850/1171 ( 73%)]  Loss:  4.882502 (4.6947)  Time: 2.152s,  475.86/s  (2.721s,  376.33/s)  LR: 8.791e-04  Data: 1.582 (2.106)
Train: 5 [ 900/1171 ( 77%)]  Loss:  4.167103 (4.6669)  Time: 0.587s, 1744.42/s  (2.703s,  378.85/s)  LR: 8.791e-04  Data: 0.021 (2.088)
Train: 5 [ 950/1171 ( 81%)]  Loss:  4.788695 (4.6730)  Time: 0.646s, 1586.12/s  (2.692s,  380.46/s)  LR: 8.791e-04  Data: 0.019 (2.076)
Train: 5 [1000/1171 ( 85%)]  Loss:  4.862228 (4.6820)  Time: 0.586s, 1746.61/s  (2.690s,  380.68/s)  LR: 8.791e-04  Data: 0.020 (2.075)
Train: 5 [1050/1171 ( 90%)]  Loss:  4.911164 (4.6924)  Time: 0.913s, 1121.25/s  (2.690s,  380.66/s)  LR: 8.791e-04  Data: 0.351 (2.076)
Train: 5 [1100/1171 ( 94%)]  Loss:  4.842646 (4.6990)  Time: 0.586s, 1748.20/s  (2.691s,  380.56/s)  LR: 8.791e-04  Data: 0.022 (2.077)
Train: 5 [1150/1171 ( 98%)]  Loss:  4.841119 (4.7049)  Time: 0.586s, 1746.61/s  (2.683s,  381.64/s)  LR: 8.791e-04  Data: 0.019 (2.069)
Train: 5 [1170/1171 (100%)]  Loss:  5.131783 (4.7220)  Time: 0.565s, 1813.43/s  (2.680s,  382.12/s)  LR: 8.791e-04  Data: 0.000 (2.066)
Test: [   0/97]  Time: 13.643 (13.643)  Loss:  0.8831 (0.8831)  Acc@1: 77.5391 (77.5391)  Acc@5: 97.1680 (97.1680)
Test: [  50/97]  Time: 0.197 (3.300)  Loss:  1.8962 (1.3069)  Acc@1: 59.9609 (73.0124)  Acc@5: 85.4492 (92.2660)
Test: [  97/97]  Time: 0.119 (3.187)  Loss:  0.8889 (1.3483)  Acc@1: 83.1845 (72.9940)  Acc@5: 94.1964 (90.8830)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-5.pth.tar', 72.9940000048828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-4.pth.tar', 67.57300003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-3.pth.tar', 58.862999982910154)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-2.pth.tar', 45.86700000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-1.pth.tar', 31.120000002441405)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-0.pth.tar', 10.033000006103515)

Train: 6 [   0/1171 (  0%)]  Loss:  5.199102 (5.1991)  Time: 11.429s,   89.59/s  (11.429s,   89.59/s)  LR: 8.292e-04  Data: 10.521 (10.521)
Train: 6 [  50/1171 (  4%)]  Loss:  4.341620 (4.7704)  Time: 0.586s, 1746.22/s  (2.576s,  397.45/s)  LR: 8.292e-04  Data: 0.020 (1.994)
Train: 6 [ 100/1171 (  9%)]  Loss:  4.646311 (4.7290)  Time: 0.587s, 1742.98/s  (2.588s,  395.66/s)  LR: 8.292e-04  Data: 0.020 (1.986)
Train: 6 [ 150/1171 ( 13%)]  Loss:  4.984395 (4.7929)  Time: 1.068s,  958.91/s  (2.591s,  395.27/s)  LR: 8.292e-04  Data: 0.340 (1.988)
Train: 6 [ 200/1171 ( 17%)]  Loss:  4.450381 (4.7244)  Time: 0.587s, 1744.46/s  (2.593s,  394.95/s)  LR: 8.292e-04  Data: 0.021 (1.984)
Train: 6 [ 250/1171 ( 21%)]  Loss:  4.612003 (4.7056)  Time: 1.459s,  701.87/s  (2.548s,  401.91/s)  LR: 8.292e-04  Data: 0.761 (1.937)
Train: 6 [ 300/1171 ( 26%)]  Loss:  3.859519 (4.5848)  Time: 0.586s, 1747.09/s  (2.511s,  407.86/s)  LR: 8.292e-04  Data: 0.022 (1.902)
Train: 6 [ 350/1171 ( 30%)]  Loss:  4.103662 (4.5246)  Time: 2.290s,  447.09/s  (2.472s,  414.20/s)  LR: 8.292e-04  Data: 1.626 (1.863)
Train: 6 [ 400/1171 ( 34%)]  Loss:  4.333393 (4.5034)  Time: 1.420s,  721.31/s  (2.438s,  419.94/s)  LR: 8.292e-04  Data: 0.823 (1.828)
Train: 6 [ 450/1171 ( 38%)]  Loss:  4.739779 (4.5270)  Time: 1.949s,  525.44/s  (2.466s,  415.21/s)  LR: 8.292e-04  Data: 1.280 (1.854)
Train: 6 [ 500/1171 ( 43%)]  Loss:  4.520576 (4.5264)  Time: 0.584s, 1753.66/s  (2.492s,  410.95/s)  LR: 8.292e-04  Data: 0.020 (1.879)
Train: 6 [ 550/1171 ( 47%)]  Loss:  4.487427 (4.5232)  Time: 2.140s,  478.46/s  (2.506s,  408.54/s)  LR: 8.292e-04  Data: 1.444 (1.894)
Train: 6 [ 600/1171 ( 51%)]  Loss:  4.804591 (4.5448)  Time: 0.584s, 1753.99/s  (2.517s,  406.89/s)  LR: 8.292e-04  Data: 0.018 (1.904)
Train: 6 [ 650/1171 ( 56%)]  Loss:  4.271487 (4.5253)  Time: 4.375s,  234.08/s  (2.519s,  406.49/s)  LR: 8.292e-04  Data: 3.704 (1.908)
Train: 6 [ 700/1171 ( 60%)]  Loss:  4.132405 (4.4991)  Time: 0.589s, 1738.97/s  (2.515s,  407.20/s)  LR: 8.292e-04  Data: 0.021 (1.904)
Train: 6 [ 750/1171 ( 64%)]  Loss:  4.379375 (4.4916)  Time: 8.097s,  126.47/s  (2.505s,  408.75/s)  LR: 8.292e-04  Data: 7.410 (1.896)
Train: 6 [ 800/1171 ( 68%)]  Loss:  4.394500 (4.4859)  Time: 0.586s, 1748.90/s  (2.521s,  406.13/s)  LR: 8.292e-04  Data: 0.020 (1.912)
Train: 6 [ 850/1171 ( 73%)]  Loss:  4.243844 (4.4725)  Time: 6.504s,  157.44/s  (2.529s,  404.95/s)  LR: 8.292e-04  Data: 5.819 (1.919)
Train: 6 [ 900/1171 ( 77%)]  Loss:  4.654849 (4.4821)  Time: 0.585s, 1750.10/s  (2.526s,  405.43/s)  LR: 8.292e-04  Data: 0.020 (1.916)
Train: 6 [ 950/1171 ( 81%)]  Loss:  4.827381 (4.4993)  Time: 4.893s,  209.28/s  (2.517s,  406.82/s)  LR: 8.292e-04  Data: 4.232 (1.908)
Train: 6 [1000/1171 ( 85%)]  Loss:  3.720957 (4.4623)  Time: 0.587s, 1743.54/s  (2.503s,  409.19/s)  LR: 8.292e-04  Data: 0.017 (1.894)
Train: 6 [1050/1171 ( 90%)]  Loss:  4.474217 (4.4628)  Time: 2.403s,  426.09/s  (2.481s,  412.73/s)  LR: 8.292e-04  Data: 1.714 (1.873)
Train: 6 [1100/1171 ( 94%)]  Loss:  4.412088 (4.4606)  Time: 0.590s, 1736.11/s  (2.466s,  415.18/s)  LR: 8.292e-04  Data: 0.017 (1.859)
Train: 6 [1150/1171 ( 98%)]  Loss:  4.165238 (4.4483)  Time: 7.335s,  139.61/s  (2.459s,  416.48/s)  LR: 8.292e-04  Data: 6.640 (1.852)
Train: 6 [1170/1171 (100%)]  Loss:  4.730709 (4.4596)  Time: 0.564s, 1814.79/s  (2.467s,  415.08/s)  LR: 8.292e-04  Data: 0.000 (1.860)
Test: [   0/97]  Time: 14.122 (14.122)  Loss:  0.6761 (0.6761)  Acc@1: 84.1797 (84.1797)  Acc@5: 98.7305 (98.7305)
Test: [  50/97]  Time: 0.197 (3.410)  Loss:  1.6067 (1.1130)  Acc@1: 65.3320 (78.1441)  Acc@5: 87.8906 (94.1674)
Test: [  97/97]  Time: 0.120 (3.259)  Loss:  0.6466 (1.1395)  Acc@1: 86.3095 (77.8700)  Acc@5: 96.5774 (93.0760)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-6.pth.tar', 77.87000000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-5.pth.tar', 72.9940000048828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-4.pth.tar', 67.57300003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-3.pth.tar', 58.862999982910154)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-2.pth.tar', 45.86700000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-1.pth.tar', 31.120000002441405)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-0.pth.tar', 10.033000006103515)

Train: 7 [   0/1171 (  0%)]  Loss:  4.771920 (4.7719)  Time: 10.728s,   95.45/s  (10.728s,   95.45/s)  LR: 7.726e-04  Data: 10.087 (10.087)
Train: 7 [  50/1171 (  4%)]  Loss:  4.309647 (4.5408)  Time: 0.584s, 1752.76/s  (2.476s,  413.50/s)  LR: 7.726e-04  Data: 0.019 (1.880)
Train: 7 [ 100/1171 (  9%)]  Loss:  4.055595 (4.3791)  Time: 0.589s, 1739.36/s  (2.396s,  427.41/s)  LR: 7.726e-04  Data: 0.018 (1.802)
Train: 7 [ 150/1171 ( 13%)]  Loss:  3.574864 (4.1780)  Time: 0.656s, 1561.09/s  (2.291s,  446.93/s)  LR: 7.726e-04  Data: 0.032 (1.695)
Train: 7 [ 200/1171 ( 17%)]  Loss:  4.701721 (4.2827)  Time: 0.585s, 1750.67/s  (2.281s,  448.88/s)  LR: 7.726e-04  Data: 0.017 (1.686)
Train: 7 [ 250/1171 ( 21%)]  Loss:  4.142895 (4.2594)  Time: 0.586s, 1746.07/s  (2.353s,  435.16/s)  LR: 7.726e-04  Data: 0.020 (1.756)
Train: 7 [ 300/1171 ( 26%)]  Loss:  4.828551 (4.3407)  Time: 1.740s,  588.51/s  (2.398s,  426.97/s)  LR: 7.726e-04  Data: 1.171 (1.798)
Train: 7 [ 350/1171 ( 30%)]  Loss:  4.956213 (4.4177)  Time: 0.584s, 1753.78/s  (2.403s,  426.17/s)  LR: 7.726e-04  Data: 0.021 (1.801)
Train: 7 [ 400/1171 ( 34%)]  Loss:  3.926732 (4.3631)  Time: 3.813s,  268.57/s  (2.426s,  422.01/s)  LR: 7.726e-04  Data: 3.121 (1.825)
Train: 7 [ 450/1171 ( 38%)]  Loss:  4.443923 (4.3712)  Time: 0.588s, 1741.51/s  (2.421s,  422.99/s)  LR: 7.726e-04  Data: 0.020 (1.820)
Train: 7 [ 500/1171 ( 43%)]  Loss:  4.569516 (4.3892)  Time: 1.811s,  565.33/s  (2.419s,  423.27/s)  LR: 7.726e-04  Data: 1.250 (1.819)
Train: 7 [ 550/1171 ( 47%)]  Loss:  4.370392 (4.3877)  Time: 0.585s, 1750.94/s  (2.418s,  423.49/s)  LR: 7.726e-04  Data: 0.019 (1.816)
Train: 7 [ 600/1171 ( 51%)]  Loss:  4.405109 (4.3890)  Time: 4.825s,  212.23/s  (2.467s,  415.11/s)  LR: 7.726e-04  Data: 4.142 (1.863)
Train: 7 [ 650/1171 ( 56%)]  Loss:  4.670334 (4.4091)  Time: 0.588s, 1740.95/s  (2.470s,  414.60/s)  LR: 7.726e-04  Data: 0.022 (1.865)
Train: 7 [ 700/1171 ( 60%)]  Loss:  4.101084 (4.3886)  Time: 4.867s,  210.40/s  (2.484s,  412.26/s)  LR: 7.726e-04  Data: 4.271 (1.878)
Train: 7 [ 750/1171 ( 64%)]  Loss:  4.261613 (4.3806)  Time: 1.661s,  616.31/s  (2.479s,  413.03/s)  LR: 7.726e-04  Data: 1.101 (1.874)
Train: 7 [ 800/1171 ( 68%)]  Loss:  4.189441 (4.3694)  Time: 3.658s,  279.90/s  (2.478s,  413.26/s)  LR: 7.726e-04  Data: 2.686 (1.871)
Train: 7 [ 850/1171 ( 73%)]  Loss:  4.420118 (4.3722)  Time: 0.585s, 1751.24/s  (2.471s,  414.33/s)  LR: 7.726e-04  Data: 0.019 (1.866)
Train: 7 [ 900/1171 ( 77%)]  Loss:  4.059123 (4.3557)  Time: 0.585s, 1750.56/s  (2.465s,  415.37/s)  LR: 7.726e-04  Data: 0.018 (1.861)
Train: 7 [ 950/1171 ( 81%)]  Loss:  4.527593 (4.3643)  Time: 0.585s, 1750.16/s  (2.479s,  413.11/s)  LR: 7.726e-04  Data: 0.021 (1.875)
Train: 7 [1000/1171 ( 85%)]  Loss:  4.336872 (4.3630)  Time: 0.583s, 1756.79/s  (2.477s,  413.44/s)  LR: 7.726e-04  Data: 0.018 (1.873)
Train: 7 [1050/1171 ( 90%)]  Loss:  4.245177 (4.3577)  Time: 0.586s, 1748.08/s  (2.486s,  411.96/s)  LR: 7.726e-04  Data: 0.021 (1.883)
Train: 7 [1100/1171 ( 94%)]  Loss:  4.497397 (4.3637)  Time: 0.856s, 1196.47/s  (2.485s,  412.02/s)  LR: 7.726e-04  Data: 0.020 (1.883)
Train: 7 [1150/1171 ( 98%)]  Loss:  3.912830 (4.3449)  Time: 0.583s, 1755.42/s  (2.487s,  411.76/s)  LR: 7.726e-04  Data: 0.018 (1.885)
Train: 7 [1170/1171 (100%)]  Loss:  4.502744 (4.3513)  Time: 0.565s, 1812.89/s  (2.486s,  411.96/s)  LR: 7.726e-04  Data: 0.000 (1.884)
Test: [   0/97]  Time: 14.223 (14.223)  Loss:  0.6989 (0.6989)  Acc@1: 85.8398 (85.8398)  Acc@5: 98.2422 (98.2422)
Test: [  50/97]  Time: 0.195 (3.198)  Loss:  1.3862 (1.0248)  Acc@1: 70.5078 (80.5128)  Acc@5: 91.2109 (95.1536)
Test: [  97/97]  Time: 0.120 (3.203)  Loss:  0.6086 (1.0396)  Acc@1: 89.8810 (80.3920)  Acc@5: 96.4286 (94.2380)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-7.pth.tar', 80.3920000415039)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-6.pth.tar', 77.87000000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-5.pth.tar', 72.9940000048828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-4.pth.tar', 67.57300003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-3.pth.tar', 58.862999982910154)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-2.pth.tar', 45.86700000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-1.pth.tar', 31.120000002441405)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-0.pth.tar', 10.033000006103515)

Train: 8 [   0/1171 (  0%)]  Loss:  3.733582 (3.7336)  Time: 17.794s,   57.55/s  (17.794s,   57.55/s)  LR: 7.106e-04  Data: 16.945 (16.945)
Train: 8 [  50/1171 (  4%)]  Loss:  3.951549 (3.8426)  Time: 0.586s, 1748.12/s  (2.789s,  367.17/s)  LR: 7.106e-04  Data: 0.021 (2.207)
Train: 8 [ 100/1171 (  9%)]  Loss:  3.828068 (3.8377)  Time: 0.586s, 1747.17/s  (2.669s,  383.61/s)  LR: 7.106e-04  Data: 0.019 (2.078)
Train: 8 [ 150/1171 ( 13%)]  Loss:  4.257762 (3.9427)  Time: 0.588s, 1740.57/s  (2.526s,  405.40/s)  LR: 7.106e-04  Data: 0.021 (1.938)
Train: 8 [ 200/1171 ( 17%)]  Loss:  4.227528 (3.9997)  Time: 0.584s, 1752.39/s  (2.461s,  416.02/s)  LR: 7.106e-04  Data: 0.021 (1.871)
Train: 8 [ 250/1171 ( 21%)]  Loss:  4.498348 (4.0828)  Time: 0.588s, 1742.30/s  (2.414s,  424.24/s)  LR: 7.106e-04  Data: 0.020 (1.819)
Train: 8 [ 300/1171 ( 26%)]  Loss:  3.948132 (4.0636)  Time: 0.584s, 1753.42/s  (2.393s,  427.96/s)  LR: 7.106e-04  Data: 0.018 (1.797)
Train: 8 [ 350/1171 ( 30%)]  Loss:  3.915377 (4.0450)  Time: 0.588s, 1740.26/s  (2.354s,  435.04/s)  LR: 7.106e-04  Data: 0.020 (1.760)
Train: 8 [ 400/1171 ( 34%)]  Loss:  3.560202 (3.9912)  Time: 0.588s, 1741.45/s  (2.404s,  425.94/s)  LR: 7.106e-04  Data: 0.020 (1.807)
Train: 8 [ 450/1171 ( 38%)]  Loss:  4.020924 (3.9941)  Time: 0.589s, 1737.91/s  (2.421s,  423.05/s)  LR: 7.106e-04  Data: 0.021 (1.820)
Train: 8 [ 500/1171 ( 43%)]  Loss:  4.322182 (4.0240)  Time: 0.584s, 1754.02/s  (2.449s,  418.07/s)  LR: 7.106e-04  Data: 0.019 (1.849)
Train: 8 [ 550/1171 ( 47%)]  Loss:  4.236130 (4.0416)  Time: 0.587s, 1745.90/s  (2.453s,  417.45/s)  LR: 7.106e-04  Data: 0.019 (1.854)
Train: 8 [ 600/1171 ( 51%)]  Loss:  4.168622 (4.0514)  Time: 0.587s, 1745.14/s  (2.463s,  415.83/s)  LR: 7.106e-04  Data: 0.022 (1.865)
Train: 8 [ 650/1171 ( 56%)]  Loss:  4.555980 (4.0875)  Time: 0.586s, 1747.81/s  (2.456s,  416.90/s)  LR: 7.106e-04  Data: 0.021 (1.858)
Train: 8 [ 700/1171 ( 60%)]  Loss:  4.277016 (4.1001)  Time: 0.585s, 1750.73/s  (2.451s,  417.72/s)  LR: 7.106e-04  Data: 0.018 (1.853)
Train: 8 [ 750/1171 ( 64%)]  Loss:  4.282429 (4.1115)  Time: 2.554s,  400.99/s  (2.462s,  415.96/s)  LR: 7.106e-04  Data: 1.640 (1.864)
Train: 8 [ 800/1171 ( 68%)]  Loss:  4.125921 (4.1123)  Time: 0.583s, 1756.40/s  (2.470s,  414.51/s)  LR: 7.106e-04  Data: 0.020 (1.870)
Train: 8 [ 850/1171 ( 73%)]  Loss:  4.568173 (4.1377)  Time: 0.585s, 1749.77/s  (2.469s,  414.73/s)  LR: 7.106e-04  Data: 0.021 (1.867)
Train: 8 [ 900/1171 ( 77%)]  Loss:  3.938547 (4.1272)  Time: 0.583s, 1755.55/s  (2.470s,  414.52/s)  LR: 7.106e-04  Data: 0.017 (1.867)
Train: 8 [ 950/1171 ( 81%)]  Loss:  4.264742 (4.1341)  Time: 0.587s, 1743.82/s  (2.467s,  415.03/s)  LR: 7.106e-04  Data: 0.020 (1.863)
Train: 8 [1000/1171 ( 85%)]  Loss:  4.720210 (4.1620)  Time: 1.163s,  880.36/s  (2.465s,  415.35/s)  LR: 7.106e-04  Data: 0.495 (1.860)
Train: 8 [1050/1171 ( 90%)]  Loss:  3.612961 (4.1370)  Time: 0.583s, 1757.88/s  (2.461s,  416.17/s)  LR: 7.106e-04  Data: 0.017 (1.856)
Train: 8 [1100/1171 ( 94%)]  Loss:  4.294122 (4.1438)  Time: 4.043s,  253.29/s  (2.468s,  414.88/s)  LR: 7.106e-04  Data: 3.364 (1.863)
Train: 8 [1150/1171 ( 98%)]  Loss:  4.439709 (4.1562)  Time: 0.586s, 1746.51/s  (2.471s,  414.44/s)  LR: 7.106e-04  Data: 0.019 (1.865)
Train: 8 [1170/1171 (100%)]  Loss:  3.979375 (4.1491)  Time: 0.565s, 1811.62/s  (2.474s,  413.97/s)  LR: 7.106e-04  Data: 0.000 (1.868)
Test: [   0/97]  Time: 14.164 (14.164)  Loss:  0.5896 (0.5896)  Acc@1: 87.0117 (87.0117)  Acc@5: 99.1211 (99.1211)
Test: [  50/97]  Time: 0.197 (3.271)  Loss:  1.3009 (0.9000)  Acc@1: 74.4141 (82.4353)  Acc@5: 91.6992 (95.8621)
Test: [  97/97]  Time: 0.119 (3.176)  Loss:  0.7125 (0.9149)  Acc@1: 87.2024 (82.2480)  Acc@5: 95.9821 (94.9570)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-8.pth.tar', 82.24799997558594)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-7.pth.tar', 80.3920000415039)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-6.pth.tar', 77.87000000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-5.pth.tar', 72.9940000048828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-4.pth.tar', 67.57300003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-3.pth.tar', 58.862999982910154)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-2.pth.tar', 45.86700000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-1.pth.tar', 31.120000002441405)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-0.pth.tar', 10.033000006103515)

Train: 9 [   0/1171 (  0%)]  Loss:  3.961390 (3.9614)  Time: 10.237s,  100.03/s  (10.237s,  100.03/s)  LR: 6.445e-04  Data: 9.652 (9.652)
Train: 9 [  50/1171 (  4%)]  Loss:  3.857463 (3.9094)  Time: 0.581s, 1761.20/s  (2.435s,  420.59/s)  LR: 6.445e-04  Data: 0.019 (1.837)
Train: 9 [ 100/1171 (  9%)]  Loss:  4.184094 (4.0010)  Time: 0.585s, 1750.37/s  (2.366s,  432.81/s)  LR: 6.445e-04  Data: 0.020 (1.763)
Train: 9 [ 150/1171 ( 13%)]  Loss:  4.055000 (4.0145)  Time: 0.587s, 1744.67/s  (2.306s,  444.15/s)  LR: 6.445e-04  Data: 0.024 (1.709)
Train: 9 [ 200/1171 ( 17%)]  Loss:  3.786822 (3.9690)  Time: 0.585s, 1751.28/s  (2.443s,  419.10/s)  LR: 6.445e-04  Data: 0.021 (1.848)
Train: 9 [ 250/1171 ( 21%)]  Loss:  3.860565 (3.9509)  Time: 0.585s, 1750.97/s  (2.412s,  424.61/s)  LR: 6.445e-04  Data: 0.018 (1.819)
Train: 9 [ 300/1171 ( 26%)]  Loss:  3.905729 (3.9444)  Time: 0.587s, 1745.39/s  (2.419s,  423.30/s)  LR: 6.445e-04  Data: 0.022 (1.824)
Train: 9 [ 350/1171 ( 30%)]  Loss:  4.002480 (3.9517)  Time: 0.586s, 1746.25/s  (2.387s,  428.94/s)  LR: 6.445e-04  Data: 0.022 (1.794)
Train: 9 [ 400/1171 ( 34%)]  Loss:  4.059464 (3.9637)  Time: 0.585s, 1751.06/s  (2.376s,  430.94/s)  LR: 6.445e-04  Data: 0.020 (1.784)
Train: 9 [ 450/1171 ( 38%)]  Loss:  4.252991 (3.9926)  Time: 0.585s, 1751.39/s  (2.351s,  435.50/s)  LR: 6.445e-04  Data: 0.019 (1.760)
Train: 9 [ 500/1171 ( 43%)]  Loss:  3.571000 (3.9543)  Time: 0.585s, 1750.49/s  (2.342s,  437.22/s)  LR: 6.445e-04  Data: 0.022 (1.751)
Train: 9 [ 550/1171 ( 47%)]  Loss:  4.214672 (3.9760)  Time: 0.582s, 1758.88/s  (2.362s,  433.50/s)  LR: 6.445e-04  Data: 0.019 (1.770)
Train: 9 [ 600/1171 ( 51%)]  Loss:  4.526220 (4.0183)  Time: 0.588s, 1741.56/s  (2.392s,  428.07/s)  LR: 6.445e-04  Data: 0.020 (1.800)
Train: 9 [ 650/1171 ( 56%)]  Loss:  4.233824 (4.0337)  Time: 0.588s, 1740.62/s  (2.403s,  426.19/s)  LR: 6.445e-04  Data: 0.019 (1.810)
Train: 9 [ 700/1171 ( 60%)]  Loss:  4.037500 (4.0339)  Time: 0.584s, 1754.14/s  (2.398s,  427.05/s)  LR: 6.445e-04  Data: 0.021 (1.806)
Train: 9 [ 750/1171 ( 64%)]  Loss:  4.824295 (4.0833)  Time: 0.584s, 1752.64/s  (2.386s,  429.18/s)  LR: 6.445e-04  Data: 0.022 (1.794)
Train: 9 [ 800/1171 ( 68%)]  Loss:  3.838673 (4.0690)  Time: 0.586s, 1746.64/s  (2.381s,  430.14/s)  LR: 6.445e-04  Data: 0.021 (1.788)
Train: 9 [ 850/1171 ( 73%)]  Loss:  4.062230 (4.0686)  Time: 0.587s, 1743.72/s  (2.367s,  432.70/s)  LR: 6.445e-04  Data: 0.022 (1.773)
Train: 9 [ 900/1171 ( 77%)]  Loss:  4.085115 (4.0694)  Time: 0.585s, 1751.71/s  (2.364s,  433.12/s)  LR: 6.445e-04  Data: 0.022 (1.771)
Train: 9 [ 950/1171 ( 81%)]  Loss:  4.235201 (4.0777)  Time: 0.586s, 1748.27/s  (2.379s,  430.48/s)  LR: 6.445e-04  Data: 0.018 (1.786)
Train: 9 [1000/1171 ( 85%)]  Loss:  4.311915 (4.0889)  Time: 0.587s, 1745.81/s  (2.389s,  428.61/s)  LR: 6.445e-04  Data: 0.021 (1.797)
Train: 9 [1050/1171 ( 90%)]  Loss:  3.849068 (4.0780)  Time: 0.597s, 1716.05/s  (2.389s,  428.61/s)  LR: 6.445e-04  Data: 0.019 (1.797)
Train: 9 [1100/1171 ( 94%)]  Loss:  4.536289 (4.0979)  Time: 0.590s, 1736.57/s  (2.393s,  427.94/s)  LR: 6.445e-04  Data: 0.025 (1.801)
Train: 9 [1150/1171 ( 98%)]  Loss:  4.083845 (4.0973)  Time: 3.681s,  278.16/s  (2.391s,  428.23/s)  LR: 6.445e-04  Data: 3.117 (1.799)
Train: 9 [1170/1171 (100%)]  Loss:  3.631108 (4.0787)  Time: 0.567s, 1804.52/s  (2.389s,  428.63/s)  LR: 6.445e-04  Data: 0.000 (1.797)
Test: [   0/97]  Time: 12.452 (12.452)  Loss:  0.5451 (0.5451)  Acc@1: 90.3320 (90.3320)  Acc@5: 98.9258 (98.9258)
Test: [  50/97]  Time: 0.198 (3.113)  Loss:  1.2309 (0.8455)  Acc@1: 75.0000 (83.8848)  Acc@5: 92.9688 (96.4614)
Test: [  97/97]  Time: 0.119 (3.222)  Loss:  0.5062 (0.8574)  Acc@1: 91.6667 (83.8010)  Acc@5: 97.9167 (95.6450)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-9.pth.tar', 83.8010000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-8.pth.tar', 82.24799997558594)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-7.pth.tar', 80.3920000415039)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-6.pth.tar', 77.87000000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-5.pth.tar', 72.9940000048828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-4.pth.tar', 67.57300003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-3.pth.tar', 58.862999982910154)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-2.pth.tar', 45.86700000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-1.pth.tar', 31.120000002441405)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-0.pth.tar', 10.033000006103515)

Train: 10 [   0/1171 (  0%)]  Loss:  3.701094 (3.7011)  Time: 12.003s,   85.31/s  (12.003s,   85.31/s)  LR: 5.754e-04  Data: 10.912 (10.912)
Train: 10 [  50/1171 (  4%)]  Loss:  3.347869 (3.5245)  Time: 0.591s, 1733.33/s  (2.616s,  391.42/s)  LR: 5.754e-04  Data: 0.021 (2.003)
Train: 10 [ 100/1171 (  9%)]  Loss:  3.272740 (3.4406)  Time: 3.445s,  297.21/s  (2.590s,  395.37/s)  LR: 5.754e-04  Data: 2.769 (1.974)
Train: 10 [ 150/1171 ( 13%)]  Loss:  3.860628 (3.5456)  Time: 0.585s, 1751.33/s  (2.510s,  407.91/s)  LR: 5.754e-04  Data: 0.019 (1.900)
Train: 10 [ 200/1171 ( 17%)]  Loss:  4.027508 (3.6420)  Time: 0.585s, 1751.36/s  (2.494s,  410.58/s)  LR: 5.754e-04  Data: 0.020 (1.884)
Train: 10 [ 250/1171 ( 21%)]  Loss:  4.274478 (3.7474)  Time: 0.583s, 1755.50/s  (2.438s,  419.98/s)  LR: 5.754e-04  Data: 0.019 (1.829)
Train: 10 [ 300/1171 ( 26%)]  Loss:  3.595909 (3.7257)  Time: 6.174s,  165.85/s  (2.412s,  424.62/s)  LR: 5.754e-04  Data: 5.579 (1.804)
Train: 10 [ 350/1171 ( 30%)]  Loss:  4.307021 (3.7984)  Time: 0.586s, 1747.18/s  (2.374s,  431.30/s)  LR: 5.754e-04  Data: 0.018 (1.766)
Train: 10 [ 400/1171 ( 34%)]  Loss:  3.972859 (3.8178)  Time: 4.912s,  208.49/s  (2.428s,  421.80/s)  LR: 5.754e-04  Data: 4.301 (1.819)
Train: 10 [ 450/1171 ( 38%)]  Loss:  3.655200 (3.8015)  Time: 0.586s, 1748.03/s  (2.444s,  419.03/s)  LR: 5.754e-04  Data: 0.018 (1.833)
Train: 10 [ 500/1171 ( 43%)]  Loss:  3.711401 (3.7933)  Time: 8.200s,  124.89/s  (2.460s,  416.20/s)  LR: 5.754e-04  Data: 7.541 (1.849)
Train: 10 [ 550/1171 ( 47%)]  Loss:  3.371636 (3.7582)  Time: 0.584s, 1752.11/s  (2.459s,  416.36/s)  LR: 5.754e-04  Data: 0.019 (1.847)
Train: 10 [ 600/1171 ( 51%)]  Loss:  3.952744 (3.7732)  Time: 7.216s,  141.90/s  (2.472s,  414.30/s)  LR: 5.754e-04  Data: 6.655 (1.861)
Train: 10 [ 650/1171 ( 56%)]  Loss:  3.646577 (3.7641)  Time: 0.586s, 1747.22/s  (2.459s,  416.43/s)  LR: 5.754e-04  Data: 0.022 (1.850)
Train: 10 [ 700/1171 ( 60%)]  Loss:  4.007531 (3.7803)  Time: 5.939s,  172.43/s  (2.455s,  417.07/s)  LR: 5.754e-04  Data: 5.283 (1.846)
Train: 10 [ 750/1171 ( 64%)]  Loss:  3.970886 (3.7923)  Time: 0.584s, 1752.40/s  (2.474s,  413.94/s)  LR: 5.754e-04  Data: 0.018 (1.864)
Train: 10 [ 800/1171 ( 68%)]  Loss:  4.009469 (3.8050)  Time: 8.544s,  119.86/s  (2.485s,  412.04/s)  LR: 5.754e-04  Data: 7.912 (1.876)
Train: 10 [ 850/1171 ( 73%)]  Loss:  3.937487 (3.8124)  Time: 0.583s, 1756.36/s  (2.479s,  413.00/s)  LR: 5.754e-04  Data: 0.021 (1.872)
Train: 10 [ 900/1171 ( 77%)]  Loss:  4.128566 (3.8290)  Time: 10.032s,  102.07/s  (2.484s,  412.21/s)  LR: 5.754e-04  Data: 8.940 (1.878)
Train: 10 [ 950/1171 ( 81%)]  Loss:  3.248274 (3.8000)  Time: 0.586s, 1748.30/s  (2.479s,  413.11/s)  LR: 5.754e-04  Data: 0.021 (1.873)
Train: 10 [1000/1171 ( 85%)]  Loss:  4.161551 (3.8172)  Time: 7.628s,  134.24/s  (2.474s,  413.83/s)  LR: 5.754e-04  Data: 7.066 (1.870)
Train: 10 [1050/1171 ( 90%)]  Loss:  3.777959 (3.8154)  Time: 0.587s, 1743.17/s  (2.464s,  415.54/s)  LR: 5.754e-04  Data: 0.021 (1.861)
Train: 10 [1100/1171 ( 94%)]  Loss:  4.405987 (3.8411)  Time: 8.468s,  120.93/s  (2.477s,  413.32/s)  LR: 5.754e-04  Data: 7.770 (1.874)
Train: 10 [1150/1171 ( 98%)]  Loss:  3.256731 (3.8168)  Time: 0.585s, 1750.06/s  (2.478s,  413.19/s)  LR: 5.754e-04  Data: 0.020 (1.876)
Train: 10 [1170/1171 (100%)]  Loss:  3.700998 (3.8121)  Time: 0.565s, 1813.33/s  (2.477s,  413.34/s)  LR: 5.754e-04  Data: 0.000 (1.874)
Test: [   0/97]  Time: 14.343 (14.343)  Loss:  0.5062 (0.5062)  Acc@1: 89.9414 (89.9414)  Acc@5: 99.3164 (99.3164)
Test: [  50/97]  Time: 0.197 (3.272)  Loss:  1.1369 (0.7963)  Acc@1: 76.6602 (84.8901)  Acc@5: 93.5547 (96.7984)
Test: [  97/97]  Time: 0.119 (3.157)  Loss:  0.5630 (0.8099)  Acc@1: 91.2202 (84.9130)  Acc@5: 98.0655 (96.0010)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-10.pth.tar', 84.91300004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-9.pth.tar', 83.8010000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-8.pth.tar', 82.24799997558594)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-7.pth.tar', 80.3920000415039)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-6.pth.tar', 77.87000000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-5.pth.tar', 72.9940000048828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-4.pth.tar', 67.57300003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-3.pth.tar', 58.862999982910154)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-2.pth.tar', 45.86700000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-1.pth.tar', 31.120000002441405)

Train: 11 [   0/1171 (  0%)]  Loss:  3.803250 (3.8033)  Time: 11.506s,   89.00/s  (11.506s,   89.00/s)  LR: 5.050e-04  Data: 10.390 (10.390)
Train: 11 [  50/1171 (  4%)]  Loss:  2.850264 (3.3268)  Time: 0.585s, 1751.75/s  (2.419s,  423.34/s)  LR: 5.050e-04  Data: 0.021 (1.833)
Train: 11 [ 100/1171 (  9%)]  Loss:  3.796241 (3.4833)  Time: 0.587s, 1745.59/s  (2.398s,  427.01/s)  LR: 5.050e-04  Data: 0.017 (1.818)
Train: 11 [ 150/1171 ( 13%)]  Loss:  3.658798 (3.5271)  Time: 0.584s, 1752.73/s  (2.398s,  427.10/s)  LR: 5.050e-04  Data: 0.018 (1.814)
Train: 11 [ 200/1171 ( 17%)]  Loss:  3.831875 (3.5881)  Time: 1.049s,  976.09/s  (2.438s,  420.09/s)  LR: 5.050e-04  Data: 0.482 (1.852)
Train: 11 [ 250/1171 ( 21%)]  Loss:  3.373033 (3.5522)  Time: 0.586s, 1747.27/s  (2.446s,  418.66/s)  LR: 5.050e-04  Data: 0.018 (1.858)
Train: 11 [ 300/1171 ( 26%)]  Loss:  4.019914 (3.6191)  Time: 0.583s, 1756.93/s  (2.471s,  414.36/s)  LR: 5.050e-04  Data: 0.018 (1.884)
Train: 11 [ 350/1171 ( 30%)]  Loss:  3.915040 (3.6561)  Time: 0.650s, 1575.33/s  (2.442s,  419.27/s)  LR: 5.050e-04  Data: 0.022 (1.854)
Train: 11 [ 400/1171 ( 34%)]  Loss:  3.038877 (3.5875)  Time: 0.584s, 1754.33/s  (2.446s,  418.66/s)  LR: 5.050e-04  Data: 0.017 (1.856)
Train: 11 [ 450/1171 ( 38%)]  Loss:  4.237544 (3.6525)  Time: 0.589s, 1738.50/s  (2.429s,  421.56/s)  LR: 5.050e-04  Data: 0.024 (1.838)
Train: 11 [ 500/1171 ( 43%)]  Loss:  4.542154 (3.7334)  Time: 4.024s,  254.50/s  (2.431s,  421.24/s)  LR: 5.050e-04  Data: 3.459 (1.838)
Train: 11 [ 550/1171 ( 47%)]  Loss:  3.256287 (3.6936)  Time: 0.587s, 1745.08/s  (2.468s,  414.83/s)  LR: 5.050e-04  Data: 0.023 (1.873)
Train: 11 [ 600/1171 ( 51%)]  Loss:  4.428784 (3.7502)  Time: 6.377s,  160.57/s  (2.498s,  409.91/s)  LR: 5.050e-04  Data: 5.714 (1.903)
Train: 11 [ 650/1171 ( 56%)]  Loss:  3.554699 (3.7362)  Time: 0.591s, 1733.62/s  (2.504s,  408.93/s)  LR: 5.050e-04  Data: 0.025 (1.909)
Train: 11 [ 700/1171 ( 60%)]  Loss:  4.413887 (3.7814)  Time: 8.116s,  126.17/s  (2.503s,  409.15/s)  LR: 5.050e-04  Data: 7.435 (1.907)
Train: 11 [ 750/1171 ( 64%)]  Loss:  3.827430 (3.7843)  Time: 0.587s, 1742.98/s  (2.492s,  410.97/s)  LR: 5.050e-04  Data: 0.021 (1.897)
Train: 11 [ 800/1171 ( 68%)]  Loss:  3.279701 (3.7546)  Time: 7.664s,  133.60/s  (2.488s,  411.59/s)  LR: 5.050e-04  Data: 7.103 (1.893)
Train: 11 [ 850/1171 ( 73%)]  Loss:  3.803912 (3.7573)  Time: 0.585s, 1750.95/s  (2.471s,  414.37/s)  LR: 5.050e-04  Data: 0.019 (1.876)
Train: 11 [ 900/1171 ( 77%)]  Loss:  3.769282 (3.7579)  Time: 7.703s,  132.93/s  (2.489s,  411.43/s)  LR: 5.050e-04  Data: 7.100 (1.894)
Train: 11 [ 950/1171 ( 81%)]  Loss:  3.876805 (3.7639)  Time: 1.760s,  581.89/s  (2.487s,  411.72/s)  LR: 5.050e-04  Data: 1.104 (1.893)
Train: 11 [1000/1171 ( 85%)]  Loss:  4.013129 (3.7758)  Time: 4.633s,  221.02/s  (2.492s,  410.94/s)  LR: 5.050e-04  Data: 3.970 (1.896)
Train: 11 [1050/1171 ( 90%)]  Loss:  3.530153 (3.7646)  Time: 0.589s, 1739.53/s  (2.483s,  412.38/s)  LR: 5.050e-04  Data: 0.022 (1.887)
Train: 11 [1100/1171 ( 94%)]  Loss:  4.364602 (3.7907)  Time: 6.178s,  165.74/s  (2.484s,  412.27/s)  LR: 5.050e-04  Data: 5.603 (1.888)
Train: 11 [1150/1171 ( 98%)]  Loss:  4.258506 (3.8102)  Time: 0.583s, 1756.73/s  (2.477s,  413.45/s)  LR: 5.050e-04  Data: 0.019 (1.881)
Train: 11 [1170/1171 (100%)]  Loss:  3.767456 (3.8085)  Time: 0.566s, 1809.63/s  (2.475s,  413.80/s)  LR: 5.050e-04  Data: 0.000 (1.879)
Test: [   0/97]  Time: 13.199 (13.199)  Loss:  0.4698 (0.4698)  Acc@1: 92.0898 (92.0898)  Acc@5: 99.2188 (99.2188)
Test: [  50/97]  Time: 0.196 (3.271)  Loss:  1.0884 (0.7412)  Acc@1: 78.2227 (86.7456)  Acc@5: 94.9219 (97.2733)
Test: [  97/97]  Time: 0.119 (3.250)  Loss:  0.4889 (0.7575)  Acc@1: 91.0714 (86.4430)  Acc@5: 97.7679 (96.5280)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-11.pth.tar', 86.44299998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-10.pth.tar', 84.91300004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-9.pth.tar', 83.8010000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-8.pth.tar', 82.24799997558594)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-7.pth.tar', 80.3920000415039)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-6.pth.tar', 77.87000000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-5.pth.tar', 72.9940000048828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-4.pth.tar', 67.57300003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-3.pth.tar', 58.862999982910154)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-2.pth.tar', 45.86700000488281)

Train: 12 [   0/1171 (  0%)]  Loss:  3.360669 (3.3607)  Time: 12.175s,   84.11/s  (12.175s,   84.11/s)  LR: 4.346e-04  Data: 10.883 (10.883)
Train: 12 [  50/1171 (  4%)]  Loss:  4.184808 (3.7727)  Time: 0.586s, 1747.93/s  (2.633s,  388.96/s)  LR: 4.346e-04  Data: 0.021 (2.013)
Train: 12 [ 100/1171 (  9%)]  Loss:  4.105271 (3.8836)  Time: 1.469s,  696.86/s  (2.532s,  404.36/s)  LR: 4.346e-04  Data: 0.905 (1.926)
Train: 12 [ 150/1171 ( 13%)]  Loss:  3.490096 (3.7852)  Time: 0.589s, 1738.68/s  (2.471s,  414.47/s)  LR: 4.346e-04  Data: 0.018 (1.872)
Train: 12 [ 200/1171 ( 17%)]  Loss:  4.008592 (3.8299)  Time: 0.611s, 1674.59/s  (2.444s,  418.98/s)  LR: 4.346e-04  Data: 0.022 (1.850)
Train: 12 [ 250/1171 ( 21%)]  Loss:  4.013916 (3.8606)  Time: 0.584s, 1752.43/s  (2.403s,  426.17/s)  LR: 4.346e-04  Data: 0.018 (1.807)
Train: 12 [ 300/1171 ( 26%)]  Loss:  4.177764 (3.9059)  Time: 0.584s, 1752.04/s  (2.388s,  428.76/s)  LR: 4.346e-04  Data: 0.022 (1.796)
Train: 12 [ 350/1171 ( 30%)]  Loss:  3.586046 (3.8659)  Time: 0.586s, 1748.82/s  (2.426s,  422.10/s)  LR: 4.346e-04  Data: 0.019 (1.834)
Train: 12 [ 400/1171 ( 34%)]  Loss:  3.693354 (3.8467)  Time: 0.587s, 1745.50/s  (2.453s,  417.49/s)  LR: 4.346e-04  Data: 0.019 (1.860)
Train: 12 [ 450/1171 ( 38%)]  Loss:  3.553702 (3.8174)  Time: 0.587s, 1745.76/s  (2.438s,  420.10/s)  LR: 4.346e-04  Data: 0.018 (1.844)
Train: 12 [ 500/1171 ( 43%)]  Loss:  4.209509 (3.8531)  Time: 0.585s, 1751.70/s  (2.446s,  418.66/s)  LR: 4.346e-04  Data: 0.021 (1.850)
Train: 12 [ 550/1171 ( 47%)]  Loss:  3.814763 (3.8499)  Time: 0.587s, 1745.63/s  (2.449s,  418.15/s)  LR: 4.346e-04  Data: 0.023 (1.852)
Train: 12 [ 600/1171 ( 51%)]  Loss:  3.674654 (3.8364)  Time: 0.584s, 1754.36/s  (2.451s,  417.83/s)  LR: 4.346e-04  Data: 0.020 (1.854)
Train: 12 [ 650/1171 ( 56%)]  Loss:  3.785347 (3.8327)  Time: 0.584s, 1752.53/s  (2.444s,  418.92/s)  LR: 4.346e-04  Data: 0.022 (1.847)
Train: 12 [ 700/1171 ( 60%)]  Loss:  3.891303 (3.8367)  Time: 0.585s, 1749.12/s  (2.467s,  415.08/s)  LR: 4.346e-04  Data: 0.023 (1.870)
Train: 12 [ 750/1171 ( 64%)]  Loss:  4.072311 (3.8514)  Time: 0.586s, 1748.44/s  (2.482s,  412.52/s)  LR: 4.346e-04  Data: 0.020 (1.883)
Train: 12 [ 800/1171 ( 68%)]  Loss:  3.557955 (3.8341)  Time: 2.795s,  366.37/s  (2.484s,  412.21/s)  LR: 4.346e-04  Data: 2.222 (1.884)
Train: 12 [ 850/1171 ( 73%)]  Loss:  3.971206 (3.8417)  Time: 0.929s, 1102.43/s  (2.488s,  411.58/s)  LR: 4.346e-04  Data: 0.210 (1.888)
Train: 12 [ 900/1171 ( 77%)]  Loss:  4.216307 (3.8615)  Time: 2.166s,  472.68/s  (2.485s,  412.15/s)  LR: 4.346e-04  Data: 1.345 (1.884)
Train: 12 [ 950/1171 ( 81%)]  Loss:  4.214269 (3.8791)  Time: 3.043s,  336.51/s  (2.483s,  412.45/s)  LR: 4.346e-04  Data: 2.446 (1.882)
Train: 12 [1000/1171 ( 85%)]  Loss:  3.742142 (3.8726)  Time: 0.590s, 1736.26/s  (2.474s,  413.89/s)  LR: 4.346e-04  Data: 0.021 (1.874)
Train: 12 [1050/1171 ( 90%)]  Loss:  3.592206 (3.8598)  Time: 3.512s,  291.55/s  (2.491s,  411.04/s)  LR: 4.346e-04  Data: 2.933 (1.891)
Train: 12 [1100/1171 ( 94%)]  Loss:  3.364712 (3.8383)  Time: 0.591s, 1732.47/s  (2.492s,  410.89/s)  LR: 4.346e-04  Data: 0.019 (1.891)
Train: 12 [1150/1171 ( 98%)]  Loss:  3.806082 (3.8370)  Time: 8.525s,  120.12/s  (2.498s,  409.90/s)  LR: 4.346e-04  Data: 7.644 (1.897)
Train: 12 [1170/1171 (100%)]  Loss:  3.035675 (3.8049)  Time: 0.566s, 1810.68/s  (2.492s,  410.91/s)  LR: 4.346e-04  Data: 0.000 (1.891)
Test: [   0/97]  Time: 13.660 (13.660)  Loss:  0.5106 (0.5106)  Acc@1: 92.8711 (92.8711)  Acc@5: 99.2188 (99.2188)
Test: [  50/97]  Time: 2.101 (3.250)  Loss:  0.8970 (0.6848)  Acc@1: 80.8594 (87.8198)  Acc@5: 95.9961 (97.4629)
Test: [  97/97]  Time: 0.119 (3.140)  Loss:  0.4559 (0.6992)  Acc@1: 92.2619 (87.4070)  Acc@5: 97.7679 (96.7530)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-12.pth.tar', 87.40700003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-11.pth.tar', 86.44299998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-10.pth.tar', 84.91300004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-9.pth.tar', 83.8010000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-8.pth.tar', 82.24799997558594)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-7.pth.tar', 80.3920000415039)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-6.pth.tar', 77.87000000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-5.pth.tar', 72.9940000048828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-4.pth.tar', 67.57300003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-3.pth.tar', 58.862999982910154)

Train: 13 [   0/1171 (  0%)]  Loss:  2.993967 (2.9940)  Time: 12.171s,   84.13/s  (12.171s,   84.13/s)  LR: 3.655e-04  Data: 10.259 (10.259)
Train: 13 [  50/1171 (  4%)]  Loss:  3.922895 (3.4584)  Time: 0.591s, 1733.15/s  (2.416s,  423.89/s)  LR: 3.655e-04  Data: 0.023 (1.800)
Train: 13 [ 100/1171 (  9%)]  Loss:  3.585419 (3.5008)  Time: 0.591s, 1731.27/s  (2.559s,  400.19/s)  LR: 3.655e-04  Data: 0.029 (1.957)
Train: 13 [ 150/1171 ( 13%)]  Loss:  3.036932 (3.3848)  Time: 0.586s, 1747.56/s  (2.545s,  402.41/s)  LR: 3.655e-04  Data: 0.024 (1.950)
Train: 13 [ 200/1171 ( 17%)]  Loss:  3.334127 (3.3747)  Time: 0.587s, 1743.53/s  (2.556s,  400.66/s)  LR: 3.655e-04  Data: 0.019 (1.957)
Train: 13 [ 250/1171 ( 21%)]  Loss:  3.710661 (3.4307)  Time: 0.586s, 1746.98/s  (2.535s,  403.89/s)  LR: 3.655e-04  Data: 0.018 (1.936)
Train: 13 [ 300/1171 ( 26%)]  Loss:  3.924482 (3.5012)  Time: 0.586s, 1747.99/s  (2.518s,  406.70/s)  LR: 3.655e-04  Data: 0.022 (1.920)
Train: 13 [ 350/1171 ( 30%)]  Loss:  4.092070 (3.5751)  Time: 0.585s, 1751.11/s  (2.482s,  412.64/s)  LR: 3.655e-04  Data: 0.018 (1.885)
Train: 13 [ 400/1171 ( 34%)]  Loss:  4.109137 (3.6344)  Time: 0.589s, 1739.30/s  (2.469s,  414.68/s)  LR: 3.655e-04  Data: 0.022 (1.869)
Train: 13 [ 450/1171 ( 38%)]  Loss:  3.737134 (3.6447)  Time: 0.583s, 1755.15/s  (2.463s,  415.79/s)  LR: 3.655e-04  Data: 0.018 (1.861)
Train: 13 [ 500/1171 ( 43%)]  Loss:  3.193498 (3.6037)  Time: 0.917s, 1117.28/s  (2.488s,  411.54/s)  LR: 3.655e-04  Data: 0.125 (1.884)
Train: 13 [ 550/1171 ( 47%)]  Loss:  3.948787 (3.6324)  Time: 0.587s, 1744.33/s  (2.508s,  408.29/s)  LR: 3.655e-04  Data: 0.019 (1.906)
Train: 13 [ 600/1171 ( 51%)]  Loss:  2.571149 (3.5508)  Time: 0.584s, 1754.78/s  (2.513s,  407.56/s)  LR: 3.655e-04  Data: 0.021 (1.909)
Train: 13 [ 650/1171 ( 56%)]  Loss:  3.380294 (3.5386)  Time: 0.590s, 1736.19/s  (2.509s,  408.21/s)  LR: 3.655e-04  Data: 0.021 (1.904)
Train: 13 [ 700/1171 ( 60%)]  Loss:  3.171639 (3.5141)  Time: 0.584s, 1754.69/s  (2.500s,  409.53/s)  LR: 3.655e-04  Data: 0.021 (1.895)
Train: 13 [ 750/1171 ( 64%)]  Loss:  3.253848 (3.4979)  Time: 0.585s, 1749.00/s  (2.485s,  412.09/s)  LR: 3.655e-04  Data: 0.020 (1.879)
Train: 13 [ 800/1171 ( 68%)]  Loss:  3.905243 (3.5218)  Time: 0.585s, 1749.08/s  (2.482s,  412.52/s)  LR: 3.655e-04  Data: 0.021 (1.877)
Train: 13 [ 850/1171 ( 73%)]  Loss:  3.616498 (3.5271)  Time: 0.583s, 1757.51/s  (2.506s,  408.69/s)  LR: 3.655e-04  Data: 0.017 (1.898)
Train: 13 [ 900/1171 ( 77%)]  Loss:  4.314726 (3.5686)  Time: 0.586s, 1746.48/s  (2.516s,  406.93/s)  LR: 3.655e-04  Data: 0.016 (1.910)
Train: 13 [ 950/1171 ( 81%)]  Loss:  3.959046 (3.5881)  Time: 0.583s, 1755.33/s  (2.518s,  406.60/s)  LR: 3.655e-04  Data: 0.017 (1.912)
Train: 13 [1000/1171 ( 85%)]  Loss:  3.653287 (3.5912)  Time: 0.584s, 1752.27/s  (2.519s,  406.50/s)  LR: 3.655e-04  Data: 0.017 (1.911)
Train: 13 [1050/1171 ( 90%)]  Loss:  3.812285 (3.6012)  Time: 3.198s,  320.16/s  (2.514s,  407.36/s)  LR: 3.655e-04  Data: 2.613 (1.906)
Train: 13 [1100/1171 ( 94%)]  Loss:  3.523409 (3.5978)  Time: 0.587s, 1744.65/s  (2.507s,  408.38/s)  LR: 3.655e-04  Data: 0.017 (1.899)
Train: 13 [1150/1171 ( 98%)]  Loss:  3.524708 (3.5948)  Time: 1.933s,  529.88/s  (2.503s,  409.04/s)  LR: 3.655e-04  Data: 1.303 (1.892)
Train: 13 [1170/1171 (100%)]  Loss:  4.159002 (3.6174)  Time: 0.565s, 1811.53/s  (2.516s,  407.02/s)  LR: 3.655e-04  Data: 0.000 (1.905)
Test: [   0/97]  Time: 14.788 (14.788)  Loss:  0.5075 (0.5075)  Acc@1: 92.3828 (92.3828)  Acc@5: 99.0234 (99.0234)
Test: [  50/97]  Time: 0.200 (3.405)  Loss:  0.8633 (0.6549)  Acc@1: 82.7148 (88.6336)  Acc@5: 95.7031 (97.6677)
Test: [  97/97]  Time: 0.119 (3.337)  Loss:  0.4877 (0.6696)  Acc@1: 91.8155 (88.1270)  Acc@5: 98.6607 (97.0010)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-13.pth.tar', 88.12699999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-12.pth.tar', 87.40700003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-11.pth.tar', 86.44299998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-10.pth.tar', 84.91300004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-9.pth.tar', 83.8010000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-8.pth.tar', 82.24799997558594)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-7.pth.tar', 80.3920000415039)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-6.pth.tar', 77.87000000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-5.pth.tar', 72.9940000048828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-4.pth.tar', 67.57300003417969)

Train: 14 [   0/1171 (  0%)]  Loss:  4.009469 (4.0095)  Time: 12.430s,   82.38/s  (12.430s,   82.38/s)  LR: 2.994e-04  Data: 11.611 (11.611)
Train: 14 [  50/1171 (  4%)]  Loss:  2.907780 (3.4586)  Time: 0.584s, 1753.93/s  (2.602s,  393.60/s)  LR: 2.994e-04  Data: 0.019 (1.990)
Train: 14 [ 100/1171 (  9%)]  Loss:  3.277099 (3.3981)  Time: 0.585s, 1749.89/s  (2.496s,  410.24/s)  LR: 2.994e-04  Data: 0.020 (1.897)
Train: 14 [ 150/1171 ( 13%)]  Loss:  3.569530 (3.4410)  Time: 0.583s, 1756.13/s  (2.425s,  422.22/s)  LR: 2.994e-04  Data: 0.019 (1.823)
Train: 14 [ 200/1171 ( 17%)]  Loss:  3.432299 (3.4392)  Time: 6.505s,  157.43/s  (2.418s,  423.50/s)  LR: 2.994e-04  Data: 5.849 (1.817)
Train: 14 [ 250/1171 ( 21%)]  Loss:  3.245771 (3.4070)  Time: 0.585s, 1751.03/s  (2.468s,  414.99/s)  LR: 2.994e-04  Data: 0.019 (1.866)
Train: 14 [ 300/1171 ( 26%)]  Loss:  3.342463 (3.3978)  Time: 7.968s,  128.51/s  (2.503s,  409.10/s)  LR: 2.994e-04  Data: 7.320 (1.902)
Train: 14 [ 350/1171 ( 30%)]  Loss:  3.426860 (3.4014)  Time: 0.586s, 1746.49/s  (2.504s,  408.99/s)  LR: 2.994e-04  Data: 0.017 (1.902)
Train: 14 [ 400/1171 ( 34%)]  Loss:  3.115573 (3.3696)  Time: 8.327s,  122.98/s  (2.509s,  408.05/s)  LR: 2.994e-04  Data: 7.616 (1.910)
Train: 14 [ 450/1171 ( 38%)]  Loss:  3.611586 (3.3938)  Time: 0.586s, 1747.26/s  (2.493s,  410.67/s)  LR: 2.994e-04  Data: 0.021 (1.894)
Train: 14 [ 500/1171 ( 43%)]  Loss:  3.539209 (3.4071)  Time: 6.618s,  154.74/s  (2.488s,  411.50/s)  LR: 2.994e-04  Data: 6.056 (1.890)
Train: 14 [ 550/1171 ( 47%)]  Loss:  3.681940 (3.4300)  Time: 0.586s, 1746.29/s  (2.472s,  414.18/s)  LR: 2.994e-04  Data: 0.021 (1.875)
Train: 14 [ 600/1171 ( 51%)]  Loss:  3.393660 (3.4272)  Time: 6.350s,  161.27/s  (2.509s,  408.18/s)  LR: 2.994e-04  Data: 5.692 (1.910)
Train: 14 [ 650/1171 ( 56%)]  Loss:  3.527528 (3.4343)  Time: 2.479s,  413.08/s  (2.519s,  406.44/s)  LR: 2.994e-04  Data: 1.807 (1.920)
Train: 14 [ 700/1171 ( 60%)]  Loss:  3.863594 (3.4630)  Time: 3.070s,  333.54/s  (2.518s,  406.67/s)  LR: 2.994e-04  Data: 2.401 (1.915)
Train: 14 [ 750/1171 ( 64%)]  Loss:  3.242204 (3.4492)  Time: 7.532s,  135.96/s  (2.517s,  406.80/s)  LR: 2.994e-04  Data: 6.971 (1.913)
Train: 14 [ 800/1171 ( 68%)]  Loss:  3.238789 (3.4368)  Time: 0.615s, 1665.35/s  (2.508s,  408.28/s)  LR: 2.994e-04  Data: 0.035 (1.903)
Train: 14 [ 850/1171 ( 73%)]  Loss:  3.278172 (3.4280)  Time: 5.514s,  185.73/s  (2.506s,  408.58/s)  LR: 2.994e-04  Data: 4.834 (1.901)
Train: 14 [ 900/1171 ( 77%)]  Loss:  3.215579 (3.4168)  Time: 4.558s,  224.66/s  (2.495s,  410.48/s)  LR: 2.994e-04  Data: 3.960 (1.888)
Train: 14 [ 950/1171 ( 81%)]  Loss:  3.296700 (3.4108)  Time: 1.098s,  932.87/s  (2.507s,  408.51/s)  LR: 2.994e-04  Data: 0.502 (1.901)
Train: 14 [1000/1171 ( 85%)]  Loss:  3.122258 (3.3971)  Time: 5.190s,  197.30/s  (2.516s,  407.05/s)  LR: 2.994e-04  Data: 4.514 (1.910)
Train: 14 [1050/1171 ( 90%)]  Loss:  3.466322 (3.4002)  Time: 1.821s,  562.20/s  (2.514s,  407.29/s)  LR: 2.994e-04  Data: 1.259 (1.907)
Train: 14 [1100/1171 ( 94%)]  Loss:  3.938894 (3.4236)  Time: 8.518s,  120.22/s  (2.517s,  406.82/s)  LR: 2.994e-04  Data: 7.900 (1.910)
Train: 14 [1150/1171 ( 98%)]  Loss:  3.306927 (3.4188)  Time: 1.593s,  642.66/s  (2.513s,  407.56/s)  LR: 2.994e-04  Data: 0.941 (1.905)
Train: 14 [1170/1171 (100%)]  Loss:  3.235130 (3.4114)  Time: 0.566s, 1809.25/s  (2.511s,  407.81/s)  LR: 2.994e-04  Data: 0.000 (1.904)
Test: [   0/97]  Time: 13.536 (13.536)  Loss:  0.4988 (0.4988)  Acc@1: 92.5781 (92.5781)  Acc@5: 99.3164 (99.3164)
Test: [  50/97]  Time: 0.197 (3.069)  Loss:  0.8712 (0.6856)  Acc@1: 86.0352 (89.3019)  Acc@5: 96.0938 (97.7635)
Test: [  97/97]  Time: 0.119 (3.165)  Loss:  0.5573 (0.6964)  Acc@1: 91.0714 (88.6980)  Acc@5: 98.0655 (97.1820)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-14.pth.tar', 88.69799998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-13.pth.tar', 88.12699999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-12.pth.tar', 87.40700003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-11.pth.tar', 86.44299998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-10.pth.tar', 84.91300004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-9.pth.tar', 83.8010000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-8.pth.tar', 82.24799997558594)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-7.pth.tar', 80.3920000415039)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-6.pth.tar', 77.87000000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-5.pth.tar', 72.9940000048828)

Train: 15 [   0/1171 (  0%)]  Loss:  3.341948 (3.3419)  Time: 16.578s,   61.77/s  (16.578s,   61.77/s)  LR: 2.374e-04  Data: 15.374 (15.374)
Train: 15 [  50/1171 (  4%)]  Loss:  3.862984 (3.6025)  Time: 0.587s, 1743.61/s  (2.775s,  368.95/s)  LR: 2.374e-04  Data: 0.021 (2.182)
Train: 15 [ 100/1171 (  9%)]  Loss:  3.631080 (3.6120)  Time: 0.591s, 1733.27/s  (2.636s,  388.41/s)  LR: 2.374e-04  Data: 0.019 (2.051)
Train: 15 [ 150/1171 ( 13%)]  Loss:  3.528613 (3.5912)  Time: 0.586s, 1747.41/s  (2.561s,  399.81/s)  LR: 2.374e-04  Data: 0.020 (1.977)
Train: 15 [ 200/1171 ( 17%)]  Loss:  3.611465 (3.5952)  Time: 2.087s,  490.71/s  (2.568s,  398.74/s)  LR: 2.374e-04  Data: 1.432 (1.982)
Train: 15 [ 250/1171 ( 21%)]  Loss:  3.849636 (3.6376)  Time: 0.582s, 1758.77/s  (2.532s,  404.44/s)  LR: 2.374e-04  Data: 0.018 (1.934)
Train: 15 [ 300/1171 ( 26%)]  Loss:  3.498811 (3.6178)  Time: 1.281s,  799.41/s  (2.513s,  407.52/s)  LR: 2.374e-04  Data: 0.720 (1.912)
Train: 15 [ 350/1171 ( 30%)]  Loss:  4.022880 (3.6684)  Time: 2.825s,  362.43/s  (2.483s,  412.48/s)  LR: 2.374e-04  Data: 2.264 (1.879)
Train: 15 [ 400/1171 ( 34%)]  Loss:  3.965605 (3.7014)  Time: 5.604s,  182.74/s  (2.551s,  401.40/s)  LR: 2.374e-04  Data: 5.043 (1.944)
Train: 15 [ 450/1171 ( 38%)]  Loss:  3.802952 (3.7116)  Time: 1.662s,  616.29/s  (2.546s,  402.22/s)  LR: 2.374e-04  Data: 1.086 (1.936)
Train: 15 [ 500/1171 ( 43%)]  Loss:  4.042056 (3.7416)  Time: 6.710s,  152.60/s  (2.565s,  399.25/s)  LR: 2.374e-04  Data: 6.148 (1.954)
Train: 15 [ 550/1171 ( 47%)]  Loss:  3.391348 (3.7124)  Time: 1.942s,  527.28/s  (2.566s,  399.10/s)  LR: 2.374e-04  Data: 1.355 (1.957)
Train: 15 [ 600/1171 ( 51%)]  Loss:  3.873754 (3.7249)  Time: 4.728s,  216.60/s  (2.562s,  399.74/s)  LR: 2.374e-04  Data: 4.163 (1.953)
Train: 15 [ 650/1171 ( 56%)]  Loss:  3.904373 (3.7377)  Time: 2.939s,  348.37/s  (2.553s,  401.16/s)  LR: 2.374e-04  Data: 2.378 (1.944)
Train: 15 [ 700/1171 ( 60%)]  Loss:  3.432085 (3.7173)  Time: 1.671s,  612.79/s  (2.550s,  401.58/s)  LR: 2.374e-04  Data: 1.096 (1.941)
Train: 15 [ 750/1171 ( 64%)]  Loss:  3.784586 (3.7215)  Time: 5.363s,  190.94/s  (2.558s,  400.36/s)  LR: 2.374e-04  Data: 4.696 (1.949)
Train: 15 [ 800/1171 ( 68%)]  Loss:  3.205918 (3.6912)  Time: 5.428s,  188.64/s  (2.567s,  398.96/s)  LR: 2.374e-04  Data: 4.792 (1.956)
Train: 15 [ 850/1171 ( 73%)]  Loss:  3.091990 (3.6579)  Time: 0.589s, 1738.80/s  (2.566s,  399.04/s)  LR: 2.374e-04  Data: 0.021 (1.957)
Train: 15 [ 900/1171 ( 77%)]  Loss:  4.199907 (3.6864)  Time: 7.666s,  133.58/s  (2.566s,  399.01/s)  LR: 2.374e-04  Data: 6.788 (1.958)
Train: 15 [ 950/1171 ( 81%)]  Loss:  3.416320 (3.6729)  Time: 0.590s, 1736.46/s  (2.558s,  400.39/s)  LR: 2.374e-04  Data: 0.027 (1.951)
Train: 15 [1000/1171 ( 85%)]  Loss:  3.702438 (3.6743)  Time: 7.927s,  129.19/s  (2.554s,  400.87/s)  LR: 2.374e-04  Data: 7.238 (1.948)
Train: 15 [1050/1171 ( 90%)]  Loss:  3.953115 (3.6870)  Time: 0.584s, 1753.85/s  (2.543s,  402.64/s)  LR: 2.374e-04  Data: 0.021 (1.938)
Train: 15 [1100/1171 ( 94%)]  Loss:  4.088313 (3.7044)  Time: 8.753s,  116.99/s  (2.565s,  399.28/s)  LR: 2.374e-04  Data: 8.122 (1.959)
Train: 15 [1150/1171 ( 98%)]  Loss:  3.613144 (3.7006)  Time: 0.586s, 1748.36/s  (2.564s,  399.38/s)  LR: 2.374e-04  Data: 0.022 (1.958)
Train: 15 [1170/1171 (100%)]  Loss:  3.999613 (3.7126)  Time: 0.567s, 1807.43/s  (2.566s,  399.02/s)  LR: 2.374e-04  Data: 0.000 (1.960)
Test: [   0/97]  Time: 14.320 (14.320)  Loss:  0.4671 (0.4671)  Acc@1: 94.0430 (94.0430)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (3.218)  Loss:  0.8172 (0.5971)  Acc@1: 84.2773 (89.9127)  Acc@5: 96.2891 (97.9818)
Test: [  97/97]  Time: 0.119 (3.148)  Loss:  0.4620 (0.6149)  Acc@1: 92.4107 (89.3430)  Acc@5: 98.3631 (97.4080)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-15.pth.tar', 89.34300004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-14.pth.tar', 88.69799998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-13.pth.tar', 88.12699999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-12.pth.tar', 87.40700003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-11.pth.tar', 86.44299998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-10.pth.tar', 84.91300004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-9.pth.tar', 83.8010000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-8.pth.tar', 82.24799997558594)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-7.pth.tar', 80.3920000415039)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-6.pth.tar', 77.87000000488281)

Train: 16 [   0/1171 (  0%)]  Loss:  3.755235 (3.7552)  Time: 10.408s,   98.38/s  (10.408s,   98.38/s)  LR: 1.808e-04  Data: 9.663 (9.663)
Train: 16 [  50/1171 (  4%)]  Loss:  3.700120 (3.7277)  Time: 0.584s, 1751.97/s  (2.440s,  419.62/s)  LR: 1.808e-04  Data: 0.019 (1.846)
Train: 16 [ 100/1171 (  9%)]  Loss:  3.476746 (3.6440)  Time: 2.583s,  396.40/s  (2.387s,  429.06/s)  LR: 1.808e-04  Data: 1.998 (1.791)
Train: 16 [ 150/1171 ( 13%)]  Loss:  3.290408 (3.5556)  Time: 0.584s, 1753.19/s  (2.537s,  403.64/s)  LR: 1.808e-04  Data: 0.018 (1.936)
Train: 16 [ 200/1171 ( 17%)]  Loss:  3.320517 (3.5086)  Time: 4.295s,  238.41/s  (2.582s,  396.54/s)  LR: 1.808e-04  Data: 3.734 (1.976)
Train: 16 [ 250/1171 ( 21%)]  Loss:  3.501426 (3.5074)  Time: 0.585s, 1749.15/s  (2.571s,  398.28/s)  LR: 1.808e-04  Data: 0.019 (1.961)
Train: 16 [ 300/1171 ( 26%)]  Loss:  3.501480 (3.5066)  Time: 7.599s,  134.75/s  (2.574s,  397.89/s)  LR: 1.808e-04  Data: 7.037 (1.968)
Train: 16 [ 350/1171 ( 30%)]  Loss:  3.511883 (3.5072)  Time: 0.584s, 1754.32/s  (2.538s,  403.44/s)  LR: 1.808e-04  Data: 0.022 (1.936)
Train: 16 [ 400/1171 ( 34%)]  Loss:  3.739122 (3.5330)  Time: 9.433s,  108.56/s  (2.541s,  402.93/s)  LR: 1.808e-04  Data: 8.727 (1.942)
Train: 16 [ 450/1171 ( 38%)]  Loss:  3.997114 (3.5794)  Time: 0.590s, 1734.92/s  (2.521s,  406.13/s)  LR: 1.808e-04  Data: 0.021 (1.924)
Train: 16 [ 500/1171 ( 43%)]  Loss:  3.445967 (3.5673)  Time: 10.637s,   96.27/s  (2.603s,  393.32/s)  LR: 1.808e-04  Data: 10.069 (2.007)
Train: 16 [ 550/1171 ( 47%)]  Loss:  3.308522 (3.5457)  Time: 0.583s, 1756.82/s  (2.629s,  389.50/s)  LR: 1.808e-04  Data: 0.020 (2.034)
Train: 16 [ 600/1171 ( 51%)]  Loss:  3.479923 (3.5407)  Time: 8.883s,  115.28/s  (2.653s,  385.94/s)  LR: 1.808e-04  Data: 8.316 (2.059)
Train: 16 [ 650/1171 ( 56%)]  Loss:  3.471570 (3.5357)  Time: 0.586s, 1746.17/s  (2.655s,  385.73/s)  LR: 1.808e-04  Data: 0.018 (2.061)
Train: 16 [ 700/1171 ( 60%)]  Loss:  3.692029 (3.5461)  Time: 6.884s,  148.76/s  (2.649s,  386.62/s)  LR: 1.808e-04  Data: 6.217 (2.055)
Train: 16 [ 750/1171 ( 64%)]  Loss:  3.578862 (3.5482)  Time: 0.588s, 1742.66/s  (2.626s,  389.91/s)  LR: 1.808e-04  Data: 0.018 (2.033)
Train: 16 [ 800/1171 ( 68%)]  Loss:  3.915146 (3.5698)  Time: 10.633s,   96.30/s  (2.655s,  385.64/s)  LR: 1.808e-04  Data: 9.939 (2.061)
Train: 16 [ 850/1171 ( 73%)]  Loss:  4.092330 (3.5988)  Time: 1.481s,  691.65/s  (2.678s,  382.31/s)  LR: 1.808e-04  Data: 0.888 (2.083)
Train: 16 [ 900/1171 ( 77%)]  Loss:  3.549671 (3.5962)  Time: 10.063s,  101.76/s  (2.694s,  380.11/s)  LR: 1.808e-04  Data: 9.384 (2.099)
Train: 16 [ 950/1171 ( 81%)]  Loss:  3.290752 (3.5809)  Time: 0.584s, 1753.37/s  (2.694s,  380.08/s)  LR: 1.808e-04  Data: 0.018 (2.099)
Train: 16 [1000/1171 ( 85%)]  Loss:  3.950893 (3.5986)  Time: 7.776s,  131.69/s  (2.701s,  379.13/s)  LR: 1.808e-04  Data: 6.690 (2.106)
Train: 16 [1050/1171 ( 90%)]  Loss:  3.348032 (3.5872)  Time: 0.589s, 1737.84/s  (2.695s,  380.02/s)  LR: 1.808e-04  Data: 0.024 (2.100)
Train: 16 [1100/1171 ( 94%)]  Loss:  3.275218 (3.5736)  Time: 4.970s,  206.03/s  (2.683s,  381.69/s)  LR: 1.808e-04  Data: 4.408 (2.087)
Train: 16 [1150/1171 ( 98%)]  Loss:  3.944673 (3.5891)  Time: 2.813s,  364.01/s  (2.691s,  380.47/s)  LR: 1.808e-04  Data: 2.252 (2.094)
Train: 16 [1170/1171 (100%)]  Loss:  3.618364 (3.5902)  Time: 0.564s, 1814.78/s  (2.688s,  381.00/s)  LR: 1.808e-04  Data: 0.000 (2.090)
Test: [   0/97]  Time: 15.131 (15.131)  Loss:  0.4286 (0.4286)  Acc@1: 93.5547 (93.5547)  Acc@5: 99.3164 (99.3164)
Test: [  50/97]  Time: 0.196 (3.384)  Loss:  0.7506 (0.5660)  Acc@1: 87.0117 (90.5178)  Acc@5: 96.3867 (98.0871)
Test: [  97/97]  Time: 0.120 (3.220)  Loss:  0.4649 (0.5822)  Acc@1: 92.7083 (89.9430)  Acc@5: 98.0655 (97.5490)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-16.pth.tar', 89.94300001708984)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-15.pth.tar', 89.34300004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-14.pth.tar', 88.69799998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-13.pth.tar', 88.12699999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-12.pth.tar', 87.40700003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-11.pth.tar', 86.44299998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-10.pth.tar', 84.91300004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-9.pth.tar', 83.8010000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-8.pth.tar', 82.24799997558594)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-7.pth.tar', 80.3920000415039)

Train: 17 [   0/1171 (  0%)]  Loss:  3.657051 (3.6571)  Time: 12.555s,   81.56/s  (12.555s,   81.56/s)  LR: 1.309e-04  Data: 11.293 (11.293)
Train: 17 [  50/1171 (  4%)]  Loss:  3.688613 (3.6728)  Time: 0.586s, 1747.25/s  (2.408s,  425.30/s)  LR: 1.309e-04  Data: 0.021 (1.789)
Train: 17 [ 100/1171 (  9%)]  Loss:  3.833485 (3.7264)  Time: 0.813s, 1260.12/s  (2.411s,  424.76/s)  LR: 1.309e-04  Data: 0.208 (1.796)
Train: 17 [ 150/1171 ( 13%)]  Loss:  3.453666 (3.6582)  Time: 0.587s, 1744.89/s  (2.356s,  434.66/s)  LR: 1.309e-04  Data: 0.022 (1.740)
Train: 17 [ 200/1171 ( 17%)]  Loss:  2.864322 (3.4994)  Time: 0.591s, 1732.12/s  (2.478s,  413.30/s)  LR: 1.309e-04  Data: 0.020 (1.864)
Train: 17 [ 250/1171 ( 21%)]  Loss:  3.652027 (3.5249)  Time: 0.586s, 1747.81/s  (2.484s,  412.20/s)  LR: 1.309e-04  Data: 0.019 (1.876)
Train: 17 [ 300/1171 ( 26%)]  Loss:  3.049946 (3.4570)  Time: 0.586s, 1748.17/s  (2.511s,  407.88/s)  LR: 1.309e-04  Data: 0.021 (1.905)
Train: 17 [ 350/1171 ( 30%)]  Loss:  3.181558 (3.4226)  Time: 0.586s, 1747.18/s  (2.497s,  410.16/s)  LR: 1.309e-04  Data: 0.017 (1.894)
Train: 17 [ 400/1171 ( 34%)]  Loss:  3.891106 (3.4746)  Time: 0.583s, 1757.01/s  (2.494s,  410.61/s)  LR: 1.309e-04  Data: 0.016 (1.894)
Train: 17 [ 450/1171 ( 38%)]  Loss:  3.880325 (3.5152)  Time: 0.584s, 1752.88/s  (2.464s,  415.57/s)  LR: 1.309e-04  Data: 0.021 (1.864)
Train: 17 [ 500/1171 ( 43%)]  Loss:  2.919489 (3.4611)  Time: 0.585s, 1749.07/s  (2.455s,  417.16/s)  LR: 1.309e-04  Data: 0.018 (1.855)
Train: 17 [ 550/1171 ( 47%)]  Loss:  3.322244 (3.4495)  Time: 0.586s, 1747.87/s  (2.467s,  415.07/s)  LR: 1.309e-04  Data: 0.019 (1.866)
Train: 17 [ 600/1171 ( 51%)]  Loss:  3.829336 (3.4787)  Time: 0.588s, 1740.52/s  (2.478s,  413.29/s)  LR: 1.309e-04  Data: 0.017 (1.878)
Train: 17 [ 650/1171 ( 56%)]  Loss:  3.630292 (3.4895)  Time: 0.591s, 1733.57/s  (2.466s,  415.29/s)  LR: 1.309e-04  Data: 0.021 (1.867)
Train: 17 [ 700/1171 ( 60%)]  Loss:  3.869251 (3.5148)  Time: 0.584s, 1752.65/s  (2.454s,  417.28/s)  LR: 1.309e-04  Data: 0.018 (1.856)
Train: 17 [ 750/1171 ( 64%)]  Loss:  3.402194 (3.5078)  Time: 0.587s, 1744.48/s  (2.437s,  420.19/s)  LR: 1.309e-04  Data: 0.022 (1.841)
Train: 17 [ 800/1171 ( 68%)]  Loss:  3.167050 (3.4878)  Time: 0.587s, 1744.73/s  (2.427s,  421.97/s)  LR: 1.309e-04  Data: 0.023 (1.830)
Train: 17 [ 850/1171 ( 73%)]  Loss:  3.394265 (3.4826)  Time: 0.588s, 1742.41/s  (2.408s,  425.31/s)  LR: 1.309e-04  Data: 0.023 (1.811)
Train: 17 [ 900/1171 ( 77%)]  Loss:  3.790760 (3.4988)  Time: 0.588s, 1740.03/s  (2.391s,  428.21/s)  LR: 1.309e-04  Data: 0.022 (1.796)
Train: 17 [ 950/1171 ( 81%)]  Loss:  3.213745 (3.4845)  Time: 0.583s, 1757.52/s  (2.399s,  426.91/s)  LR: 1.309e-04  Data: 0.018 (1.803)
Train: 17 [1000/1171 ( 85%)]  Loss:  3.409727 (3.4810)  Time: 0.583s, 1757.85/s  (2.404s,  426.04/s)  LR: 1.309e-04  Data: 0.020 (1.809)
Train: 17 [1050/1171 ( 90%)]  Loss:  2.993748 (3.4588)  Time: 0.587s, 1745.14/s  (2.394s,  427.75/s)  LR: 1.309e-04  Data: 0.020 (1.800)
Train: 17 [1100/1171 ( 94%)]  Loss:  3.412613 (3.4568)  Time: 0.586s, 1747.29/s  (2.393s,  427.92/s)  LR: 1.309e-04  Data: 0.019 (1.799)
Train: 17 [1150/1171 ( 98%)]  Loss:  3.315072 (3.4509)  Time: 0.585s, 1750.18/s  (2.386s,  429.13/s)  LR: 1.309e-04  Data: 0.018 (1.791)
Train: 17 [1170/1171 (100%)]  Loss:  3.265178 (3.4435)  Time: 0.565s, 1811.34/s  (2.381s,  430.01/s)  LR: 1.309e-04  Data: 0.000 (1.787)
Test: [   0/97]  Time: 13.510 (13.510)  Loss:  0.4049 (0.4049)  Acc@1: 94.7266 (94.7266)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.203 (3.068)  Loss:  0.7397 (0.5562)  Acc@1: 85.8398 (90.7475)  Acc@5: 96.4844 (98.1254)
Test: [  97/97]  Time: 0.119 (3.028)  Loss:  0.4367 (0.5721)  Acc@1: 93.6012 (90.1930)  Acc@5: 98.6607 (97.5970)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-17.pth.tar', 90.1930000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-16.pth.tar', 89.94300001708984)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-15.pth.tar', 89.34300004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-14.pth.tar', 88.69799998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-13.pth.tar', 88.12699999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-12.pth.tar', 87.40700003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-11.pth.tar', 86.44299998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-10.pth.tar', 84.91300004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-9.pth.tar', 83.8010000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-8.pth.tar', 82.24799997558594)

Train: 18 [   0/1171 (  0%)]  Loss:  3.223624 (3.2236)  Time: 16.400s,   62.44/s  (16.400s,   62.44/s)  LR: 8.858e-05  Data: 15.201 (15.201)
Train: 18 [  50/1171 (  4%)]  Loss:  3.673629 (3.4486)  Time: 0.585s, 1749.36/s  (2.832s,  361.54/s)  LR: 8.858e-05  Data: 0.019 (2.220)
Train: 18 [ 100/1171 (  9%)]  Loss:  3.597207 (3.4982)  Time: 0.587s, 1744.57/s  (2.676s,  382.72/s)  LR: 8.858e-05  Data: 0.021 (2.066)
Train: 18 [ 150/1171 ( 13%)]  Loss:  3.014570 (3.3773)  Time: 0.588s, 1741.42/s  (2.557s,  400.46/s)  LR: 8.858e-05  Data: 0.020 (1.949)
Train: 18 [ 200/1171 ( 17%)]  Loss:  3.147626 (3.3313)  Time: 0.583s, 1755.64/s  (2.506s,  408.59/s)  LR: 8.858e-05  Data: 0.020 (1.898)
Train: 18 [ 250/1171 ( 21%)]  Loss:  3.270818 (3.3212)  Time: 0.779s, 1314.17/s  (2.454s,  417.29/s)  LR: 8.858e-05  Data: 0.020 (1.847)
Train: 18 [ 300/1171 ( 26%)]  Loss:  3.143330 (3.2958)  Time: 0.589s, 1739.51/s  (2.426s,  422.10/s)  LR: 8.858e-05  Data: 0.023 (1.821)
Train: 18 [ 350/1171 ( 30%)]  Loss:  3.977034 (3.3810)  Time: 0.589s, 1737.50/s  (2.388s,  428.89/s)  LR: 8.858e-05  Data: 0.021 (1.783)
Train: 18 [ 400/1171 ( 34%)]  Loss:  3.321418 (3.3744)  Time: 4.367s,  234.51/s  (2.433s,  420.82/s)  LR: 8.858e-05  Data: 3.805 (1.827)
Train: 18 [ 450/1171 ( 38%)]  Loss:  3.653947 (3.4023)  Time: 0.585s, 1749.78/s  (2.450s,  418.00/s)  LR: 8.858e-05  Data: 0.021 (1.844)
Train: 18 [ 500/1171 ( 43%)]  Loss:  3.378489 (3.4002)  Time: 2.561s,  399.84/s  (2.445s,  418.85/s)  LR: 8.858e-05  Data: 1.999 (1.838)
Train: 18 [ 550/1171 ( 47%)]  Loss:  3.648405 (3.4208)  Time: 0.587s, 1743.61/s  (2.453s,  417.53/s)  LR: 8.858e-05  Data: 0.020 (1.844)
Train: 18 [ 600/1171 ( 51%)]  Loss:  4.024216 (3.4673)  Time: 3.534s,  289.72/s  (2.459s,  416.47/s)  LR: 8.858e-05  Data: 2.968 (1.849)
Train: 18 [ 650/1171 ( 56%)]  Loss:  3.642339 (3.4798)  Time: 0.585s, 1750.76/s  (2.457s,  416.78/s)  LR: 8.858e-05  Data: 0.022 (1.845)
Train: 18 [ 700/1171 ( 60%)]  Loss:  3.630707 (3.4898)  Time: 2.751s,  372.17/s  (2.458s,  416.64/s)  LR: 8.858e-05  Data: 2.071 (1.847)
Train: 18 [ 750/1171 ( 64%)]  Loss:  3.715616 (3.5039)  Time: 0.585s, 1750.96/s  (2.472s,  414.21/s)  LR: 8.858e-05  Data: 0.021 (1.860)
Train: 18 [ 800/1171 ( 68%)]  Loss:  3.032444 (3.4762)  Time: 3.636s,  281.66/s  (2.481s,  412.82/s)  LR: 8.858e-05  Data: 3.074 (1.868)
Train: 18 [ 850/1171 ( 73%)]  Loss:  2.693613 (3.4327)  Time: 0.589s, 1737.10/s  (2.483s,  412.45/s)  LR: 8.858e-05  Data: 0.022 (1.870)
Train: 18 [ 900/1171 ( 77%)]  Loss:  3.109749 (3.4157)  Time: 7.269s,  140.86/s  (2.482s,  412.58/s)  LR: 8.858e-05  Data: 6.599 (1.869)
Train: 18 [ 950/1171 ( 81%)]  Loss:  4.125099 (3.4512)  Time: 0.590s, 1734.61/s  (2.462s,  415.95/s)  LR: 8.858e-05  Data: 0.019 (1.849)
Train: 18 [1000/1171 ( 85%)]  Loss:  3.279856 (3.4430)  Time: 4.435s,  230.88/s  (2.449s,  418.07/s)  LR: 8.858e-05  Data: 3.774 (1.836)
Train: 18 [1050/1171 ( 90%)]  Loss:  3.033685 (3.4244)  Time: 0.588s, 1742.40/s  (2.435s,  420.62/s)  LR: 8.858e-05  Data: 0.020 (1.821)
Train: 18 [1100/1171 ( 94%)]  Loss:  3.585932 (3.4315)  Time: 3.637s,  281.54/s  (2.421s,  422.91/s)  LR: 8.858e-05  Data: 2.993 (1.809)
Train: 18 [1150/1171 ( 98%)]  Loss:  3.307547 (3.4263)  Time: 0.585s, 1748.95/s  (2.429s,  421.55/s)  LR: 8.858e-05  Data: 0.021 (1.816)
Train: 18 [1170/1171 (100%)]  Loss:  4.044419 (3.4510)  Time: 0.565s, 1811.88/s  (2.428s,  421.79/s)  LR: 8.858e-05  Data: 0.000 (1.815)
Test: [   0/97]  Time: 15.678 (15.678)  Loss:  0.4185 (0.4185)  Acc@1: 94.1406 (94.1406)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.196 (3.358)  Loss:  0.7741 (0.5474)  Acc@1: 85.1562 (91.1439)  Acc@5: 96.2891 (98.1828)
Test: [  97/97]  Time: 0.119 (3.271)  Loss:  0.4254 (0.5626)  Acc@1: 93.8988 (90.6070)  Acc@5: 98.8095 (97.7300)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-18.pth.tar', 90.60700001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-17.pth.tar', 90.1930000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-16.pth.tar', 89.94300001708984)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-15.pth.tar', 89.34300004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-14.pth.tar', 88.69799998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-13.pth.tar', 88.12699999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-12.pth.tar', 87.40700003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-11.pth.tar', 86.44299998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-10.pth.tar', 84.91300004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-9.pth.tar', 83.8010000341797)

Train: 19 [   0/1171 (  0%)]  Loss:  3.902117 (3.9021)  Time: 10.281s,   99.61/s  (10.281s,   99.61/s)  LR: 5.473e-05  Data: 9.399 (9.399)
Train: 19 [  50/1171 (  4%)]  Loss:  2.511083 (3.2066)  Time: 0.585s, 1750.47/s  (2.326s,  440.17/s)  LR: 5.473e-05  Data: 0.018 (1.727)
Train: 19 [ 100/1171 (  9%)]  Loss:  3.045817 (3.1530)  Time: 0.590s, 1735.23/s  (2.298s,  445.61/s)  LR: 5.473e-05  Data: 0.022 (1.707)
Train: 19 [ 150/1171 ( 13%)]  Loss:  3.612515 (3.2679)  Time: 0.589s, 1739.66/s  (2.252s,  454.79/s)  LR: 5.473e-05  Data: 0.022 (1.659)
Train: 19 [ 200/1171 ( 17%)]  Loss:  3.635446 (3.3414)  Time: 0.583s, 1755.76/s  (2.343s,  437.09/s)  LR: 5.473e-05  Data: 0.020 (1.749)
Train: 19 [ 250/1171 ( 21%)]  Loss:  3.448253 (3.3592)  Time: 4.541s,  225.49/s  (2.428s,  421.79/s)  LR: 5.473e-05  Data: 3.850 (1.831)
Train: 19 [ 300/1171 ( 26%)]  Loss:  3.251415 (3.3438)  Time: 0.587s, 1745.38/s  (2.486s,  411.88/s)  LR: 5.473e-05  Data: 0.020 (1.888)
Train: 19 [ 350/1171 ( 30%)]  Loss:  3.741122 (3.3935)  Time: 0.586s, 1747.86/s  (2.492s,  410.97/s)  LR: 5.473e-05  Data: 0.020 (1.895)
Train: 19 [ 400/1171 ( 34%)]  Loss:  3.128633 (3.3640)  Time: 0.584s, 1754.67/s  (2.505s,  408.71/s)  LR: 5.473e-05  Data: 0.019 (1.910)
Train: 19 [ 450/1171 ( 38%)]  Loss:  3.361712 (3.3638)  Time: 6.004s,  170.55/s  (2.498s,  410.00/s)  LR: 5.473e-05  Data: 5.356 (1.902)
Train: 19 [ 500/1171 ( 43%)]  Loss:  2.923812 (3.3238)  Time: 0.588s, 1742.77/s  (2.477s,  413.48/s)  LR: 5.473e-05  Data: 0.021 (1.879)
Train: 19 [ 550/1171 ( 47%)]  Loss:  3.909595 (3.3726)  Time: 3.148s,  325.28/s  (2.473s,  414.14/s)  LR: 5.473e-05  Data: 2.585 (1.873)
Train: 19 [ 600/1171 ( 51%)]  Loss:  3.646158 (3.3937)  Time: 0.584s, 1752.99/s  (2.489s,  411.39/s)  LR: 5.473e-05  Data: 0.019 (1.890)
Train: 19 [ 650/1171 ( 56%)]  Loss:  3.801327 (3.4228)  Time: 3.237s,  316.31/s  (2.483s,  412.38/s)  LR: 5.473e-05  Data: 2.668 (1.884)
Train: 19 [ 700/1171 ( 60%)]  Loss:  3.227291 (3.4098)  Time: 0.590s, 1734.98/s  (2.483s,  412.44/s)  LR: 5.473e-05  Data: 0.020 (1.883)
Train: 19 [ 750/1171 ( 64%)]  Loss:  2.729292 (3.3672)  Time: 2.774s,  369.18/s  (2.473s,  414.07/s)  LR: 5.473e-05  Data: 2.115 (1.872)
Train: 19 [ 800/1171 ( 68%)]  Loss:  3.929718 (3.4003)  Time: 0.592s, 1728.35/s  (2.463s,  415.80/s)  LR: 5.473e-05  Data: 0.019 (1.862)
Train: 19 [ 850/1171 ( 73%)]  Loss:  3.441555 (3.4026)  Time: 0.586s, 1748.08/s  (2.448s,  418.34/s)  LR: 5.473e-05  Data: 0.020 (1.847)
Train: 19 [ 900/1171 ( 77%)]  Loss:  3.566488 (3.4112)  Time: 0.587s, 1743.00/s  (2.441s,  419.51/s)  LR: 5.473e-05  Data: 0.021 (1.841)
Train: 19 [ 950/1171 ( 81%)]  Loss:  3.738070 (3.4276)  Time: 0.585s, 1750.47/s  (2.453s,  417.40/s)  LR: 5.473e-05  Data: 0.021 (1.854)
Train: 19 [1000/1171 ( 85%)]  Loss:  3.884144 (3.4493)  Time: 0.586s, 1747.81/s  (2.458s,  416.61/s)  LR: 5.473e-05  Data: 0.019 (1.858)
Train: 19 [1050/1171 ( 90%)]  Loss:  3.184786 (3.4373)  Time: 0.730s, 1401.84/s  (2.450s,  417.89/s)  LR: 5.473e-05  Data: 0.076 (1.851)
Train: 19 [1100/1171 ( 94%)]  Loss:  3.678467 (3.4478)  Time: 0.587s, 1745.78/s  (2.453s,  417.41/s)  LR: 5.473e-05  Data: 0.023 (1.855)
Train: 19 [1150/1171 ( 98%)]  Loss:  3.649156 (3.4562)  Time: 0.588s, 1742.55/s  (2.444s,  419.02/s)  LR: 5.473e-05  Data: 0.019 (1.846)
Train: 19 [1170/1171 (100%)]  Loss:  3.750947 (3.4680)  Time: 0.565s, 1813.87/s  (2.441s,  419.48/s)  LR: 5.473e-05  Data: 0.000 (1.843)
Test: [   0/97]  Time: 13.134 (13.134)  Loss:  0.4149 (0.4149)  Acc@1: 94.2383 (94.2383)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.201 (3.102)  Loss:  0.7065 (0.5362)  Acc@1: 87.4023 (91.3584)  Acc@5: 97.5586 (98.2652)
Test: [  97/97]  Time: 0.119 (3.247)  Loss:  0.4087 (0.5517)  Acc@1: 93.4524 (90.7940)  Acc@5: 98.8095 (97.7990)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-19.pth.tar', 90.79400002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-18.pth.tar', 90.60700001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-17.pth.tar', 90.1930000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-16.pth.tar', 89.94300001708984)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-15.pth.tar', 89.34300004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-14.pth.tar', 88.69799998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-13.pth.tar', 88.12699999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-12.pth.tar', 87.40700003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-11.pth.tar', 86.44299998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-10.pth.tar', 84.91300004882812)

Train: 20 [   0/1171 (  0%)]  Loss:  3.404586 (3.4046)  Time: 14.266s,   71.78/s  (14.266s,   71.78/s)  LR: 3.005e-05  Data: 13.344 (13.344)
Train: 20 [  50/1171 (  4%)]  Loss:  3.466664 (3.4356)  Time: 2.111s,  484.98/s  (2.915s,  351.33/s)  LR: 3.005e-05  Data: 1.463 (2.324)
Train: 20 [ 100/1171 (  9%)]  Loss:  3.654294 (3.5085)  Time: 0.584s, 1754.84/s  (2.855s,  358.68/s)  LR: 3.005e-05  Data: 0.020 (2.263)
Train: 20 [ 150/1171 ( 13%)]  Loss:  3.245817 (3.4428)  Time: 1.338s,  765.14/s  (2.768s,  369.88/s)  LR: 3.005e-05  Data: 0.591 (2.163)
Train: 20 [ 200/1171 ( 17%)]  Loss:  2.832323 (3.3207)  Time: 0.591s, 1732.31/s  (2.733s,  374.62/s)  LR: 3.005e-05  Data: 0.022 (2.130)
Train: 20 [ 250/1171 ( 21%)]  Loss:  3.785374 (3.3982)  Time: 0.585s, 1750.86/s  (2.679s,  382.29/s)  LR: 3.005e-05  Data: 0.018 (2.078)
Train: 20 [ 300/1171 ( 26%)]  Loss:  3.828983 (3.4597)  Time: 0.585s, 1751.79/s  (2.633s,  388.90/s)  LR: 3.005e-05  Data: 0.021 (2.036)
Train: 20 [ 350/1171 ( 30%)]  Loss:  3.888643 (3.5133)  Time: 0.586s, 1748.04/s  (2.632s,  389.02/s)  LR: 3.005e-05  Data: 0.018 (2.035)
Train: 20 [ 400/1171 ( 34%)]  Loss:  3.597153 (3.5226)  Time: 0.589s, 1739.86/s  (2.634s,  388.69/s)  LR: 3.005e-05  Data: 0.024 (2.038)
Train: 20 [ 450/1171 ( 38%)]  Loss:  3.553119 (3.5257)  Time: 0.584s, 1754.59/s  (2.624s,  390.20/s)  LR: 3.005e-05  Data: 0.019 (2.029)
Train: 20 [ 500/1171 ( 43%)]  Loss:  3.787905 (3.5495)  Time: 0.585s, 1749.70/s  (2.628s,  389.63/s)  LR: 3.005e-05  Data: 0.020 (2.034)
Train: 20 [ 550/1171 ( 47%)]  Loss:  3.068169 (3.5094)  Time: 0.583s, 1755.09/s  (2.608s,  392.58/s)  LR: 3.005e-05  Data: 0.019 (2.015)
Train: 20 [ 600/1171 ( 51%)]  Loss:  3.030730 (3.4726)  Time: 0.588s, 1740.45/s  (2.609s,  392.43/s)  LR: 3.005e-05  Data: 0.022 (2.016)
Train: 20 [ 650/1171 ( 56%)]  Loss:  2.543363 (3.4062)  Time: 0.584s, 1752.23/s  (2.591s,  395.18/s)  LR: 3.005e-05  Data: 0.019 (1.998)
Train: 20 [ 700/1171 ( 60%)]  Loss:  3.448395 (3.4090)  Time: 0.587s, 1745.21/s  (2.616s,  391.42/s)  LR: 3.005e-05  Data: 0.020 (2.023)
Train: 20 [ 750/1171 ( 64%)]  Loss:  3.785314 (3.4326)  Time: 0.586s, 1747.66/s  (2.631s,  389.18/s)  LR: 3.005e-05  Data: 0.020 (2.036)
Train: 20 [ 800/1171 ( 68%)]  Loss:  4.055755 (3.4692)  Time: 0.584s, 1754.58/s  (2.650s,  386.44/s)  LR: 3.005e-05  Data: 0.020 (2.055)
Train: 20 [ 850/1171 ( 73%)]  Loss:  2.900050 (3.4376)  Time: 0.584s, 1752.16/s  (2.644s,  387.23/s)  LR: 3.005e-05  Data: 0.018 (2.047)
Train: 20 [ 900/1171 ( 77%)]  Loss:  3.001522 (3.4146)  Time: 0.584s, 1754.04/s  (2.644s,  387.32/s)  LR: 3.005e-05  Data: 0.020 (2.046)
Train: 20 [ 950/1171 ( 81%)]  Loss:  3.923335 (3.4401)  Time: 0.588s, 1741.60/s  (2.637s,  388.30/s)  LR: 3.005e-05  Data: 0.021 (2.040)
Train: 20 [1000/1171 ( 85%)]  Loss:  3.746142 (3.4546)  Time: 0.586s, 1747.36/s  (2.628s,  389.69/s)  LR: 3.005e-05  Data: 0.021 (2.030)
Train: 20 [1050/1171 ( 90%)]  Loss:  4.122942 (3.4850)  Time: 1.302s,  786.34/s  (2.637s,  388.32/s)  LR: 3.005e-05  Data: 0.708 (2.039)
Train: 20 [1100/1171 ( 94%)]  Loss:  2.991467 (3.4636)  Time: 4.672s,  219.16/s  (2.635s,  388.65/s)  LR: 3.005e-05  Data: 4.092 (2.036)
Train: 20 [1150/1171 ( 98%)]  Loss:  3.666513 (3.4720)  Time: 0.587s, 1744.65/s  (2.627s,  389.75/s)  LR: 3.005e-05  Data: 0.020 (2.028)
Train: 20 [1170/1171 (100%)]  Loss:  3.612993 (3.4777)  Time: 0.563s, 1819.62/s  (2.627s,  389.72/s)  LR: 3.005e-05  Data: 0.000 (2.028)
Test: [   0/97]  Time: 14.013 (14.013)  Loss:  0.4014 (0.4014)  Acc@1: 94.5312 (94.5312)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 2.191 (3.051)  Loss:  0.6870 (0.5301)  Acc@1: 87.9883 (91.5441)  Acc@5: 97.0703 (98.2805)
Test: [  97/97]  Time: 0.119 (2.885)  Loss:  0.4288 (0.5471)  Acc@1: 93.3036 (90.9500)  Acc@5: 98.9583 (97.8240)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-20.pth.tar', 90.95000001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-19.pth.tar', 90.79400002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-18.pth.tar', 90.60700001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-17.pth.tar', 90.1930000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-16.pth.tar', 89.94300001708984)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-15.pth.tar', 89.34300004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-14.pth.tar', 88.69799998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-13.pth.tar', 88.12699999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-12.pth.tar', 87.40700003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-11.pth.tar', 86.44299998535156)

Train: 21 [   0/1171 (  0%)]  Loss:  3.536236 (3.5362)  Time: 10.403s,   98.43/s  (10.403s,   98.43/s)  LR: 1.504e-05  Data: 9.368 (9.368)
Train: 21 [  50/1171 (  4%)]  Loss:  3.088065 (3.3122)  Time: 0.586s, 1748.14/s  (2.328s,  439.84/s)  LR: 1.504e-05  Data: 0.017 (1.738)
Train: 21 [ 100/1171 (  9%)]  Loss:  3.323515 (3.3159)  Time: 0.585s, 1751.89/s  (2.500s,  409.53/s)  LR: 1.504e-05  Data: 0.021 (1.908)
Train: 21 [ 150/1171 ( 13%)]  Loss:  3.873514 (3.4553)  Time: 0.586s, 1747.63/s  (2.505s,  408.84/s)  LR: 1.504e-05  Data: 0.019 (1.916)
Train: 21 [ 200/1171 ( 17%)]  Loss:  3.225313 (3.4093)  Time: 0.587s, 1743.32/s  (2.535s,  403.97/s)  LR: 1.504e-05  Data: 0.020 (1.948)
Train: 21 [ 250/1171 ( 21%)]  Loss:  3.390736 (3.4062)  Time: 0.588s, 1740.84/s  (2.516s,  407.05/s)  LR: 1.504e-05  Data: 0.018 (1.931)
Train: 21 [ 300/1171 ( 26%)]  Loss:  3.266070 (3.3862)  Time: 0.588s, 1742.66/s  (2.515s,  407.09/s)  LR: 1.504e-05  Data: 0.020 (1.930)
Train: 21 [ 350/1171 ( 30%)]  Loss:  2.839594 (3.3179)  Time: 0.589s, 1739.27/s  (2.487s,  411.79/s)  LR: 1.504e-05  Data: 0.020 (1.901)
Train: 21 [ 400/1171 ( 34%)]  Loss:  3.214010 (3.3063)  Time: 0.584s, 1752.02/s  (2.480s,  412.92/s)  LR: 1.504e-05  Data: 0.018 (1.895)
Train: 21 [ 450/1171 ( 38%)]  Loss:  3.614817 (3.3372)  Time: 0.586s, 1748.18/s  (2.479s,  413.09/s)  LR: 1.504e-05  Data: 0.019 (1.895)
Train: 21 [ 500/1171 ( 43%)]  Loss:  3.569841 (3.3583)  Time: 0.586s, 1747.18/s  (2.503s,  409.08/s)  LR: 1.504e-05  Data: 0.021 (1.919)
Train: 21 [ 550/1171 ( 47%)]  Loss:  3.018771 (3.3300)  Time: 0.589s, 1737.58/s  (2.510s,  407.89/s)  LR: 1.504e-05  Data: 0.022 (1.925)
Train: 21 [ 600/1171 ( 51%)]  Loss:  3.812941 (3.3672)  Time: 0.584s, 1753.99/s  (2.528s,  405.04/s)  LR: 1.504e-05  Data: 0.019 (1.943)
Train: 21 [ 650/1171 ( 56%)]  Loss:  3.539452 (3.3795)  Time: 0.587s, 1743.72/s  (2.529s,  404.94/s)  LR: 1.504e-05  Data: 0.020 (1.943)
Train: 21 [ 700/1171 ( 60%)]  Loss:  3.349340 (3.3775)  Time: 0.587s, 1745.54/s  (2.529s,  404.85/s)  LR: 1.504e-05  Data: 0.022 (1.943)
Train: 21 [ 750/1171 ( 64%)]  Loss:  3.543496 (3.3879)  Time: 0.585s, 1749.38/s  (2.514s,  407.31/s)  LR: 1.504e-05  Data: 0.018 (1.928)
Train: 21 [ 800/1171 ( 68%)]  Loss:  3.940531 (3.4204)  Time: 0.585s, 1749.52/s  (2.523s,  405.80/s)  LR: 1.504e-05  Data: 0.018 (1.937)
Train: 21 [ 850/1171 ( 73%)]  Loss:  3.402566 (3.4194)  Time: 0.585s, 1751.85/s  (2.524s,  405.71/s)  LR: 1.504e-05  Data: 0.021 (1.937)
Train: 21 [ 900/1171 ( 77%)]  Loss:  3.420353 (3.4194)  Time: 0.583s, 1756.39/s  (2.535s,  403.95/s)  LR: 1.504e-05  Data: 0.019 (1.948)
Train: 21 [ 950/1171 ( 81%)]  Loss:  3.193083 (3.4081)  Time: 0.585s, 1749.85/s  (2.531s,  404.57/s)  LR: 1.504e-05  Data: 0.021 (1.944)
Train: 21 [1000/1171 ( 85%)]  Loss:  3.665685 (3.4204)  Time: 0.586s, 1746.29/s  (2.529s,  404.95/s)  LR: 1.504e-05  Data: 0.021 (1.940)
Train: 21 [1050/1171 ( 90%)]  Loss:  3.653122 (3.4310)  Time: 0.584s, 1752.72/s  (2.516s,  406.92/s)  LR: 1.504e-05  Data: 0.018 (1.928)
Train: 21 [1100/1171 ( 94%)]  Loss:  3.355484 (3.4277)  Time: 0.585s, 1749.19/s  (2.510s,  407.89/s)  LR: 1.504e-05  Data: 0.019 (1.921)
Train: 21 [1150/1171 ( 98%)]  Loss:  3.076735 (3.4131)  Time: 0.587s, 1744.93/s  (2.496s,  410.28/s)  LR: 1.504e-05  Data: 0.021 (1.907)
Train: 21 [1170/1171 (100%)]  Loss:  3.797136 (3.4284)  Time: 0.566s, 1808.96/s  (2.505s,  408.73/s)  LR: 1.504e-05  Data: 0.000 (1.916)
Test: [   0/97]  Time: 15.984 (15.984)  Loss:  0.4093 (0.4093)  Acc@1: 94.5312 (94.5312)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.197 (3.383)  Loss:  0.6892 (0.5334)  Acc@1: 87.8906 (91.6379)  Acc@5: 97.0703 (98.3073)
Test: [  97/97]  Time: 0.119 (3.259)  Loss:  0.4411 (0.5499)  Acc@1: 93.3036 (91.0570)  Acc@5: 98.8095 (97.8540)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-21.pth.tar', 91.05700001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-20.pth.tar', 90.95000001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-19.pth.tar', 90.79400002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-18.pth.tar', 90.60700001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-17.pth.tar', 90.1930000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-16.pth.tar', 89.94300001708984)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-15.pth.tar', 89.34300004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-14.pth.tar', 88.69799998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-13.pth.tar', 88.12699999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-12.pth.tar', 87.40700003173828)

*** Best metric: 91.05700001464844 (epoch 21)

wandb: Waiting for W&B process to finish, PID 35699
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210523_201758-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1/logs/debug.log
wandb: Find internal logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210523_201758-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 21
wandb:   train_loss 3.42842
wandb:    eval_loss 0.5499
wandb:    eval_top1 91.057
wandb:    eval_top5 97.854
wandb:     _runtime 71285
wandb:   _timestamp 1621839963
wandb:        _step 21
wandb: Run history:
wandb:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   train_loss ‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    eval_loss ‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    eval_top1 ‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:    eval_top5 ‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced PreTraining_vit_deit_tiny_patch16_224_fake_1k: https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
--End--
Mon May 24 16:06:15 JST 2021

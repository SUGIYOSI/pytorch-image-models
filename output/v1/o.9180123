--Start--
Tue Jun 1 22:37:32 JST 2021
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 0
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
wandb: Currently logged in as: gyama_x (use `wandb login --relogin` to force relogin)
wandb: WARNING Tried to auto resume run with id PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k but id PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1 is set.
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.10.27
wandb: Resuming run PreTraining_vit_deit_tiny_patch16_224_fake_1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gyama_x/pytorch-image-models
wandb: üöÄ View run at https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run data is saved locally in /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210601_223738-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run `wandb offline` to turn off syncing.

Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar' (epoch 121)
Using native Torch DistributedDataParallel.
Scheduled epochs: 144
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Train: 122 [   0/1171 (  0%)]  Loss:  2.917833 (2.9178)  Time: 10.677s,   95.91/s  (10.677s,   95.91/s)  LR: 6.593e-05  Data: 9.642 (9.642)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 122 [  50/1171 (  4%)]  Loss:  3.010994 (2.9644)  Time: 0.584s, 1754.65/s  (2.283s,  448.49/s)  LR: 6.593e-05  Data: 0.020 (1.688)
Train: 122 [ 100/1171 (  9%)]  Loss:  3.020309 (2.9830)  Time: 0.584s, 1752.07/s  (2.138s,  479.03/s)  LR: 6.593e-05  Data: 0.020 (1.552)
Train: 122 [ 150/1171 ( 13%)]  Loss:  2.579151 (2.8821)  Time: 0.584s, 1753.45/s  (2.106s,  486.34/s)  LR: 6.593e-05  Data: 0.020 (1.515)
Train: 122 [ 200/1171 ( 17%)]  Loss:  2.744208 (2.8545)  Time: 0.715s, 1431.91/s  (2.204s,  464.65/s)  LR: 6.593e-05  Data: 0.143 (1.611)
Train: 122 [ 250/1171 ( 21%)]  Loss:  2.403639 (2.7794)  Time: 0.584s, 1754.64/s  (2.181s,  469.58/s)  LR: 6.593e-05  Data: 0.019 (1.581)
Train: 122 [ 300/1171 ( 26%)]  Loss:  3.163528 (2.8342)  Time: 2.694s,  380.15/s  (2.191s,  467.45/s)  LR: 6.593e-05  Data: 2.132 (1.592)
Train: 122 [ 350/1171 ( 30%)]  Loss:  2.577994 (2.8022)  Time: 2.527s,  405.18/s  (2.211s,  463.05/s)  LR: 6.593e-05  Data: 1.903 (1.611)
Train: 122 [ 400/1171 ( 34%)]  Loss:  2.950588 (2.8187)  Time: 3.815s,  268.39/s  (2.231s,  458.93/s)  LR: 6.593e-05  Data: 3.254 (1.629)
Train: 122 [ 450/1171 ( 38%)]  Loss:  2.614558 (2.7983)  Time: 0.582s, 1760.37/s  (2.223s,  460.65/s)  LR: 6.593e-05  Data: 0.017 (1.620)
Train: 122 [ 500/1171 ( 43%)]  Loss:  2.603367 (2.7806)  Time: 0.589s, 1739.80/s  (2.222s,  460.94/s)  LR: 6.593e-05  Data: 0.021 (1.618)
Train: 122 [ 550/1171 ( 47%)]  Loss:  2.770290 (2.7797)  Time: 0.585s, 1750.04/s  (2.200s,  465.54/s)  LR: 6.593e-05  Data: 0.021 (1.597)
Train: 122 [ 600/1171 ( 51%)]  Loss:  3.418436 (2.8288)  Time: 5.905s,  173.42/s  (2.219s,  461.37/s)  LR: 6.593e-05  Data: 5.217 (1.614)
Train: 122 [ 650/1171 ( 56%)]  Loss:  2.770446 (2.8247)  Time: 0.589s, 1737.45/s  (2.205s,  464.33/s)  LR: 6.593e-05  Data: 0.019 (1.600)
Train: 122 [ 700/1171 ( 60%)]  Loss:  2.983515 (2.8353)  Time: 6.808s,  150.40/s  (2.212s,  462.93/s)  LR: 6.593e-05  Data: 6.248 (1.608)
Train: 122 [ 750/1171 ( 64%)]  Loss:  3.060949 (2.8494)  Time: 0.587s, 1743.01/s  (2.215s,  462.32/s)  LR: 6.593e-05  Data: 0.021 (1.612)
Train: 122 [ 800/1171 ( 68%)]  Loss:  2.833188 (2.8484)  Time: 7.328s,  139.75/s  (2.225s,  460.32/s)  LR: 6.593e-05  Data: 6.652 (1.623)
Train: 122 [ 850/1171 ( 73%)]  Loss:  2.807042 (2.8461)  Time: 0.589s, 1738.75/s  (2.223s,  460.73/s)  LR: 6.593e-05  Data: 0.024 (1.622)
Train: 122 [ 900/1171 ( 77%)]  Loss:  3.180018 (2.8637)  Time: 5.690s,  179.95/s  (2.220s,  461.35/s)  LR: 6.593e-05  Data: 5.129 (1.620)
Train: 122 [ 950/1171 ( 81%)]  Loss:  2.697173 (2.8554)  Time: 0.592s, 1729.91/s  (2.206s,  464.14/s)  LR: 6.593e-05  Data: 0.023 (1.607)
Train: 122 [1000/1171 ( 85%)]  Loss:  2.408276 (2.8341)  Time: 2.498s,  409.88/s  (2.193s,  466.99/s)  LR: 6.593e-05  Data: 1.793 (1.593)
Train: 122 [1050/1171 ( 90%)]  Loss:  3.014874 (2.8423)  Time: 0.588s, 1742.91/s  (2.199s,  465.60/s)  LR: 6.593e-05  Data: 0.023 (1.600)
Train: 122 [1100/1171 ( 94%)]  Loss:  3.009286 (2.8496)  Time: 2.164s,  473.29/s  (2.205s,  464.48/s)  LR: 6.593e-05  Data: 1.579 (1.605)
Train: 122 [1150/1171 ( 98%)]  Loss:  2.500792 (2.8350)  Time: 2.477s,  413.48/s  (2.202s,  465.10/s)  LR: 6.593e-05  Data: 1.805 (1.602)
Train: 122 [1170/1171 (100%)]  Loss:  2.838766 (2.8352)  Time: 0.568s, 1802.87/s  (2.200s,  465.49/s)  LR: 6.593e-05  Data: 0.000 (1.600)
Test: [   0/97]  Time: 12.875 (12.875)  Loss:  0.2748 (0.2748)  Acc@1: 96.8750 (96.8750)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (2.901)  Loss:  0.4719 (0.3554)  Acc@1: 92.0898 (94.9314)  Acc@5: 98.3398 (98.9047)
Test: [  97/97]  Time: 0.426 (2.782)  Loss:  0.3421 (0.3703)  Acc@1: 93.6012 (94.3610)  Acc@5: 98.9583 (98.7310)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-122.pth.tar', 94.3610000390625)

Train: 123 [   0/1171 (  0%)]  Loss:  2.860315 (2.8603)  Time: 8.893s,  115.15/s  (8.893s,  115.15/s)  LR: 6.105e-05  Data: 8.294 (8.294)
Train: 123 [  50/1171 (  4%)]  Loss:  3.220364 (3.0403)  Time: 0.588s, 1741.43/s  (2.039s,  502.30/s)  LR: 6.105e-05  Data: 0.022 (1.463)
Train: 123 [ 100/1171 (  9%)]  Loss:  2.968432 (3.0164)  Time: 0.588s, 1741.92/s  (2.018s,  507.41/s)  LR: 6.105e-05  Data: 0.021 (1.440)
Train: 123 [ 150/1171 ( 13%)]  Loss:  2.751940 (2.9503)  Time: 0.590s, 1734.43/s  (2.115s,  484.12/s)  LR: 6.105e-05  Data: 0.026 (1.535)
Train: 123 [ 200/1171 ( 17%)]  Loss:  2.718399 (2.9039)  Time: 0.584s, 1754.33/s  (2.189s,  467.83/s)  LR: 6.105e-05  Data: 0.020 (1.605)
Train: 123 [ 250/1171 ( 21%)]  Loss:  2.634767 (2.8590)  Time: 0.589s, 1737.61/s  (2.186s,  468.52/s)  LR: 6.105e-05  Data: 0.022 (1.598)
Train: 123 [ 300/1171 ( 26%)]  Loss:  2.790116 (2.8492)  Time: 0.587s, 1744.42/s  (2.209s,  463.65/s)  LR: 6.105e-05  Data: 0.019 (1.621)
Train: 123 [ 350/1171 ( 30%)]  Loss:  2.720075 (2.8331)  Time: 0.586s, 1746.10/s  (2.202s,  465.05/s)  LR: 6.105e-05  Data: 0.020 (1.614)
Train: 123 [ 400/1171 ( 34%)]  Loss:  3.210410 (2.8750)  Time: 6.314s,  162.17/s  (2.211s,  463.15/s)  LR: 6.105e-05  Data: 5.753 (1.621)
Train: 123 [ 450/1171 ( 38%)]  Loss:  2.699580 (2.8574)  Time: 0.588s, 1742.11/s  (2.193s,  466.87/s)  LR: 6.105e-05  Data: 0.023 (1.604)
Train: 123 [ 500/1171 ( 43%)]  Loss:  2.932874 (2.8643)  Time: 6.219s,  164.66/s  (2.199s,  465.62/s)  LR: 6.105e-05  Data: 5.545 (1.609)
Train: 123 [ 550/1171 ( 47%)]  Loss:  2.731175 (2.8532)  Time: 1.010s, 1014.05/s  (2.238s,  457.61/s)  LR: 6.105e-05  Data: 0.446 (1.647)
Train: 123 [ 600/1171 ( 51%)]  Loss:  3.179077 (2.8783)  Time: 6.457s,  158.59/s  (2.271s,  450.81/s)  LR: 6.105e-05  Data: 5.893 (1.680)
Train: 123 [ 650/1171 ( 56%)]  Loss:  2.958876 (2.8840)  Time: 0.588s, 1742.37/s  (2.279s,  449.30/s)  LR: 6.105e-05  Data: 0.024 (1.686)
Train: 123 [ 700/1171 ( 60%)]  Loss:  2.787258 (2.8776)  Time: 6.839s,  149.73/s  (2.287s,  447.79/s)  LR: 6.105e-05  Data: 6.214 (1.694)
Train: 123 [ 750/1171 ( 64%)]  Loss:  2.791176 (2.8722)  Time: 0.666s, 1536.98/s  (2.285s,  448.21/s)  LR: 6.105e-05  Data: 0.021 (1.690)
Train: 123 [ 800/1171 ( 68%)]  Loss:  2.674599 (2.8606)  Time: 6.192s,  165.37/s  (2.281s,  448.84/s)  LR: 6.105e-05  Data: 5.630 (1.687)
Train: 123 [ 850/1171 ( 73%)]  Loss:  2.868236 (2.8610)  Time: 0.590s, 1736.65/s  (2.272s,  450.71/s)  LR: 6.105e-05  Data: 0.026 (1.677)
Train: 123 [ 900/1171 ( 77%)]  Loss:  3.018005 (2.8692)  Time: 5.039s,  203.22/s  (2.271s,  450.97/s)  LR: 6.105e-05  Data: 4.477 (1.674)
Train: 123 [ 950/1171 ( 81%)]  Loss:  3.070150 (2.8793)  Time: 0.586s, 1746.68/s  (2.286s,  447.90/s)  LR: 6.105e-05  Data: 0.023 (1.689)
Train: 123 [1000/1171 ( 85%)]  Loss:  3.378347 (2.9031)  Time: 7.528s,  136.03/s  (2.294s,  446.37/s)  LR: 6.105e-05  Data: 6.805 (1.698)
Train: 123 [1050/1171 ( 90%)]  Loss:  3.127775 (2.9133)  Time: 0.587s, 1745.23/s  (2.290s,  447.09/s)  LR: 6.105e-05  Data: 0.024 (1.694)
Train: 123 [1100/1171 ( 94%)]  Loss:  2.967963 (2.9156)  Time: 6.162s,  166.18/s  (2.290s,  447.13/s)  LR: 6.105e-05  Data: 5.498 (1.693)
Train: 123 [1150/1171 ( 98%)]  Loss:  2.738410 (2.9083)  Time: 0.585s, 1749.18/s  (2.285s,  448.16/s)  LR: 6.105e-05  Data: 0.020 (1.688)
Train: 123 [1170/1171 (100%)]  Loss:  2.810735 (2.9044)  Time: 0.568s, 1803.34/s  (2.283s,  448.55/s)  LR: 6.105e-05  Data: 0.000 (1.686)
Test: [   0/97]  Time: 13.015 (13.015)  Loss:  0.2912 (0.2912)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.201 (2.927)  Loss:  0.4430 (0.3561)  Acc@1: 92.6758 (94.9104)  Acc@5: 98.4375 (98.8664)
Test: [  97/97]  Time: 0.120 (2.887)  Loss:  0.3478 (0.3710)  Acc@1: 93.7500 (94.2850)  Acc@5: 99.2560 (98.6950)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-122.pth.tar', 94.3610000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-123.pth.tar', 94.28500005126953)

Train: 124 [   0/1171 (  0%)]  Loss:  2.885436 (2.8854)  Time: 16.625s,   61.59/s  (16.625s,   61.59/s)  LR: 5.638e-05  Data: 15.298 (15.298)
Train: 124 [  50/1171 (  4%)]  Loss:  2.752243 (2.8188)  Time: 0.585s, 1749.26/s  (2.409s,  425.14/s)  LR: 5.638e-05  Data: 0.021 (1.813)
Train: 124 [ 100/1171 (  9%)]  Loss:  3.354518 (2.9974)  Time: 0.585s, 1751.07/s  (2.331s,  439.33/s)  LR: 5.638e-05  Data: 0.022 (1.737)
Train: 124 [ 150/1171 ( 13%)]  Loss:  2.399521 (2.8479)  Time: 0.586s, 1747.81/s  (2.263s,  452.43/s)  LR: 5.638e-05  Data: 0.022 (1.673)
Train: 124 [ 200/1171 ( 17%)]  Loss:  2.622269 (2.8028)  Time: 0.777s, 1318.23/s  (2.268s,  451.53/s)  LR: 5.638e-05  Data: 0.109 (1.676)
Train: 124 [ 250/1171 ( 21%)]  Loss:  2.697403 (2.7852)  Time: 0.583s, 1755.04/s  (2.230s,  459.28/s)  LR: 5.638e-05  Data: 0.021 (1.630)
Train: 124 [ 300/1171 ( 26%)]  Loss:  2.488011 (2.7428)  Time: 1.330s,  769.69/s  (2.215s,  462.30/s)  LR: 5.638e-05  Data: 0.759 (1.613)
Train: 124 [ 350/1171 ( 30%)]  Loss:  2.821586 (2.7526)  Time: 0.831s, 1232.42/s  (2.185s,  468.63/s)  LR: 5.638e-05  Data: 0.260 (1.583)
Train: 124 [ 400/1171 ( 34%)]  Loss:  2.948658 (2.7744)  Time: 2.855s,  358.71/s  (2.168s,  472.28/s)  LR: 5.638e-05  Data: 2.280 (1.568)
Train: 124 [ 450/1171 ( 38%)]  Loss:  2.824437 (2.7794)  Time: 0.584s, 1751.93/s  (2.198s,  465.94/s)  LR: 5.638e-05  Data: 0.022 (1.597)
Train: 124 [ 500/1171 ( 43%)]  Loss:  2.935939 (2.7936)  Time: 4.828s,  212.09/s  (2.222s,  460.78/s)  LR: 5.638e-05  Data: 4.148 (1.622)
Train: 124 [ 550/1171 ( 47%)]  Loss:  2.840569 (2.7975)  Time: 0.585s, 1751.08/s  (2.237s,  457.84/s)  LR: 5.638e-05  Data: 0.022 (1.636)
Train: 124 [ 600/1171 ( 51%)]  Loss:  2.764754 (2.7950)  Time: 4.064s,  251.97/s  (2.256s,  453.81/s)  LR: 5.638e-05  Data: 3.478 (1.656)
Train: 124 [ 650/1171 ( 56%)]  Loss:  3.044100 (2.8128)  Time: 2.018s,  507.31/s  (2.262s,  452.77/s)  LR: 5.638e-05  Data: 1.437 (1.659)
Train: 124 [ 700/1171 ( 60%)]  Loss:  3.107577 (2.8325)  Time: 0.806s, 1270.73/s  (2.265s,  452.19/s)  LR: 5.638e-05  Data: 0.117 (1.660)
Train: 124 [ 750/1171 ( 64%)]  Loss:  2.971282 (2.8411)  Time: 1.124s,  910.73/s  (2.257s,  453.61/s)  LR: 5.638e-05  Data: 0.553 (1.653)
Train: 124 [ 800/1171 ( 68%)]  Loss:  3.003141 (2.8507)  Time: 4.610s,  222.11/s  (2.271s,  450.97/s)  LR: 5.638e-05  Data: 4.046 (1.667)
Train: 124 [ 850/1171 ( 73%)]  Loss:  2.338268 (2.8222)  Time: 0.975s, 1050.02/s  (2.274s,  450.38/s)  LR: 5.638e-05  Data: 0.358 (1.668)
Train: 124 [ 900/1171 ( 77%)]  Loss:  2.621552 (2.8116)  Time: 6.092s,  168.09/s  (2.289s,  447.37/s)  LR: 5.638e-05  Data: 5.416 (1.684)
Train: 124 [ 950/1171 ( 81%)]  Loss:  3.292878 (2.8357)  Time: 0.770s, 1329.01/s  (2.281s,  448.83/s)  LR: 5.638e-05  Data: 0.175 (1.676)
Train: 124 [1000/1171 ( 85%)]  Loss:  2.690767 (2.8288)  Time: 10.047s,  101.92/s  (2.288s,  447.55/s)  LR: 5.638e-05  Data: 9.128 (1.683)
Train: 124 [1050/1171 ( 90%)]  Loss:  2.978659 (2.8356)  Time: 0.589s, 1738.77/s  (2.284s,  448.31/s)  LR: 5.638e-05  Data: 0.021 (1.680)
Train: 124 [1100/1171 ( 94%)]  Loss:  3.266757 (2.8544)  Time: 6.683s,  153.23/s  (2.284s,  448.31/s)  LR: 5.638e-05  Data: 6.044 (1.680)
Train: 124 [1150/1171 ( 98%)]  Loss:  2.988157 (2.8599)  Time: 1.089s,  940.50/s  (2.277s,  449.76/s)  LR: 5.638e-05  Data: 0.502 (1.674)
Train: 124 [1170/1171 (100%)]  Loss:  3.143003 (2.8713)  Time: 0.566s, 1809.20/s  (2.274s,  450.32/s)  LR: 5.638e-05  Data: 0.000 (1.671)
Test: [   0/97]  Time: 12.180 (12.180)  Loss:  0.3224 (0.3224)  Acc@1: 96.4844 (96.4844)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.198 (3.343)  Loss:  0.5009 (0.4026)  Acc@1: 92.0898 (94.8817)  Acc@5: 98.4375 (98.8626)
Test: [  97/97]  Time: 0.119 (3.244)  Loss:  0.3629 (0.4108)  Acc@1: 94.6429 (94.3860)  Acc@5: 99.2560 (98.7010)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-124.pth.tar', 94.38600002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-122.pth.tar', 94.3610000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-123.pth.tar', 94.28500005126953)

Train: 125 [   0/1171 (  0%)]  Loss:  3.107194 (3.1072)  Time: 10.195s,  100.45/s  (10.195s,  100.45/s)  LR: 5.192e-05  Data: 9.596 (9.596)
Train: 125 [  50/1171 (  4%)]  Loss:  2.949048 (3.0281)  Time: 0.953s, 1074.60/s  (2.422s,  422.76/s)  LR: 5.192e-05  Data: 0.021 (1.821)
Train: 125 [ 100/1171 (  9%)]  Loss:  2.792259 (2.9495)  Time: 0.587s, 1744.83/s  (2.359s,  434.15/s)  LR: 5.192e-05  Data: 0.021 (1.754)
Train: 125 [ 150/1171 ( 13%)]  Loss:  2.893929 (2.9356)  Time: 2.352s,  435.33/s  (2.300s,  445.16/s)  LR: 5.192e-05  Data: 1.790 (1.689)
Train: 125 [ 200/1171 ( 17%)]  Loss:  2.468955 (2.8423)  Time: 0.588s, 1742.59/s  (2.278s,  449.58/s)  LR: 5.192e-05  Data: 0.021 (1.667)
Train: 125 [ 250/1171 ( 21%)]  Loss:  2.785968 (2.8329)  Time: 1.263s,  810.79/s  (2.264s,  452.32/s)  LR: 5.192e-05  Data: 0.498 (1.651)
Train: 125 [ 300/1171 ( 26%)]  Loss:  3.146848 (2.8777)  Time: 0.588s, 1740.89/s  (2.308s,  443.61/s)  LR: 5.192e-05  Data: 0.019 (1.696)
Train: 125 [ 350/1171 ( 30%)]  Loss:  3.243557 (2.9235)  Time: 3.259s,  314.22/s  (2.322s,  440.95/s)  LR: 5.192e-05  Data: 2.646 (1.711)
Train: 125 [ 400/1171 ( 34%)]  Loss:  2.637381 (2.8917)  Time: 0.588s, 1741.13/s  (2.304s,  444.49/s)  LR: 5.192e-05  Data: 0.021 (1.695)
Train: 125 [ 450/1171 ( 38%)]  Loss:  3.222973 (2.9248)  Time: 5.706s,  179.47/s  (2.306s,  444.13/s)  LR: 5.192e-05  Data: 5.057 (1.698)
Train: 125 [ 500/1171 ( 43%)]  Loss:  2.815698 (2.9149)  Time: 0.586s, 1746.54/s  (2.310s,  443.32/s)  LR: 5.192e-05  Data: 0.020 (1.703)
Train: 125 [ 550/1171 ( 47%)]  Loss:  2.530399 (2.8829)  Time: 7.796s,  131.36/s  (2.334s,  438.78/s)  LR: 5.192e-05  Data: 7.222 (1.728)
Train: 125 [ 600/1171 ( 51%)]  Loss:  2.501339 (2.8535)  Time: 0.588s, 1742.33/s  (2.328s,  439.82/s)  LR: 5.192e-05  Data: 0.021 (1.721)
Train: 125 [ 650/1171 ( 56%)]  Loss:  3.169563 (2.8761)  Time: 6.700s,  152.83/s  (2.313s,  442.73/s)  LR: 5.192e-05  Data: 6.136 (1.707)
Train: 125 [ 700/1171 ( 60%)]  Loss:  2.707962 (2.8649)  Time: 0.588s, 1741.66/s  (2.330s,  439.57/s)  LR: 5.192e-05  Data: 0.021 (1.725)
Train: 125 [ 750/1171 ( 64%)]  Loss:  2.951325 (2.8703)  Time: 7.562s,  135.41/s  (2.338s,  438.02/s)  LR: 5.192e-05  Data: 6.891 (1.735)
Train: 125 [ 800/1171 ( 68%)]  Loss:  3.030507 (2.8797)  Time: 0.588s, 1740.24/s  (2.334s,  438.65/s)  LR: 5.192e-05  Data: 0.022 (1.732)
Train: 125 [ 850/1171 ( 73%)]  Loss:  3.114028 (2.8927)  Time: 7.609s,  134.59/s  (2.338s,  438.05/s)  LR: 5.192e-05  Data: 7.021 (1.736)
Train: 125 [ 900/1171 ( 77%)]  Loss:  2.889702 (2.8926)  Time: 0.593s, 1725.55/s  (2.328s,  439.86/s)  LR: 5.192e-05  Data: 0.026 (1.727)
Train: 125 [ 950/1171 ( 81%)]  Loss:  2.711387 (2.8835)  Time: 7.591s,  134.89/s  (2.326s,  440.31/s)  LR: 5.192e-05  Data: 6.941 (1.726)
Train: 125 [1000/1171 ( 85%)]  Loss:  2.793852 (2.8792)  Time: 0.588s, 1740.05/s  (2.319s,  441.51/s)  LR: 5.192e-05  Data: 0.021 (1.720)
Train: 125 [1050/1171 ( 90%)]  Loss:  2.950548 (2.8825)  Time: 9.846s,  104.00/s  (2.332s,  439.16/s)  LR: 5.192e-05  Data: 9.285 (1.733)
Train: 125 [1100/1171 ( 94%)]  Loss:  2.956564 (2.8857)  Time: 0.587s, 1744.44/s  (2.328s,  439.92/s)  LR: 5.192e-05  Data: 0.020 (1.730)
Train: 125 [1150/1171 ( 98%)]  Loss:  3.097828 (2.8945)  Time: 7.206s,  142.09/s  (2.336s,  438.29/s)  LR: 5.192e-05  Data: 6.602 (1.739)
Train: 125 [1170/1171 (100%)]  Loss:  3.109468 (2.9031)  Time: 0.564s, 1816.86/s  (2.330s,  439.50/s)  LR: 5.192e-05  Data: 0.000 (1.732)
Test: [   0/97]  Time: 13.222 (13.222)  Loss:  0.2755 (0.2755)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (3.087)  Loss:  0.4611 (0.3533)  Acc@1: 91.5039 (95.0253)  Acc@5: 98.2422 (98.9526)
Test: [  97/97]  Time: 0.120 (2.980)  Loss:  0.3381 (0.3657)  Acc@1: 94.1964 (94.4700)  Acc@5: 99.2560 (98.7680)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-125.pth.tar', 94.46999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-124.pth.tar', 94.38600002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-122.pth.tar', 94.3610000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-123.pth.tar', 94.28500005126953)

Train: 126 [   0/1171 (  0%)]  Loss:  2.762401 (2.7624)  Time: 10.814s,   94.69/s  (10.814s,   94.69/s)  LR: 4.768e-05  Data: 9.611 (9.611)
Train: 126 [  50/1171 (  4%)]  Loss:  2.725756 (2.7441)  Time: 0.590s, 1736.51/s  (2.256s,  453.90/s)  LR: 4.768e-05  Data: 0.020 (1.659)
Train: 126 [ 100/1171 (  9%)]  Loss:  2.609005 (2.6991)  Time: 0.587s, 1744.10/s  (2.216s,  462.15/s)  LR: 4.768e-05  Data: 0.020 (1.624)
Train: 126 [ 150/1171 ( 13%)]  Loss:  2.732242 (2.7074)  Time: 0.587s, 1745.27/s  (2.284s,  448.25/s)  LR: 4.768e-05  Data: 0.024 (1.692)
Train: 126 [ 200/1171 ( 17%)]  Loss:  3.328405 (2.8316)  Time: 0.587s, 1744.15/s  (2.281s,  449.02/s)  LR: 4.768e-05  Data: 0.024 (1.692)
Train: 126 [ 250/1171 ( 21%)]  Loss:  2.762396 (2.8200)  Time: 0.585s, 1749.05/s  (2.295s,  446.21/s)  LR: 4.768e-05  Data: 0.021 (1.708)
Train: 126 [ 300/1171 ( 26%)]  Loss:  3.032590 (2.8504)  Time: 2.121s,  482.70/s  (2.320s,  441.30/s)  LR: 4.768e-05  Data: 1.411 (1.728)
Train: 126 [ 350/1171 ( 30%)]  Loss:  2.565613 (2.8148)  Time: 0.586s, 1748.47/s  (2.311s,  443.01/s)  LR: 4.768e-05  Data: 0.022 (1.712)
Train: 126 [ 400/1171 ( 34%)]  Loss:  3.275323 (2.8660)  Time: 0.586s, 1746.00/s  (2.292s,  446.75/s)  LR: 4.768e-05  Data: 0.020 (1.691)
Train: 126 [ 450/1171 ( 38%)]  Loss:  2.778017 (2.8572)  Time: 0.587s, 1744.24/s  (2.284s,  448.30/s)  LR: 4.768e-05  Data: 0.018 (1.685)
Train: 126 [ 500/1171 ( 43%)]  Loss:  2.976942 (2.8681)  Time: 0.591s, 1733.76/s  (2.275s,  450.16/s)  LR: 4.768e-05  Data: 0.019 (1.677)
Train: 126 [ 550/1171 ( 47%)]  Loss:  2.987439 (2.8780)  Time: 0.587s, 1745.64/s  (2.310s,  443.34/s)  LR: 4.768e-05  Data: 0.018 (1.713)
Train: 126 [ 600/1171 ( 51%)]  Loss:  3.237981 (2.9057)  Time: 0.591s, 1732.18/s  (2.318s,  441.67/s)  LR: 4.768e-05  Data: 0.022 (1.722)
Train: 126 [ 650/1171 ( 56%)]  Loss:  3.175293 (2.9250)  Time: 0.586s, 1747.08/s  (2.332s,  439.14/s)  LR: 4.768e-05  Data: 0.019 (1.736)
Train: 126 [ 700/1171 ( 60%)]  Loss:  3.084495 (2.9356)  Time: 0.587s, 1743.42/s  (2.326s,  440.24/s)  LR: 4.768e-05  Data: 0.017 (1.731)
Train: 126 [ 750/1171 ( 64%)]  Loss:  2.622066 (2.9160)  Time: 0.589s, 1739.65/s  (2.324s,  440.64/s)  LR: 4.768e-05  Data: 0.020 (1.730)
Train: 126 [ 800/1171 ( 68%)]  Loss:  3.220301 (2.9339)  Time: 0.591s, 1733.57/s  (2.310s,  443.34/s)  LR: 4.768e-05  Data: 0.017 (1.716)
Train: 126 [ 850/1171 ( 73%)]  Loss:  2.852262 (2.9294)  Time: 0.588s, 1741.28/s  (2.307s,  443.94/s)  LR: 4.768e-05  Data: 0.021 (1.713)
Train: 126 [ 900/1171 ( 77%)]  Loss:  2.922097 (2.9290)  Time: 0.590s, 1736.83/s  (2.295s,  446.28/s)  LR: 4.768e-05  Data: 0.020 (1.701)
Train: 126 [ 950/1171 ( 81%)]  Loss:  3.226898 (2.9439)  Time: 0.588s, 1741.42/s  (2.316s,  442.19/s)  LR: 4.768e-05  Data: 0.021 (1.723)
Train: 126 [1000/1171 ( 85%)]  Loss:  2.573708 (2.9262)  Time: 0.586s, 1746.23/s  (2.316s,  442.06/s)  LR: 4.768e-05  Data: 0.019 (1.724)
Train: 126 [1050/1171 ( 90%)]  Loss:  2.347846 (2.9000)  Time: 0.587s, 1743.64/s  (2.321s,  441.12/s)  LR: 4.768e-05  Data: 0.025 (1.728)
Train: 126 [1100/1171 ( 94%)]  Loss:  3.113278 (2.9092)  Time: 0.587s, 1744.29/s  (2.319s,  441.54/s)  LR: 4.768e-05  Data: 0.021 (1.726)
Train: 126 [1150/1171 ( 98%)]  Loss:  3.028905 (2.9142)  Time: 0.587s, 1744.24/s  (2.320s,  441.42/s)  LR: 4.768e-05  Data: 0.024 (1.727)
Train: 126 [1170/1171 (100%)]  Loss:  2.730326 (2.9069)  Time: 0.566s, 1809.50/s  (2.320s,  441.46/s)  LR: 4.768e-05  Data: 0.000 (1.727)
Test: [   0/97]  Time: 12.729 (12.729)  Loss:  0.3139 (0.3139)  Acc@1: 96.4844 (96.4844)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (2.975)  Loss:  0.4659 (0.3781)  Acc@1: 92.8711 (94.9946)  Acc@5: 98.1445 (98.9162)
Test: [  97/97]  Time: 0.119 (3.016)  Loss:  0.3408 (0.3906)  Acc@1: 94.6429 (94.4520)  Acc@5: 99.4048 (98.7550)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-125.pth.tar', 94.46999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-126.pth.tar', 94.45200002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-124.pth.tar', 94.38600002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-122.pth.tar', 94.3610000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-123.pth.tar', 94.28500005126953)

Train: 127 [   0/1171 (  0%)]  Loss:  2.772337 (2.7723)  Time: 14.884s,   68.80/s  (14.884s,   68.80/s)  LR: 4.366e-05  Data: 13.769 (13.769)
Train: 127 [  50/1171 (  4%)]  Loss:  2.492091 (2.6322)  Time: 0.588s, 1741.71/s  (2.433s,  420.86/s)  LR: 4.366e-05  Data: 0.022 (1.839)
Train: 127 [ 100/1171 (  9%)]  Loss:  2.956235 (2.7402)  Time: 0.585s, 1750.90/s  (2.437s,  420.18/s)  LR: 4.366e-05  Data: 0.021 (1.849)
Train: 127 [ 150/1171 ( 13%)]  Loss:  3.128361 (2.8373)  Time: 2.607s,  392.74/s  (2.385s,  429.31/s)  LR: 4.366e-05  Data: 1.858 (1.793)
Train: 127 [ 200/1171 ( 17%)]  Loss:  2.466432 (2.7631)  Time: 1.983s,  516.35/s  (2.360s,  433.98/s)  LR: 4.366e-05  Data: 1.417 (1.765)
Train: 127 [ 250/1171 ( 21%)]  Loss:  2.636006 (2.7419)  Time: 1.829s,  559.88/s  (2.327s,  439.99/s)  LR: 4.366e-05  Data: 1.238 (1.731)
Train: 127 [ 300/1171 ( 26%)]  Loss:  3.327235 (2.8255)  Time: 1.044s,  980.94/s  (2.322s,  441.03/s)  LR: 4.366e-05  Data: 0.482 (1.720)
Train: 127 [ 350/1171 ( 30%)]  Loss:  3.336449 (2.8894)  Time: 4.866s,  210.45/s  (2.307s,  443.96/s)  LR: 4.366e-05  Data: 4.110 (1.704)
Train: 127 [ 400/1171 ( 34%)]  Loss:  3.007050 (2.9025)  Time: 0.587s, 1745.68/s  (2.342s,  437.17/s)  LR: 4.366e-05  Data: 0.019 (1.740)
Train: 127 [ 450/1171 ( 38%)]  Loss:  3.086865 (2.9209)  Time: 5.351s,  191.37/s  (2.348s,  436.17/s)  LR: 4.366e-05  Data: 4.789 (1.744)
Train: 127 [ 500/1171 ( 43%)]  Loss:  2.644938 (2.8958)  Time: 0.587s, 1743.96/s  (2.362s,  433.50/s)  LR: 4.366e-05  Data: 0.020 (1.760)
Train: 127 [ 550/1171 ( 47%)]  Loss:  2.821591 (2.8896)  Time: 3.050s,  335.71/s  (2.380s,  430.25/s)  LR: 4.366e-05  Data: 2.338 (1.776)
Train: 127 [ 600/1171 ( 51%)]  Loss:  3.045403 (2.9016)  Time: 0.586s, 1746.07/s  (2.389s,  428.68/s)  LR: 4.366e-05  Data: 0.019 (1.784)
Train: 127 [ 650/1171 ( 56%)]  Loss:  2.971547 (2.9066)  Time: 4.106s,  249.40/s  (2.384s,  429.47/s)  LR: 4.366e-05  Data: 3.457 (1.778)
Train: 127 [ 700/1171 ( 60%)]  Loss:  2.647768 (2.8894)  Time: 0.585s, 1751.38/s  (2.376s,  430.90/s)  LR: 4.366e-05  Data: 0.020 (1.769)
Train: 127 [ 750/1171 ( 64%)]  Loss:  2.817603 (2.8849)  Time: 6.265s,  163.44/s  (2.381s,  430.04/s)  LR: 4.366e-05  Data: 5.585 (1.773)
Train: 127 [ 800/1171 ( 68%)]  Loss:  3.089245 (2.8969)  Time: 0.587s, 1744.37/s  (2.395s,  427.57/s)  LR: 4.366e-05  Data: 0.020 (1.787)
Train: 127 [ 850/1171 ( 73%)]  Loss:  3.086874 (2.9074)  Time: 2.784s,  367.80/s  (2.403s,  426.08/s)  LR: 4.366e-05  Data: 2.222 (1.796)
Train: 127 [ 900/1171 ( 77%)]  Loss:  2.749341 (2.8991)  Time: 0.586s, 1746.80/s  (2.407s,  425.37/s)  LR: 4.366e-05  Data: 0.019 (1.800)
Train: 127 [ 950/1171 ( 81%)]  Loss:  3.134988 (2.9109)  Time: 0.588s, 1742.46/s  (2.401s,  426.46/s)  LR: 4.366e-05  Data: 0.022 (1.794)
Train: 127 [1000/1171 ( 85%)]  Loss:  3.126168 (2.9212)  Time: 0.586s, 1746.59/s  (2.399s,  426.87/s)  LR: 4.366e-05  Data: 0.017 (1.793)
Train: 127 [1050/1171 ( 90%)]  Loss:  3.219665 (2.9347)  Time: 0.588s, 1740.41/s  (2.388s,  428.85/s)  LR: 4.366e-05  Data: 0.021 (1.782)
Train: 127 [1100/1171 ( 94%)]  Loss:  3.125832 (2.9430)  Time: 0.584s, 1754.46/s  (2.371s,  431.88/s)  LR: 4.366e-05  Data: 0.019 (1.766)
Train: 127 [1150/1171 ( 98%)]  Loss:  3.085708 (2.9490)  Time: 0.585s, 1750.09/s  (2.373s,  431.49/s)  LR: 4.366e-05  Data: 0.019 (1.769)
Train: 127 [1170/1171 (100%)]  Loss:  3.355434 (2.9652)  Time: 0.565s, 1812.50/s  (2.372s,  431.75/s)  LR: 4.366e-05  Data: 0.000 (1.768)
Test: [   0/97]  Time: 12.344 (12.344)  Loss:  0.2896 (0.2896)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (3.114)  Loss:  0.4399 (0.3613)  Acc@1: 92.0898 (94.9851)  Acc@5: 98.7305 (98.9085)
Test: [  97/97]  Time: 0.119 (3.043)  Loss:  0.3331 (0.3735)  Acc@1: 95.0893 (94.4870)  Acc@5: 99.2560 (98.7750)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-127.pth.tar', 94.48700000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-125.pth.tar', 94.46999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-126.pth.tar', 94.45200002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-124.pth.tar', 94.38600002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-122.pth.tar', 94.3610000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-123.pth.tar', 94.28500005126953)

Train: 128 [   0/1171 (  0%)]  Loss:  3.318139 (3.3181)  Time: 11.316s,   90.50/s  (11.316s,   90.50/s)  LR: 3.985e-05  Data: 10.386 (10.386)
Train: 128 [  50/1171 (  4%)]  Loss:  2.819467 (3.0688)  Time: 0.588s, 1741.78/s  (2.346s,  436.47/s)  LR: 3.985e-05  Data: 0.020 (1.766)
Train: 128 [ 100/1171 (  9%)]  Loss:  3.115611 (3.0844)  Time: 0.586s, 1746.34/s  (2.322s,  441.01/s)  LR: 3.985e-05  Data: 0.021 (1.732)
Train: 128 [ 150/1171 ( 13%)]  Loss:  3.237905 (3.1228)  Time: 0.589s, 1737.67/s  (2.266s,  451.97/s)  LR: 3.985e-05  Data: 0.026 (1.674)
Train: 128 [ 200/1171 ( 17%)]  Loss:  2.990692 (3.0964)  Time: 0.785s, 1305.28/s  (2.249s,  455.40/s)  LR: 3.985e-05  Data: 0.204 (1.649)
Train: 128 [ 250/1171 ( 21%)]  Loss:  3.025732 (3.0846)  Time: 0.588s, 1742.40/s  (2.305s,  444.26/s)  LR: 3.985e-05  Data: 0.024 (1.701)
Train: 128 [ 300/1171 ( 26%)]  Loss:  2.491858 (2.9999)  Time: 4.771s,  214.65/s  (2.332s,  439.02/s)  LR: 3.985e-05  Data: 4.196 (1.732)
Train: 128 [ 350/1171 ( 30%)]  Loss:  2.476727 (2.9345)  Time: 0.587s, 1744.68/s  (2.333s,  438.94/s)  LR: 3.985e-05  Data: 0.021 (1.732)
Train: 128 [ 400/1171 ( 34%)]  Loss:  2.778646 (2.9172)  Time: 6.828s,  149.97/s  (2.337s,  438.25/s)  LR: 3.985e-05  Data: 6.240 (1.735)
Train: 128 [ 450/1171 ( 38%)]  Loss:  3.164848 (2.9420)  Time: 0.587s, 1743.50/s  (2.326s,  440.31/s)  LR: 3.985e-05  Data: 0.024 (1.724)
Train: 128 [ 500/1171 ( 43%)]  Loss:  3.040171 (2.9509)  Time: 6.823s,  150.07/s  (2.335s,  438.51/s)  LR: 3.985e-05  Data: 6.169 (1.734)
Train: 128 [ 550/1171 ( 47%)]  Loss:  2.840269 (2.9417)  Time: 0.586s, 1746.29/s  (2.331s,  439.29/s)  LR: 3.985e-05  Data: 0.023 (1.731)
Train: 128 [ 600/1171 ( 51%)]  Loss:  3.148997 (2.9576)  Time: 7.901s,  129.60/s  (2.338s,  438.03/s)  LR: 3.985e-05  Data: 7.323 (1.738)
Train: 128 [ 650/1171 ( 56%)]  Loss:  2.890253 (2.9528)  Time: 1.342s,  763.03/s  (2.356s,  434.67/s)  LR: 3.985e-05  Data: 0.664 (1.757)
Train: 128 [ 700/1171 ( 60%)]  Loss:  2.799333 (2.9426)  Time: 5.007s,  204.52/s  (2.362s,  433.46/s)  LR: 3.985e-05  Data: 4.445 (1.763)
Train: 128 [ 750/1171 ( 64%)]  Loss:  2.924536 (2.9414)  Time: 1.646s,  622.10/s  (2.354s,  435.09/s)  LR: 3.985e-05  Data: 0.960 (1.754)
Train: 128 [ 800/1171 ( 68%)]  Loss:  2.973724 (2.9433)  Time: 2.662s,  384.71/s  (2.355s,  434.81/s)  LR: 3.985e-05  Data: 1.636 (1.753)
Train: 128 [ 850/1171 ( 73%)]  Loss:  2.880519 (2.9399)  Time: 0.719s, 1424.88/s  (2.346s,  436.55/s)  LR: 3.985e-05  Data: 0.022 (1.744)
Train: 128 [ 900/1171 ( 77%)]  Loss:  3.054204 (2.9459)  Time: 0.587s, 1744.31/s  (2.342s,  437.15/s)  LR: 3.985e-05  Data: 0.022 (1.741)
Train: 128 [ 950/1171 ( 81%)]  Loss:  3.134542 (2.9553)  Time: 0.591s, 1733.19/s  (2.332s,  439.09/s)  LR: 3.985e-05  Data: 0.023 (1.732)
Train: 128 [1000/1171 ( 85%)]  Loss:  2.508876 (2.9340)  Time: 1.605s,  638.16/s  (2.345s,  436.63/s)  LR: 3.985e-05  Data: 1.015 (1.745)
Train: 128 [1050/1171 ( 90%)]  Loss:  3.157401 (2.9442)  Time: 0.593s, 1727.08/s  (2.341s,  437.35/s)  LR: 3.985e-05  Data: 0.026 (1.742)
Train: 128 [1100/1171 ( 94%)]  Loss:  2.779644 (2.9370)  Time: 1.387s,  738.48/s  (2.351s,  435.59/s)  LR: 3.985e-05  Data: 0.791 (1.751)
Train: 128 [1150/1171 ( 98%)]  Loss:  2.819489 (2.9321)  Time: 0.588s, 1741.43/s  (2.349s,  436.02/s)  LR: 3.985e-05  Data: 0.022 (1.749)
Train: 128 [1170/1171 (100%)]  Loss:  3.278409 (2.9460)  Time: 0.564s, 1814.49/s  (2.348s,  436.19/s)  LR: 3.985e-05  Data: 0.000 (1.748)
Test: [   0/97]  Time: 12.368 (12.368)  Loss:  0.3289 (0.3289)  Acc@1: 96.3867 (96.3867)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.198 (3.014)  Loss:  0.4645 (0.3909)  Acc@1: 92.6758 (95.0310)  Acc@5: 98.4375 (98.9430)
Test: [  97/97]  Time: 0.119 (2.954)  Loss:  0.3618 (0.4009)  Acc@1: 94.1964 (94.5390)  Acc@5: 99.2560 (98.7890)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-128.pth.tar', 94.53899998535157)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-127.pth.tar', 94.48700000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-125.pth.tar', 94.46999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-126.pth.tar', 94.45200002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-124.pth.tar', 94.38600002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-122.pth.tar', 94.3610000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-123.pth.tar', 94.28500005126953)

Train: 129 [   0/1171 (  0%)]  Loss:  3.245386 (3.2454)  Time: 9.882s,  103.62/s  (9.882s,  103.62/s)  LR: 3.627e-05  Data: 9.236 (9.236)
Train: 129 [  50/1171 (  4%)]  Loss:  2.867537 (3.0565)  Time: 0.589s, 1739.46/s  (2.226s,  459.96/s)  LR: 3.627e-05  Data: 0.023 (1.624)
Train: 129 [ 100/1171 (  9%)]  Loss:  2.733867 (2.9489)  Time: 0.585s, 1749.31/s  (2.422s,  422.86/s)  LR: 3.627e-05  Data: 0.019 (1.801)
Train: 129 [ 150/1171 ( 13%)]  Loss:  2.507460 (2.8386)  Time: 0.591s, 1733.85/s  (2.347s,  436.24/s)  LR: 3.627e-05  Data: 0.024 (1.734)
Train: 129 [ 200/1171 ( 17%)]  Loss:  3.368654 (2.9446)  Time: 1.490s,  687.34/s  (2.348s,  436.09/s)  LR: 3.627e-05  Data: 0.928 (1.741)
Train: 129 [ 250/1171 ( 21%)]  Loss:  2.920622 (2.9406)  Time: 0.590s, 1736.53/s  (2.345s,  436.61/s)  LR: 3.627e-05  Data: 0.024 (1.740)
Train: 129 [ 300/1171 ( 26%)]  Loss:  3.238559 (2.9832)  Time: 0.587s, 1744.73/s  (2.337s,  438.16/s)  LR: 3.627e-05  Data: 0.020 (1.731)
Train: 129 [ 350/1171 ( 30%)]  Loss:  3.499105 (3.0476)  Time: 0.591s, 1731.59/s  (2.321s,  441.11/s)  LR: 3.627e-05  Data: 0.025 (1.716)
Train: 129 [ 400/1171 ( 34%)]  Loss:  2.503323 (2.9872)  Time: 1.425s,  718.72/s  (2.314s,  442.56/s)  LR: 3.627e-05  Data: 0.705 (1.707)
Train: 129 [ 450/1171 ( 38%)]  Loss:  3.107811 (2.9992)  Time: 0.587s, 1744.78/s  (2.293s,  446.60/s)  LR: 3.627e-05  Data: 0.024 (1.688)
Train: 129 [ 500/1171 ( 43%)]  Loss:  3.079427 (3.0065)  Time: 2.427s,  421.93/s  (2.348s,  436.04/s)  LR: 3.627e-05  Data: 1.844 (1.743)
Train: 129 [ 550/1171 ( 47%)]  Loss:  2.881030 (2.9961)  Time: 0.594s, 1724.19/s  (2.363s,  433.35/s)  LR: 3.627e-05  Data: 0.026 (1.758)
Train: 129 [ 600/1171 ( 51%)]  Loss:  3.076612 (3.0023)  Time: 4.583s,  223.42/s  (2.381s,  430.02/s)  LR: 3.627e-05  Data: 3.912 (1.777)
Train: 129 [ 650/1171 ( 56%)]  Loss:  3.188788 (3.0156)  Time: 0.589s, 1737.24/s  (2.384s,  429.44/s)  LR: 3.627e-05  Data: 0.022 (1.780)
Train: 129 [ 700/1171 ( 60%)]  Loss:  2.772288 (2.9994)  Time: 6.452s,  158.72/s  (2.380s,  430.21/s)  LR: 3.627e-05  Data: 5.864 (1.777)
Train: 129 [ 750/1171 ( 64%)]  Loss:  2.916419 (2.9942)  Time: 0.593s, 1726.08/s  (2.365s,  432.89/s)  LR: 3.627e-05  Data: 0.018 (1.764)
Train: 129 [ 800/1171 ( 68%)]  Loss:  2.905226 (2.9889)  Time: 6.989s,  146.51/s  (2.364s,  433.19/s)  LR: 3.627e-05  Data: 6.428 (1.763)
Train: 129 [ 850/1171 ( 73%)]  Loss:  2.927603 (2.9855)  Time: 0.590s, 1734.86/s  (2.372s,  431.69/s)  LR: 3.627e-05  Data: 0.019 (1.773)
Train: 129 [ 900/1171 ( 77%)]  Loss:  2.765667 (2.9740)  Time: 9.785s,  104.65/s  (2.373s,  431.54/s)  LR: 3.627e-05  Data: 8.546 (1.773)
Train: 129 [ 950/1171 ( 81%)]  Loss:  3.107856 (2.9807)  Time: 0.589s, 1739.07/s  (2.371s,  431.87/s)  LR: 3.627e-05  Data: 0.019 (1.772)
Train: 129 [1000/1171 ( 85%)]  Loss:  3.014640 (2.9823)  Time: 7.220s,  141.83/s  (2.374s,  431.37/s)  LR: 3.627e-05  Data: 6.658 (1.776)
Train: 129 [1050/1171 ( 90%)]  Loss:  2.846400 (2.9761)  Time: 0.588s, 1740.08/s  (2.367s,  432.62/s)  LR: 3.627e-05  Data: 0.019 (1.770)
Train: 129 [1100/1171 ( 94%)]  Loss:  3.088651 (2.9810)  Time: 7.315s,  139.98/s  (2.364s,  433.19/s)  LR: 3.627e-05  Data: 6.669 (1.767)
Train: 129 [1150/1171 ( 98%)]  Loss:  2.667582 (2.9679)  Time: 0.589s, 1738.15/s  (2.355s,  434.77/s)  LR: 3.627e-05  Data: 0.021 (1.760)
Train: 129 [1170/1171 (100%)]  Loss:  3.146749 (2.9751)  Time: 0.566s, 1810.38/s  (2.353s,  435.24/s)  LR: 3.627e-05  Data: 0.000 (1.757)
Test: [   0/97]  Time: 12.063 (12.063)  Loss:  0.3007 (0.3007)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.197 (3.206)  Loss:  0.4418 (0.3725)  Acc@1: 92.7734 (95.0157)  Acc@5: 98.3398 (98.9105)
Test: [  97/97]  Time: 0.119 (3.130)  Loss:  0.3457 (0.3854)  Acc@1: 94.3452 (94.4530)  Acc@5: 99.2560 (98.7470)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-128.pth.tar', 94.53899998535157)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-127.pth.tar', 94.48700000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-125.pth.tar', 94.46999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-129.pth.tar', 94.45299999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-126.pth.tar', 94.45200002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-124.pth.tar', 94.38600002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-122.pth.tar', 94.3610000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-123.pth.tar', 94.28500005126953)

Train: 130 [   0/1171 (  0%)]  Loss:  2.576329 (2.5763)  Time: 10.829s,   94.56/s  (10.829s,   94.56/s)  LR: 3.291e-05  Data: 10.032 (10.032)
Train: 130 [  50/1171 (  4%)]  Loss:  2.825458 (2.7009)  Time: 1.314s,  779.34/s  (2.396s,  427.37/s)  LR: 3.291e-05  Data: 0.707 (1.797)
Train: 130 [ 100/1171 (  9%)]  Loss:  2.660077 (2.6873)  Time: 0.586s, 1748.80/s  (2.338s,  437.93/s)  LR: 3.291e-05  Data: 0.020 (1.746)
Train: 130 [ 150/1171 ( 13%)]  Loss:  2.891527 (2.7383)  Time: 0.587s, 1745.61/s  (2.270s,  451.10/s)  LR: 3.291e-05  Data: 0.023 (1.681)
Train: 130 [ 200/1171 ( 17%)]  Loss:  2.897772 (2.7702)  Time: 0.583s, 1755.51/s  (2.260s,  453.12/s)  LR: 3.291e-05  Data: 0.019 (1.669)
Train: 130 [ 250/1171 ( 21%)]  Loss:  3.066551 (2.8196)  Time: 0.639s, 1602.11/s  (2.228s,  459.65/s)  LR: 3.291e-05  Data: 0.023 (1.634)
Train: 130 [ 300/1171 ( 26%)]  Loss:  2.716179 (2.8048)  Time: 3.696s,  277.03/s  (2.227s,  459.74/s)  LR: 3.291e-05  Data: 3.121 (1.633)
Train: 130 [ 350/1171 ( 30%)]  Loss:  2.723847 (2.7947)  Time: 0.588s, 1740.09/s  (2.270s,  451.10/s)  LR: 3.291e-05  Data: 0.025 (1.675)
Train: 130 [ 400/1171 ( 34%)]  Loss:  2.429340 (2.7541)  Time: 2.908s,  352.14/s  (2.300s,  445.29/s)  LR: 3.291e-05  Data: 2.324 (1.704)
Train: 130 [ 450/1171 ( 38%)]  Loss:  2.754601 (2.7542)  Time: 1.238s,  826.92/s  (2.299s,  445.50/s)  LR: 3.291e-05  Data: 0.590 (1.701)
Train: 130 [ 500/1171 ( 43%)]  Loss:  3.157773 (2.7909)  Time: 1.673s,  612.14/s  (2.326s,  440.22/s)  LR: 3.291e-05  Data: 1.031 (1.725)
Train: 130 [ 550/1171 ( 47%)]  Loss:  3.037231 (2.8114)  Time: 4.265s,  240.12/s  (2.338s,  438.07/s)  LR: 3.291e-05  Data: 3.683 (1.736)
Train: 130 [ 600/1171 ( 51%)]  Loss:  2.809606 (2.8113)  Time: 1.382s,  740.84/s  (2.341s,  437.40/s)  LR: 3.291e-05  Data: 0.711 (1.739)
Train: 130 [ 650/1171 ( 56%)]  Loss:  3.223194 (2.8407)  Time: 2.638s,  388.20/s  (2.338s,  438.05/s)  LR: 3.291e-05  Data: 1.800 (1.733)
Train: 130 [ 700/1171 ( 60%)]  Loss:  2.995989 (2.8510)  Time: 4.757s,  215.24/s  (2.356s,  434.64/s)  LR: 3.291e-05  Data: 4.064 (1.748)
Train: 130 [ 750/1171 ( 64%)]  Loss:  2.850413 (2.8510)  Time: 1.305s,  784.75/s  (2.347s,  436.32/s)  LR: 3.291e-05  Data: 0.737 (1.739)
Train: 130 [ 800/1171 ( 68%)]  Loss:  2.907381 (2.8543)  Time: 3.018s,  339.30/s  (2.352s,  435.44/s)  LR: 3.291e-05  Data: 2.371 (1.741)
Train: 130 [ 850/1171 ( 73%)]  Loss:  3.219311 (2.8746)  Time: 1.385s,  739.31/s  (2.353s,  435.13/s)  LR: 3.291e-05  Data: 0.811 (1.741)
Train: 130 [ 900/1171 ( 77%)]  Loss:  2.915310 (2.8767)  Time: 4.857s,  210.84/s  (2.354s,  434.98/s)  LR: 3.291e-05  Data: 4.295 (1.741)
Train: 130 [ 950/1171 ( 81%)]  Loss:  3.094947 (2.8876)  Time: 0.587s, 1744.25/s  (2.346s,  436.45/s)  LR: 3.291e-05  Data: 0.024 (1.734)
Train: 130 [1000/1171 ( 85%)]  Loss:  3.256622 (2.9052)  Time: 7.482s,  136.86/s  (2.344s,  436.91/s)  LR: 3.291e-05  Data: 6.541 (1.732)
Train: 130 [1050/1171 ( 90%)]  Loss:  2.564413 (2.8897)  Time: 0.584s, 1752.13/s  (2.332s,  439.11/s)  LR: 3.291e-05  Data: 0.019 (1.722)
Train: 130 [1100/1171 ( 94%)]  Loss:  2.992498 (2.8942)  Time: 7.898s,  129.65/s  (2.347s,  436.30/s)  LR: 3.291e-05  Data: 7.337 (1.738)
Train: 130 [1150/1171 ( 98%)]  Loss:  3.010736 (2.8990)  Time: 0.587s, 1743.76/s  (2.344s,  436.82/s)  LR: 3.291e-05  Data: 0.019 (1.736)
Train: 130 [1170/1171 (100%)]  Loss:  2.706393 (2.8913)  Time: 0.564s, 1815.21/s  (2.346s,  436.47/s)  LR: 3.291e-05  Data: 0.000 (1.738)
Test: [   0/97]  Time: 12.747 (12.747)  Loss:  0.2902 (0.2902)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.6094 (99.6094)
Test: [  50/97]  Time: 0.199 (3.107)  Loss:  0.4330 (0.3590)  Acc@1: 92.8711 (95.0100)  Acc@5: 98.4375 (98.9220)
Test: [  97/97]  Time: 0.119 (3.043)  Loss:  0.3237 (0.3705)  Acc@1: 95.0893 (94.4940)  Acc@5: 99.1071 (98.7570)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-128.pth.tar', 94.53899998535157)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-130.pth.tar', 94.49400000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-127.pth.tar', 94.48700000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-125.pth.tar', 94.46999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-129.pth.tar', 94.45299999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-126.pth.tar', 94.45200002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-124.pth.tar', 94.38600002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-122.pth.tar', 94.3610000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-123.pth.tar', 94.28500005126953)

Train: 131 [   0/1171 (  0%)]  Loss:  2.763560 (2.7636)  Time: 9.956s,  102.85/s  (9.956s,  102.85/s)  LR: 2.978e-05  Data: 9.395 (9.395)
Train: 131 [  50/1171 (  4%)]  Loss:  2.645511 (2.7045)  Time: 0.584s, 1752.43/s  (2.283s,  448.49/s)  LR: 2.978e-05  Data: 0.019 (1.702)
Train: 131 [ 100/1171 (  9%)]  Loss:  3.155411 (2.8548)  Time: 0.587s, 1745.41/s  (2.256s,  453.93/s)  LR: 2.978e-05  Data: 0.019 (1.667)
Train: 131 [ 150/1171 ( 13%)]  Loss:  3.010023 (2.8936)  Time: 0.591s, 1731.49/s  (2.191s,  467.27/s)  LR: 2.978e-05  Data: 0.022 (1.594)
Train: 131 [ 200/1171 ( 17%)]  Loss:  2.749836 (2.8649)  Time: 2.484s,  412.28/s  (2.299s,  445.39/s)  LR: 2.978e-05  Data: 1.826 (1.699)
Train: 131 [ 250/1171 ( 21%)]  Loss:  2.764466 (2.8481)  Time: 1.040s,  984.52/s  (2.315s,  442.30/s)  LR: 2.978e-05  Data: 0.478 (1.704)
Train: 131 [ 300/1171 ( 26%)]  Loss:  2.846132 (2.8478)  Time: 3.224s,  317.57/s  (2.325s,  440.38/s)  LR: 2.978e-05  Data: 2.556 (1.713)
Train: 131 [ 350/1171 ( 30%)]  Loss:  2.782992 (2.8397)  Time: 4.238s,  241.63/s  (2.319s,  441.56/s)  LR: 2.978e-05  Data: 3.431 (1.708)
Train: 131 [ 400/1171 ( 34%)]  Loss:  2.771348 (2.8321)  Time: 0.986s, 1038.68/s  (2.316s,  442.11/s)  LR: 2.978e-05  Data: 0.302 (1.704)
Train: 131 [ 450/1171 ( 38%)]  Loss:  2.972881 (2.8462)  Time: 1.235s,  829.47/s  (2.309s,  443.42/s)  LR: 2.978e-05  Data: 0.576 (1.698)
Train: 131 [ 500/1171 ( 43%)]  Loss:  2.628785 (2.8264)  Time: 0.586s, 1747.55/s  (2.318s,  441.80/s)  LR: 2.978e-05  Data: 0.018 (1.707)
Train: 131 [ 550/1171 ( 47%)]  Loss:  2.908705 (2.8333)  Time: 3.010s,  340.24/s  (2.347s,  436.34/s)  LR: 2.978e-05  Data: 2.383 (1.736)
Train: 131 [ 600/1171 ( 51%)]  Loss:  3.348058 (2.8729)  Time: 0.588s, 1741.31/s  (2.359s,  433.99/s)  LR: 2.978e-05  Data: 0.020 (1.749)
Train: 131 [ 650/1171 ( 56%)]  Loss:  2.954141 (2.8787)  Time: 0.590s, 1736.16/s  (2.369s,  432.20/s)  LR: 2.978e-05  Data: 0.025 (1.761)
Train: 131 [ 700/1171 ( 60%)]  Loss:  3.025619 (2.8885)  Time: 0.586s, 1748.85/s  (2.373s,  431.45/s)  LR: 2.978e-05  Data: 0.018 (1.766)
Train: 131 [ 750/1171 ( 64%)]  Loss:  3.345319 (2.9170)  Time: 0.586s, 1748.11/s  (2.365s,  432.98/s)  LR: 2.978e-05  Data: 0.020 (1.759)
Train: 131 [ 800/1171 ( 68%)]  Loss:  2.819777 (2.9113)  Time: 0.588s, 1741.14/s  (2.361s,  433.64/s)  LR: 2.978e-05  Data: 0.020 (1.757)
Train: 131 [ 850/1171 ( 73%)]  Loss:  2.840408 (2.9074)  Time: 0.589s, 1739.96/s  (2.351s,  435.63/s)  LR: 2.978e-05  Data: 0.021 (1.747)
Train: 131 [ 900/1171 ( 77%)]  Loss:  2.967780 (2.9106)  Time: 0.585s, 1751.89/s  (2.347s,  436.24/s)  LR: 2.978e-05  Data: 0.019 (1.745)
Train: 131 [ 950/1171 ( 81%)]  Loss:  3.056764 (2.9179)  Time: 0.587s, 1745.16/s  (2.358s,  434.34/s)  LR: 2.978e-05  Data: 0.022 (1.756)
Train: 131 [1000/1171 ( 85%)]  Loss:  3.159451 (2.9294)  Time: 0.586s, 1747.13/s  (2.361s,  433.66/s)  LR: 2.978e-05  Data: 0.019 (1.761)
Train: 131 [1050/1171 ( 90%)]  Loss:  2.892732 (2.9277)  Time: 0.588s, 1741.06/s  (2.354s,  435.09/s)  LR: 2.978e-05  Data: 0.023 (1.754)
Train: 131 [1100/1171 ( 94%)]  Loss:  3.329626 (2.9452)  Time: 1.285s,  796.95/s  (2.358s,  434.24/s)  LR: 2.978e-05  Data: 0.723 (1.759)
Train: 131 [1150/1171 ( 98%)]  Loss:  3.018946 (2.9483)  Time: 0.587s, 1743.92/s  (2.356s,  434.69/s)  LR: 2.978e-05  Data: 0.024 (1.756)
Train: 131 [1170/1171 (100%)]  Loss:  2.687352 (2.9378)  Time: 0.564s, 1816.39/s  (2.354s,  434.92/s)  LR: 2.978e-05  Data: 0.000 (1.755)
Test: [   0/97]  Time: 13.256 (13.256)  Loss:  0.3021 (0.3021)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (2.988)  Loss:  0.4528 (0.3680)  Acc@1: 92.3828 (95.1574)  Acc@5: 98.4375 (98.9277)
Test: [  97/97]  Time: 0.133 (2.900)  Loss:  0.3256 (0.3808)  Acc@1: 94.6429 (94.6030)  Acc@5: 99.2560 (98.7710)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-131.pth.tar', 94.60300002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-128.pth.tar', 94.53899998535157)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-130.pth.tar', 94.49400000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-127.pth.tar', 94.48700000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-125.pth.tar', 94.46999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-129.pth.tar', 94.45299999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-126.pth.tar', 94.45200002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-124.pth.tar', 94.38600002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-122.pth.tar', 94.3610000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-123.pth.tar', 94.28500005126953)

Train: 132 [   0/1171 (  0%)]  Loss:  2.663551 (2.6636)  Time: 10.782s,   94.97/s  (10.782s,   94.97/s)  LR: 2.687e-05  Data: 10.156 (10.156)
Train: 132 [  50/1171 (  4%)]  Loss:  2.404714 (2.5341)  Time: 0.587s, 1745.54/s  (2.673s,  383.10/s)  LR: 2.687e-05  Data: 0.021 (2.073)
Train: 132 [ 100/1171 (  9%)]  Loss:  2.371854 (2.4800)  Time: 2.687s,  381.10/s  (2.568s,  398.73/s)  LR: 2.687e-05  Data: 2.125 (1.969)
Train: 132 [ 150/1171 ( 13%)]  Loss:  2.828614 (2.5672)  Time: 0.586s, 1748.08/s  (2.434s,  420.65/s)  LR: 2.687e-05  Data: 0.018 (1.833)
Train: 132 [ 200/1171 ( 17%)]  Loss:  2.967585 (2.6473)  Time: 4.082s,  250.85/s  (2.410s,  424.85/s)  LR: 2.687e-05  Data: 3.411 (1.809)
Train: 132 [ 250/1171 ( 21%)]  Loss:  3.176440 (2.7355)  Time: 0.586s, 1747.65/s  (2.359s,  433.99/s)  LR: 2.687e-05  Data: 0.021 (1.760)
Train: 132 [ 300/1171 ( 26%)]  Loss:  2.563919 (2.7110)  Time: 3.696s,  277.08/s  (2.346s,  436.41/s)  LR: 2.687e-05  Data: 3.131 (1.746)
Train: 132 [ 350/1171 ( 30%)]  Loss:  3.099838 (2.7596)  Time: 0.614s, 1668.05/s  (2.315s,  442.32/s)  LR: 2.687e-05  Data: 0.020 (1.714)
Train: 132 [ 400/1171 ( 34%)]  Loss:  2.973225 (2.7833)  Time: 7.749s,  132.15/s  (2.337s,  438.10/s)  LR: 2.687e-05  Data: 7.153 (1.735)
Train: 132 [ 450/1171 ( 38%)]  Loss:  2.608707 (2.7658)  Time: 0.584s, 1754.81/s  (2.335s,  438.52/s)  LR: 2.687e-05  Data: 0.018 (1.734)
Train: 132 [ 500/1171 ( 43%)]  Loss:  2.799798 (2.7689)  Time: 2.895s,  353.69/s  (2.355s,  434.87/s)  LR: 2.687e-05  Data: 2.223 (1.753)
Train: 132 [ 550/1171 ( 47%)]  Loss:  2.553075 (2.7509)  Time: 0.584s, 1752.07/s  (2.362s,  433.51/s)  LR: 2.687e-05  Data: 0.017 (1.760)
Train: 132 [ 600/1171 ( 51%)]  Loss:  2.882508 (2.7611)  Time: 5.020s,  203.98/s  (2.371s,  431.95/s)  LR: 2.687e-05  Data: 4.458 (1.767)
Train: 132 [ 650/1171 ( 56%)]  Loss:  2.659517 (2.7538)  Time: 0.585s, 1749.79/s  (2.365s,  432.90/s)  LR: 2.687e-05  Data: 0.018 (1.761)
Train: 132 [ 700/1171 ( 60%)]  Loss:  2.982884 (2.7691)  Time: 5.536s,  184.98/s  (2.360s,  433.92/s)  LR: 2.687e-05  Data: 4.876 (1.756)
Train: 132 [ 750/1171 ( 64%)]  Loss:  2.847851 (2.7740)  Time: 0.584s, 1753.73/s  (2.346s,  436.52/s)  LR: 2.687e-05  Data: 0.019 (1.743)
Train: 132 [ 800/1171 ( 68%)]  Loss:  2.971127 (2.7856)  Time: 5.689s,  179.99/s  (2.368s,  432.39/s)  LR: 2.687e-05  Data: 5.121 (1.765)
Train: 132 [ 850/1171 ( 73%)]  Loss:  2.835627 (2.7884)  Time: 0.592s, 1729.92/s  (2.373s,  431.48/s)  LR: 2.687e-05  Data: 0.020 (1.770)
Train: 132 [ 900/1171 ( 77%)]  Loss:  3.015591 (2.8003)  Time: 4.219s,  242.69/s  (2.377s,  430.80/s)  LR: 2.687e-05  Data: 3.649 (1.773)
Train: 132 [ 950/1171 ( 81%)]  Loss:  2.354833 (2.7781)  Time: 0.585s, 1749.56/s  (2.373s,  431.58/s)  LR: 2.687e-05  Data: 0.018 (1.768)
Train: 132 [1000/1171 ( 85%)]  Loss:  3.125808 (2.7946)  Time: 7.444s,  137.57/s  (2.373s,  431.46/s)  LR: 2.687e-05  Data: 6.813 (1.769)
Train: 132 [1050/1171 ( 90%)]  Loss:  2.888284 (2.7989)  Time: 0.588s, 1742.11/s  (2.363s,  433.31/s)  LR: 2.687e-05  Data: 0.023 (1.760)
Train: 132 [1100/1171 ( 94%)]  Loss:  3.270594 (2.8194)  Time: 3.396s,  301.51/s  (2.358s,  434.33/s)  LR: 2.687e-05  Data: 2.530 (1.754)
Train: 132 [1150/1171 ( 98%)]  Loss:  2.384752 (2.8013)  Time: 0.586s, 1746.27/s  (2.349s,  435.87/s)  LR: 2.687e-05  Data: 0.020 (1.746)
Train: 132 [1170/1171 (100%)]  Loss:  2.758458 (2.7996)  Time: 0.566s, 1810.47/s  (2.364s,  433.23/s)  LR: 2.687e-05  Data: 0.000 (1.760)
Test: [   0/97]  Time: 12.187 (12.187)  Loss:  0.2967 (0.2967)  Acc@1: 96.8750 (96.8750)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (2.983)  Loss:  0.4565 (0.3684)  Acc@1: 91.9922 (95.0866)  Acc@5: 98.3398 (98.9085)
Test: [  97/97]  Time: 0.119 (2.944)  Loss:  0.3311 (0.3793)  Acc@1: 95.0893 (94.6350)  Acc@5: 99.4048 (98.7840)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-132.pth.tar', 94.63500000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-131.pth.tar', 94.60300002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-128.pth.tar', 94.53899998535157)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-130.pth.tar', 94.49400000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-127.pth.tar', 94.48700000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-125.pth.tar', 94.46999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-129.pth.tar', 94.45299999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-126.pth.tar', 94.45200002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-124.pth.tar', 94.38600002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-122.pth.tar', 94.3610000390625)

Train: 133 [   0/1171 (  0%)]  Loss:  2.769237 (2.7692)  Time: 11.388s,   89.92/s  (11.388s,   89.92/s)  LR: 2.419e-05  Data: 10.337 (10.337)
Train: 133 [  50/1171 (  4%)]  Loss:  2.172061 (2.4706)  Time: 0.586s, 1746.85/s  (2.310s,  443.28/s)  LR: 2.419e-05  Data: 0.020 (1.722)
Train: 133 [ 100/1171 (  9%)]  Loss:  2.825036 (2.5888)  Time: 0.588s, 1741.38/s  (2.230s,  459.09/s)  LR: 2.419e-05  Data: 0.022 (1.643)
Train: 133 [ 150/1171 ( 13%)]  Loss:  2.706100 (2.6181)  Time: 0.584s, 1752.62/s  (2.139s,  478.76/s)  LR: 2.419e-05  Data: 0.020 (1.545)
Train: 133 [ 200/1171 ( 17%)]  Loss:  2.832943 (2.6611)  Time: 0.587s, 1745.51/s  (2.120s,  482.97/s)  LR: 2.419e-05  Data: 0.022 (1.528)
Train: 133 [ 250/1171 ( 21%)]  Loss:  2.338831 (2.6074)  Time: 0.585s, 1749.68/s  (2.085s,  491.13/s)  LR: 2.419e-05  Data: 0.020 (1.493)
Train: 133 [ 300/1171 ( 26%)]  Loss:  3.017868 (2.6660)  Time: 0.834s, 1227.58/s  (2.182s,  469.28/s)  LR: 2.419e-05  Data: 0.102 (1.588)
Train: 133 [ 350/1171 ( 30%)]  Loss:  2.956970 (2.7024)  Time: 0.586s, 1746.12/s  (2.238s,  457.63/s)  LR: 2.419e-05  Data: 0.022 (1.620)
Train: 133 [ 400/1171 ( 34%)]  Loss:  2.313149 (2.6591)  Time: 0.592s, 1730.52/s  (2.231s,  458.92/s)  LR: 2.419e-05  Data: 0.023 (1.605)
Train: 133 [ 450/1171 ( 38%)]  Loss:  3.226495 (2.7159)  Time: 0.588s, 1742.32/s  (2.254s,  454.37/s)  LR: 2.419e-05  Data: 0.021 (1.628)
Train: 133 [ 500/1171 ( 43%)]  Loss:  3.559037 (2.7925)  Time: 0.587s, 1745.19/s  (2.277s,  449.68/s)  LR: 2.419e-05  Data: 0.020 (1.654)
Train: 133 [ 550/1171 ( 47%)]  Loss:  2.393156 (2.7592)  Time: 0.586s, 1748.73/s  (2.276s,  449.86/s)  LR: 2.419e-05  Data: 0.022 (1.652)
Train: 133 [ 600/1171 ( 51%)]  Loss:  3.376492 (2.8067)  Time: 2.100s,  487.69/s  (2.284s,  448.38/s)  LR: 2.419e-05  Data: 1.429 (1.663)
Train: 133 [ 650/1171 ( 56%)]  Loss:  2.706867 (2.7996)  Time: 0.587s, 1744.93/s  (2.331s,  439.25/s)  LR: 2.419e-05  Data: 0.018 (1.707)
Train: 133 [ 700/1171 ( 60%)]  Loss:  3.301057 (2.8330)  Time: 4.205s,  243.50/s  (2.346s,  436.40/s)  LR: 2.419e-05  Data: 3.503 (1.723)
Train: 133 [ 750/1171 ( 64%)]  Loss:  2.821381 (2.8323)  Time: 0.587s, 1743.50/s  (2.350s,  435.67/s)  LR: 2.419e-05  Data: 0.019 (1.728)
Train: 133 [ 800/1171 ( 68%)]  Loss:  2.429246 (2.8086)  Time: 1.795s,  570.32/s  (2.364s,  433.25/s)  LR: 2.419e-05  Data: 1.234 (1.738)
Train: 133 [ 850/1171 ( 73%)]  Loss:  2.732431 (2.8044)  Time: 2.989s,  342.60/s  (2.361s,  433.64/s)  LR: 2.419e-05  Data: 2.104 (1.730)
Train: 133 [ 900/1171 ( 77%)]  Loss:  2.792486 (2.8037)  Time: 0.590s, 1734.25/s  (2.353s,  435.22/s)  LR: 2.419e-05  Data: 0.022 (1.716)
Train: 133 [ 950/1171 ( 81%)]  Loss:  3.049996 (2.8160)  Time: 7.021s,  145.85/s  (2.356s,  434.69/s)  LR: 2.419e-05  Data: 1.962 (1.707)
Train: 133 [1000/1171 ( 85%)]  Loss:  3.091178 (2.8291)  Time: 0.589s, 1738.63/s  (2.343s,  437.13/s)  LR: 2.419e-05  Data: 0.021 (1.688)
Train: 133 [1050/1171 ( 90%)]  Loss:  2.705187 (2.8235)  Time: 6.602s,  155.11/s  (2.382s,  429.89/s)  LR: 2.419e-05  Data: 6.039 (1.731)
Train: 133 [1100/1171 ( 94%)]  Loss:  3.233190 (2.8413)  Time: 0.589s, 1739.08/s  (2.382s,  429.87/s)  LR: 2.419e-05  Data: 0.020 (1.732)
Train: 133 [1150/1171 ( 98%)]  Loss:  3.273549 (2.8593)  Time: 7.767s,  131.84/s  (2.383s,  429.63/s)  LR: 2.419e-05  Data: 7.058 (1.734)
Train: 133 [1170/1171 (100%)]  Loss:  2.811615 (2.8574)  Time: 0.567s, 1806.74/s  (2.380s,  430.21/s)  LR: 2.419e-05  Data: 0.000 (1.730)
Test: [   0/97]  Time: 18.563 (18.563)  Loss:  0.2920 (0.2920)  Acc@1: 96.8750 (96.8750)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 2.305 (4.279)  Loss:  0.4811 (0.3757)  Acc@1: 91.6992 (95.1057)  Acc@5: 98.3398 (98.9143)
Test: [  97/97]  Time: 0.119 (3.820)  Loss:  0.3337 (0.3855)  Acc@1: 94.7917 (94.6290)  Acc@5: 99.4048 (98.7870)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-132.pth.tar', 94.63500000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-133.pth.tar', 94.62899998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-131.pth.tar', 94.60300002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-128.pth.tar', 94.53899998535157)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-130.pth.tar', 94.49400000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-127.pth.tar', 94.48700000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-125.pth.tar', 94.46999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-129.pth.tar', 94.45299999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-126.pth.tar', 94.45200002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-124.pth.tar', 94.38600002197266)

Train: 134 [   0/1171 (  0%)]  Loss:  2.518575 (2.5186)  Time: 13.090s,   78.22/s  (13.090s,   78.22/s)  LR: 2.173e-05  Data: 11.981 (11.981)
Train: 134 [  50/1171 (  4%)]  Loss:  3.122928 (2.8208)  Time: 0.585s, 1751.46/s  (2.734s,  374.49/s)  LR: 2.173e-05  Data: 0.022 (2.099)
Train: 134 [ 100/1171 (  9%)]  Loss:  3.145432 (2.9290)  Time: 3.418s,  299.57/s  (2.957s,  346.35/s)  LR: 2.173e-05  Data: 2.856 (2.276)
Train: 134 [ 150/1171 ( 13%)]  Loss:  2.585164 (2.8430)  Time: 0.584s, 1752.98/s  (2.764s,  370.44/s)  LR: 2.173e-05  Data: 0.021 (2.092)
Train: 134 [ 200/1171 ( 17%)]  Loss:  2.984509 (2.8713)  Time: 0.588s, 1740.28/s  (2.691s,  380.47/s)  LR: 2.173e-05  Data: 0.020 (2.026)
Train: 134 [ 250/1171 ( 21%)]  Loss:  3.173609 (2.9217)  Time: 0.587s, 1744.57/s  (2.693s,  380.26/s)  LR: 2.173e-05  Data: 0.020 (2.040)
Train: 134 [ 300/1171 ( 26%)]  Loss:  3.161726 (2.9560)  Time: 0.589s, 1737.55/s  (2.649s,  386.54/s)  LR: 2.173e-05  Data: 0.019 (2.008)
Train: 134 [ 350/1171 ( 30%)]  Loss:  2.789490 (2.9352)  Time: 0.585s, 1751.08/s  (2.638s,  388.16/s)  LR: 2.173e-05  Data: 0.020 (2.003)
Train: 134 [ 400/1171 ( 34%)]  Loss:  2.936721 (2.9354)  Time: 0.587s, 1744.82/s  (2.704s,  378.66/s)  LR: 2.173e-05  Data: 0.020 (2.074)
Train: 134 [ 450/1171 ( 38%)]  Loss:  2.774232 (2.9192)  Time: 0.586s, 1747.77/s  (2.673s,  383.04/s)  LR: 2.173e-05  Data: 0.020 (2.047)
Train: 134 [ 500/1171 ( 43%)]  Loss:  3.245503 (2.9489)  Time: 0.592s, 1730.86/s  (2.666s,  384.13/s)  LR: 2.173e-05  Data: 0.019 (2.042)
Train: 134 [ 550/1171 ( 47%)]  Loss:  2.847548 (2.9405)  Time: 0.586s, 1747.58/s  (2.648s,  386.66/s)  LR: 2.173e-05  Data: 0.022 (2.027)
Train: 134 [ 600/1171 ( 51%)]  Loss:  2.743100 (2.9253)  Time: 0.588s, 1742.40/s  (2.637s,  388.36/s)  LR: 2.173e-05  Data: 0.021 (2.018)
Train: 134 [ 650/1171 ( 56%)]  Loss:  2.895126 (2.9231)  Time: 0.587s, 1744.44/s  (2.610s,  392.34/s)  LR: 2.173e-05  Data: 0.023 (1.992)
Train: 134 [ 700/1171 ( 60%)]  Loss:  2.867307 (2.9194)  Time: 0.588s, 1740.22/s  (2.619s,  390.96/s)  LR: 2.173e-05  Data: 0.020 (2.000)
Train: 134 [ 750/1171 ( 64%)]  Loss:  3.050953 (2.9276)  Time: 0.589s, 1739.15/s  (2.633s,  388.88/s)  LR: 2.173e-05  Data: 0.021 (2.008)
Train: 134 [ 800/1171 ( 68%)]  Loss:  2.687523 (2.9135)  Time: 0.588s, 1742.86/s  (2.614s,  391.81/s)  LR: 2.173e-05  Data: 0.020 (1.990)
Train: 134 [ 850/1171 ( 73%)]  Loss:  3.024267 (2.9197)  Time: 0.587s, 1744.14/s  (2.595s,  394.60/s)  LR: 2.173e-05  Data: 0.020 (1.973)
Train: 134 [ 900/1171 ( 77%)]  Loss:  3.294310 (2.9394)  Time: 5.766s,  177.61/s  (2.596s,  394.42/s)  LR: 2.173e-05  Data: 5.166 (1.973)
Train: 134 [ 950/1171 ( 81%)]  Loss:  3.180190 (2.9514)  Time: 0.861s, 1189.74/s  (2.580s,  396.89/s)  LR: 2.173e-05  Data: 0.299 (1.953)
Train: 134 [1000/1171 ( 85%)]  Loss:  2.806246 (2.9445)  Time: 1.214s,  843.25/s  (2.564s,  399.39/s)  LR: 2.173e-05  Data: 0.553 (1.933)
Train: 134 [1050/1171 ( 90%)]  Loss:  2.781137 (2.9371)  Time: 0.770s, 1330.37/s  (2.543s,  402.67/s)  LR: 2.173e-05  Data: 0.181 (1.912)
Train: 134 [1100/1171 ( 94%)]  Loss:  2.605977 (2.9227)  Time: 13.449s,   76.14/s  (2.555s,  400.78/s)  LR: 2.173e-05  Data: 12.861 (1.925)
Train: 134 [1150/1171 ( 98%)]  Loss:  2.923943 (2.9227)  Time: 1.495s,  685.08/s  (2.551s,  401.45/s)  LR: 2.173e-05  Data: 0.933 (1.917)
Train: 134 [1170/1171 (100%)]  Loss:  2.379631 (2.9010)  Time: 0.565s, 1813.59/s  (2.549s,  401.80/s)  LR: 2.173e-05  Data: 0.000 (1.915)
Test: [   0/97]  Time: 26.981 (26.981)  Loss:  0.3004 (0.3004)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.198 (4.113)  Loss:  0.4669 (0.3683)  Acc@1: 91.9922 (95.1804)  Acc@5: 98.2422 (98.9277)
Test: [  97/97]  Time: 0.120 (3.464)  Loss:  0.3321 (0.3797)  Acc@1: 94.7917 (94.6520)  Acc@5: 99.4048 (98.7850)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-134.pth.tar', 94.6520000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-132.pth.tar', 94.63500000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-133.pth.tar', 94.62899998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-131.pth.tar', 94.60300002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-128.pth.tar', 94.53899998535157)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-130.pth.tar', 94.49400000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-127.pth.tar', 94.48700000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-125.pth.tar', 94.46999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-129.pth.tar', 94.45299999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-126.pth.tar', 94.45200002197265)

Train: 135 [   0/1171 (  0%)]  Loss:  2.389495 (2.3895)  Time: 10.642s,   96.23/s  (10.642s,   96.23/s)  LR: 1.951e-05  Data: 9.889 (9.889)
Train: 135 [  50/1171 (  4%)]  Loss:  2.988704 (2.6891)  Time: 0.586s, 1748.09/s  (2.201s,  465.27/s)  LR: 1.951e-05  Data: 0.020 (1.616)
Train: 135 [ 100/1171 (  9%)]  Loss:  2.752288 (2.7102)  Time: 0.586s, 1747.74/s  (2.167s,  472.60/s)  LR: 1.951e-05  Data: 0.023 (1.578)
Train: 135 [ 150/1171 ( 13%)]  Loss:  2.289984 (2.6051)  Time: 0.590s, 1734.41/s  (2.107s,  486.04/s)  LR: 1.951e-05  Data: 0.019 (1.519)
Train: 135 [ 200/1171 ( 17%)]  Loss:  2.605235 (2.6051)  Time: 0.587s, 1744.89/s  (2.244s,  456.39/s)  LR: 1.951e-05  Data: 0.024 (1.657)
Train: 135 [ 250/1171 ( 21%)]  Loss:  2.810396 (2.6394)  Time: 0.584s, 1754.25/s  (2.237s,  457.67/s)  LR: 1.951e-05  Data: 0.021 (1.650)
Train: 135 [ 300/1171 ( 26%)]  Loss:  2.993293 (2.6899)  Time: 0.586s, 1748.67/s  (2.259s,  453.30/s)  LR: 1.951e-05  Data: 0.023 (1.673)
Train: 135 [ 350/1171 ( 30%)]  Loss:  3.134627 (2.7455)  Time: 0.590s, 1736.47/s  (2.261s,  452.90/s)  LR: 1.951e-05  Data: 0.017 (1.674)
Train: 135 [ 400/1171 ( 34%)]  Loss:  3.169502 (2.7926)  Time: 0.592s, 1729.69/s  (2.270s,  451.01/s)  LR: 1.951e-05  Data: 0.030 (1.683)
Train: 135 [ 450/1171 ( 38%)]  Loss:  2.895237 (2.8029)  Time: 0.589s, 1737.82/s  (2.261s,  452.94/s)  LR: 1.951e-05  Data: 0.020 (1.673)
Train: 135 [ 500/1171 ( 43%)]  Loss:  2.390150 (2.7654)  Time: 0.588s, 1740.49/s  (2.278s,  449.57/s)  LR: 1.951e-05  Data: 0.024 (1.689)
Train: 135 [ 550/1171 ( 47%)]  Loss:  2.940708 (2.7800)  Time: 0.590s, 1735.97/s  (2.280s,  449.03/s)  LR: 1.951e-05  Data: 0.020 (1.692)
Train: 135 [ 600/1171 ( 51%)]  Loss:  2.037663 (2.7229)  Time: 0.592s, 1729.16/s  (2.333s,  438.85/s)  LR: 1.951e-05  Data: 0.023 (1.745)
Train: 135 [ 650/1171 ( 56%)]  Loss:  2.649264 (2.7176)  Time: 0.588s, 1741.83/s  (2.343s,  437.12/s)  LR: 1.951e-05  Data: 0.021 (1.755)
Train: 135 [ 700/1171 ( 60%)]  Loss:  2.351891 (2.6932)  Time: 0.588s, 1742.62/s  (2.353s,  435.14/s)  LR: 1.951e-05  Data: 0.023 (1.765)
Train: 135 [ 750/1171 ( 64%)]  Loss:  2.404396 (2.6752)  Time: 0.589s, 1739.15/s  (2.350s,  435.77/s)  LR: 1.951e-05  Data: 0.023 (1.763)
Train: 135 [ 800/1171 ( 68%)]  Loss:  3.041992 (2.6968)  Time: 0.587s, 1743.92/s  (2.354s,  434.99/s)  LR: 1.951e-05  Data: 0.024 (1.767)
Train: 135 [ 850/1171 ( 73%)]  Loss:  2.836383 (2.7045)  Time: 0.590s, 1736.89/s  (2.347s,  436.36/s)  LR: 1.951e-05  Data: 0.026 (1.760)
Train: 135 [ 900/1171 ( 77%)]  Loss:  3.303751 (2.7361)  Time: 0.585s, 1749.90/s  (2.346s,  436.53/s)  LR: 1.951e-05  Data: 0.023 (1.759)
Train: 135 [ 950/1171 ( 81%)]  Loss:  2.990844 (2.7488)  Time: 0.588s, 1741.63/s  (2.362s,  433.56/s)  LR: 1.951e-05  Data: 0.019 (1.775)
Train: 135 [1000/1171 ( 85%)]  Loss:  2.825997 (2.7525)  Time: 0.587s, 1745.62/s  (2.366s,  432.87/s)  LR: 1.951e-05  Data: 0.024 (1.779)
Train: 135 [1050/1171 ( 90%)]  Loss:  3.001364 (2.7638)  Time: 0.587s, 1744.60/s  (2.361s,  433.72/s)  LR: 1.951e-05  Data: 0.022 (1.775)
Train: 135 [1100/1171 ( 94%)]  Loss:  2.736600 (2.7626)  Time: 0.589s, 1739.55/s  (2.366s,  432.85/s)  LR: 1.951e-05  Data: 0.023 (1.779)
Train: 135 [1150/1171 ( 98%)]  Loss:  2.678546 (2.7591)  Time: 0.587s, 1743.99/s  (2.363s,  433.42/s)  LR: 1.951e-05  Data: 0.021 (1.776)
Train: 135 [1170/1171 (100%)]  Loss:  3.151713 (2.7748)  Time: 0.566s, 1809.23/s  (2.364s,  433.25/s)  LR: 1.951e-05  Data: 0.000 (1.777)
Test: [   0/97]  Time: 12.676 (12.676)  Loss:  0.2955 (0.2955)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (2.986)  Loss:  0.4675 (0.3623)  Acc@1: 91.6016 (95.2321)  Acc@5: 98.3398 (98.9430)
Test: [  97/97]  Time: 0.120 (2.930)  Loss:  0.3281 (0.3732)  Acc@1: 94.7917 (94.7200)  Acc@5: 99.4048 (98.7980)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-135.pth.tar', 94.71999998291015)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-134.pth.tar', 94.6520000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-132.pth.tar', 94.63500000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-133.pth.tar', 94.62899998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-131.pth.tar', 94.60300002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-128.pth.tar', 94.53899998535157)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-130.pth.tar', 94.49400000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-127.pth.tar', 94.48700000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-125.pth.tar', 94.46999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-129.pth.tar', 94.45299999755859)

Train: 136 [   0/1171 (  0%)]  Loss:  3.122838 (3.1228)  Time: 14.027s,   73.00/s  (14.027s,   73.00/s)  LR: 1.752e-05  Data: 12.812 (12.812)
Train: 136 [  50/1171 (  4%)]  Loss:  2.318316 (2.7206)  Time: 0.591s, 1733.23/s  (2.648s,  386.64/s)  LR: 1.752e-05  Data: 0.019 (2.039)
Train: 136 [ 100/1171 (  9%)]  Loss:  2.693913 (2.7117)  Time: 1.746s,  586.32/s  (2.562s,  399.66/s)  LR: 1.752e-05  Data: 1.185 (1.955)
Train: 136 [ 150/1171 ( 13%)]  Loss:  2.837856 (2.7432)  Time: 0.583s, 1755.44/s  (2.470s,  414.59/s)  LR: 1.752e-05  Data: 0.019 (1.856)
Train: 136 [ 200/1171 ( 17%)]  Loss:  2.586417 (2.7119)  Time: 0.592s, 1729.81/s  (2.429s,  421.62/s)  LR: 1.752e-05  Data: 0.021 (1.818)
Train: 136 [ 250/1171 ( 21%)]  Loss:  2.541480 (2.6835)  Time: 0.590s, 1736.19/s  (2.379s,  430.47/s)  LR: 1.752e-05  Data: 0.024 (1.771)
Train: 136 [ 300/1171 ( 26%)]  Loss:  2.684657 (2.6836)  Time: 0.585s, 1749.93/s  (2.357s,  434.49/s)  LR: 1.752e-05  Data: 0.023 (1.752)
Train: 136 [ 350/1171 ( 30%)]  Loss:  2.657708 (2.6804)  Time: 0.592s, 1730.59/s  (2.320s,  441.42/s)  LR: 1.752e-05  Data: 0.026 (1.718)
Train: 136 [ 400/1171 ( 34%)]  Loss:  2.408452 (2.6502)  Time: 0.585s, 1751.20/s  (2.339s,  437.80/s)  LR: 1.752e-05  Data: 0.020 (1.736)
Train: 136 [ 450/1171 ( 38%)]  Loss:  2.882404 (2.6734)  Time: 0.589s, 1739.48/s  (2.330s,  439.43/s)  LR: 1.752e-05  Data: 0.023 (1.729)
Train: 136 [ 500/1171 ( 43%)]  Loss:  2.709017 (2.6766)  Time: 0.588s, 1742.09/s  (2.363s,  433.34/s)  LR: 1.752e-05  Data: 0.019 (1.763)
Train: 136 [ 550/1171 ( 47%)]  Loss:  2.886174 (2.6941)  Time: 0.588s, 1740.79/s  (2.370s,  432.02/s)  LR: 1.752e-05  Data: 0.023 (1.770)
Train: 136 [ 600/1171 ( 51%)]  Loss:  2.716727 (2.6958)  Time: 0.584s, 1754.55/s  (2.386s,  429.23/s)  LR: 1.752e-05  Data: 0.019 (1.788)
Train: 136 [ 650/1171 ( 56%)]  Loss:  2.758532 (2.7003)  Time: 0.590s, 1734.26/s  (2.377s,  430.79/s)  LR: 1.752e-05  Data: 0.023 (1.780)
Train: 136 [ 700/1171 ( 60%)]  Loss:  3.085368 (2.7260)  Time: 0.587s, 1744.05/s  (2.370s,  432.11/s)  LR: 1.752e-05  Data: 0.018 (1.773)
Train: 136 [ 750/1171 ( 64%)]  Loss:  2.506171 (2.7123)  Time: 0.590s, 1736.68/s  (2.359s,  434.08/s)  LR: 1.752e-05  Data: 0.023 (1.761)
Train: 136 [ 800/1171 ( 68%)]  Loss:  2.588678 (2.7050)  Time: 0.585s, 1749.81/s  (2.382s,  429.93/s)  LR: 1.752e-05  Data: 0.021 (1.783)
Train: 136 [ 850/1171 ( 73%)]  Loss:  2.651954 (2.7020)  Time: 0.590s, 1735.97/s  (2.386s,  429.21/s)  LR: 1.752e-05  Data: 0.025 (1.788)
Train: 136 [ 900/1171 ( 77%)]  Loss:  2.607728 (2.6971)  Time: 0.587s, 1745.56/s  (2.391s,  428.21/s)  LR: 1.752e-05  Data: 0.020 (1.794)
Train: 136 [ 950/1171 ( 81%)]  Loss:  2.548323 (2.6896)  Time: 0.587s, 1745.17/s  (2.385s,  429.39/s)  LR: 1.752e-05  Data: 0.022 (1.788)
Train: 136 [1000/1171 ( 85%)]  Loss:  2.541350 (2.6826)  Time: 1.800s,  568.95/s  (2.386s,  429.16/s)  LR: 1.752e-05  Data: 1.131 (1.789)
Train: 136 [1050/1171 ( 90%)]  Loss:  2.728584 (2.6847)  Time: 0.587s, 1744.75/s  (2.378s,  430.59/s)  LR: 1.752e-05  Data: 0.022 (1.780)
Train: 136 [1100/1171 ( 94%)]  Loss:  3.139959 (2.7045)  Time: 2.213s,  462.65/s  (2.388s,  428.86/s)  LR: 1.752e-05  Data: 1.624 (1.790)
Train: 136 [1150/1171 ( 98%)]  Loss:  2.735469 (2.7058)  Time: 0.586s, 1747.93/s  (2.396s,  427.43/s)  LR: 1.752e-05  Data: 0.018 (1.798)
Train: 136 [1170/1171 (100%)]  Loss:  2.735485 (2.7069)  Time: 0.565s, 1810.93/s  (2.396s,  427.34/s)  LR: 1.752e-05  Data: 0.000 (1.799)
Test: [   0/97]  Time: 11.822 (11.822)  Loss:  0.3113 (0.3113)  Acc@1: 96.8750 (96.8750)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.204 (3.082)  Loss:  0.4587 (0.3754)  Acc@1: 92.6758 (95.1708)  Acc@5: 98.4375 (98.9411)
Test: [  97/97]  Time: 0.119 (3.055)  Loss:  0.3362 (0.3859)  Acc@1: 95.2381 (94.6950)  Acc@5: 99.4048 (98.7960)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-135.pth.tar', 94.71999998291015)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-136.pth.tar', 94.69500001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-134.pth.tar', 94.6520000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-132.pth.tar', 94.63500000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-133.pth.tar', 94.62899998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-131.pth.tar', 94.60300002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-128.pth.tar', 94.53899998535157)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-130.pth.tar', 94.49400000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-127.pth.tar', 94.48700000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-125.pth.tar', 94.46999998535156)

Train: 137 [   0/1171 (  0%)]  Loss:  2.654829 (2.6548)  Time: 10.968s,   93.36/s  (10.968s,   93.36/s)  LR: 1.576e-05  Data: 9.944 (9.944)
Train: 137 [  50/1171 (  4%)]  Loss:  3.038105 (2.8465)  Time: 0.586s, 1747.97/s  (2.343s,  437.11/s)  LR: 1.576e-05  Data: 0.019 (1.721)
Train: 137 [ 100/1171 (  9%)]  Loss:  2.742697 (2.8119)  Time: 3.742s,  273.64/s  (2.314s,  442.58/s)  LR: 1.576e-05  Data: 2.849 (1.706)
Train: 137 [ 150/1171 ( 13%)]  Loss:  2.809202 (2.8112)  Time: 0.587s, 1744.38/s  (2.245s,  456.11/s)  LR: 1.576e-05  Data: 0.019 (1.640)
Train: 137 [ 200/1171 ( 17%)]  Loss:  2.861524 (2.8213)  Time: 1.712s,  598.19/s  (2.245s,  456.22/s)  LR: 1.576e-05  Data: 1.053 (1.645)
Train: 137 [ 250/1171 ( 21%)]  Loss:  3.108569 (2.8692)  Time: 0.590s, 1735.28/s  (2.296s,  445.95/s)  LR: 1.576e-05  Data: 0.019 (1.693)
Train: 137 [ 300/1171 ( 26%)]  Loss:  2.864682 (2.8685)  Time: 0.590s, 1735.91/s  (2.335s,  438.53/s)  LR: 1.576e-05  Data: 0.025 (1.735)
Train: 137 [ 350/1171 ( 30%)]  Loss:  3.170966 (2.9063)  Time: 0.587s, 1745.70/s  (2.341s,  437.37/s)  LR: 1.576e-05  Data: 0.020 (1.742)
Train: 137 [ 400/1171 ( 34%)]  Loss:  3.224966 (2.9417)  Time: 0.587s, 1745.22/s  (2.352s,  435.33/s)  LR: 1.576e-05  Data: 0.022 (1.754)
Train: 137 [ 450/1171 ( 38%)]  Loss:  2.991029 (2.9467)  Time: 0.588s, 1740.99/s  (2.341s,  437.35/s)  LR: 1.576e-05  Data: 0.023 (1.745)
Train: 137 [ 500/1171 ( 43%)]  Loss:  3.220925 (2.9716)  Time: 0.591s, 1733.85/s  (2.349s,  436.02/s)  LR: 1.576e-05  Data: 0.025 (1.752)
Train: 137 [ 550/1171 ( 47%)]  Loss:  2.676826 (2.9470)  Time: 0.594s, 1722.82/s  (2.344s,  436.77/s)  LR: 1.576e-05  Data: 0.018 (1.749)
Train: 137 [ 600/1171 ( 51%)]  Loss:  3.104837 (2.9592)  Time: 0.588s, 1741.98/s  (2.366s,  432.74/s)  LR: 1.576e-05  Data: 0.024 (1.771)
Train: 137 [ 650/1171 ( 56%)]  Loss:  2.964196 (2.9595)  Time: 0.589s, 1739.16/s  (2.372s,  431.74/s)  LR: 1.576e-05  Data: 0.020 (1.778)
Train: 137 [ 700/1171 ( 60%)]  Loss:  2.739742 (2.9449)  Time: 0.600s, 1707.88/s  (2.417s,  423.61/s)  LR: 1.576e-05  Data: 0.032 (1.824)
Train: 137 [ 750/1171 ( 64%)]  Loss:  3.064512 (2.9524)  Time: 0.591s, 1732.99/s  (2.410s,  424.83/s)  LR: 1.576e-05  Data: 0.018 (1.817)
Train: 137 [ 800/1171 ( 68%)]  Loss:  2.523890 (2.9271)  Time: 0.590s, 1735.29/s  (2.411s,  424.78/s)  LR: 1.576e-05  Data: 0.026 (1.818)
Train: 137 [ 850/1171 ( 73%)]  Loss:  2.531837 (2.9052)  Time: 0.592s, 1730.12/s  (2.401s,  426.48/s)  LR: 1.576e-05  Data: 0.018 (1.808)
Train: 137 [ 900/1171 ( 77%)]  Loss:  3.302855 (2.9261)  Time: 0.585s, 1749.80/s  (2.398s,  427.08/s)  LR: 1.576e-05  Data: 0.023 (1.806)
Train: 137 [ 950/1171 ( 81%)]  Loss:  2.825393 (2.9211)  Time: 0.591s, 1732.73/s  (2.386s,  429.09/s)  LR: 1.576e-05  Data: 0.024 (1.795)
Train: 137 [1000/1171 ( 85%)]  Loss:  2.975839 (2.9237)  Time: 0.586s, 1746.50/s  (2.403s,  426.06/s)  LR: 1.576e-05  Data: 0.023 (1.811)
Train: 137 [1050/1171 ( 90%)]  Loss:  3.111897 (2.9322)  Time: 0.587s, 1745.60/s  (2.401s,  426.47/s)  LR: 1.576e-05  Data: 0.024 (1.810)
Train: 137 [1100/1171 ( 94%)]  Loss:  3.227445 (2.9451)  Time: 0.586s, 1746.95/s  (2.401s,  426.52/s)  LR: 1.576e-05  Data: 0.023 (1.810)
Train: 137 [1150/1171 ( 98%)]  Loss:  2.933658 (2.9446)  Time: 0.587s, 1744.05/s  (2.395s,  427.50/s)  LR: 1.576e-05  Data: 0.024 (1.805)
Train: 137 [1170/1171 (100%)]  Loss:  3.193126 (2.9545)  Time: 0.566s, 1810.49/s  (2.393s,  427.90/s)  LR: 1.576e-05  Data: 0.000 (1.802)
Test: [   0/97]  Time: 12.650 (12.650)  Loss:  0.2901 (0.2901)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (3.009)  Loss:  0.4430 (0.3529)  Acc@1: 91.8945 (95.2608)  Acc@5: 98.4375 (98.9200)
Test: [  97/97]  Time: 0.119 (2.933)  Loss:  0.3231 (0.3649)  Acc@1: 94.6429 (94.6890)  Acc@5: 99.4048 (98.7960)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-135.pth.tar', 94.71999998291015)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-136.pth.tar', 94.69500001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-137.pth.tar', 94.68900002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-134.pth.tar', 94.6520000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-132.pth.tar', 94.63500000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-133.pth.tar', 94.62899998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-131.pth.tar', 94.60300002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-128.pth.tar', 94.53899998535157)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-130.pth.tar', 94.49400000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-127.pth.tar', 94.48700000732421)

Train: 138 [   0/1171 (  0%)]  Loss:  3.050841 (3.0508)  Time: 10.067s,  101.72/s  (10.067s,  101.72/s)  LR: 1.423e-05  Data: 9.113 (9.113)
Train: 138 [  50/1171 (  4%)]  Loss:  3.091255 (3.0710)  Time: 0.590s, 1736.44/s  (2.472s,  414.23/s)  LR: 1.423e-05  Data: 0.019 (1.878)
Train: 138 [ 100/1171 (  9%)]  Loss:  2.813423 (2.9852)  Time: 0.591s, 1733.71/s  (2.495s,  410.36/s)  LR: 1.423e-05  Data: 0.020 (1.896)
Train: 138 [ 150/1171 ( 13%)]  Loss:  2.560378 (2.8790)  Time: 0.585s, 1750.83/s  (2.437s,  420.14/s)  LR: 1.423e-05  Data: 0.017 (1.840)
Train: 138 [ 200/1171 ( 17%)]  Loss:  2.643883 (2.8320)  Time: 0.586s, 1746.94/s  (2.442s,  419.32/s)  LR: 1.423e-05  Data: 0.019 (1.841)
Train: 138 [ 250/1171 ( 21%)]  Loss:  2.745090 (2.8175)  Time: 0.587s, 1744.38/s  (2.395s,  427.64/s)  LR: 1.423e-05  Data: 0.018 (1.794)
Train: 138 [ 300/1171 ( 26%)]  Loss:  2.808083 (2.8161)  Time: 0.590s, 1736.64/s  (2.434s,  420.71/s)  LR: 1.423e-05  Data: 0.024 (1.830)
Train: 138 [ 350/1171 ( 30%)]  Loss:  2.837983 (2.8189)  Time: 0.591s, 1733.81/s  (2.409s,  425.00/s)  LR: 1.423e-05  Data: 0.018 (1.807)
Train: 138 [ 400/1171 ( 34%)]  Loss:  3.010837 (2.8402)  Time: 0.586s, 1746.58/s  (2.395s,  427.62/s)  LR: 1.423e-05  Data: 0.025 (1.792)
Train: 138 [ 450/1171 ( 38%)]  Loss:  3.228191 (2.8790)  Time: 1.453s,  704.80/s  (2.412s,  424.52/s)  LR: 1.423e-05  Data: 0.891 (1.809)
Train: 138 [ 500/1171 ( 43%)]  Loss:  2.758781 (2.8681)  Time: 0.587s, 1743.01/s  (2.416s,  423.83/s)  LR: 1.423e-05  Data: 0.023 (1.813)
Train: 138 [ 550/1171 ( 47%)]  Loss:  2.733071 (2.8568)  Time: 2.360s,  433.87/s  (2.423s,  422.70/s)  LR: 1.423e-05  Data: 1.784 (1.821)
Train: 138 [ 600/1171 ( 51%)]  Loss:  2.822619 (2.8542)  Time: 0.588s, 1742.02/s  (2.433s,  420.86/s)  LR: 1.423e-05  Data: 0.024 (1.830)
Train: 138 [ 650/1171 ( 56%)]  Loss:  2.811650 (2.8511)  Time: 3.421s,  299.36/s  (2.433s,  420.86/s)  LR: 1.423e-05  Data: 2.834 (1.829)
Train: 138 [ 700/1171 ( 60%)]  Loss:  2.784427 (2.8467)  Time: 0.590s, 1736.40/s  (2.425s,  422.20/s)  LR: 1.423e-05  Data: 0.023 (1.822)
Train: 138 [ 750/1171 ( 64%)]  Loss:  2.837678 (2.8461)  Time: 0.589s, 1737.13/s  (2.410s,  424.83/s)  LR: 1.423e-05  Data: 0.026 (1.808)
Train: 138 [ 800/1171 ( 68%)]  Loss:  3.037714 (2.8574)  Time: 0.587s, 1743.02/s  (2.424s,  422.46/s)  LR: 1.423e-05  Data: 0.023 (1.822)
Train: 138 [ 850/1171 ( 73%)]  Loss:  3.326986 (2.8835)  Time: 0.587s, 1745.03/s  (2.416s,  423.89/s)  LR: 1.423e-05  Data: 0.024 (1.815)
Train: 138 [ 900/1171 ( 77%)]  Loss:  2.981336 (2.8886)  Time: 0.587s, 1743.50/s  (2.423s,  422.58/s)  LR: 1.423e-05  Data: 0.024 (1.822)
Train: 138 [ 950/1171 ( 81%)]  Loss:  2.576123 (2.8730)  Time: 2.058s,  497.48/s  (2.418s,  423.53/s)  LR: 1.423e-05  Data: 1.394 (1.817)
Train: 138 [1000/1171 ( 85%)]  Loss:  3.151165 (2.8863)  Time: 0.589s, 1740.01/s  (2.417s,  423.67/s)  LR: 1.423e-05  Data: 0.023 (1.816)
Train: 138 [1050/1171 ( 90%)]  Loss:  2.848598 (2.8846)  Time: 2.768s,  369.88/s  (2.414s,  424.21/s)  LR: 1.423e-05  Data: 2.112 (1.812)
Train: 138 [1100/1171 ( 94%)]  Loss:  2.558308 (2.8704)  Time: 2.018s,  507.54/s  (2.404s,  425.95/s)  LR: 1.423e-05  Data: 1.435 (1.802)
Train: 138 [1150/1171 ( 98%)]  Loss:  3.206855 (2.8844)  Time: 4.457s,  229.74/s  (2.396s,  427.32/s)  LR: 1.423e-05  Data: 3.892 (1.792)
Train: 138 [1170/1171 (100%)]  Loss:  2.822643 (2.8819)  Time: 0.565s, 1812.95/s  (2.389s,  428.54/s)  LR: 1.423e-05  Data: 0.000 (1.785)
Test: [   0/97]  Time: 17.249 (17.249)  Loss:  0.2870 (0.2870)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (3.666)  Loss:  0.4324 (0.3473)  Acc@1: 92.5781 (95.2589)  Acc@5: 98.3398 (98.9430)
Test: [  97/97]  Time: 0.120 (3.351)  Loss:  0.3100 (0.3581)  Acc@1: 94.9405 (94.7530)  Acc@5: 99.1071 (98.8130)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-138.pth.tar', 94.7529999951172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-135.pth.tar', 94.71999998291015)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-136.pth.tar', 94.69500001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-137.pth.tar', 94.68900002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-134.pth.tar', 94.6520000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-132.pth.tar', 94.63500000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-133.pth.tar', 94.62899998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-131.pth.tar', 94.60300002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-128.pth.tar', 94.53899998535157)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-130.pth.tar', 94.49400000732422)

Train: 139 [   0/1171 (  0%)]  Loss:  2.888401 (2.8884)  Time: 11.343s,   90.28/s  (11.343s,   90.28/s)  LR: 1.294e-05  Data: 10.255 (10.255)
Train: 139 [  50/1171 (  4%)]  Loss:  2.939198 (2.9138)  Time: 0.587s, 1744.39/s  (2.364s,  433.21/s)  LR: 1.294e-05  Data: 0.023 (1.774)
Train: 139 [ 100/1171 (  9%)]  Loss:  3.071375 (2.9663)  Time: 0.586s, 1747.16/s  (2.279s,  449.31/s)  LR: 1.294e-05  Data: 0.021 (1.681)
Train: 139 [ 150/1171 ( 13%)]  Loss:  2.796984 (2.9240)  Time: 0.586s, 1746.24/s  (2.228s,  459.54/s)  LR: 1.294e-05  Data: 0.021 (1.628)
Train: 139 [ 200/1171 ( 17%)]  Loss:  2.294037 (2.7980)  Time: 0.585s, 1749.04/s  (2.219s,  461.50/s)  LR: 1.294e-05  Data: 0.023 (1.621)
Train: 139 [ 250/1171 ( 21%)]  Loss:  3.006229 (2.8327)  Time: 0.584s, 1752.21/s  (2.201s,  465.30/s)  LR: 1.294e-05  Data: 0.018 (1.603)
Train: 139 [ 300/1171 ( 26%)]  Loss:  2.659886 (2.8080)  Time: 1.639s,  624.59/s  (2.272s,  450.70/s)  LR: 1.294e-05  Data: 1.077 (1.664)
Train: 139 [ 350/1171 ( 30%)]  Loss:  2.587332 (2.7804)  Time: 0.588s, 1742.08/s  (2.242s,  456.72/s)  LR: 1.294e-05  Data: 0.019 (1.636)
Train: 139 [ 400/1171 ( 34%)]  Loss:  3.073727 (2.8130)  Time: 3.582s,  285.91/s  (2.262s,  452.67/s)  LR: 1.294e-05  Data: 2.887 (1.653)
Train: 139 [ 450/1171 ( 38%)]  Loss:  3.161081 (2.8478)  Time: 0.584s, 1753.49/s  (2.258s,  453.49/s)  LR: 1.294e-05  Data: 0.019 (1.648)
Train: 139 [ 500/1171 ( 43%)]  Loss:  2.436224 (2.8104)  Time: 3.653s,  280.33/s  (2.262s,  452.72/s)  LR: 1.294e-05  Data: 2.999 (1.652)
Train: 139 [ 550/1171 ( 47%)]  Loss:  2.769261 (2.8070)  Time: 0.585s, 1751.13/s  (2.271s,  450.92/s)  LR: 1.294e-05  Data: 0.020 (1.661)
Train: 139 [ 600/1171 ( 51%)]  Loss:  3.114255 (2.8306)  Time: 2.511s,  407.78/s  (2.274s,  450.34/s)  LR: 1.294e-05  Data: 1.926 (1.666)
Train: 139 [ 650/1171 ( 56%)]  Loss:  2.933900 (2.8380)  Time: 0.586s, 1747.27/s  (2.280s,  449.17/s)  LR: 1.294e-05  Data: 0.020 (1.672)
Train: 139 [ 700/1171 ( 60%)]  Loss:  3.208705 (2.8627)  Time: 0.587s, 1743.02/s  (2.297s,  445.84/s)  LR: 1.294e-05  Data: 0.018 (1.690)
Train: 139 [ 750/1171 ( 64%)]  Loss:  2.773490 (2.8571)  Time: 0.584s, 1754.51/s  (2.302s,  444.84/s)  LR: 1.294e-05  Data: 0.021 (1.697)
Train: 139 [ 800/1171 ( 68%)]  Loss:  2.595348 (2.8417)  Time: 0.584s, 1753.66/s  (2.318s,  441.67/s)  LR: 1.294e-05  Data: 0.020 (1.715)
Train: 139 [ 850/1171 ( 73%)]  Loss:  2.756560 (2.8370)  Time: 0.582s, 1758.88/s  (2.328s,  439.82/s)  LR: 1.294e-05  Data: 0.019 (1.725)
Train: 139 [ 900/1171 ( 77%)]  Loss:  3.188976 (2.8555)  Time: 0.584s, 1753.01/s  (2.321s,  441.18/s)  LR: 1.294e-05  Data: 0.019 (1.719)
Train: 139 [ 950/1171 ( 81%)]  Loss:  2.698955 (2.8477)  Time: 0.585s, 1751.41/s  (2.319s,  441.61/s)  LR: 1.294e-05  Data: 0.020 (1.718)
Train: 139 [1000/1171 ( 85%)]  Loss:  2.660989 (2.8388)  Time: 0.589s, 1738.02/s  (2.312s,  442.97/s)  LR: 1.294e-05  Data: 0.018 (1.712)
Train: 139 [1050/1171 ( 90%)]  Loss:  2.528579 (2.8247)  Time: 0.584s, 1752.00/s  (2.332s,  439.04/s)  LR: 1.294e-05  Data: 0.020 (1.732)
Train: 139 [1100/1171 ( 94%)]  Loss:  2.779149 (2.8227)  Time: 0.589s, 1737.28/s  (2.322s,  440.92/s)  LR: 1.294e-05  Data: 0.023 (1.723)
Train: 139 [1150/1171 ( 98%)]  Loss:  2.709476 (2.8180)  Time: 0.584s, 1753.99/s  (2.329s,  439.68/s)  LR: 1.294e-05  Data: 0.019 (1.730)
Train: 139 [1170/1171 (100%)]  Loss:  2.651841 (2.8114)  Time: 0.565s, 1811.21/s  (2.328s,  439.79/s)  LR: 1.294e-05  Data: 0.000 (1.730)
Test: [   0/97]  Time: 11.968 (11.968)  Loss:  0.2900 (0.2900)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.201 (3.040)  Loss:  0.4489 (0.3572)  Acc@1: 92.4805 (95.2436)  Acc@5: 98.3398 (98.9488)
Test: [  97/97]  Time: 0.120 (2.962)  Loss:  0.3132 (0.3684)  Acc@1: 94.7917 (94.7500)  Acc@5: 99.4048 (98.8010)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-138.pth.tar', 94.7529999951172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-139.pth.tar', 94.74999998291015)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-135.pth.tar', 94.71999998291015)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-136.pth.tar', 94.69500001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-137.pth.tar', 94.68900002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-134.pth.tar', 94.6520000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-132.pth.tar', 94.63500000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-133.pth.tar', 94.62899998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-131.pth.tar', 94.60300002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-128.pth.tar', 94.53899998535157)

Train: 140 [   0/1171 (  0%)]  Loss:  2.628489 (2.6285)  Time: 10.089s,  101.50/s  (10.089s,  101.50/s)  LR: 1.188e-05  Data: 8.954 (8.954)
Train: 140 [  50/1171 (  4%)]  Loss:  3.043511 (2.8360)  Time: 0.589s, 1739.11/s  (2.247s,  455.75/s)  LR: 1.188e-05  Data: 0.024 (1.659)
Train: 140 [ 100/1171 (  9%)]  Loss:  2.889424 (2.8538)  Time: 0.586s, 1746.64/s  (2.226s,  459.98/s)  LR: 1.188e-05  Data: 0.023 (1.639)
Train: 140 [ 150/1171 ( 13%)]  Loss:  2.570795 (2.7831)  Time: 0.585s, 1750.42/s  (2.338s,  437.99/s)  LR: 1.188e-05  Data: 0.021 (1.750)
Train: 140 [ 200/1171 ( 17%)]  Loss:  2.600577 (2.7466)  Time: 0.872s, 1174.79/s  (2.312s,  442.81/s)  LR: 1.188e-05  Data: 0.209 (1.720)
Train: 140 [ 250/1171 ( 21%)]  Loss:  2.628937 (2.7270)  Time: 0.587s, 1745.28/s  (2.308s,  443.59/s)  LR: 1.188e-05  Data: 0.018 (1.713)
Train: 140 [ 300/1171 ( 26%)]  Loss:  2.642124 (2.7148)  Time: 1.686s,  607.20/s  (2.296s,  445.94/s)  LR: 1.188e-05  Data: 1.016 (1.696)
Train: 140 [ 350/1171 ( 30%)]  Loss:  3.151961 (2.7695)  Time: 0.588s, 1742.40/s  (2.273s,  450.56/s)  LR: 1.188e-05  Data: 0.021 (1.671)
Train: 140 [ 400/1171 ( 34%)]  Loss:  2.727730 (2.7648)  Time: 0.585s, 1750.39/s  (2.267s,  451.70/s)  LR: 1.188e-05  Data: 0.020 (1.666)
Train: 140 [ 450/1171 ( 38%)]  Loss:  3.015091 (2.7899)  Time: 0.589s, 1739.63/s  (2.301s,  445.05/s)  LR: 1.188e-05  Data: 0.026 (1.699)
Train: 140 [ 500/1171 ( 43%)]  Loss:  2.772465 (2.7883)  Time: 0.584s, 1753.54/s  (2.314s,  442.57/s)  LR: 1.188e-05  Data: 0.021 (1.713)
Train: 140 [ 550/1171 ( 47%)]  Loss:  2.971826 (2.8036)  Time: 0.590s, 1735.53/s  (2.364s,  433.15/s)  LR: 1.188e-05  Data: 0.025 (1.763)
Train: 140 [ 600/1171 ( 51%)]  Loss:  3.467160 (2.8546)  Time: 1.900s,  538.90/s  (2.367s,  432.70/s)  LR: 1.188e-05  Data: 1.254 (1.764)
Train: 140 [ 650/1171 ( 56%)]  Loss:  2.980661 (2.8636)  Time: 0.588s, 1740.74/s  (2.376s,  430.94/s)  LR: 1.188e-05  Data: 0.024 (1.773)
Train: 140 [ 700/1171 ( 60%)]  Loss:  2.961189 (2.8701)  Time: 2.778s,  368.56/s  (2.379s,  430.41/s)  LR: 1.188e-05  Data: 2.062 (1.775)
Train: 140 [ 750/1171 ( 64%)]  Loss:  3.036064 (2.8805)  Time: 1.982s,  516.68/s  (2.375s,  431.24/s)  LR: 1.188e-05  Data: 1.408 (1.770)
Train: 140 [ 800/1171 ( 68%)]  Loss:  2.512132 (2.8588)  Time: 1.455s,  703.98/s  (2.364s,  433.25/s)  LR: 1.188e-05  Data: 0.866 (1.759)
Train: 140 [ 850/1171 ( 73%)]  Loss:  2.253941 (2.8252)  Time: 3.801s,  269.41/s  (2.360s,  433.88/s)  LR: 1.188e-05  Data: 3.223 (1.755)
Train: 140 [ 900/1171 ( 77%)]  Loss:  2.532185 (2.8098)  Time: 2.934s,  348.97/s  (2.374s,  431.28/s)  LR: 1.188e-05  Data: 2.276 (1.768)
Train: 140 [ 950/1171 ( 81%)]  Loss:  3.212864 (2.8300)  Time: 4.676s,  218.97/s  (2.364s,  433.24/s)  LR: 1.188e-05  Data: 3.954 (1.756)
Train: 140 [1000/1171 ( 85%)]  Loss:  2.673029 (2.8225)  Time: 3.483s,  294.00/s  (2.365s,  432.97/s)  LR: 1.188e-05  Data: 2.818 (1.757)
Train: 140 [1050/1171 ( 90%)]  Loss:  2.405201 (2.8035)  Time: 4.142s,  247.22/s  (2.362s,  433.61/s)  LR: 1.188e-05  Data: 3.575 (1.753)
Train: 140 [1100/1171 ( 94%)]  Loss:  2.937275 (2.8093)  Time: 5.478s,  186.95/s  (2.359s,  434.14/s)  LR: 1.188e-05  Data: 4.844 (1.751)
Train: 140 [1150/1171 ( 98%)]  Loss:  2.763463 (2.8074)  Time: 0.586s, 1748.92/s  (2.349s,  435.86/s)  LR: 1.188e-05  Data: 0.019 (1.741)
Train: 140 [1170/1171 (100%)]  Loss:  3.215886 (2.8238)  Time: 0.565s, 1810.90/s  (2.347s,  436.22/s)  LR: 1.188e-05  Data: 0.000 (1.740)
Test: [   0/97]  Time: 12.501 (12.501)  Loss:  0.2867 (0.2867)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (2.876)  Loss:  0.4391 (0.3548)  Acc@1: 92.7734 (95.2876)  Acc@5: 98.5352 (98.9468)
Test: [  97/97]  Time: 0.119 (3.031)  Loss:  0.3168 (0.3648)  Acc@1: 94.7917 (94.7880)  Acc@5: 99.2560 (98.8140)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-140.pth.tar', 94.78799998291015)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-138.pth.tar', 94.7529999951172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-139.pth.tar', 94.74999998291015)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-135.pth.tar', 94.71999998291015)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-136.pth.tar', 94.69500001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-137.pth.tar', 94.68900002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-134.pth.tar', 94.6520000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-132.pth.tar', 94.63500000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-133.pth.tar', 94.62899998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-131.pth.tar', 94.60300002197266)

Train: 141 [   0/1171 (  0%)]  Loss:  3.171343 (3.1713)  Time: 18.505s,   55.34/s  (18.505s,   55.34/s)  LR: 1.106e-05  Data: 17.252 (17.252)
Train: 141 [  50/1171 (  4%)]  Loss:  2.154367 (2.6629)  Time: 0.585s, 1749.02/s  (2.752s,  372.04/s)  LR: 1.106e-05  Data: 0.020 (2.124)
Train: 141 [ 100/1171 (  9%)]  Loss:  2.612285 (2.6460)  Time: 1.555s,  658.52/s  (2.494s,  410.52/s)  LR: 1.106e-05  Data: 0.987 (1.889)
Train: 141 [ 150/1171 ( 13%)]  Loss:  2.934937 (2.7182)  Time: 0.591s, 1732.27/s  (2.397s,  427.23/s)  LR: 1.106e-05  Data: 0.025 (1.794)
Train: 141 [ 200/1171 ( 17%)]  Loss:  3.053862 (2.7854)  Time: 5.526s,  185.32/s  (2.355s,  434.84/s)  LR: 1.106e-05  Data: 4.960 (1.755)
Train: 141 [ 250/1171 ( 21%)]  Loss:  2.951710 (2.8131)  Time: 0.587s, 1745.31/s  (2.300s,  445.28/s)  LR: 1.106e-05  Data: 0.021 (1.699)
Train: 141 [ 300/1171 ( 26%)]  Loss:  2.851156 (2.8185)  Time: 3.355s,  305.17/s  (2.263s,  452.49/s)  LR: 1.106e-05  Data: 2.780 (1.663)
Train: 141 [ 350/1171 ( 30%)]  Loss:  3.079702 (2.8512)  Time: 0.589s, 1739.74/s  (2.225s,  460.32/s)  LR: 1.106e-05  Data: 0.021 (1.624)
Train: 141 [ 400/1171 ( 34%)]  Loss:  2.601665 (2.8234)  Time: 6.280s,  163.06/s  (2.271s,  450.87/s)  LR: 1.106e-05  Data: 5.698 (1.672)
Train: 141 [ 450/1171 ( 38%)]  Loss:  2.752781 (2.8164)  Time: 0.588s, 1741.68/s  (2.239s,  457.41/s)  LR: 1.106e-05  Data: 0.021 (1.641)
Train: 141 [ 500/1171 ( 43%)]  Loss:  2.437234 (2.7819)  Time: 5.851s,  175.00/s  (2.261s,  452.85/s)  LR: 1.106e-05  Data: 5.165 (1.663)
Train: 141 [ 550/1171 ( 47%)]  Loss:  3.154777 (2.8130)  Time: 0.587s, 1743.27/s  (2.255s,  454.10/s)  LR: 1.106e-05  Data: 0.022 (1.657)
Train: 141 [ 600/1171 ( 51%)]  Loss:  2.968151 (2.8249)  Time: 5.057s,  202.50/s  (2.267s,  451.70/s)  LR: 1.106e-05  Data: 4.470 (1.668)
Train: 141 [ 650/1171 ( 56%)]  Loss:  3.071989 (2.8426)  Time: 0.587s, 1744.46/s  (2.256s,  453.98/s)  LR: 1.106e-05  Data: 0.022 (1.657)
Train: 141 [ 700/1171 ( 60%)]  Loss:  2.663268 (2.8306)  Time: 3.088s,  331.58/s  (2.252s,  454.81/s)  LR: 1.106e-05  Data: 2.493 (1.653)
Train: 141 [ 750/1171 ( 64%)]  Loss:  2.293795 (2.7971)  Time: 0.586s, 1747.85/s  (2.237s,  457.80/s)  LR: 1.106e-05  Data: 0.021 (1.638)
Train: 141 [ 800/1171 ( 68%)]  Loss:  3.313063 (2.8274)  Time: 2.984s,  343.19/s  (2.258s,  453.51/s)  LR: 1.106e-05  Data: 2.397 (1.660)
Train: 141 [ 850/1171 ( 73%)]  Loss:  2.846893 (2.8285)  Time: 0.587s, 1743.89/s  (2.242s,  456.70/s)  LR: 1.106e-05  Data: 0.024 (1.643)
Train: 141 [ 900/1171 ( 77%)]  Loss:  2.999928 (2.8375)  Time: 1.301s,  787.11/s  (2.251s,  455.00/s)  LR: 1.106e-05  Data: 0.646 (1.652)
Train: 141 [ 950/1171 ( 81%)]  Loss:  3.154904 (2.8534)  Time: 0.587s, 1744.19/s  (2.246s,  455.98/s)  LR: 1.106e-05  Data: 0.022 (1.646)
Train: 141 [1000/1171 ( 85%)]  Loss:  3.139199 (2.8670)  Time: 0.630s, 1626.62/s  (2.261s,  452.94/s)  LR: 1.106e-05  Data: 0.035 (1.661)
Train: 141 [1050/1171 ( 90%)]  Loss:  2.594675 (2.8546)  Time: 0.588s, 1741.44/s  (2.248s,  455.47/s)  LR: 1.106e-05  Data: 0.025 (1.649)
Train: 141 [1100/1171 ( 94%)]  Loss:  3.108093 (2.8656)  Time: 0.587s, 1745.51/s  (2.246s,  455.87/s)  LR: 1.106e-05  Data: 0.019 (1.648)
Train: 141 [1150/1171 ( 98%)]  Loss:  3.077939 (2.8745)  Time: 0.588s, 1741.53/s  (2.238s,  457.55/s)  LR: 1.106e-05  Data: 0.020 (1.640)
Train: 141 [1170/1171 (100%)]  Loss:  3.083004 (2.8828)  Time: 0.565s, 1811.96/s  (2.238s,  457.54/s)  LR: 1.106e-05  Data: 0.000 (1.640)
Test: [   0/97]  Time: 19.291 (19.291)  Loss:  0.2890 (0.2890)  Acc@1: 97.0703 (97.0703)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.202 (3.148)  Loss:  0.4325 (0.3574)  Acc@1: 92.9688 (95.2761)  Acc@5: 98.4375 (98.9334)
Test: [  97/97]  Time: 0.119 (2.976)  Loss:  0.3191 (0.3693)  Acc@1: 95.0893 (94.7600)  Acc@5: 99.4048 (98.8060)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-140.pth.tar', 94.78799998291015)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-141.pth.tar', 94.76000000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-138.pth.tar', 94.7529999951172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-139.pth.tar', 94.74999998291015)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-135.pth.tar', 94.71999998291015)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-136.pth.tar', 94.69500001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-137.pth.tar', 94.68900002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-134.pth.tar', 94.6520000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-132.pth.tar', 94.63500000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-133.pth.tar', 94.62899998291016)

Train: 142 [   0/1171 (  0%)]  Loss:  2.864143 (2.8641)  Time: 9.598s,  106.69/s  (9.598s,  106.69/s)  LR: 1.047e-05  Data: 8.872 (8.872)
Train: 142 [  50/1171 (  4%)]  Loss:  2.744483 (2.8043)  Time: 0.586s, 1746.92/s  (2.142s,  478.12/s)  LR: 1.047e-05  Data: 0.019 (1.548)
Train: 142 [ 100/1171 (  9%)]  Loss:  2.882219 (2.8303)  Time: 2.405s,  425.77/s  (2.099s,  487.79/s)  LR: 1.047e-05  Data: 1.818 (1.502)
Train: 142 [ 150/1171 ( 13%)]  Loss:  2.673070 (2.7910)  Time: 0.587s, 1744.39/s  (2.083s,  491.53/s)  LR: 1.047e-05  Data: 0.023 (1.480)
Train: 142 [ 200/1171 ( 17%)]  Loss:  2.428169 (2.7184)  Time: 2.972s,  344.58/s  (2.078s,  492.73/s)  LR: 1.047e-05  Data: 2.402 (1.472)
Train: 142 [ 250/1171 ( 21%)]  Loss:  3.075116 (2.7779)  Time: 0.584s, 1753.13/s  (2.075s,  493.47/s)  LR: 1.047e-05  Data: 0.019 (1.472)
Train: 142 [ 300/1171 ( 26%)]  Loss:  3.141304 (2.8298)  Time: 1.084s,  944.59/s  (2.131s,  480.60/s)  LR: 1.047e-05  Data: 0.521 (1.528)
Train: 142 [ 350/1171 ( 30%)]  Loss:  3.246429 (2.8819)  Time: 0.584s, 1752.67/s  (2.147s,  477.02/s)  LR: 1.047e-05  Data: 0.018 (1.544)
Train: 142 [ 400/1171 ( 34%)]  Loss:  3.024893 (2.8978)  Time: 0.583s, 1757.37/s  (2.143s,  477.74/s)  LR: 1.047e-05  Data: 0.019 (1.540)
Train: 142 [ 450/1171 ( 38%)]  Loss:  2.940249 (2.9020)  Time: 0.585s, 1750.08/s  (2.157s,  474.64/s)  LR: 1.047e-05  Data: 0.019 (1.555)
Train: 142 [ 500/1171 ( 43%)]  Loss:  3.166136 (2.9260)  Time: 1.735s,  590.14/s  (2.168s,  472.27/s)  LR: 1.047e-05  Data: 1.171 (1.566)
Train: 142 [ 550/1171 ( 47%)]  Loss:  2.477056 (2.8886)  Time: 0.802s, 1276.84/s  (2.179s,  469.94/s)  LR: 1.047e-05  Data: 0.230 (1.577)
Train: 142 [ 600/1171 ( 51%)]  Loss:  2.586645 (2.8654)  Time: 0.584s, 1752.16/s  (2.194s,  466.65/s)  LR: 1.047e-05  Data: 0.021 (1.593)
Train: 142 [ 650/1171 ( 56%)]  Loss:  2.153188 (2.8145)  Time: 0.585s, 1751.39/s  (2.223s,  460.71/s)  LR: 1.047e-05  Data: 0.020 (1.621)
Train: 142 [ 700/1171 ( 60%)]  Loss:  2.921773 (2.8217)  Time: 0.586s, 1748.52/s  (2.256s,  453.93/s)  LR: 1.047e-05  Data: 0.018 (1.655)
Train: 142 [ 750/1171 ( 64%)]  Loss:  3.048207 (2.8358)  Time: 0.588s, 1742.39/s  (2.253s,  454.48/s)  LR: 1.047e-05  Data: 0.021 (1.652)
Train: 142 [ 800/1171 ( 68%)]  Loss:  3.237676 (2.8595)  Time: 0.585s, 1750.65/s  (2.261s,  452.89/s)  LR: 1.047e-05  Data: 0.019 (1.659)
Train: 142 [ 850/1171 ( 73%)]  Loss:  2.340877 (2.8306)  Time: 0.584s, 1752.02/s  (2.266s,  451.98/s)  LR: 1.047e-05  Data: 0.021 (1.664)
Train: 142 [ 900/1171 ( 77%)]  Loss:  2.550969 (2.8159)  Time: 0.584s, 1752.32/s  (2.265s,  452.14/s)  LR: 1.047e-05  Data: 0.018 (1.664)
Train: 142 [ 950/1171 ( 81%)]  Loss:  3.236516 (2.8370)  Time: 0.583s, 1755.17/s  (2.259s,  453.31/s)  LR: 1.047e-05  Data: 0.019 (1.659)
Train: 142 [1000/1171 ( 85%)]  Loss:  3.066394 (2.8479)  Time: 0.582s, 1759.69/s  (2.257s,  453.78/s)  LR: 1.047e-05  Data: 0.018 (1.658)
Train: 142 [1050/1171 ( 90%)]  Loss:  3.349496 (2.8707)  Time: 0.584s, 1752.45/s  (2.250s,  455.11/s)  LR: 1.047e-05  Data: 0.019 (1.651)
Train: 142 [1100/1171 ( 94%)]  Loss:  2.604455 (2.8591)  Time: 0.587s, 1745.66/s  (2.268s,  451.53/s)  LR: 1.047e-05  Data: 0.023 (1.669)
Train: 142 [1150/1171 ( 98%)]  Loss:  3.098809 (2.8691)  Time: 0.588s, 1741.92/s  (2.259s,  453.37/s)  LR: 1.047e-05  Data: 0.024 (1.661)
Train: 142 [1170/1171 (100%)]  Loss:  3.039618 (2.8759)  Time: 0.564s, 1814.62/s  (2.262s,  452.67/s)  LR: 1.047e-05  Data: 0.000 (1.665)
Test: [   0/97]  Time: 13.883 (13.883)  Loss:  0.2867 (0.2867)  Acc@1: 96.8750 (96.8750)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 4.556 (3.109)  Loss:  0.4502 (0.3542)  Acc@1: 92.7734 (95.3106)  Acc@5: 98.3398 (98.9507)
Test: [  97/97]  Time: 0.119 (2.924)  Loss:  0.3211 (0.3644)  Acc@1: 95.0893 (94.8120)  Acc@5: 99.4048 (98.8270)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-142.pth.tar', 94.81200000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-140.pth.tar', 94.78799998291015)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-141.pth.tar', 94.76000000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-138.pth.tar', 94.7529999951172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-139.pth.tar', 94.74999998291015)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-135.pth.tar', 94.71999998291015)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-136.pth.tar', 94.69500001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-137.pth.tar', 94.68900002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-134.pth.tar', 94.6520000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-132.pth.tar', 94.63500000732422)

Train: 143 [   0/1171 (  0%)]  Loss:  3.032797 (3.0328)  Time: 9.813s,  104.35/s  (9.813s,  104.35/s)  LR: 1.012e-05  Data: 8.967 (8.967)
Train: 143 [  50/1171 (  4%)]  Loss:  2.592942 (2.8129)  Time: 0.586s, 1746.78/s  (2.234s,  458.42/s)  LR: 1.012e-05  Data: 0.019 (1.649)
Train: 143 [ 100/1171 (  9%)]  Loss:  2.780011 (2.8019)  Time: 0.587s, 1744.88/s  (2.177s,  470.30/s)  LR: 1.012e-05  Data: 0.022 (1.595)
Train: 143 [ 150/1171 ( 13%)]  Loss:  3.236080 (2.9105)  Time: 0.588s, 1741.44/s  (2.110s,  485.41/s)  LR: 1.012e-05  Data: 0.020 (1.525)
Train: 143 [ 200/1171 ( 17%)]  Loss:  2.715469 (2.8715)  Time: 0.586s, 1747.54/s  (2.246s,  456.02/s)  LR: 1.012e-05  Data: 0.020 (1.657)
Train: 143 [ 250/1171 ( 21%)]  Loss:  2.807896 (2.8609)  Time: 0.594s, 1723.88/s  (2.207s,  463.90/s)  LR: 1.012e-05  Data: 0.019 (1.620)
Train: 143 [ 300/1171 ( 26%)]  Loss:  2.634101 (2.8285)  Time: 0.584s, 1752.02/s  (2.306s,  444.12/s)  LR: 1.012e-05  Data: 0.020 (1.717)
Train: 143 [ 350/1171 ( 30%)]  Loss:  2.369330 (2.7711)  Time: 0.587s, 1744.32/s  (2.265s,  452.10/s)  LR: 1.012e-05  Data: 0.019 (1.677)
Train: 143 [ 400/1171 ( 34%)]  Loss:  2.747359 (2.7684)  Time: 0.586s, 1746.07/s  (2.267s,  451.77/s)  LR: 1.012e-05  Data: 0.020 (1.679)
Train: 143 [ 450/1171 ( 38%)]  Loss:  2.997367 (2.7913)  Time: 0.586s, 1747.14/s  (2.247s,  455.80/s)  LR: 1.012e-05  Data: 0.019 (1.660)
Train: 143 [ 500/1171 ( 43%)]  Loss:  3.010629 (2.8113)  Time: 0.584s, 1752.66/s  (2.254s,  454.28/s)  LR: 1.012e-05  Data: 0.019 (1.668)
Train: 143 [ 550/1171 ( 47%)]  Loss:  2.571835 (2.7913)  Time: 0.586s, 1746.62/s  (2.247s,  455.69/s)  LR: 1.012e-05  Data: 0.022 (1.661)
Train: 143 [ 600/1171 ( 51%)]  Loss:  3.060292 (2.8120)  Time: 0.585s, 1751.47/s  (2.294s,  446.44/s)  LR: 1.012e-05  Data: 0.021 (1.705)
Train: 143 [ 650/1171 ( 56%)]  Loss:  2.969887 (2.8233)  Time: 0.585s, 1751.20/s  (2.276s,  449.90/s)  LR: 1.012e-05  Data: 0.021 (1.689)
Train: 143 [ 700/1171 ( 60%)]  Loss:  2.853115 (2.8253)  Time: 0.590s, 1735.00/s  (2.289s,  447.30/s)  LR: 1.012e-05  Data: 0.023 (1.702)
Train: 143 [ 750/1171 ( 64%)]  Loss:  2.884379 (2.8290)  Time: 0.589s, 1739.88/s  (2.276s,  449.89/s)  LR: 1.012e-05  Data: 0.021 (1.689)
Train: 143 [ 800/1171 ( 68%)]  Loss:  3.314145 (2.8575)  Time: 0.586s, 1748.63/s  (2.284s,  448.28/s)  LR: 1.012e-05  Data: 0.022 (1.697)
Train: 143 [ 850/1171 ( 73%)]  Loss:  2.818402 (2.8553)  Time: 0.586s, 1747.69/s  (2.277s,  449.73/s)  LR: 1.012e-05  Data: 0.021 (1.689)
Train: 143 [ 900/1171 ( 77%)]  Loss:  2.822482 (2.8536)  Time: 0.588s, 1740.22/s  (2.272s,  450.72/s)  LR: 1.012e-05  Data: 0.024 (1.684)
Train: 143 [ 950/1171 ( 81%)]  Loss:  2.664941 (2.8442)  Time: 0.590s, 1736.79/s  (2.264s,  452.28/s)  LR: 1.012e-05  Data: 0.024 (1.676)
Train: 143 [1000/1171 ( 85%)]  Loss:  2.998134 (2.8515)  Time: 0.586s, 1746.29/s  (2.284s,  448.25/s)  LR: 1.012e-05  Data: 0.023 (1.696)
Train: 143 [1050/1171 ( 90%)]  Loss:  3.003997 (2.8584)  Time: 0.588s, 1740.49/s  (2.276s,  449.86/s)  LR: 1.012e-05  Data: 0.022 (1.688)
Train: 143 [1100/1171 ( 94%)]  Loss:  2.780589 (2.8551)  Time: 0.588s, 1742.97/s  (2.282s,  448.73/s)  LR: 1.012e-05  Data: 0.023 (1.693)
Train: 143 [1150/1171 ( 98%)]  Loss:  2.650724 (2.8465)  Time: 0.588s, 1742.82/s  (2.273s,  450.50/s)  LR: 1.012e-05  Data: 0.021 (1.684)
Train: 143 [1170/1171 (100%)]  Loss:  3.146349 (2.8585)  Time: 0.566s, 1809.04/s  (2.273s,  450.60/s)  LR: 1.012e-05  Data: 0.000 (1.683)
Test: [   0/97]  Time: 12.309 (12.309)  Loss:  0.2877 (0.2877)  Acc@1: 96.5820 (96.5820)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.202 (3.246)  Loss:  0.4395 (0.3549)  Acc@1: 92.9688 (95.3202)  Acc@5: 98.3398 (98.9354)
Test: [  97/97]  Time: 0.120 (3.050)  Loss:  0.3169 (0.3654)  Acc@1: 94.7917 (94.8040)  Acc@5: 99.4048 (98.8170)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-142.pth.tar', 94.81200000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-143.pth.tar', 94.8040000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-140.pth.tar', 94.78799998291015)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-141.pth.tar', 94.76000000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-138.pth.tar', 94.7529999951172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-139.pth.tar', 94.74999998291015)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-135.pth.tar', 94.71999998291015)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-136.pth.tar', 94.69500001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-137.pth.tar', 94.68900002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-134.pth.tar', 94.6520000341797)

*** Best metric: 94.81200000732422 (epoch 142)

wandb: Waiting for W&B process to finish, PID 29781
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210601_223738-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1/logs/debug.log
wandb: Find internal logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210601_223738-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1/logs/debug-internal.log
wandb: Run summary:
wandb:   train_loss 2.85853
wandb:        _step 143
wandb:        epoch 143
wandb:     _runtime 462124
wandb:    eval_loss 0.36536
wandb:    eval_top1 94.804
wandb:    eval_top5 98.817
wandb:   _timestamp 1622621602
wandb: Run history:
wandb:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   train_loss ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñá‚ñà‚ñÜ‚ñá‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñÅ‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÖ
wandb:    eval_loss ‚ñÉ‚ñÉ‚ñà‚ñÇ‚ñÖ‚ñÉ‚ñá‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:    eval_top1 ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà
wandb:    eval_top5 ‚ñÉ‚ñÅ‚ñÅ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá
wandb:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced PreTraining_vit_deit_tiny_patch16_224_fake_1k: https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
--End--
Wed Jun 2 17:13:33 JST 2021

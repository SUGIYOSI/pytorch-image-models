--Start--
Sat May 29 12:07:32 JST 2021
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 1
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
wandb: Currently logged in as: gyama_x (use `wandb login --relogin` to force relogin)
wandb: WARNING Tried to auto resume run with id PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k but id PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1 is set.
wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
wandb: Tracking run with wandb version 0.10.27
wandb: Resuming run PreTraining_vit_deit_tiny_patch16_224_fake_1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gyama_x/pytorch-image-models
wandb: üöÄ View run at https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run data is saved locally in /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210529_120814-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run `wandb offline` to turn off syncing.

Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar' (epoch 87)
Using native Torch DistributedDataParallel.
Scheduled epochs: 100
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Train: 88 [   0/1171 (  0%)]  Loss:  2.937215 (2.9372)  Time: 15.298s,   66.93/s  (15.298s,   66.93/s)  LR: 4.476e-05  Data: 14.069 (14.069)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 88 [  50/1171 (  4%)]  Loss:  3.084869 (3.0110)  Time: 0.581s, 1761.62/s  (2.334s,  438.68/s)  LR: 4.476e-05  Data: 0.017 (1.743)
Train: 88 [ 100/1171 (  9%)]  Loss:  3.086463 (3.0362)  Time: 0.585s, 1750.66/s  (2.230s,  459.14/s)  LR: 4.476e-05  Data: 0.021 (1.646)
Train: 88 [ 150/1171 ( 13%)]  Loss:  2.696071 (2.9512)  Time: 0.585s, 1750.07/s  (2.112s,  484.78/s)  LR: 4.476e-05  Data: 0.019 (1.528)
Train: 88 [ 200/1171 ( 17%)]  Loss:  2.694495 (2.8998)  Time: 0.587s, 1743.15/s  (2.176s,  470.48/s)  LR: 4.476e-05  Data: 0.015 (1.584)
Train: 88 [ 250/1171 ( 21%)]  Loss:  2.412186 (2.8185)  Time: 1.869s,  547.76/s  (2.172s,  471.50/s)  LR: 4.476e-05  Data: 1.190 (1.574)
Train: 88 [ 300/1171 ( 26%)]  Loss:  3.065793 (2.8539)  Time: 0.586s, 1747.73/s  (2.180s,  469.74/s)  LR: 4.476e-05  Data: 0.020 (1.581)
Train: 88 [ 350/1171 ( 30%)]  Loss:  2.615464 (2.8241)  Time: 0.583s, 1757.69/s  (2.164s,  473.10/s)  LR: 4.476e-05  Data: 0.019 (1.569)
Train: 88 [ 400/1171 ( 34%)]  Loss:  3.098988 (2.8546)  Time: 0.586s, 1747.03/s  (2.165s,  473.05/s)  LR: 4.476e-05  Data: 0.023 (1.571)
Train: 88 [ 450/1171 ( 38%)]  Loss:  2.604167 (2.8296)  Time: 0.581s, 1763.41/s  (2.139s,  478.70/s)  LR: 4.476e-05  Data: 0.017 (1.545)
Train: 88 [ 500/1171 ( 43%)]  Loss:  2.599390 (2.8086)  Time: 0.585s, 1750.31/s  (2.128s,  481.10/s)  LR: 4.476e-05  Data: 0.022 (1.535)
Train: 88 [ 550/1171 ( 47%)]  Loss:  2.881354 (2.8147)  Time: 0.583s, 1757.66/s  (2.111s,  485.17/s)  LR: 4.476e-05  Data: 0.018 (1.518)
Train: 88 [ 600/1171 ( 51%)]  Loss:  3.416803 (2.8610)  Time: 0.585s, 1750.56/s  (2.124s,  482.16/s)  LR: 4.476e-05  Data: 0.019 (1.531)
Train: 88 [ 650/1171 ( 56%)]  Loss:  2.816036 (2.8578)  Time: 0.585s, 1750.92/s  (2.116s,  483.98/s)  LR: 4.476e-05  Data: 0.022 (1.524)
Train: 88 [ 700/1171 ( 60%)]  Loss:  3.062152 (2.8714)  Time: 3.433s,  298.27/s  (2.133s,  480.18/s)  LR: 4.476e-05  Data: 2.859 (1.541)
Train: 88 [ 750/1171 ( 64%)]  Loss:  3.341294 (2.9008)  Time: 0.585s, 1750.07/s  (2.144s,  477.64/s)  LR: 4.476e-05  Data: 0.018 (1.551)
Train: 88 [ 800/1171 ( 68%)]  Loss:  2.819327 (2.8960)  Time: 0.585s, 1750.01/s  (2.158s,  474.41/s)  LR: 4.476e-05  Data: 0.023 (1.565)
Train: 88 [ 850/1171 ( 73%)]  Loss:  2.907065 (2.8966)  Time: 0.582s, 1759.23/s  (2.164s,  473.30/s)  LR: 4.476e-05  Data: 0.016 (1.572)
Train: 88 [ 900/1171 ( 77%)]  Loss:  3.152944 (2.9101)  Time: 0.589s, 1739.27/s  (2.169s,  472.06/s)  LR: 4.476e-05  Data: 0.020 (1.578)
Train: 88 [ 950/1171 ( 81%)]  Loss:  2.582233 (2.8937)  Time: 0.585s, 1751.61/s  (2.164s,  473.25/s)  LR: 4.476e-05  Data: 0.019 (1.573)
Train: 88 [1000/1171 ( 85%)]  Loss:  2.455047 (2.8728)  Time: 0.585s, 1750.54/s  (2.180s,  469.76/s)  LR: 4.476e-05  Data: 0.021 (1.589)
Train: 88 [1050/1171 ( 90%)]  Loss:  2.868670 (2.8726)  Time: 0.584s, 1753.19/s  (2.184s,  468.77/s)  LR: 4.476e-05  Data: 0.018 (1.593)
Train: 88 [1100/1171 ( 94%)]  Loss:  2.927516 (2.8750)  Time: 0.905s, 1131.60/s  (2.190s,  467.50/s)  LR: 4.476e-05  Data: 0.021 (1.600)
Train: 88 [1150/1171 ( 98%)]  Loss:  2.525619 (2.8605)  Time: 0.584s, 1754.73/s  (2.190s,  467.56/s)  LR: 4.476e-05  Data: 0.019 (1.599)
Train: 88 [1170/1171 (100%)]  Loss:  2.924391 (2.8630)  Time: 0.563s, 1818.85/s  (2.191s,  467.28/s)  LR: 4.476e-05  Data: 0.000 (1.601)
Test: [   0/97]  Time: 13.840 (13.840)  Loss:  0.2906 (0.2906)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.196 (2.893)  Loss:  0.4785 (0.3713)  Acc@1: 91.0156 (94.7036)  Acc@5: 98.3398 (98.8760)
Test: [  97/97]  Time: 0.431 (2.811)  Loss:  0.3098 (0.3866)  Acc@1: 95.5357 (94.1990)  Acc@5: 98.9583 (98.6750)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-88.pth.tar', 94.19900004394532)

Train: 89 [   0/1171 (  0%)]  Loss:  2.922747 (2.9227)  Time: 9.423s,  108.67/s  (9.423s,  108.67/s)  LR: 3.926e-05  Data: 8.159 (8.159)
Train: 89 [  50/1171 (  4%)]  Loss:  3.221162 (3.0720)  Time: 0.586s, 1746.88/s  (2.066s,  495.76/s)  LR: 3.926e-05  Data: 0.022 (1.472)
Train: 89 [ 100/1171 (  9%)]  Loss:  3.024206 (3.0560)  Time: 1.602s,  639.26/s  (2.139s,  478.65/s)  LR: 3.926e-05  Data: 1.033 (1.546)
Train: 89 [ 150/1171 ( 13%)]  Loss:  2.844038 (3.0030)  Time: 0.585s, 1750.12/s  (2.162s,  473.53/s)  LR: 3.926e-05  Data: 0.019 (1.567)
Train: 89 [ 200/1171 ( 17%)]  Loss:  2.804940 (2.9634)  Time: 0.587s, 1745.64/s  (2.217s,  461.81/s)  LR: 3.926e-05  Data: 0.025 (1.620)
Train: 89 [ 250/1171 ( 21%)]  Loss:  2.690326 (2.9179)  Time: 0.583s, 1755.42/s  (2.214s,  462.54/s)  LR: 3.926e-05  Data: 0.020 (1.619)
Train: 89 [ 300/1171 ( 26%)]  Loss:  2.757903 (2.8950)  Time: 0.587s, 1745.09/s  (2.231s,  458.96/s)  LR: 3.926e-05  Data: 0.022 (1.638)
Train: 89 [ 350/1171 ( 30%)]  Loss:  2.731054 (2.8745)  Time: 0.583s, 1756.62/s  (2.217s,  461.93/s)  LR: 3.926e-05  Data: 0.021 (1.621)
Train: 89 [ 400/1171 ( 34%)]  Loss:  3.298444 (2.9216)  Time: 0.583s, 1757.76/s  (2.208s,  463.83/s)  LR: 3.926e-05  Data: 0.017 (1.612)
Train: 89 [ 450/1171 ( 38%)]  Loss:  2.704587 (2.8999)  Time: 1.030s,  994.29/s  (2.190s,  467.63/s)  LR: 3.926e-05  Data: 0.362 (1.593)
Train: 89 [ 500/1171 ( 43%)]  Loss:  3.002590 (2.9093)  Time: 0.591s, 1733.46/s  (2.185s,  468.68/s)  LR: 3.926e-05  Data: 0.016 (1.589)
Train: 89 [ 550/1171 ( 47%)]  Loss:  2.879516 (2.9068)  Time: 4.017s,  254.91/s  (2.224s,  460.39/s)  LR: 3.926e-05  Data: 3.453 (1.629)
Train: 89 [ 600/1171 ( 51%)]  Loss:  3.119821 (2.9232)  Time: 0.586s, 1746.91/s  (2.253s,  454.51/s)  LR: 3.926e-05  Data: 0.017 (1.659)
Train: 89 [ 650/1171 ( 56%)]  Loss:  3.069981 (2.9337)  Time: 0.583s, 1756.75/s  (2.256s,  453.84/s)  LR: 3.926e-05  Data: 0.020 (1.663)
Train: 89 [ 700/1171 ( 60%)]  Loss:  2.876306 (2.9298)  Time: 0.585s, 1750.67/s  (2.267s,  451.62/s)  LR: 3.926e-05  Data: 0.017 (1.675)
Train: 89 [ 750/1171 ( 64%)]  Loss:  2.786856 (2.9209)  Time: 0.582s, 1758.17/s  (2.256s,  453.90/s)  LR: 3.926e-05  Data: 0.020 (1.665)
Train: 89 [ 800/1171 ( 68%)]  Loss:  2.702743 (2.9081)  Time: 0.586s, 1748.23/s  (2.254s,  454.33/s)  LR: 3.926e-05  Data: 0.019 (1.662)
Train: 89 [ 850/1171 ( 73%)]  Loss:  2.995924 (2.9130)  Time: 0.582s, 1759.33/s  (2.242s,  456.69/s)  LR: 3.926e-05  Data: 0.020 (1.652)
Train: 89 [ 900/1171 ( 77%)]  Loss:  3.004011 (2.9177)  Time: 0.584s, 1753.26/s  (2.250s,  455.06/s)  LR: 3.926e-05  Data: 0.019 (1.660)
Train: 89 [ 950/1171 ( 81%)]  Loss:  3.070189 (2.9254)  Time: 0.602s, 1699.69/s  (2.253s,  454.57/s)  LR: 3.926e-05  Data: 0.022 (1.663)
Train: 89 [1000/1171 ( 85%)]  Loss:  3.295172 (2.9430)  Time: 0.584s, 1754.11/s  (2.260s,  453.02/s)  LR: 3.926e-05  Data: 0.017 (1.671)
Train: 89 [1050/1171 ( 90%)]  Loss:  3.251517 (2.9570)  Time: 0.584s, 1754.71/s  (2.259s,  453.36/s)  LR: 3.926e-05  Data: 0.022 (1.670)
Train: 89 [1100/1171 ( 94%)]  Loss:  2.984021 (2.9582)  Time: 0.582s, 1758.27/s  (2.263s,  452.50/s)  LR: 3.926e-05  Data: 0.018 (1.674)
Train: 89 [1150/1171 ( 98%)]  Loss:  2.818306 (2.9523)  Time: 0.583s, 1755.50/s  (2.258s,  453.54/s)  LR: 3.926e-05  Data: 0.022 (1.669)
Train: 89 [1170/1171 (100%)]  Loss:  2.843046 (2.9480)  Time: 0.564s, 1814.81/s  (2.257s,  453.75/s)  LR: 3.926e-05  Data: 0.000 (1.668)
Test: [   0/97]  Time: 12.383 (12.383)  Loss:  0.2978 (0.2978)  Acc@1: 96.5820 (96.5820)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.197 (2.804)  Loss:  0.4735 (0.3671)  Acc@1: 91.3086 (94.7649)  Acc@5: 98.2422 (98.8549)
Test: [  97/97]  Time: 0.120 (2.827)  Loss:  0.3350 (0.3807)  Acc@1: 94.3452 (94.1890)  Acc@5: 99.1071 (98.6630)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-88.pth.tar', 94.19900004394532)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-89.pth.tar', 94.1889999975586)

Train: 90 [   0/1171 (  0%)]  Loss:  2.839758 (2.8398)  Time: 13.526s,   75.71/s  (13.526s,   75.71/s)  LR: 3.423e-05  Data: 12.808 (12.808)
Train: 90 [  50/1171 (  4%)]  Loss:  2.698754 (2.7693)  Time: 0.586s, 1748.38/s  (2.421s,  422.93/s)  LR: 3.423e-05  Data: 0.022 (1.838)
Train: 90 [ 100/1171 (  9%)]  Loss:  3.403871 (2.9808)  Time: 2.196s,  466.40/s  (2.392s,  428.05/s)  LR: 3.423e-05  Data: 1.547 (1.805)
Train: 90 [ 150/1171 ( 13%)]  Loss:  2.377305 (2.8299)  Time: 0.588s, 1742.33/s  (2.306s,  444.04/s)  LR: 3.423e-05  Data: 0.022 (1.717)
Train: 90 [ 200/1171 ( 17%)]  Loss:  2.724870 (2.8089)  Time: 0.939s, 1090.33/s  (2.276s,  449.87/s)  LR: 3.423e-05  Data: 0.263 (1.687)
Train: 90 [ 250/1171 ( 21%)]  Loss:  2.771010 (2.8026)  Time: 0.586s, 1748.81/s  (2.232s,  458.83/s)  LR: 3.423e-05  Data: 0.024 (1.642)
Train: 90 [ 300/1171 ( 26%)]  Loss:  2.503081 (2.7598)  Time: 2.238s,  457.62/s  (2.209s,  463.64/s)  LR: 3.423e-05  Data: 1.671 (1.618)
Train: 90 [ 350/1171 ( 30%)]  Loss:  2.791073 (2.7637)  Time: 0.587s, 1744.83/s  (2.175s,  470.82/s)  LR: 3.423e-05  Data: 0.024 (1.582)
Train: 90 [ 400/1171 ( 34%)]  Loss:  3.029509 (2.7932)  Time: 1.301s,  787.02/s  (2.154s,  475.33/s)  LR: 3.423e-05  Data: 0.740 (1.560)
Train: 90 [ 450/1171 ( 38%)]  Loss:  2.836041 (2.7975)  Time: 0.585s, 1750.47/s  (2.179s,  469.86/s)  LR: 3.423e-05  Data: 0.021 (1.584)
Train: 90 [ 500/1171 ( 43%)]  Loss:  2.966270 (2.8129)  Time: 4.043s,  253.26/s  (2.206s,  464.11/s)  LR: 3.423e-05  Data: 3.379 (1.610)
Train: 90 [ 550/1171 ( 47%)]  Loss:  2.839364 (2.8151)  Time: 0.585s, 1751.15/s  (2.211s,  463.05/s)  LR: 3.423e-05  Data: 0.018 (1.615)
Train: 90 [ 600/1171 ( 51%)]  Loss:  2.719608 (2.8077)  Time: 3.734s,  274.24/s  (2.228s,  459.67/s)  LR: 3.423e-05  Data: 3.068 (1.630)
Train: 90 [ 650/1171 ( 56%)]  Loss:  2.978743 (2.8199)  Time: 0.764s, 1339.51/s  (2.223s,  460.55/s)  LR: 3.423e-05  Data: 0.117 (1.626)
Train: 90 [ 700/1171 ( 60%)]  Loss:  3.035857 (2.8343)  Time: 1.981s,  516.90/s  (2.222s,  460.88/s)  LR: 3.423e-05  Data: 1.303 (1.622)
Train: 90 [ 750/1171 ( 64%)]  Loss:  2.992955 (2.8443)  Time: 0.588s, 1741.41/s  (2.212s,  462.84/s)  LR: 3.423e-05  Data: 0.020 (1.612)
Train: 90 [ 800/1171 ( 68%)]  Loss:  2.865399 (2.8455)  Time: 2.727s,  375.46/s  (2.207s,  464.06/s)  LR: 3.423e-05  Data: 2.048 (1.607)
Train: 90 [ 850/1171 ( 73%)]  Loss:  2.323065 (2.8165)  Time: 1.203s,  850.90/s  (2.222s,  460.83/s)  LR: 3.423e-05  Data: 0.568 (1.621)
Train: 90 [ 900/1171 ( 77%)]  Loss:  2.722305 (2.8115)  Time: 2.095s,  488.76/s  (2.232s,  458.83/s)  LR: 3.423e-05  Data: 1.450 (1.631)
Train: 90 [ 950/1171 ( 81%)]  Loss:  3.263452 (2.8341)  Time: 2.511s,  407.82/s  (2.239s,  457.30/s)  LR: 3.423e-05  Data: 1.948 (1.638)
Train: 90 [1000/1171 ( 85%)]  Loss:  2.760790 (2.8306)  Time: 0.586s, 1747.56/s  (2.241s,  456.90/s)  LR: 3.423e-05  Data: 0.021 (1.639)
Train: 90 [1050/1171 ( 90%)]  Loss:  3.018871 (2.8392)  Time: 3.295s,  310.76/s  (2.245s,  456.22/s)  LR: 3.423e-05  Data: 2.640 (1.643)
Train: 90 [1100/1171 ( 94%)]  Loss:  3.307883 (2.8596)  Time: 0.585s, 1749.53/s  (2.239s,  457.28/s)  LR: 3.423e-05  Data: 0.020 (1.638)
Train: 90 [1150/1171 ( 98%)]  Loss:  2.916540 (2.8619)  Time: 6.594s,  155.30/s  (2.239s,  457.27/s)  LR: 3.423e-05  Data: 5.917 (1.639)
Train: 90 [1170/1171 (100%)]  Loss:  3.195710 (2.8753)  Time: 0.563s, 1817.48/s  (2.234s,  458.45/s)  LR: 3.423e-05  Data: 0.000 (1.633)
Test: [   0/97]  Time: 11.916 (11.916)  Loss:  0.3072 (0.3072)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.196 (3.122)  Loss:  0.4898 (0.3856)  Acc@1: 91.7969 (94.8491)  Acc@5: 98.1445 (98.8568)
Test: [  97/97]  Time: 0.119 (3.091)  Loss:  0.3554 (0.3981)  Acc@1: 94.3452 (94.3070)  Acc@5: 98.9583 (98.6700)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-90.pth.tar', 94.30700004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-88.pth.tar', 94.19900004394532)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-89.pth.tar', 94.1889999975586)

Train: 91 [   0/1171 (  0%)]  Loss:  3.101218 (3.1012)  Time: 10.530s,   97.25/s  (10.530s,   97.25/s)  LR: 2.965e-05  Data: 9.390 (9.390)
Train: 91 [  50/1171 (  4%)]  Loss:  3.080999 (3.0911)  Time: 0.585s, 1750.37/s  (2.398s,  427.11/s)  LR: 2.965e-05  Data: 0.020 (1.805)
Train: 91 [ 100/1171 (  9%)]  Loss:  2.815852 (2.9994)  Time: 0.587s, 1745.89/s  (2.330s,  439.46/s)  LR: 2.965e-05  Data: 0.018 (1.740)
Train: 91 [ 150/1171 ( 13%)]  Loss:  2.923915 (2.9805)  Time: 0.589s, 1739.53/s  (2.234s,  458.40/s)  LR: 2.965e-05  Data: 0.024 (1.643)
Train: 91 [ 200/1171 ( 17%)]  Loss:  2.532800 (2.8910)  Time: 0.583s, 1757.38/s  (2.200s,  465.41/s)  LR: 2.965e-05  Data: 0.020 (1.610)
Train: 91 [ 250/1171 ( 21%)]  Loss:  2.853203 (2.8847)  Time: 2.953s,  346.77/s  (2.166s,  472.74/s)  LR: 2.965e-05  Data: 2.285 (1.571)
Train: 91 [ 300/1171 ( 26%)]  Loss:  3.195173 (2.9290)  Time: 2.093s,  489.14/s  (2.140s,  478.40/s)  LR: 2.965e-05  Data: 1.431 (1.540)
Train: 91 [ 350/1171 ( 30%)]  Loss:  3.325451 (2.9786)  Time: 1.720s,  595.48/s  (2.172s,  471.40/s)  LR: 2.965e-05  Data: 1.157 (1.569)
Train: 91 [ 400/1171 ( 34%)]  Loss:  2.836831 (2.9628)  Time: 2.065s,  495.82/s  (2.194s,  466.64/s)  LR: 2.965e-05  Data: 1.440 (1.591)
Train: 91 [ 450/1171 ( 38%)]  Loss:  3.109920 (2.9775)  Time: 1.192s,  859.20/s  (2.193s,  466.86/s)  LR: 2.965e-05  Data: 0.630 (1.589)
Train: 91 [ 500/1171 ( 43%)]  Loss:  2.780874 (2.9597)  Time: 1.719s,  595.63/s  (2.201s,  465.21/s)  LR: 2.965e-05  Data: 1.153 (1.597)
Train: 91 [ 550/1171 ( 47%)]  Loss:  2.612378 (2.9307)  Time: 0.641s, 1598.17/s  (2.203s,  464.90/s)  LR: 2.965e-05  Data: 0.078 (1.597)
Train: 91 [ 600/1171 ( 51%)]  Loss:  2.447430 (2.8935)  Time: 0.585s, 1751.39/s  (2.206s,  464.08/s)  LR: 2.965e-05  Data: 0.022 (1.601)
Train: 91 [ 650/1171 ( 56%)]  Loss:  3.194503 (2.9150)  Time: 1.420s,  720.98/s  (2.199s,  465.68/s)  LR: 2.965e-05  Data: 0.856 (1.594)
Train: 91 [ 700/1171 ( 60%)]  Loss:  2.746721 (2.9038)  Time: 0.935s, 1094.94/s  (2.197s,  466.18/s)  LR: 2.965e-05  Data: 0.274 (1.593)
Train: 91 [ 750/1171 ( 64%)]  Loss:  3.023800 (2.9113)  Time: 0.585s, 1749.55/s  (2.214s,  462.54/s)  LR: 2.965e-05  Data: 0.020 (1.611)
Train: 91 [ 800/1171 ( 68%)]  Loss:  3.278367 (2.9329)  Time: 0.585s, 1751.20/s  (2.226s,  460.12/s)  LR: 2.965e-05  Data: 0.020 (1.623)
Train: 91 [ 850/1171 ( 73%)]  Loss:  3.192591 (2.9473)  Time: 0.585s, 1751.34/s  (2.230s,  459.25/s)  LR: 2.965e-05  Data: 0.021 (1.629)
Train: 91 [ 900/1171 ( 77%)]  Loss:  2.924179 (2.9461)  Time: 0.585s, 1750.18/s  (2.241s,  456.87/s)  LR: 2.965e-05  Data: 0.020 (1.641)
Train: 91 [ 950/1171 ( 81%)]  Loss:  2.769058 (2.9373)  Time: 1.499s,  683.09/s  (2.238s,  457.55/s)  LR: 2.965e-05  Data: 0.903 (1.639)
Train: 91 [1000/1171 ( 85%)]  Loss:  2.812257 (2.9313)  Time: 0.582s, 1759.04/s  (2.238s,  457.49/s)  LR: 2.965e-05  Data: 0.020 (1.638)
Train: 91 [1050/1171 ( 90%)]  Loss:  3.020766 (2.9354)  Time: 0.585s, 1750.70/s  (2.231s,  458.99/s)  LR: 2.965e-05  Data: 0.022 (1.631)
Train: 91 [1100/1171 ( 94%)]  Loss:  2.952096 (2.9361)  Time: 0.596s, 1718.67/s  (2.227s,  459.74/s)  LR: 2.965e-05  Data: 0.018 (1.628)
Train: 91 [1150/1171 ( 98%)]  Loss:  3.125908 (2.9440)  Time: 0.590s, 1734.59/s  (2.236s,  458.01/s)  LR: 2.965e-05  Data: 0.023 (1.636)
Train: 91 [1170/1171 (100%)]  Loss:  3.069488 (2.9490)  Time: 0.565s, 1811.23/s  (2.239s,  457.34/s)  LR: 2.965e-05  Data: 0.000 (1.640)
Test: [   0/97]  Time: 12.696 (12.696)  Loss:  0.2859 (0.2859)  Acc@1: 96.9727 (96.9727)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.197 (2.983)  Loss:  0.4451 (0.3573)  Acc@1: 92.0898 (94.8893)  Acc@5: 98.2422 (98.8971)
Test: [  97/97]  Time: 0.120 (2.926)  Loss:  0.3191 (0.3707)  Acc@1: 94.4940 (94.3290)  Acc@5: 99.1071 (98.7060)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-91.pth.tar', 94.32900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-90.pth.tar', 94.30700004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-88.pth.tar', 94.19900004394532)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-89.pth.tar', 94.1889999975586)

Train: 92 [   0/1171 (  0%)]  Loss:  2.880884 (2.8809)  Time: 10.319s,   99.23/s  (10.319s,   99.23/s)  LR: 2.555e-05  Data: 9.546 (9.546)
Train: 92 [  50/1171 (  4%)]  Loss:  2.736421 (2.8087)  Time: 0.584s, 1753.04/s  (2.181s,  469.55/s)  LR: 2.555e-05  Data: 0.019 (1.584)
Train: 92 [ 100/1171 (  9%)]  Loss:  2.677987 (2.7651)  Time: 0.584s, 1754.87/s  (2.143s,  477.76/s)  LR: 2.555e-05  Data: 0.020 (1.549)
Train: 92 [ 150/1171 ( 13%)]  Loss:  2.712922 (2.7521)  Time: 0.584s, 1754.55/s  (2.083s,  491.63/s)  LR: 2.555e-05  Data: 0.019 (1.494)
Train: 92 [ 200/1171 ( 17%)]  Loss:  3.334339 (2.8685)  Time: 0.590s, 1736.24/s  (2.073s,  493.96/s)  LR: 2.555e-05  Data: 0.021 (1.486)
Train: 92 [ 250/1171 ( 21%)]  Loss:  2.820016 (2.8604)  Time: 0.583s, 1756.83/s  (2.116s,  483.83/s)  LR: 2.555e-05  Data: 0.019 (1.528)
Train: 92 [ 300/1171 ( 26%)]  Loss:  3.126958 (2.8985)  Time: 2.525s,  405.48/s  (2.151s,  476.04/s)  LR: 2.555e-05  Data: 1.851 (1.561)
Train: 92 [ 350/1171 ( 30%)]  Loss:  2.598541 (2.8610)  Time: 0.588s, 1742.75/s  (2.151s,  476.00/s)  LR: 2.555e-05  Data: 0.021 (1.559)
Train: 92 [ 400/1171 ( 34%)]  Loss:  3.221610 (2.9011)  Time: 2.528s,  405.02/s  (2.162s,  473.57/s)  LR: 2.555e-05  Data: 1.882 (1.567)
Train: 92 [ 450/1171 ( 38%)]  Loss:  2.807389 (2.8917)  Time: 0.582s, 1759.10/s  (2.157s,  474.64/s)  LR: 2.555e-05  Data: 0.020 (1.562)
Train: 92 [ 500/1171 ( 43%)]  Loss:  2.898224 (2.8923)  Time: 3.233s,  316.71/s  (2.169s,  472.07/s)  LR: 2.555e-05  Data: 2.667 (1.573)
Train: 92 [ 550/1171 ( 47%)]  Loss:  2.914502 (2.8941)  Time: 0.583s, 1755.34/s  (2.168s,  472.36/s)  LR: 2.555e-05  Data: 0.019 (1.572)
Train: 92 [ 600/1171 ( 51%)]  Loss:  3.234892 (2.9204)  Time: 1.050s,  975.37/s  (2.179s,  469.87/s)  LR: 2.555e-05  Data: 0.479 (1.583)
Train: 92 [ 650/1171 ( 56%)]  Loss:  3.249485 (2.9439)  Time: 0.584s, 1753.75/s  (2.204s,  464.71/s)  LR: 2.555e-05  Data: 0.019 (1.606)
Train: 92 [ 700/1171 ( 60%)]  Loss:  3.023652 (2.9492)  Time: 2.582s,  396.57/s  (2.224s,  460.33/s)  LR: 2.555e-05  Data: 1.815 (1.627)
Train: 92 [ 750/1171 ( 64%)]  Loss:  2.727381 (2.9353)  Time: 0.590s, 1736.97/s  (2.228s,  459.61/s)  LR: 2.555e-05  Data: 0.021 (1.630)
Train: 92 [ 800/1171 ( 68%)]  Loss:  3.123481 (2.9464)  Time: 1.333s,  767.98/s  (2.233s,  458.49/s)  LR: 2.555e-05  Data: 0.755 (1.635)
Train: 92 [ 850/1171 ( 73%)]  Loss:  2.854722 (2.9413)  Time: 0.588s, 1742.89/s  (2.235s,  458.21/s)  LR: 2.555e-05  Data: 0.022 (1.635)
Train: 92 [ 900/1171 ( 77%)]  Loss:  3.016220 (2.9452)  Time: 0.585s, 1750.49/s  (2.235s,  458.24/s)  LR: 2.555e-05  Data: 0.022 (1.635)
Train: 92 [ 950/1171 ( 81%)]  Loss:  3.301863 (2.9631)  Time: 0.585s, 1750.66/s  (2.228s,  459.57/s)  LR: 2.555e-05  Data: 0.021 (1.628)
Train: 92 [1000/1171 ( 85%)]  Loss:  2.639763 (2.9477)  Time: 0.587s, 1745.64/s  (2.227s,  459.81/s)  LR: 2.555e-05  Data: 0.021 (1.627)
Train: 92 [1050/1171 ( 90%)]  Loss:  2.274674 (2.9171)  Time: 0.585s, 1751.06/s  (2.240s,  457.10/s)  LR: 2.555e-05  Data: 0.020 (1.641)
Train: 92 [1100/1171 ( 94%)]  Loss:  3.158516 (2.9276)  Time: 0.586s, 1746.71/s  (2.251s,  454.86/s)  LR: 2.555e-05  Data: 0.023 (1.652)
Train: 92 [1150/1171 ( 98%)]  Loss:  3.197467 (2.9388)  Time: 2.295s,  446.21/s  (2.254s,  454.32/s)  LR: 2.555e-05  Data: 1.729 (1.655)
Train: 92 [1170/1171 (100%)]  Loss:  2.775445 (2.9323)  Time: 0.563s, 1819.53/s  (2.255s,  454.09/s)  LR: 2.555e-05  Data: 0.000 (1.657)
Test: [   0/97]  Time: 12.955 (12.955)  Loss:  0.3067 (0.3067)  Acc@1: 96.4844 (96.4844)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.198 (3.024)  Loss:  0.4833 (0.3838)  Acc@1: 91.7969 (94.8300)  Acc@5: 98.2422 (98.8683)
Test: [  97/97]  Time: 0.119 (2.900)  Loss:  0.3441 (0.3959)  Acc@1: 94.7917 (94.2920)  Acc@5: 98.9583 (98.6880)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-91.pth.tar', 94.32900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-90.pth.tar', 94.30700004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-92.pth.tar', 94.2920000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-88.pth.tar', 94.19900004394532)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-89.pth.tar', 94.1889999975586)

Train: 93 [   0/1171 (  0%)]  Loss:  2.674566 (2.6746)  Time: 9.914s,  103.29/s  (9.914s,  103.29/s)  LR: 2.192e-05  Data: 9.026 (9.026)
Train: 93 [  50/1171 (  4%)]  Loss:  2.546589 (2.6106)  Time: 0.583s, 1757.84/s  (2.215s,  462.23/s)  LR: 2.192e-05  Data: 0.019 (1.616)
Train: 93 [ 100/1171 (  9%)]  Loss:  2.940748 (2.7206)  Time: 3.215s,  318.55/s  (2.177s,  470.34/s)  LR: 2.192e-05  Data: 2.532 (1.579)
Train: 93 [ 150/1171 ( 13%)]  Loss:  3.021023 (2.7957)  Time: 0.580s, 1765.21/s  (2.266s,  451.92/s)  LR: 2.192e-05  Data: 0.017 (1.672)
Train: 93 [ 200/1171 ( 17%)]  Loss:  2.456519 (2.7279)  Time: 0.587s, 1744.65/s  (2.305s,  444.28/s)  LR: 2.192e-05  Data: 0.020 (1.707)
Train: 93 [ 250/1171 ( 21%)]  Loss:  2.717833 (2.7262)  Time: 0.585s, 1750.96/s  (2.287s,  447.72/s)  LR: 2.192e-05  Data: 0.020 (1.693)
Train: 93 [ 300/1171 ( 26%)]  Loss:  3.359727 (2.8167)  Time: 0.583s, 1757.14/s  (2.295s,  446.28/s)  LR: 2.192e-05  Data: 0.018 (1.702)
Train: 93 [ 350/1171 ( 30%)]  Loss:  3.467006 (2.8980)  Time: 0.583s, 1756.64/s  (2.269s,  451.32/s)  LR: 2.192e-05  Data: 0.017 (1.679)
Train: 93 [ 400/1171 ( 34%)]  Loss:  3.023316 (2.9119)  Time: 0.582s, 1758.27/s  (2.253s,  454.56/s)  LR: 2.192e-05  Data: 0.017 (1.664)
Train: 93 [ 450/1171 ( 38%)]  Loss:  3.177428 (2.9385)  Time: 0.583s, 1757.66/s  (2.222s,  460.80/s)  LR: 2.192e-05  Data: 0.017 (1.634)
Train: 93 [ 500/1171 ( 43%)]  Loss:  2.763629 (2.9226)  Time: 0.582s, 1758.05/s  (2.212s,  462.99/s)  LR: 2.192e-05  Data: 0.017 (1.624)
Train: 93 [ 550/1171 ( 47%)]  Loss:  2.845835 (2.9162)  Time: 0.583s, 1755.29/s  (2.241s,  457.01/s)  LR: 2.192e-05  Data: 0.021 (1.653)
Train: 93 [ 600/1171 ( 51%)]  Loss:  3.171167 (2.9358)  Time: 0.583s, 1757.12/s  (2.273s,  450.55/s)  LR: 2.192e-05  Data: 0.017 (1.685)
Train: 93 [ 650/1171 ( 56%)]  Loss:  3.013353 (2.9413)  Time: 0.585s, 1751.60/s  (2.277s,  449.81/s)  LR: 2.192e-05  Data: 0.019 (1.688)
Train: 93 [ 700/1171 ( 60%)]  Loss:  2.755719 (2.9290)  Time: 0.585s, 1750.19/s  (2.282s,  448.73/s)  LR: 2.192e-05  Data: 0.018 (1.694)
Train: 93 [ 750/1171 ( 64%)]  Loss:  2.756145 (2.9182)  Time: 0.583s, 1755.59/s  (2.277s,  449.65/s)  LR: 2.192e-05  Data: 0.020 (1.689)
Train: 93 [ 800/1171 ( 68%)]  Loss:  3.183453 (2.9338)  Time: 0.588s, 1742.26/s  (2.277s,  449.75/s)  LR: 2.192e-05  Data: 0.020 (1.687)
Train: 93 [ 850/1171 ( 73%)]  Loss:  3.117458 (2.9440)  Time: 0.821s, 1247.33/s  (2.267s,  451.80/s)  LR: 2.192e-05  Data: 0.181 (1.677)
Train: 93 [ 900/1171 ( 77%)]  Loss:  2.680879 (2.9301)  Time: 0.584s, 1753.75/s  (2.276s,  449.92/s)  LR: 2.192e-05  Data: 0.020 (1.687)
Train: 93 [ 950/1171 ( 81%)]  Loss:  3.028685 (2.9351)  Time: 0.585s, 1750.10/s  (2.280s,  449.12/s)  LR: 2.192e-05  Data: 0.019 (1.690)
Train: 93 [1000/1171 ( 85%)]  Loss:  3.222312 (2.9487)  Time: 0.585s, 1751.05/s  (2.289s,  447.30/s)  LR: 2.192e-05  Data: 0.016 (1.699)
Train: 93 [1050/1171 ( 90%)]  Loss:  3.274702 (2.9635)  Time: 0.582s, 1759.40/s  (2.287s,  447.81/s)  LR: 2.192e-05  Data: 0.018 (1.697)
Train: 93 [1100/1171 ( 94%)]  Loss:  3.170234 (2.9725)  Time: 0.582s, 1758.88/s  (2.290s,  447.18/s)  LR: 2.192e-05  Data: 0.019 (1.701)
Train: 93 [1150/1171 ( 98%)]  Loss:  3.169488 (2.9807)  Time: 0.586s, 1748.08/s  (2.286s,  447.86/s)  LR: 2.192e-05  Data: 0.023 (1.697)
Train: 93 [1170/1171 (100%)]  Loss:  3.429717 (2.9987)  Time: 0.566s, 1810.34/s  (2.285s,  448.19/s)  LR: 2.192e-05  Data: 0.000 (1.695)
Test: [   0/97]  Time: 11.945 (11.945)  Loss:  0.3013 (0.3013)  Acc@1: 96.5820 (96.5820)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (2.896)  Loss:  0.4617 (0.3681)  Acc@1: 91.7969 (94.9142)  Acc@5: 98.4375 (98.8837)
Test: [  97/97]  Time: 0.119 (2.941)  Loss:  0.3361 (0.3799)  Acc@1: 94.9405 (94.3630)  Acc@5: 99.1071 (98.7020)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-93.pth.tar', 94.36300004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-91.pth.tar', 94.32900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-90.pth.tar', 94.30700004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-92.pth.tar', 94.2920000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-88.pth.tar', 94.19900004394532)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-89.pth.tar', 94.1889999975586)

Train: 94 [   0/1171 (  0%)]  Loss:  3.453351 (3.4534)  Time: 11.540s,   88.74/s  (11.540s,   88.74/s)  LR: 1.877e-05  Data: 10.499 (10.499)
Train: 94 [  50/1171 (  4%)]  Loss:  2.898134 (3.1757)  Time: 0.588s, 1742.75/s  (2.505s,  408.84/s)  LR: 1.877e-05  Data: 0.017 (1.923)
Train: 94 [ 100/1171 (  9%)]  Loss:  3.105396 (3.1523)  Time: 0.581s, 1761.21/s  (2.472s,  414.31/s)  LR: 1.877e-05  Data: 0.017 (1.881)
Train: 94 [ 150/1171 ( 13%)]  Loss:  3.234560 (3.1729)  Time: 0.760s, 1346.95/s  (2.390s,  428.39/s)  LR: 1.877e-05  Data: 0.052 (1.801)
Train: 94 [ 200/1171 ( 17%)]  Loss:  3.021147 (3.1425)  Time: 0.584s, 1751.97/s  (2.362s,  433.49/s)  LR: 1.877e-05  Data: 0.019 (1.774)
Train: 94 [ 250/1171 ( 21%)]  Loss:  3.082795 (3.1326)  Time: 0.600s, 1705.94/s  (2.310s,  443.36/s)  LR: 1.877e-05  Data: 0.039 (1.722)
Train: 94 [ 300/1171 ( 26%)]  Loss:  2.416391 (3.0303)  Time: 0.670s, 1527.30/s  (2.286s,  448.03/s)  LR: 1.877e-05  Data: 0.097 (1.700)
Train: 94 [ 350/1171 ( 30%)]  Loss:  2.573031 (2.9731)  Time: 1.380s,  741.86/s  (2.255s,  454.17/s)  LR: 1.877e-05  Data: 0.703 (1.664)
Train: 94 [ 400/1171 ( 34%)]  Loss:  2.947943 (2.9703)  Time: 1.046s,  978.51/s  (2.284s,  448.38/s)  LR: 1.877e-05  Data: 0.457 (1.685)
Train: 94 [ 450/1171 ( 38%)]  Loss:  3.170542 (2.9903)  Time: 4.485s,  228.32/s  (2.290s,  447.15/s)  LR: 1.877e-05  Data: 3.791 (1.689)
Train: 94 [ 500/1171 ( 43%)]  Loss:  2.984966 (2.9898)  Time: 0.586s, 1748.64/s  (2.297s,  445.73/s)  LR: 1.877e-05  Data: 0.019 (1.696)
Train: 94 [ 550/1171 ( 47%)]  Loss:  3.057083 (2.9954)  Time: 2.589s,  395.56/s  (2.306s,  443.99/s)  LR: 1.877e-05  Data: 1.521 (1.703)
Train: 94 [ 600/1171 ( 51%)]  Loss:  3.369294 (3.0242)  Time: 0.999s, 1024.86/s  (2.307s,  443.96/s)  LR: 1.877e-05  Data: 0.413 (1.701)
Train: 94 [ 650/1171 ( 56%)]  Loss:  2.957903 (3.0195)  Time: 0.584s, 1753.45/s  (2.304s,  444.44/s)  LR: 1.877e-05  Data: 0.022 (1.699)
Train: 94 [ 700/1171 ( 60%)]  Loss:  2.727352 (3.0000)  Time: 1.847s,  554.29/s  (2.290s,  447.09/s)  LR: 1.877e-05  Data: 1.241 (1.687)
Train: 94 [ 750/1171 ( 64%)]  Loss:  3.016580 (3.0010)  Time: 0.583s, 1756.49/s  (2.278s,  449.55/s)  LR: 1.877e-05  Data: 0.020 (1.675)
Train: 94 [ 800/1171 ( 68%)]  Loss:  2.940543 (2.9975)  Time: 0.581s, 1762.54/s  (2.283s,  448.59/s)  LR: 1.877e-05  Data: 0.019 (1.681)
Train: 94 [ 850/1171 ( 73%)]  Loss:  2.865743 (2.9902)  Time: 0.586s, 1748.10/s  (2.288s,  447.54/s)  LR: 1.877e-05  Data: 0.021 (1.685)
Train: 94 [ 900/1171 ( 77%)]  Loss:  3.115551 (2.9968)  Time: 0.585s, 1751.69/s  (2.291s,  447.05/s)  LR: 1.877e-05  Data: 0.020 (1.687)
Train: 94 [ 950/1171 ( 81%)]  Loss:  3.141723 (3.0040)  Time: 0.585s, 1749.60/s  (2.291s,  446.88/s)  LR: 1.877e-05  Data: 0.019 (1.688)
Train: 94 [1000/1171 ( 85%)]  Loss:  2.567162 (2.9832)  Time: 0.583s, 1755.42/s  (2.291s,  446.94/s)  LR: 1.877e-05  Data: 0.020 (1.688)
Train: 94 [1050/1171 ( 90%)]  Loss:  3.045791 (2.9860)  Time: 0.586s, 1746.71/s  (2.290s,  447.11/s)  LR: 1.877e-05  Data: 0.017 (1.687)
Train: 94 [1100/1171 ( 94%)]  Loss:  2.872361 (2.9811)  Time: 0.592s, 1729.19/s  (2.285s,  448.17/s)  LR: 1.877e-05  Data: 0.020 (1.683)
Train: 94 [1150/1171 ( 98%)]  Loss:  2.803909 (2.9737)  Time: 0.584s, 1754.08/s  (2.275s,  450.19/s)  LR: 1.877e-05  Data: 0.018 (1.672)
Train: 94 [1170/1171 (100%)]  Loss:  3.324615 (2.9878)  Time: 0.563s, 1818.92/s  (2.280s,  449.13/s)  LR: 1.877e-05  Data: 0.000 (1.678)
Test: [   0/97]  Time: 15.569 (15.569)  Loss:  0.3038 (0.3038)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.198 (3.150)  Loss:  0.4647 (0.3739)  Acc@1: 91.8945 (94.9717)  Acc@5: 98.3398 (98.8683)
Test: [  97/97]  Time: 0.119 (3.065)  Loss:  0.3385 (0.3873)  Acc@1: 94.3452 (94.3620)  Acc@5: 98.8095 (98.6780)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-93.pth.tar', 94.36300004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-94.pth.tar', 94.36200004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-91.pth.tar', 94.32900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-90.pth.tar', 94.30700004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-92.pth.tar', 94.2920000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-88.pth.tar', 94.19900004394532)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-89.pth.tar', 94.1889999975586)

Train: 95 [   0/1171 (  0%)]  Loss:  3.254592 (3.2546)  Time: 10.684s,   95.85/s  (10.684s,   95.85/s)  LR: 1.609e-05  Data: 9.908 (9.908)
Train: 95 [  50/1171 (  4%)]  Loss:  2.836423 (3.0455)  Time: 0.580s, 1764.61/s  (2.349s,  436.02/s)  LR: 1.609e-05  Data: 0.018 (1.761)
Train: 95 [ 100/1171 (  9%)]  Loss:  2.816906 (2.9693)  Time: 0.676s, 1515.01/s  (2.309s,  443.44/s)  LR: 1.609e-05  Data: 0.019 (1.713)
Train: 95 [ 150/1171 ( 13%)]  Loss:  2.509280 (2.8543)  Time: 0.582s, 1760.07/s  (2.231s,  459.04/s)  LR: 1.609e-05  Data: 0.018 (1.636)
Train: 95 [ 200/1171 ( 17%)]  Loss:  3.356906 (2.9548)  Time: 2.135s,  479.53/s  (2.220s,  461.36/s)  LR: 1.609e-05  Data: 1.558 (1.627)
Train: 95 [ 250/1171 ( 21%)]  Loss:  2.918119 (2.9487)  Time: 0.582s, 1758.81/s  (2.183s,  469.17/s)  LR: 1.609e-05  Data: 0.018 (1.585)
Train: 95 [ 300/1171 ( 26%)]  Loss:  3.323146 (3.0022)  Time: 1.840s,  556.48/s  (2.243s,  456.51/s)  LR: 1.609e-05  Data: 1.276 (1.642)
Train: 95 [ 350/1171 ( 30%)]  Loss:  3.371013 (3.0483)  Time: 0.585s, 1750.25/s  (2.250s,  455.07/s)  LR: 1.609e-05  Data: 0.018 (1.652)
Train: 95 [ 400/1171 ( 34%)]  Loss:  2.593713 (2.9978)  Time: 0.586s, 1747.21/s  (2.261s,  452.89/s)  LR: 1.609e-05  Data: 0.022 (1.665)
Train: 95 [ 450/1171 ( 38%)]  Loss:  3.166454 (3.0147)  Time: 0.588s, 1741.91/s  (2.251s,  454.94/s)  LR: 1.609e-05  Data: 0.018 (1.656)
Train: 95 [ 500/1171 ( 43%)]  Loss:  3.075452 (3.0202)  Time: 2.476s,  413.52/s  (2.259s,  453.21/s)  LR: 1.609e-05  Data: 1.909 (1.666)
Train: 95 [ 550/1171 ( 47%)]  Loss:  2.918671 (3.0117)  Time: 0.586s, 1747.77/s  (2.257s,  453.66/s)  LR: 1.609e-05  Data: 0.018 (1.663)
Train: 95 [ 600/1171 ( 51%)]  Loss:  3.085766 (3.0174)  Time: 1.065s,  961.28/s  (2.258s,  453.55/s)  LR: 1.609e-05  Data: 0.378 (1.663)
Train: 95 [ 650/1171 ( 56%)]  Loss:  3.221199 (3.0320)  Time: 0.586s, 1748.41/s  (2.242s,  456.68/s)  LR: 1.609e-05  Data: 0.020 (1.647)
Train: 95 [ 700/1171 ( 60%)]  Loss:  2.893362 (3.0227)  Time: 2.736s,  374.23/s  (2.278s,  449.47/s)  LR: 1.609e-05  Data: 2.159 (1.682)
Train: 95 [ 750/1171 ( 64%)]  Loss:  2.904922 (3.0154)  Time: 0.585s, 1749.70/s  (2.283s,  448.58/s)  LR: 1.609e-05  Data: 0.017 (1.687)
Train: 95 [ 800/1171 ( 68%)]  Loss:  3.016186 (3.0154)  Time: 5.491s,  186.49/s  (2.291s,  447.03/s)  LR: 1.609e-05  Data: 4.706 (1.695)
Train: 95 [ 850/1171 ( 73%)]  Loss:  3.003988 (3.0148)  Time: 0.583s, 1755.88/s  (2.286s,  447.95/s)  LR: 1.609e-05  Data: 0.017 (1.690)
Train: 95 [ 900/1171 ( 77%)]  Loss:  2.735915 (3.0001)  Time: 6.968s,  146.97/s  (2.283s,  448.45/s)  LR: 1.609e-05  Data: 6.291 (1.688)
Train: 95 [ 950/1171 ( 81%)]  Loss:  3.227159 (3.0115)  Time: 0.586s, 1747.85/s  (2.271s,  450.91/s)  LR: 1.609e-05  Data: 0.017 (1.676)
Train: 95 [1000/1171 ( 85%)]  Loss:  3.053360 (3.0135)  Time: 6.420s,  159.49/s  (2.264s,  452.24/s)  LR: 1.609e-05  Data: 5.847 (1.670)
Train: 95 [1050/1171 ( 90%)]  Loss:  2.920906 (3.0092)  Time: 0.584s, 1752.96/s  (2.249s,  455.33/s)  LR: 1.609e-05  Data: 0.018 (1.655)
Train: 95 [1100/1171 ( 94%)]  Loss:  3.186107 (3.0169)  Time: 7.462s,  137.22/s  (2.258s,  453.57/s)  LR: 1.609e-05  Data: 6.789 (1.664)
Train: 95 [1150/1171 ( 98%)]  Loss:  2.674325 (3.0027)  Time: 0.585s, 1749.24/s  (2.258s,  453.57/s)  LR: 1.609e-05  Data: 0.020 (1.665)
Train: 95 [1170/1171 (100%)]  Loss:  3.125634 (3.0076)  Time: 0.563s, 1819.44/s  (2.258s,  453.51/s)  LR: 1.609e-05  Data: 0.000 (1.665)
Test: [   0/97]  Time: 12.726 (12.726)  Loss:  0.3048 (0.3048)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.198 (3.004)  Loss:  0.4918 (0.3801)  Acc@1: 91.1133 (94.9257)  Acc@5: 98.3398 (98.8798)
Test: [  97/97]  Time: 0.119 (2.919)  Loss:  0.3429 (0.3908)  Acc@1: 94.0476 (94.3780)  Acc@5: 99.2560 (98.6940)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-95.pth.tar', 94.37800002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-93.pth.tar', 94.36300004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-94.pth.tar', 94.36200004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-91.pth.tar', 94.32900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-90.pth.tar', 94.30700004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-92.pth.tar', 94.2920000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-88.pth.tar', 94.19900004394532)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-89.pth.tar', 94.1889999975586)

Train: 96 [   0/1171 (  0%)]  Loss:  2.610860 (2.6109)  Time: 9.525s,  107.51/s  (9.525s,  107.51/s)  LR: 1.390e-05  Data: 8.688 (8.688)
Train: 96 [  50/1171 (  4%)]  Loss:  2.719812 (2.6653)  Time: 0.583s, 1757.61/s  (2.112s,  484.84/s)  LR: 1.390e-05  Data: 0.018 (1.529)
Train: 96 [ 100/1171 (  9%)]  Loss:  2.777143 (2.7026)  Time: 0.584s, 1753.65/s  (2.062s,  496.54/s)  LR: 1.390e-05  Data: 0.021 (1.479)
Train: 96 [ 150/1171 ( 13%)]  Loss:  2.944468 (2.7631)  Time: 0.585s, 1750.40/s  (2.007s,  510.30/s)  LR: 1.390e-05  Data: 0.021 (1.420)
Train: 96 [ 200/1171 ( 17%)]  Loss:  2.920220 (2.7945)  Time: 0.582s, 1758.43/s  (2.088s,  490.37/s)  LR: 1.390e-05  Data: 0.020 (1.498)
Train: 96 [ 250/1171 ( 21%)]  Loss:  3.155869 (2.8547)  Time: 0.590s, 1734.43/s  (2.109s,  485.59/s)  LR: 1.390e-05  Data: 0.022 (1.518)
Train: 96 [ 300/1171 ( 26%)]  Loss:  2.815702 (2.8492)  Time: 0.826s, 1240.36/s  (2.136s,  479.29/s)  LR: 1.390e-05  Data: 0.263 (1.547)
Train: 96 [ 350/1171 ( 30%)]  Loss:  2.761905 (2.8382)  Time: 1.901s,  538.75/s  (2.140s,  478.57/s)  LR: 1.390e-05  Data: 1.336 (1.552)
Train: 96 [ 400/1171 ( 34%)]  Loss:  2.435127 (2.7935)  Time: 0.584s, 1753.61/s  (2.172s,  471.36/s)  LR: 1.390e-05  Data: 0.022 (1.584)
Train: 96 [ 450/1171 ( 38%)]  Loss:  2.785077 (2.7926)  Time: 0.586s, 1746.80/s  (2.171s,  471.71/s)  LR: 1.390e-05  Data: 0.023 (1.583)
Train: 96 [ 500/1171 ( 43%)]  Loss:  3.119876 (2.8224)  Time: 0.582s, 1759.78/s  (2.174s,  470.92/s)  LR: 1.390e-05  Data: 0.020 (1.587)
Train: 96 [ 550/1171 ( 47%)]  Loss:  3.134478 (2.8484)  Time: 0.586s, 1747.25/s  (2.161s,  473.82/s)  LR: 1.390e-05  Data: 0.023 (1.574)
Train: 96 [ 600/1171 ( 51%)]  Loss:  3.000373 (2.8601)  Time: 0.616s, 1663.02/s  (2.192s,  467.16/s)  LR: 1.390e-05  Data: 0.024 (1.605)
Train: 96 [ 650/1171 ( 56%)]  Loss:  3.261334 (2.8887)  Time: 0.902s, 1134.71/s  (2.204s,  464.52/s)  LR: 1.390e-05  Data: 0.253 (1.617)
Train: 96 [ 700/1171 ( 60%)]  Loss:  3.052664 (2.8997)  Time: 0.583s, 1754.94/s  (2.219s,  461.39/s)  LR: 1.390e-05  Data: 0.021 (1.630)
Train: 96 [ 750/1171 ( 64%)]  Loss:  2.939087 (2.9021)  Time: 1.163s,  880.66/s  (2.218s,  461.68/s)  LR: 1.390e-05  Data: 0.589 (1.627)
Train: 96 [ 800/1171 ( 68%)]  Loss:  2.863704 (2.8999)  Time: 0.585s, 1751.26/s  (2.215s,  462.25/s)  LR: 1.390e-05  Data: 0.021 (1.624)
Train: 96 [ 850/1171 ( 73%)]  Loss:  3.240011 (2.9188)  Time: 2.597s,  394.28/s  (2.207s,  464.04/s)  LR: 1.390e-05  Data: 1.964 (1.614)
Train: 96 [ 900/1171 ( 77%)]  Loss:  2.842734 (2.9148)  Time: 0.585s, 1749.92/s  (2.197s,  466.01/s)  LR: 1.390e-05  Data: 0.018 (1.604)
Train: 96 [ 950/1171 ( 81%)]  Loss:  3.076785 (2.9229)  Time: 0.740s, 1383.95/s  (2.184s,  468.86/s)  LR: 1.390e-05  Data: 0.164 (1.590)
Train: 96 [1000/1171 ( 85%)]  Loss:  3.270433 (2.9394)  Time: 0.585s, 1749.67/s  (2.191s,  467.36/s)  LR: 1.390e-05  Data: 0.017 (1.597)
Train: 96 [1050/1171 ( 90%)]  Loss:  2.561240 (2.9222)  Time: 0.585s, 1751.58/s  (2.194s,  466.73/s)  LR: 1.390e-05  Data: 0.021 (1.601)
Train: 96 [1100/1171 ( 94%)]  Loss:  3.050503 (2.9278)  Time: 0.585s, 1750.42/s  (2.205s,  464.42/s)  LR: 1.390e-05  Data: 0.017 (1.612)
Train: 96 [1150/1171 ( 98%)]  Loss:  3.190993 (2.9388)  Time: 0.585s, 1749.03/s  (2.205s,  464.36/s)  LR: 1.390e-05  Data: 0.023 (1.612)
Train: 96 [1170/1171 (100%)]  Loss:  2.755527 (2.9314)  Time: 0.566s, 1809.58/s  (2.207s,  463.97/s)  LR: 1.390e-05  Data: 0.000 (1.614)
Test: [   0/97]  Time: 13.177 (13.177)  Loss:  0.2985 (0.2985)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (2.971)  Loss:  0.4663 (0.3724)  Acc@1: 91.5039 (94.9027)  Acc@5: 98.2422 (98.8683)
Test: [  97/97]  Time: 0.119 (2.884)  Loss:  0.3312 (0.3831)  Acc@1: 94.7917 (94.3960)  Acc@5: 99.1071 (98.6870)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-96.pth.tar', 94.39600003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-95.pth.tar', 94.37800002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-93.pth.tar', 94.36300004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-94.pth.tar', 94.36200004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-91.pth.tar', 94.32900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-90.pth.tar', 94.30700004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-92.pth.tar', 94.2920000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-88.pth.tar', 94.19900004394532)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-89.pth.tar', 94.1889999975586)

Train: 97 [   0/1171 (  0%)]  Loss:  2.699771 (2.6998)  Time: 10.508s,   97.45/s  (10.508s,   97.45/s)  LR: 1.220e-05  Data: 9.477 (9.477)
Train: 97 [  50/1171 (  4%)]  Loss:  2.767931 (2.7339)  Time: 0.588s, 1741.47/s  (2.183s,  469.11/s)  LR: 1.220e-05  Data: 0.022 (1.584)
Train: 97 [ 100/1171 (  9%)]  Loss:  3.054788 (2.8408)  Time: 0.583s, 1756.60/s  (2.346s,  436.51/s)  LR: 1.220e-05  Data: 0.020 (1.750)
Train: 97 [ 150/1171 ( 13%)]  Loss:  2.942511 (2.8663)  Time: 0.586s, 1748.54/s  (2.257s,  453.62/s)  LR: 1.220e-05  Data: 0.021 (1.661)
Train: 97 [ 200/1171 ( 17%)]  Loss:  2.775068 (2.8480)  Time: 0.590s, 1736.45/s  (2.286s,  447.93/s)  LR: 1.220e-05  Data: 0.024 (1.691)
Train: 97 [ 250/1171 ( 21%)]  Loss:  2.729647 (2.8283)  Time: 0.584s, 1754.79/s  (2.277s,  449.73/s)  LR: 1.220e-05  Data: 0.019 (1.684)
Train: 97 [ 300/1171 ( 26%)]  Loss:  2.783799 (2.8219)  Time: 0.583s, 1757.39/s  (2.275s,  450.11/s)  LR: 1.220e-05  Data: 0.018 (1.681)
Train: 97 [ 350/1171 ( 30%)]  Loss:  2.836155 (2.8237)  Time: 0.585s, 1749.55/s  (2.260s,  453.00/s)  LR: 1.220e-05  Data: 0.021 (1.666)
Train: 97 [ 400/1171 ( 34%)]  Loss:  2.800185 (2.8211)  Time: 0.584s, 1754.82/s  (2.260s,  453.06/s)  LR: 1.220e-05  Data: 0.020 (1.667)
Train: 97 [ 450/1171 ( 38%)]  Loss:  2.964120 (2.8354)  Time: 0.584s, 1751.96/s  (2.246s,  455.93/s)  LR: 1.220e-05  Data: 0.022 (1.654)
Train: 97 [ 500/1171 ( 43%)]  Loss:  2.659989 (2.8195)  Time: 0.586s, 1746.45/s  (2.286s,  447.85/s)  LR: 1.220e-05  Data: 0.018 (1.693)
Train: 97 [ 550/1171 ( 47%)]  Loss:  3.011137 (2.8354)  Time: 0.584s, 1752.00/s  (2.303s,  444.63/s)  LR: 1.220e-05  Data: 0.020 (1.709)
Train: 97 [ 600/1171 ( 51%)]  Loss:  3.263773 (2.8684)  Time: 0.585s, 1748.99/s  (2.317s,  441.89/s)  LR: 1.220e-05  Data: 0.019 (1.720)
Train: 97 [ 650/1171 ( 56%)]  Loss:  3.134622 (2.8874)  Time: 0.585s, 1751.64/s  (2.320s,  441.46/s)  LR: 1.220e-05  Data: 0.020 (1.723)
Train: 97 [ 700/1171 ( 60%)]  Loss:  3.110654 (2.9023)  Time: 0.584s, 1752.42/s  (2.311s,  443.16/s)  LR: 1.220e-05  Data: 0.020 (1.715)
Train: 97 [ 750/1171 ( 64%)]  Loss:  3.470627 (2.9378)  Time: 0.588s, 1740.74/s  (2.304s,  444.35/s)  LR: 1.220e-05  Data: 0.022 (1.707)
Train: 97 [ 800/1171 ( 68%)]  Loss:  2.876304 (2.9342)  Time: 0.585s, 1750.06/s  (2.301s,  444.99/s)  LR: 1.220e-05  Data: 0.019 (1.702)
Train: 97 [ 850/1171 ( 73%)]  Loss:  2.904910 (2.9326)  Time: 0.584s, 1753.24/s  (2.295s,  446.19/s)  LR: 1.220e-05  Data: 0.019 (1.696)
Train: 97 [ 900/1171 ( 77%)]  Loss:  2.996366 (2.9359)  Time: 3.553s,  288.20/s  (2.313s,  442.64/s)  LR: 1.220e-05  Data: 2.883 (1.715)
Train: 97 [ 950/1171 ( 81%)]  Loss:  3.088109 (2.9435)  Time: 0.585s, 1751.65/s  (2.332s,  439.20/s)  LR: 1.220e-05  Data: 0.019 (1.732)
Train: 97 [1000/1171 ( 85%)]  Loss:  3.195470 (2.9555)  Time: 0.586s, 1748.70/s  (2.332s,  439.04/s)  LR: 1.220e-05  Data: 0.017 (1.733)
Train: 97 [1050/1171 ( 90%)]  Loss:  2.915549 (2.9537)  Time: 0.585s, 1751.32/s  (2.331s,  439.33/s)  LR: 1.220e-05  Data: 0.020 (1.731)
Train: 97 [1100/1171 ( 94%)]  Loss:  3.385690 (2.9725)  Time: 0.585s, 1751.56/s  (2.324s,  440.55/s)  LR: 1.220e-05  Data: 0.018 (1.725)
Train: 97 [1150/1171 ( 98%)]  Loss:  3.066791 (2.9764)  Time: 0.584s, 1753.78/s  (2.317s,  442.03/s)  LR: 1.220e-05  Data: 0.020 (1.717)
Train: 97 [1170/1171 (100%)]  Loss:  2.650883 (2.9634)  Time: 0.567s, 1807.34/s  (2.314s,  442.61/s)  LR: 1.220e-05  Data: 0.000 (1.714)
Test: [   0/97]  Time: 12.039 (12.039)  Loss:  0.3006 (0.3006)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.206 (2.852)  Loss:  0.4795 (0.3833)  Acc@1: 91.3086 (94.9027)  Acc@5: 98.2422 (98.8837)
Test: [  97/97]  Time: 0.119 (3.001)  Loss:  0.3412 (0.3940)  Acc@1: 94.6429 (94.3820)  Acc@5: 99.1071 (98.6970)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-96.pth.tar', 94.39600003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-97.pth.tar', 94.38200002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-95.pth.tar', 94.37800002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-93.pth.tar', 94.36300004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-94.pth.tar', 94.36200004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-91.pth.tar', 94.32900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-90.pth.tar', 94.30700004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-92.pth.tar', 94.2920000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-88.pth.tar', 94.19900004394532)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-89.pth.tar', 94.1889999975586)

Train: 98 [   0/1171 (  0%)]  Loss:  2.686294 (2.6863)  Time: 10.592s,   96.67/s  (10.592s,   96.67/s)  LR: 1.098e-05  Data: 9.870 (9.870)
Train: 98 [  50/1171 (  4%)]  Loss:  2.418972 (2.5526)  Time: 0.585s, 1750.70/s  (2.494s,  410.63/s)  LR: 1.098e-05  Data: 0.022 (1.908)
Train: 98 [ 100/1171 (  9%)]  Loss:  2.540886 (2.5487)  Time: 0.585s, 1749.53/s  (2.420s,  423.18/s)  LR: 1.098e-05  Data: 0.018 (1.824)
Train: 98 [ 150/1171 ( 13%)]  Loss:  2.810246 (2.6141)  Time: 0.588s, 1742.35/s  (2.339s,  437.74/s)  LR: 1.098e-05  Data: 0.022 (1.743)
Train: 98 [ 200/1171 ( 17%)]  Loss:  2.962229 (2.6837)  Time: 0.583s, 1755.26/s  (2.336s,  438.32/s)  LR: 1.098e-05  Data: 0.020 (1.739)
Train: 98 [ 250/1171 ( 21%)]  Loss:  3.157352 (2.7627)  Time: 0.588s, 1742.74/s  (2.289s,  447.40/s)  LR: 1.098e-05  Data: 0.022 (1.695)
Train: 98 [ 300/1171 ( 26%)]  Loss:  2.712869 (2.7555)  Time: 0.587s, 1744.85/s  (2.273s,  450.60/s)  LR: 1.098e-05  Data: 0.022 (1.681)
Train: 98 [ 350/1171 ( 30%)]  Loss:  3.204949 (2.8117)  Time: 0.588s, 1742.32/s  (2.265s,  452.13/s)  LR: 1.098e-05  Data: 0.023 (1.671)
Train: 98 [ 400/1171 ( 34%)]  Loss:  3.122411 (2.8462)  Time: 0.585s, 1750.87/s  (2.304s,  444.51/s)  LR: 1.098e-05  Data: 0.021 (1.710)
Train: 98 [ 450/1171 ( 38%)]  Loss:  2.735538 (2.8352)  Time: 0.584s, 1753.36/s  (2.313s,  442.65/s)  LR: 1.098e-05  Data: 0.019 (1.721)
Train: 98 [ 500/1171 ( 43%)]  Loss:  2.843352 (2.8359)  Time: 0.587s, 1745.71/s  (2.329s,  439.64/s)  LR: 1.098e-05  Data: 0.020 (1.738)
Train: 98 [ 550/1171 ( 47%)]  Loss:  2.513681 (2.8091)  Time: 0.587s, 1743.46/s  (2.334s,  438.81/s)  LR: 1.098e-05  Data: 0.019 (1.743)
Train: 98 [ 600/1171 ( 51%)]  Loss:  2.939975 (2.8191)  Time: 1.113s,  919.65/s  (2.344s,  436.85/s)  LR: 1.098e-05  Data: 0.551 (1.752)
Train: 98 [ 650/1171 ( 56%)]  Loss:  2.823454 (2.8194)  Time: 0.585s, 1750.24/s  (2.334s,  438.72/s)  LR: 1.098e-05  Data: 0.019 (1.742)
Train: 98 [ 700/1171 ( 60%)]  Loss:  2.949195 (2.8281)  Time: 0.588s, 1742.14/s  (2.330s,  439.55/s)  LR: 1.098e-05  Data: 0.025 (1.739)
Train: 98 [ 750/1171 ( 64%)]  Loss:  2.895421 (2.8323)  Time: 0.585s, 1751.13/s  (2.341s,  437.43/s)  LR: 1.098e-05  Data: 0.019 (1.750)
Train: 98 [ 800/1171 ( 68%)]  Loss:  2.906533 (2.8367)  Time: 0.587s, 1744.50/s  (2.349s,  435.97/s)  LR: 1.098e-05  Data: 0.022 (1.759)
Train: 98 [ 850/1171 ( 73%)]  Loss:  3.002936 (2.8459)  Time: 0.584s, 1753.79/s  (2.343s,  437.02/s)  LR: 1.098e-05  Data: 0.018 (1.754)
Train: 98 [ 900/1171 ( 77%)]  Loss:  3.112993 (2.8600)  Time: 0.590s, 1735.56/s  (2.344s,  436.91/s)  LR: 1.098e-05  Data: 0.024 (1.755)
Train: 98 [ 950/1171 ( 81%)]  Loss:  2.484230 (2.8412)  Time: 0.585s, 1749.42/s  (2.338s,  438.05/s)  LR: 1.098e-05  Data: 0.020 (1.749)
Train: 98 [1000/1171 ( 85%)]  Loss:  3.164314 (2.8566)  Time: 0.587s, 1744.01/s  (2.332s,  439.02/s)  LR: 1.098e-05  Data: 0.022 (1.744)
Train: 98 [1050/1171 ( 90%)]  Loss:  3.018621 (2.8639)  Time: 0.586s, 1745.99/s  (2.324s,  440.58/s)  LR: 1.098e-05  Data: 0.017 (1.737)
Train: 98 [1100/1171 ( 94%)]  Loss:  3.339834 (2.8846)  Time: 0.588s, 1741.95/s  (2.318s,  441.73/s)  LR: 1.098e-05  Data: 0.022 (1.730)
Train: 98 [1150/1171 ( 98%)]  Loss:  2.480257 (2.8678)  Time: 0.588s, 1741.30/s  (2.327s,  439.97/s)  LR: 1.098e-05  Data: 0.021 (1.739)
Train: 98 [1170/1171 (100%)]  Loss:  2.743317 (2.8628)  Time: 0.564s, 1816.84/s  (2.324s,  440.68/s)  LR: 1.098e-05  Data: 0.000 (1.736)
Test: [   0/97]  Time: 12.337 (12.337)  Loss:  0.2991 (0.2991)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.197 (3.160)  Loss:  0.4770 (0.3726)  Acc@1: 91.6016 (95.0176)  Acc@5: 98.1445 (98.8798)
Test: [  97/97]  Time: 0.119 (3.065)  Loss:  0.3406 (0.3855)  Acc@1: 94.3452 (94.4340)  Acc@5: 98.9583 (98.6970)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-98.pth.tar', 94.43399999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-96.pth.tar', 94.39600003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-97.pth.tar', 94.38200002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-95.pth.tar', 94.37800002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-93.pth.tar', 94.36300004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-94.pth.tar', 94.36200004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-91.pth.tar', 94.32900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-90.pth.tar', 94.30700004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-92.pth.tar', 94.2920000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-88.pth.tar', 94.19900004394532)

Train: 99 [   0/1171 (  0%)]  Loss:  2.732734 (2.7327)  Time: 10.777s,   95.02/s  (10.777s,   95.02/s)  LR: 1.024e-05  Data: 9.655 (9.655)
Train: 99 [  50/1171 (  4%)]  Loss:  2.164264 (2.4485)  Time: 0.585s, 1750.64/s  (2.308s,  443.71/s)  LR: 1.024e-05  Data: 0.021 (1.705)
Train: 99 [ 100/1171 (  9%)]  Loss:  2.870243 (2.5891)  Time: 0.583s, 1754.94/s  (2.259s,  453.30/s)  LR: 1.024e-05  Data: 0.019 (1.665)
Train: 99 [ 150/1171 ( 13%)]  Loss:  2.706642 (2.6185)  Time: 0.589s, 1739.35/s  (2.230s,  459.14/s)  LR: 1.024e-05  Data: 0.020 (1.634)
Train: 99 [ 200/1171 ( 17%)]  Loss:  2.890738 (2.6729)  Time: 0.582s, 1758.99/s  (2.207s,  464.02/s)  LR: 1.024e-05  Data: 0.018 (1.607)
Train: 99 [ 250/1171 ( 21%)]  Loss:  2.449635 (2.6357)  Time: 0.586s, 1747.90/s  (2.289s,  447.26/s)  LR: 1.024e-05  Data: 0.018 (1.684)
Train: 99 [ 300/1171 ( 26%)]  Loss:  2.990015 (2.6863)  Time: 2.521s,  406.25/s  (2.311s,  443.13/s)  LR: 1.024e-05  Data: 1.858 (1.706)
Train: 99 [ 350/1171 ( 30%)]  Loss:  3.049958 (2.7318)  Time: 0.586s, 1746.81/s  (2.316s,  442.17/s)  LR: 1.024e-05  Data: 0.020 (1.708)
Train: 99 [ 400/1171 ( 34%)]  Loss:  2.241570 (2.6773)  Time: 3.400s,  301.19/s  (2.315s,  442.38/s)  LR: 1.024e-05  Data: 2.751 (1.705)
Train: 99 [ 450/1171 ( 38%)]  Loss:  3.263887 (2.7360)  Time: 0.586s, 1748.75/s  (2.307s,  443.94/s)  LR: 1.024e-05  Data: 0.022 (1.695)
Train: 99 [ 500/1171 ( 43%)]  Loss:  3.486711 (2.8042)  Time: 4.331s,  236.46/s  (2.317s,  441.92/s)  LR: 1.024e-05  Data: 3.751 (1.705)
Train: 99 [ 550/1171 ( 47%)]  Loss:  2.494234 (2.7784)  Time: 0.583s, 1755.62/s  (2.314s,  442.52/s)  LR: 1.024e-05  Data: 0.020 (1.704)
Train: 99 [ 600/1171 ( 51%)]  Loss:  3.222166 (2.8125)  Time: 1.311s,  781.12/s  (2.345s,  436.63/s)  LR: 1.024e-05  Data: 0.641 (1.734)
Train: 99 [ 650/1171 ( 56%)]  Loss:  2.676371 (2.8028)  Time: 0.585s, 1751.45/s  (2.354s,  434.92/s)  LR: 1.024e-05  Data: 0.022 (1.744)
Train: 99 [ 700/1171 ( 60%)]  Loss:  3.357990 (2.8398)  Time: 0.585s, 1749.21/s  (2.352s,  435.47/s)  LR: 1.024e-05  Data: 0.022 (1.743)
Train: 99 [ 750/1171 ( 64%)]  Loss:  2.939853 (2.8461)  Time: 0.583s, 1755.97/s  (2.352s,  435.38/s)  LR: 1.024e-05  Data: 0.021 (1.744)
Train: 99 [ 800/1171 ( 68%)]  Loss:  2.498270 (2.8256)  Time: 0.586s, 1747.64/s  (2.342s,  437.25/s)  LR: 1.024e-05  Data: 0.021 (1.735)
Train: 99 [ 850/1171 ( 73%)]  Loss:  2.855812 (2.8273)  Time: 0.585s, 1750.59/s  (2.339s,  437.81/s)  LR: 1.024e-05  Data: 0.021 (1.732)
Train: 99 [ 900/1171 ( 77%)]  Loss:  2.833181 (2.8276)  Time: 0.586s, 1747.54/s  (2.328s,  439.93/s)  LR: 1.024e-05  Data: 0.021 (1.722)
Train: 99 [ 950/1171 ( 81%)]  Loss:  3.081460 (2.8403)  Time: 0.584s, 1751.94/s  (2.321s,  441.18/s)  LR: 1.024e-05  Data: 0.021 (1.715)
Train: 99 [1000/1171 ( 85%)]  Loss:  3.114764 (2.8534)  Time: 0.584s, 1752.61/s  (2.334s,  438.74/s)  LR: 1.024e-05  Data: 0.018 (1.728)
Train: 99 [1050/1171 ( 90%)]  Loss:  2.723916 (2.8475)  Time: 0.584s, 1752.01/s  (2.334s,  438.74/s)  LR: 1.024e-05  Data: 0.019 (1.728)
Train: 99 [1100/1171 ( 94%)]  Loss:  3.268118 (2.8658)  Time: 0.592s, 1728.89/s  (2.329s,  439.71/s)  LR: 1.024e-05  Data: 0.027 (1.723)
Train: 99 [1150/1171 ( 98%)]  Loss:  3.351920 (2.8860)  Time: 0.584s, 1752.82/s  (2.327s,  440.01/s)  LR: 1.024e-05  Data: 0.020 (1.723)
Train: 99 [1170/1171 (100%)]  Loss:  2.853608 (2.8847)  Time: 0.563s, 1817.95/s  (2.326s,  440.32/s)  LR: 1.024e-05  Data: 0.000 (1.721)
Test: [   0/97]  Time: 12.832 (12.832)  Loss:  0.3038 (0.3038)  Acc@1: 96.5820 (96.5820)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.197 (2.989)  Loss:  0.4840 (0.3804)  Acc@1: 91.6992 (95.0291)  Acc@5: 98.3398 (98.8798)
Test: [  97/97]  Time: 0.119 (2.918)  Loss:  0.3512 (0.3943)  Acc@1: 94.4940 (94.4350)  Acc@5: 99.1071 (98.6840)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-99.pth.tar', 94.43500000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-98.pth.tar', 94.43399999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-96.pth.tar', 94.39600003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-97.pth.tar', 94.38200002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-95.pth.tar', 94.37800002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-93.pth.tar', 94.36300004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-94.pth.tar', 94.36200004882812)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-91.pth.tar', 94.32900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-90.pth.tar', 94.30700004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-92.pth.tar', 94.2920000341797)

*** Best metric: 94.43500000976563 (epoch 99)

wandb: Waiting for W&B process to finish, PID 2776
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210529_120814-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1/logs/debug.log
wandb: Find internal logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210529_120814-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1/logs/debug-internal.log
wandb: Run summary:
wandb:   _timestamp 1622293062
wandb:   train_loss 2.88472
wandb:        _step 99
wandb:        epoch 99
wandb:     _runtime 327369
wandb:    eval_loss 0.39431
wandb:    eval_top1 94.435
wandb:    eval_top5 98.684
wandb: Run history:
wandb:        epoch ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb:   train_loss ‚ñÅ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñà‚ñá‚ñà‚ñÑ‚ñÜ‚ñÅ‚ñÇ
wandb:    eval_loss ‚ñÖ‚ñÑ‚ñà‚ñÅ‚ñá‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñá
wandb:    eval_top1 ‚ñÅ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñà‚ñà
wandb:    eval_top5 ‚ñÉ‚ñÅ‚ñÇ‚ñà‚ñÖ‚ñá‚ñÉ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÑ
wandb:     _runtime ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb:   _timestamp ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb:        _step ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced PreTraining_vit_deit_tiny_patch16_224_fake_1k: https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
--End--
Sat May 29 21:57:52 JST 2021

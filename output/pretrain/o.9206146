--Start--
Sun Jun 6 13:51:21 JST 2021
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 3
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
wandb: Currently logged in as: gyama_x (use `wandb login --relogin` to force relogin)
wandb: WARNING Tried to auto resume run with id PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k but id PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1 is set.
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.10.27
wandb: Resuming run PreTraining_vit_deit_tiny_patch16_224_fake_1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gyama_x/pytorch-image-models
wandb: üöÄ View run at https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run data is saved locally in /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210606_135127-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run `wandb offline` to turn off syncing.

Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar' (epoch 187)
Using native Torch DistributedDataParallel.
Scheduled epochs: 210
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Train: 188 [   0/1171 (  0%)]  Loss:  2.869595 (2.8696)  Time: 13.961s,   73.34/s  (13.961s,   73.34/s)  LR: 3.657e-05  Data: 12.781 (12.781)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 188 [  50/1171 (  4%)]  Loss:  2.913282 (2.8914)  Time: 0.583s, 1757.28/s  (2.334s,  438.70/s)  LR: 3.657e-05  Data: 0.018 (1.723)
Train: 188 [ 100/1171 (  9%)]  Loss:  2.947426 (2.9101)  Time: 2.637s,  388.30/s  (2.138s,  479.06/s)  LR: 3.657e-05  Data: 1.986 (1.531)
Train: 188 [ 150/1171 ( 13%)]  Loss:  2.509738 (2.8100)  Time: 0.583s, 1756.40/s  (2.117s,  483.73/s)  LR: 3.657e-05  Data: 0.020 (1.515)
Train: 188 [ 200/1171 ( 17%)]  Loss:  2.534971 (2.7550)  Time: 3.232s,  316.86/s  (2.078s,  492.86/s)  LR: 3.657e-05  Data: 2.426 (1.475)
Train: 188 [ 250/1171 ( 21%)]  Loss:  2.368723 (2.6906)  Time: 0.583s, 1755.25/s  (2.067s,  495.47/s)  LR: 3.657e-05  Data: 0.019 (1.464)
Train: 188 [ 300/1171 ( 26%)]  Loss:  2.942859 (2.7267)  Time: 1.703s,  601.20/s  (2.061s,  496.81/s)  LR: 3.657e-05  Data: 1.140 (1.457)
Train: 188 [ 350/1171 ( 30%)]  Loss:  2.526462 (2.7016)  Time: 0.583s, 1755.74/s  (2.071s,  494.54/s)  LR: 3.657e-05  Data: 0.021 (1.468)
Train: 188 [ 400/1171 ( 34%)]  Loss:  2.999870 (2.7348)  Time: 2.527s,  405.22/s  (2.077s,  493.05/s)  LR: 3.657e-05  Data: 1.965 (1.475)
Train: 188 [ 450/1171 ( 38%)]  Loss:  2.580977 (2.7194)  Time: 0.584s, 1753.13/s  (2.072s,  494.24/s)  LR: 3.657e-05  Data: 0.019 (1.471)
Train: 188 [ 500/1171 ( 43%)]  Loss:  2.592140 (2.7078)  Time: 6.286s,  162.90/s  (2.078s,  492.83/s)  LR: 3.657e-05  Data: 5.617 (1.477)
Train: 188 [ 550/1171 ( 47%)]  Loss:  2.658777 (2.7037)  Time: 0.585s, 1750.88/s  (2.068s,  495.11/s)  LR: 3.657e-05  Data: 0.018 (1.469)
Train: 188 [ 600/1171 ( 51%)]  Loss:  3.248043 (2.7456)  Time: 6.327s,  161.84/s  (2.067s,  495.44/s)  LR: 3.657e-05  Data: 5.668 (1.469)
Train: 188 [ 650/1171 ( 56%)]  Loss:  2.726938 (2.7443)  Time: 0.582s, 1759.42/s  (2.058s,  497.62/s)  LR: 3.657e-05  Data: 0.020 (1.461)
Train: 188 [ 700/1171 ( 60%)]  Loss:  2.847359 (2.7511)  Time: 6.811s,  150.35/s  (2.088s,  490.52/s)  LR: 3.657e-05  Data: 6.249 (1.491)
Train: 188 [ 750/1171 ( 64%)]  Loss:  3.151624 (2.7762)  Time: 0.583s, 1755.40/s  (2.092s,  489.38/s)  LR: 3.657e-05  Data: 0.020 (1.497)
Train: 188 [ 800/1171 ( 68%)]  Loss:  2.737724 (2.7739)  Time: 7.365s,  139.04/s  (2.109s,  485.64/s)  LR: 3.657e-05  Data: 6.803 (1.514)
Train: 188 [ 850/1171 ( 73%)]  Loss:  2.820691 (2.7765)  Time: 0.585s, 1751.10/s  (2.116s,  483.93/s)  LR: 3.657e-05  Data: 0.021 (1.522)
Train: 188 [ 900/1171 ( 77%)]  Loss:  2.964424 (2.7864)  Time: 6.497s,  157.62/s  (2.122s,  482.66/s)  LR: 3.657e-05  Data: 5.934 (1.529)
Train: 188 [ 950/1171 ( 81%)]  Loss:  2.583149 (2.7762)  Time: 0.585s, 1750.33/s  (2.123s,  482.44/s)  LR: 3.657e-05  Data: 0.019 (1.529)
Train: 188 [1000/1171 ( 85%)]  Loss:  2.322231 (2.7546)  Time: 6.171s,  165.94/s  (2.127s,  481.35/s)  LR: 3.657e-05  Data: 5.472 (1.535)
Train: 188 [1050/1171 ( 90%)]  Loss:  2.803712 (2.7569)  Time: 0.583s, 1755.91/s  (2.124s,  482.04/s)  LR: 3.657e-05  Data: 0.019 (1.531)
Train: 188 [1100/1171 ( 94%)]  Loss:  2.757770 (2.7569)  Time: 6.363s,  160.93/s  (2.145s,  477.35/s)  LR: 3.657e-05  Data: 5.802 (1.551)
Train: 188 [1150/1171 ( 98%)]  Loss:  2.431695 (2.7433)  Time: 0.584s, 1752.72/s  (2.149s,  476.43/s)  LR: 3.657e-05  Data: 0.017 (1.555)
Train: 188 [1170/1171 (100%)]  Loss:  2.807525 (2.7459)  Time: 0.563s, 1817.90/s  (2.152s,  475.90/s)  LR: 3.657e-05  Data: 0.000 (1.558)
Test: [   0/97]  Time: 13.725 (13.725)  Loss:  0.2682 (0.2682)  Acc@1: 97.2656 (97.2656)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.196 (3.095)  Loss:  0.4357 (0.3349)  Acc@1: 91.8945 (95.3393)  Acc@5: 98.6328 (98.9717)
Test: [  97/97]  Time: 0.494 (2.992)  Loss:  0.3099 (0.3490)  Acc@1: 94.7917 (94.8060)  Acc@5: 99.4048 (98.8440)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 189 [   0/1171 (  0%)]  Loss:  2.730507 (2.7305)  Time: 10.378s,   98.67/s  (10.378s,   98.67/s)  LR: 3.423e-05  Data: 9.291 (9.291)
Train: 189 [  50/1171 (  4%)]  Loss:  3.104414 (2.9175)  Time: 0.581s, 1762.78/s  (2.439s,  419.78/s)  LR: 3.423e-05  Data: 0.018 (1.850)
Train: 189 [ 100/1171 (  9%)]  Loss:  2.944406 (2.9264)  Time: 4.212s,  243.14/s  (2.410s,  424.91/s)  LR: 3.423e-05  Data: 3.635 (1.811)
Train: 189 [ 150/1171 ( 13%)]  Loss:  2.753321 (2.8832)  Time: 0.585s, 1751.13/s  (2.274s,  450.34/s)  LR: 3.423e-05  Data: 0.022 (1.675)
Train: 189 [ 200/1171 ( 17%)]  Loss:  2.643484 (2.8352)  Time: 3.088s,  331.55/s  (2.335s,  438.51/s)  LR: 3.423e-05  Data: 2.523 (1.735)
Train: 189 [ 250/1171 ( 21%)]  Loss:  2.557747 (2.7890)  Time: 0.584s, 1754.64/s  (2.328s,  439.87/s)  LR: 3.423e-05  Data: 0.021 (1.729)
Train: 189 [ 300/1171 ( 26%)]  Loss:  2.688493 (2.7746)  Time: 1.686s,  607.50/s  (2.346s,  436.46/s)  LR: 3.423e-05  Data: 1.116 (1.746)
Train: 189 [ 350/1171 ( 30%)]  Loss:  2.665340 (2.7610)  Time: 0.584s, 1754.51/s  (2.331s,  439.39/s)  LR: 3.423e-05  Data: 0.021 (1.731)
Train: 189 [ 400/1171 ( 34%)]  Loss:  3.162632 (2.8056)  Time: 0.586s, 1746.01/s  (2.326s,  440.29/s)  LR: 3.423e-05  Data: 0.017 (1.728)
Train: 189 [ 450/1171 ( 38%)]  Loss:  2.599531 (2.7850)  Time: 0.582s, 1758.66/s  (2.307s,  443.89/s)  LR: 3.423e-05  Data: 0.019 (1.710)
Train: 189 [ 500/1171 ( 43%)]  Loss:  2.946583 (2.7997)  Time: 0.585s, 1751.40/s  (2.307s,  443.96/s)  LR: 3.423e-05  Data: 0.019 (1.710)
Train: 189 [ 550/1171 ( 47%)]  Loss:  2.688604 (2.7904)  Time: 0.581s, 1762.59/s  (2.307s,  443.78/s)  LR: 3.423e-05  Data: 0.018 (1.711)
Train: 189 [ 600/1171 ( 51%)]  Loss:  3.027462 (2.8087)  Time: 0.584s, 1752.45/s  (2.341s,  437.37/s)  LR: 3.423e-05  Data: 0.018 (1.745)
Train: 189 [ 650/1171 ( 56%)]  Loss:  2.845961 (2.8113)  Time: 0.584s, 1752.95/s  (2.350s,  435.71/s)  LR: 3.423e-05  Data: 0.022 (1.756)
Train: 189 [ 700/1171 ( 60%)]  Loss:  2.798045 (2.8104)  Time: 0.584s, 1752.00/s  (2.355s,  434.76/s)  LR: 3.423e-05  Data: 0.019 (1.761)
Train: 189 [ 750/1171 ( 64%)]  Loss:  2.780400 (2.8086)  Time: 0.584s, 1754.17/s  (2.348s,  436.13/s)  LR: 3.423e-05  Data: 0.018 (1.754)
Train: 189 [ 800/1171 ( 68%)]  Loss:  2.743222 (2.8047)  Time: 0.586s, 1747.58/s  (2.341s,  437.40/s)  LR: 3.423e-05  Data: 0.022 (1.747)
Train: 189 [ 850/1171 ( 73%)]  Loss:  2.795224 (2.8042)  Time: 0.585s, 1749.55/s  (2.333s,  438.89/s)  LR: 3.423e-05  Data: 0.019 (1.739)
Train: 189 [ 900/1171 ( 77%)]  Loss:  2.892958 (2.8089)  Time: 0.584s, 1754.21/s  (2.325s,  440.44/s)  LR: 3.423e-05  Data: 0.021 (1.731)
Train: 189 [ 950/1171 ( 81%)]  Loss:  2.950021 (2.8159)  Time: 0.585s, 1751.65/s  (2.332s,  439.10/s)  LR: 3.423e-05  Data: 0.018 (1.738)
Train: 189 [1000/1171 ( 85%)]  Loss:  3.284010 (2.8382)  Time: 2.937s,  348.64/s  (2.334s,  438.75/s)  LR: 3.423e-05  Data: 2.204 (1.738)
Train: 189 [1050/1171 ( 90%)]  Loss:  3.247530 (2.8568)  Time: 2.793s,  366.60/s  (2.334s,  438.68/s)  LR: 3.423e-05  Data: 2.223 (1.738)
Train: 189 [1100/1171 ( 94%)]  Loss:  2.878079 (2.8577)  Time: 1.400s,  731.19/s  (2.333s,  438.98/s)  LR: 3.423e-05  Data: 0.740 (1.734)
Train: 189 [1150/1171 ( 98%)]  Loss:  2.633932 (2.8484)  Time: 1.053s,  972.75/s  (2.330s,  439.43/s)  LR: 3.423e-05  Data: 0.476 (1.731)
Train: 189 [1170/1171 (100%)]  Loss:  2.743854 (2.8442)  Time: 0.565s, 1812.05/s  (2.330s,  439.48/s)  LR: 3.423e-05  Data: 0.000 (1.730)
Test: [   0/97]  Time: 13.206 (13.206)  Loss:  0.2807 (0.2807)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.197 (3.002)  Loss:  0.4075 (0.3365)  Acc@1: 92.9688 (95.3719)  Acc@5: 98.5352 (98.9698)
Test: [  97/97]  Time: 0.120 (2.890)  Loss:  0.3224 (0.3483)  Acc@1: 94.0476 (94.8740)  Acc@5: 99.5536 (98.8480)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 190 [   0/1171 (  0%)]  Loss:  2.669693 (2.6697)  Time: 9.875s,  103.70/s  (9.875s,  103.70/s)  LR: 3.199e-05  Data: 9.253 (9.253)
Train: 190 [  50/1171 (  4%)]  Loss:  2.702344 (2.6860)  Time: 0.589s, 1738.78/s  (2.650s,  386.37/s)  LR: 3.199e-05  Data: 0.020 (2.072)
Train: 190 [ 100/1171 (  9%)]  Loss:  3.222105 (2.8647)  Time: 0.587s, 1745.47/s  (2.550s,  401.51/s)  LR: 3.199e-05  Data: 0.024 (1.970)
Train: 190 [ 150/1171 ( 13%)]  Loss:  2.293488 (2.7219)  Time: 0.586s, 1748.19/s  (2.449s,  418.07/s)  LR: 3.199e-05  Data: 0.024 (1.872)
Train: 190 [ 200/1171 ( 17%)]  Loss:  2.562088 (2.6899)  Time: 0.587s, 1744.01/s  (2.412s,  424.48/s)  LR: 3.199e-05  Data: 0.018 (1.833)
Train: 190 [ 250/1171 ( 21%)]  Loss:  2.669061 (2.6865)  Time: 0.585s, 1751.45/s  (2.354s,  435.00/s)  LR: 3.199e-05  Data: 0.020 (1.773)
Train: 190 [ 300/1171 ( 26%)]  Loss:  2.431005 (2.6500)  Time: 0.584s, 1752.77/s  (2.351s,  435.63/s)  LR: 3.199e-05  Data: 0.017 (1.768)
Train: 190 [ 350/1171 ( 30%)]  Loss:  2.728569 (2.6598)  Time: 0.597s, 1714.95/s  (2.313s,  442.68/s)  LR: 3.199e-05  Data: 0.034 (1.729)
Train: 190 [ 400/1171 ( 34%)]  Loss:  2.787885 (2.6740)  Time: 0.585s, 1749.86/s  (2.298s,  445.63/s)  LR: 3.199e-05  Data: 0.021 (1.714)
Train: 190 [ 450/1171 ( 38%)]  Loss:  2.656718 (2.6723)  Time: 0.582s, 1758.32/s  (2.320s,  441.31/s)  LR: 3.199e-05  Data: 0.020 (1.733)
Train: 190 [ 500/1171 ( 43%)]  Loss:  2.804749 (2.6843)  Time: 2.146s,  477.10/s  (2.360s,  433.83/s)  LR: 3.199e-05  Data: 1.483 (1.772)
Train: 190 [ 550/1171 ( 47%)]  Loss:  2.823846 (2.6960)  Time: 0.585s, 1750.60/s  (2.376s,  430.95/s)  LR: 3.199e-05  Data: 0.018 (1.785)
Train: 190 [ 600/1171 ( 51%)]  Loss:  2.650625 (2.6925)  Time: 7.618s,  134.42/s  (2.395s,  427.58/s)  LR: 3.199e-05  Data: 6.960 (1.803)
Train: 190 [ 650/1171 ( 56%)]  Loss:  2.852142 (2.7039)  Time: 0.583s, 1756.96/s  (2.386s,  429.20/s)  LR: 3.199e-05  Data: 0.020 (1.795)
Train: 190 [ 700/1171 ( 60%)]  Loss:  2.891826 (2.7164)  Time: 6.924s,  147.90/s  (2.387s,  429.02/s)  LR: 3.199e-05  Data: 6.259 (1.796)
Train: 190 [ 750/1171 ( 64%)]  Loss:  2.772033 (2.7199)  Time: 0.585s, 1751.37/s  (2.374s,  431.28/s)  LR: 3.199e-05  Data: 0.018 (1.783)
Train: 190 [ 800/1171 ( 68%)]  Loss:  2.779440 (2.7234)  Time: 7.306s,  140.15/s  (2.392s,  428.15/s)  LR: 3.199e-05  Data: 6.729 (1.800)
Train: 190 [ 850/1171 ( 73%)]  Loss:  2.259297 (2.6976)  Time: 0.588s, 1741.32/s  (2.390s,  428.53/s)  LR: 3.199e-05  Data: 0.021 (1.799)
Train: 190 [ 900/1171 ( 77%)]  Loss:  2.577216 (2.6913)  Time: 7.260s,  141.04/s  (2.396s,  427.37/s)  LR: 3.199e-05  Data: 6.582 (1.806)
Train: 190 [ 950/1171 ( 81%)]  Loss:  3.126130 (2.7130)  Time: 0.585s, 1750.27/s  (2.389s,  428.56/s)  LR: 3.199e-05  Data: 0.020 (1.800)
Train: 190 [1000/1171 ( 85%)]  Loss:  2.653971 (2.7102)  Time: 7.188s,  142.46/s  (2.388s,  428.77/s)  LR: 3.199e-05  Data: 6.597 (1.799)
Train: 190 [1050/1171 ( 90%)]  Loss:  2.854133 (2.7167)  Time: 0.583s, 1756.83/s  (2.380s,  430.25/s)  LR: 3.199e-05  Data: 0.020 (1.791)
Train: 190 [1100/1171 ( 94%)]  Loss:  3.210702 (2.7382)  Time: 6.895s,  148.51/s  (2.377s,  430.88/s)  LR: 3.199e-05  Data: 6.259 (1.788)
Train: 190 [1150/1171 ( 98%)]  Loss:  2.809616 (2.7412)  Time: 0.591s, 1733.33/s  (2.365s,  433.02/s)  LR: 3.199e-05  Data: 0.018 (1.776)
Train: 190 [1170/1171 (100%)]  Loss:  2.990417 (2.7512)  Time: 0.565s, 1810.88/s  (2.373s,  431.50/s)  LR: 3.199e-05  Data: 0.000 (1.785)
Test: [   0/97]  Time: 15.139 (15.139)  Loss:  0.3123 (0.3123)  Acc@1: 96.3867 (96.3867)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (3.239)  Loss:  0.4477 (0.3766)  Acc@1: 92.8711 (95.2742)  Acc@5: 98.5352 (98.9622)
Test: [  97/97]  Time: 0.120 (3.144)  Loss:  0.3539 (0.3830)  Acc@1: 94.1964 (94.8600)  Acc@5: 99.2560 (98.8580)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-190.pth.tar', 94.85999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 191 [   0/1171 (  0%)]  Loss:  3.005894 (3.0059)  Time: 11.201s,   91.42/s  (11.201s,   91.42/s)  LR: 2.986e-05  Data: 10.383 (10.383)
Train: 191 [  50/1171 (  4%)]  Loss:  2.864635 (2.9353)  Time: 0.590s, 1735.56/s  (2.357s,  434.54/s)  LR: 2.986e-05  Data: 0.020 (1.768)
Train: 191 [ 100/1171 (  9%)]  Loss:  2.749014 (2.8732)  Time: 0.732s, 1399.31/s  (2.309s,  443.56/s)  LR: 2.986e-05  Data: 0.050 (1.708)
Train: 191 [ 150/1171 ( 13%)]  Loss:  2.872023 (2.8729)  Time: 0.873s, 1173.45/s  (2.260s,  453.01/s)  LR: 2.986e-05  Data: 0.310 (1.654)
Train: 191 [ 200/1171 ( 17%)]  Loss:  2.435916 (2.7855)  Time: 3.880s,  263.92/s  (2.266s,  452.00/s)  LR: 2.986e-05  Data: 3.195 (1.660)
Train: 191 [ 250/1171 ( 21%)]  Loss:  2.827721 (2.7925)  Time: 0.583s, 1756.00/s  (2.226s,  459.97/s)  LR: 2.986e-05  Data: 0.019 (1.622)
Train: 191 [ 300/1171 ( 26%)]  Loss:  3.084616 (2.8343)  Time: 4.299s,  238.17/s  (2.319s,  441.57/s)  LR: 2.986e-05  Data: 3.696 (1.717)
Train: 191 [ 350/1171 ( 30%)]  Loss:  3.116214 (2.8695)  Time: 0.586s, 1747.64/s  (2.329s,  439.77/s)  LR: 2.986e-05  Data: 0.019 (1.727)
Train: 191 [ 400/1171 ( 34%)]  Loss:  2.716761 (2.8525)  Time: 1.246s,  821.78/s  (2.341s,  437.46/s)  LR: 2.986e-05  Data: 0.648 (1.741)
Train: 191 [ 450/1171 ( 38%)]  Loss:  2.999547 (2.8672)  Time: 0.585s, 1751.08/s  (2.339s,  437.73/s)  LR: 2.986e-05  Data: 0.019 (1.740)
Train: 191 [ 500/1171 ( 43%)]  Loss:  2.743469 (2.8560)  Time: 6.392s,  160.19/s  (2.347s,  436.32/s)  LR: 2.986e-05  Data: 5.812 (1.747)
Train: 191 [ 550/1171 ( 47%)]  Loss:  2.489735 (2.8255)  Time: 0.584s, 1752.72/s  (2.340s,  437.68/s)  LR: 2.986e-05  Data: 0.020 (1.739)
Train: 191 [ 600/1171 ( 51%)]  Loss:  2.338063 (2.7880)  Time: 6.711s,  152.59/s  (2.345s,  436.72/s)  LR: 2.986e-05  Data: 6.149 (1.745)
Train: 191 [ 650/1171 ( 56%)]  Loss:  3.008670 (2.8037)  Time: 0.582s, 1760.32/s  (2.364s,  433.24/s)  LR: 2.986e-05  Data: 0.018 (1.765)
Train: 191 [ 700/1171 ( 60%)]  Loss:  2.714504 (2.7978)  Time: 7.587s,  134.97/s  (2.379s,  430.37/s)  LR: 2.986e-05  Data: 6.856 (1.781)
Train: 191 [ 750/1171 ( 64%)]  Loss:  2.906489 (2.8046)  Time: 0.584s, 1752.46/s  (2.378s,  430.53/s)  LR: 2.986e-05  Data: 0.019 (1.782)
Train: 191 [ 800/1171 ( 68%)]  Loss:  3.028430 (2.8177)  Time: 7.908s,  129.49/s  (2.382s,  429.96/s)  LR: 2.986e-05  Data: 7.229 (1.785)
Train: 191 [ 850/1171 ( 73%)]  Loss:  3.066564 (2.8316)  Time: 0.586s, 1747.40/s  (2.376s,  430.89/s)  LR: 2.986e-05  Data: 0.019 (1.780)
Train: 191 [ 900/1171 ( 77%)]  Loss:  2.893536 (2.8348)  Time: 5.429s,  188.61/s  (2.372s,  431.70/s)  LR: 2.986e-05  Data: 4.759 (1.776)
Train: 191 [ 950/1171 ( 81%)]  Loss:  2.591660 (2.8227)  Time: 0.583s, 1757.01/s  (2.361s,  433.80/s)  LR: 2.986e-05  Data: 0.021 (1.764)
Train: 191 [1000/1171 ( 85%)]  Loss:  2.663649 (2.8151)  Time: 6.528s,  156.86/s  (2.350s,  435.66/s)  LR: 2.986e-05  Data: 5.964 (1.754)
Train: 191 [1050/1171 ( 90%)]  Loss:  2.924868 (2.8201)  Time: 0.586s, 1746.91/s  (2.362s,  433.54/s)  LR: 2.986e-05  Data: 0.021 (1.764)
Train: 191 [1100/1171 ( 94%)]  Loss:  2.827333 (2.8204)  Time: 5.573s,  183.76/s  (2.369s,  432.25/s)  LR: 2.986e-05  Data: 4.806 (1.770)
Train: 191 [1150/1171 ( 98%)]  Loss:  3.037720 (2.8295)  Time: 0.586s, 1748.02/s  (2.368s,  432.45/s)  LR: 2.986e-05  Data: 0.018 (1.769)
Train: 191 [1170/1171 (100%)]  Loss:  3.037030 (2.8378)  Time: 0.566s, 1809.47/s  (2.368s,  432.50/s)  LR: 2.986e-05  Data: 0.000 (1.769)
Test: [   0/97]  Time: 13.342 (13.342)  Loss:  0.2591 (0.2591)  Acc@1: 97.0703 (97.0703)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.197 (3.068)  Loss:  0.4185 (0.3265)  Acc@1: 93.0664 (95.4676)  Acc@5: 98.2422 (98.9928)
Test: [  97/97]  Time: 0.120 (2.960)  Loss:  0.3074 (0.3395)  Acc@1: 94.7917 (94.9800)  Acc@5: 99.4048 (98.8710)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-190.pth.tar', 94.85999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 192 [   0/1171 (  0%)]  Loss:  2.757354 (2.7574)  Time: 10.172s,  100.67/s  (10.172s,  100.67/s)  LR: 2.784e-05  Data: 9.364 (9.364)
Train: 192 [  50/1171 (  4%)]  Loss:  2.674491 (2.7159)  Time: 0.827s, 1238.70/s  (2.253s,  454.57/s)  LR: 2.784e-05  Data: 0.162 (1.641)
Train: 192 [ 100/1171 (  9%)]  Loss:  2.543864 (2.6586)  Time: 2.742s,  373.47/s  (2.387s,  429.02/s)  LR: 2.784e-05  Data: 2.177 (1.769)
Train: 192 [ 150/1171 ( 13%)]  Loss:  2.757117 (2.6832)  Time: 2.841s,  360.47/s  (2.389s,  428.66/s)  LR: 2.784e-05  Data: 2.272 (1.775)
Train: 192 [ 200/1171 ( 17%)]  Loss:  3.214539 (2.7895)  Time: 2.974s,  344.32/s  (2.393s,  427.94/s)  LR: 2.784e-05  Data: 2.409 (1.779)
Train: 192 [ 250/1171 ( 21%)]  Loss:  2.593409 (2.7568)  Time: 2.867s,  357.19/s  (2.351s,  435.51/s)  LR: 2.784e-05  Data: 2.157 (1.741)
Train: 192 [ 300/1171 ( 26%)]  Loss:  2.957979 (2.7855)  Time: 0.589s, 1739.97/s  (2.337s,  438.12/s)  LR: 2.784e-05  Data: 0.019 (1.728)
Train: 192 [ 350/1171 ( 30%)]  Loss:  2.592787 (2.7614)  Time: 4.751s,  215.54/s  (2.332s,  439.11/s)  LR: 2.784e-05  Data: 4.177 (1.721)
Train: 192 [ 400/1171 ( 34%)]  Loss:  3.087431 (2.7977)  Time: 0.588s, 1740.88/s  (2.315s,  442.39/s)  LR: 2.784e-05  Data: 0.018 (1.703)
Train: 192 [ 450/1171 ( 38%)]  Loss:  2.685796 (2.7865)  Time: 6.244s,  164.00/s  (2.310s,  443.37/s)  LR: 2.784e-05  Data: 5.522 (1.698)
Train: 192 [ 500/1171 ( 43%)]  Loss:  2.849681 (2.7922)  Time: 0.586s, 1747.21/s  (2.334s,  438.81/s)  LR: 2.784e-05  Data: 0.019 (1.724)
Train: 192 [ 550/1171 ( 47%)]  Loss:  2.859117 (2.7978)  Time: 5.791s,  176.83/s  (2.373s,  431.44/s)  LR: 2.784e-05  Data: 5.200 (1.764)
Train: 192 [ 600/1171 ( 51%)]  Loss:  3.061148 (2.8181)  Time: 0.584s, 1754.51/s  (2.388s,  428.73/s)  LR: 2.784e-05  Data: 0.017 (1.780)
Train: 192 [ 650/1171 ( 56%)]  Loss:  3.151846 (2.8419)  Time: 0.588s, 1742.48/s  (2.405s,  425.84/s)  LR: 2.784e-05  Data: 0.019 (1.798)
Train: 192 [ 700/1171 ( 60%)]  Loss:  2.943206 (2.8487)  Time: 0.584s, 1752.43/s  (2.399s,  426.76/s)  LR: 2.784e-05  Data: 0.019 (1.794)
Train: 192 [ 750/1171 ( 64%)]  Loss:  2.569348 (2.8312)  Time: 0.585s, 1749.34/s  (2.396s,  427.31/s)  LR: 2.784e-05  Data: 0.017 (1.793)
Train: 192 [ 800/1171 ( 68%)]  Loss:  3.006528 (2.8415)  Time: 0.587s, 1743.78/s  (2.382s,  429.82/s)  LR: 2.784e-05  Data: 0.019 (1.780)
Train: 192 [ 850/1171 ( 73%)]  Loss:  2.695258 (2.8334)  Time: 0.585s, 1750.93/s  (2.394s,  427.80/s)  LR: 2.784e-05  Data: 0.018 (1.792)
Train: 192 [ 900/1171 ( 77%)]  Loss:  2.885871 (2.8361)  Time: 0.583s, 1757.63/s  (2.394s,  427.79/s)  LR: 2.784e-05  Data: 0.021 (1.792)
Train: 192 [ 950/1171 ( 81%)]  Loss:  3.202065 (2.8544)  Time: 0.588s, 1740.28/s  (2.400s,  426.73/s)  LR: 2.784e-05  Data: 0.019 (1.798)
Train: 192 [1000/1171 ( 85%)]  Loss:  2.579258 (2.8413)  Time: 0.583s, 1755.08/s  (2.390s,  428.40/s)  LR: 2.784e-05  Data: 0.021 (1.789)
Train: 192 [1050/1171 ( 90%)]  Loss:  2.252475 (2.8146)  Time: 3.011s,  340.11/s  (2.389s,  428.69/s)  LR: 2.784e-05  Data: 2.449 (1.788)
Train: 192 [1100/1171 ( 94%)]  Loss:  2.923633 (2.8193)  Time: 0.580s, 1764.57/s  (2.378s,  430.55/s)  LR: 2.784e-05  Data: 0.018 (1.777)
Train: 192 [1150/1171 ( 98%)]  Loss:  3.020586 (2.8277)  Time: 0.590s, 1735.01/s  (2.375s,  431.14/s)  LR: 2.784e-05  Data: 0.019 (1.774)
Train: 192 [1170/1171 (100%)]  Loss:  2.564308 (2.8172)  Time: 0.564s, 1814.02/s  (2.372s,  431.73/s)  LR: 2.784e-05  Data: 0.000 (1.771)
Test: [   0/97]  Time: 12.099 (12.099)  Loss:  0.2913 (0.2913)  Acc@1: 96.9727 (96.9727)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.198 (3.147)  Loss:  0.4252 (0.3602)  Acc@1: 93.4570 (95.3316)  Acc@5: 98.5352 (98.9737)
Test: [  97/97]  Time: 0.119 (3.060)  Loss:  0.3268 (0.3690)  Acc@1: 93.8988 (94.9250)  Acc@5: 99.4048 (98.8590)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-192.pth.tar', 94.92500001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-190.pth.tar', 94.85999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 193 [   0/1171 (  0%)]  Loss:  2.607219 (2.6072)  Time: 9.154s,  111.87/s  (9.154s,  111.87/s)  LR: 2.592e-05  Data: 8.475 (8.475)
Train: 193 [  50/1171 (  4%)]  Loss:  2.447003 (2.5271)  Time: 0.587s, 1744.41/s  (2.350s,  435.77/s)  LR: 2.592e-05  Data: 0.019 (1.754)
Train: 193 [ 100/1171 (  9%)]  Loss:  2.805738 (2.6200)  Time: 0.587s, 1743.06/s  (2.332s,  439.07/s)  LR: 2.592e-05  Data: 0.019 (1.742)
Train: 193 [ 150/1171 ( 13%)]  Loss:  3.072420 (2.7331)  Time: 0.585s, 1751.37/s  (2.278s,  449.49/s)  LR: 2.592e-05  Data: 0.021 (1.689)
Train: 193 [ 200/1171 ( 17%)]  Loss:  2.409353 (2.6683)  Time: 1.334s,  767.55/s  (2.268s,  451.46/s)  LR: 2.592e-05  Data: 0.287 (1.675)
Train: 193 [ 250/1171 ( 21%)]  Loss:  2.601033 (2.6571)  Time: 0.584s, 1752.39/s  (2.241s,  457.04/s)  LR: 2.592e-05  Data: 0.021 (1.650)
Train: 193 [ 300/1171 ( 26%)]  Loss:  3.280618 (2.7462)  Time: 1.300s,  787.58/s  (2.238s,  457.54/s)  LR: 2.592e-05  Data: 0.738 (1.645)
Train: 193 [ 350/1171 ( 30%)]  Loss:  3.299001 (2.8153)  Time: 0.581s, 1762.50/s  (2.273s,  450.49/s)  LR: 2.592e-05  Data: 0.019 (1.681)
Train: 193 [ 400/1171 ( 34%)]  Loss:  2.869695 (2.8213)  Time: 0.585s, 1749.89/s  (2.313s,  442.67/s)  LR: 2.592e-05  Data: 0.019 (1.723)
Train: 193 [ 450/1171 ( 38%)]  Loss:  2.945346 (2.8337)  Time: 0.583s, 1756.07/s  (2.322s,  441.00/s)  LR: 2.592e-05  Data: 0.018 (1.730)
Train: 193 [ 500/1171 ( 43%)]  Loss:  2.637925 (2.8159)  Time: 2.677s,  382.55/s  (2.343s,  437.09/s)  LR: 2.592e-05  Data: 2.027 (1.750)
Train: 193 [ 550/1171 ( 47%)]  Loss:  2.813928 (2.8158)  Time: 0.584s, 1754.14/s  (2.346s,  436.45/s)  LR: 2.592e-05  Data: 0.018 (1.753)
Train: 193 [ 600/1171 ( 51%)]  Loss:  2.935879 (2.8250)  Time: 4.832s,  211.93/s  (2.360s,  433.85/s)  LR: 2.592e-05  Data: 4.237 (1.766)
Train: 193 [ 650/1171 ( 56%)]  Loss:  2.919106 (2.8317)  Time: 0.587s, 1744.93/s  (2.352s,  435.43/s)  LR: 2.592e-05  Data: 0.019 (1.755)
Train: 193 [ 700/1171 ( 60%)]  Loss:  2.628658 (2.8182)  Time: 3.378s,  303.15/s  (2.371s,  431.93/s)  LR: 2.592e-05  Data: 2.702 (1.771)
Train: 193 [ 750/1171 ( 64%)]  Loss:  2.722079 (2.8122)  Time: 0.588s, 1740.88/s  (2.377s,  430.75/s)  LR: 2.592e-05  Data: 0.020 (1.777)
Train: 193 [ 800/1171 ( 68%)]  Loss:  2.969154 (2.8214)  Time: 4.063s,  252.04/s  (2.380s,  430.17/s)  LR: 2.592e-05  Data: 3.454 (1.779)
Train: 193 [ 850/1171 ( 73%)]  Loss:  3.007814 (2.8318)  Time: 0.858s, 1193.96/s  (2.382s,  429.90/s)  LR: 2.592e-05  Data: 0.270 (1.780)
Train: 193 [ 900/1171 ( 77%)]  Loss:  2.641653 (2.8218)  Time: 1.136s,  901.75/s  (2.376s,  430.99/s)  LR: 2.592e-05  Data: 0.459 (1.774)
Train: 193 [ 950/1171 ( 81%)]  Loss:  3.038750 (2.8326)  Time: 0.585s, 1751.52/s  (2.375s,  431.20/s)  LR: 2.592e-05  Data: 0.020 (1.773)
Train: 193 [1000/1171 ( 85%)]  Loss:  3.099591 (2.8453)  Time: 0.588s, 1742.17/s  (2.365s,  432.97/s)  LR: 2.592e-05  Data: 0.018 (1.763)
Train: 193 [1050/1171 ( 90%)]  Loss:  3.110873 (2.8574)  Time: 0.586s, 1747.17/s  (2.362s,  433.60/s)  LR: 2.592e-05  Data: 0.017 (1.759)
Train: 193 [1100/1171 ( 94%)]  Loss:  3.067892 (2.8666)  Time: 0.591s, 1733.12/s  (2.370s,  432.14/s)  LR: 2.592e-05  Data: 0.021 (1.767)
Train: 193 [1150/1171 ( 98%)]  Loss:  3.048208 (2.8741)  Time: 0.587s, 1743.92/s  (2.376s,  430.92/s)  LR: 2.592e-05  Data: 0.020 (1.775)
Train: 193 [1170/1171 (100%)]  Loss:  3.278320 (2.8903)  Time: 0.565s, 1813.66/s  (2.381s,  430.11/s)  LR: 2.592e-05  Data: 0.000 (1.779)
Test: [   0/97]  Time: 14.344 (14.344)  Loss:  0.2827 (0.2827)  Acc@1: 97.0703 (97.0703)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.200 (3.171)  Loss:  0.4249 (0.3480)  Acc@1: 93.3594 (95.4944)  Acc@5: 98.3398 (98.9775)
Test: [  97/97]  Time: 0.119 (3.109)  Loss:  0.3077 (0.3592)  Acc@1: 94.9405 (94.9950)  Acc@5: 99.1071 (98.8660)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-192.pth.tar', 94.92500001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-190.pth.tar', 94.85999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 194 [   0/1171 (  0%)]  Loss:  3.276244 (3.2762)  Time: 10.719s,   95.54/s  (10.719s,   95.54/s)  LR: 2.411e-05  Data: 9.695 (9.695)
Train: 194 [  50/1171 (  4%)]  Loss:  2.683336 (2.9798)  Time: 0.703s, 1457.41/s  (2.312s,  443.00/s)  LR: 2.411e-05  Data: 0.134 (1.710)
Train: 194 [ 100/1171 (  9%)]  Loss:  3.043861 (3.0011)  Time: 2.725s,  375.83/s  (2.277s,  449.73/s)  LR: 2.411e-05  Data: 2.080 (1.670)
Train: 194 [ 150/1171 ( 13%)]  Loss:  3.184123 (3.0469)  Time: 0.586s, 1746.67/s  (2.275s,  450.07/s)  LR: 2.411e-05  Data: 0.019 (1.672)
Train: 194 [ 200/1171 ( 17%)]  Loss:  2.917004 (3.0209)  Time: 3.152s,  324.84/s  (2.358s,  434.26/s)  LR: 2.411e-05  Data: 2.591 (1.753)
Train: 194 [ 250/1171 ( 21%)]  Loss:  2.897722 (3.0004)  Time: 0.582s, 1757.98/s  (2.365s,  432.91/s)  LR: 2.411e-05  Data: 0.018 (1.760)
Train: 194 [ 300/1171 ( 26%)]  Loss:  2.263815 (2.8952)  Time: 3.947s,  259.45/s  (2.386s,  429.13/s)  LR: 2.411e-05  Data: 3.373 (1.784)
Train: 194 [ 350/1171 ( 30%)]  Loss:  2.432518 (2.8373)  Time: 0.583s, 1755.39/s  (2.376s,  431.06/s)  LR: 2.411e-05  Data: 0.020 (1.774)
Train: 194 [ 400/1171 ( 34%)]  Loss:  2.804586 (2.8337)  Time: 5.959s,  171.83/s  (2.377s,  430.86/s)  LR: 2.411e-05  Data: 5.378 (1.775)
Train: 194 [ 450/1171 ( 38%)]  Loss:  3.023409 (2.8527)  Time: 0.588s, 1742.07/s  (2.362s,  433.58/s)  LR: 2.411e-05  Data: 0.022 (1.758)
Train: 194 [ 500/1171 ( 43%)]  Loss:  2.816257 (2.8494)  Time: 6.850s,  149.49/s  (2.359s,  434.07/s)  LR: 2.411e-05  Data: 6.194 (1.755)
Train: 194 [ 550/1171 ( 47%)]  Loss:  2.808813 (2.8460)  Time: 0.592s, 1730.91/s  (2.385s,  429.41/s)  LR: 2.411e-05  Data: 0.021 (1.782)
Train: 194 [ 600/1171 ( 51%)]  Loss:  3.136384 (2.8683)  Time: 8.059s,  127.07/s  (2.414s,  424.21/s)  LR: 2.411e-05  Data: 7.465 (1.813)
Train: 194 [ 650/1171 ( 56%)]  Loss:  2.707700 (2.8568)  Time: 0.583s, 1755.50/s  (2.420s,  423.09/s)  LR: 2.411e-05  Data: 0.019 (1.821)
Train: 194 [ 700/1171 ( 60%)]  Loss:  2.705938 (2.8468)  Time: 7.588s,  134.96/s  (2.425s,  422.35/s)  LR: 2.411e-05  Data: 6.997 (1.826)
Train: 194 [ 750/1171 ( 64%)]  Loss:  2.710761 (2.8383)  Time: 0.584s, 1752.42/s  (2.414s,  424.13/s)  LR: 2.411e-05  Data: 0.022 (1.817)
Train: 194 [ 800/1171 ( 68%)]  Loss:  2.867316 (2.8400)  Time: 8.049s,  127.22/s  (2.413s,  424.32/s)  LR: 2.411e-05  Data: 7.441 (1.817)
Train: 194 [ 850/1171 ( 73%)]  Loss:  2.738827 (2.8344)  Time: 0.584s, 1752.74/s  (2.400s,  426.71/s)  LR: 2.411e-05  Data: 0.022 (1.803)
Train: 194 [ 900/1171 ( 77%)]  Loss:  2.999631 (2.8431)  Time: 8.400s,  121.91/s  (2.415s,  423.99/s)  LR: 2.411e-05  Data: 7.820 (1.819)
Train: 194 [ 950/1171 ( 81%)]  Loss:  3.102527 (2.8560)  Time: 0.580s, 1765.54/s  (2.418s,  423.42/s)  LR: 2.411e-05  Data: 0.017 (1.823)
Train: 194 [1000/1171 ( 85%)]  Loss:  2.457056 (2.8370)  Time: 7.150s,  143.21/s  (2.423s,  422.54/s)  LR: 2.411e-05  Data: 6.531 (1.828)
Train: 194 [1050/1171 ( 90%)]  Loss:  2.933705 (2.8414)  Time: 0.670s, 1528.26/s  (2.419s,  423.26/s)  LR: 2.411e-05  Data: 0.018 (1.824)
Train: 194 [1100/1171 ( 94%)]  Loss:  2.713157 (2.8359)  Time: 6.356s,  161.12/s  (2.418s,  423.45/s)  LR: 2.411e-05  Data: 5.770 (1.823)
Train: 194 [1150/1171 ( 98%)]  Loss:  2.662082 (2.8286)  Time: 1.386s,  738.61/s  (2.413s,  424.40/s)  LR: 2.411e-05  Data: 0.820 (1.817)
Train: 194 [1170/1171 (100%)]  Loss:  3.193675 (2.8432)  Time: 0.566s, 1810.50/s  (2.410s,  424.87/s)  LR: 2.411e-05  Data: 0.000 (1.815)
Test: [   0/97]  Time: 13.018 (13.018)  Loss:  0.2981 (0.2981)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (3.075)  Loss:  0.4553 (0.3667)  Acc@1: 92.3828 (95.3278)  Acc@5: 98.3398 (98.9602)
Test: [  97/97]  Time: 0.120 (3.183)  Loss:  0.3314 (0.3779)  Acc@1: 94.9405 (94.8590)  Acc@5: 99.2560 (98.8540)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-192.pth.tar', 94.92500001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-190.pth.tar', 94.85999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-194.pth.tar', 94.85899999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 195 [   0/1171 (  0%)]  Loss:  3.090482 (3.0905)  Time: 12.628s,   81.09/s  (12.628s,   81.09/s)  LR: 2.241e-05  Data: 12.025 (12.025)
Train: 195 [  50/1171 (  4%)]  Loss:  2.759597 (2.9250)  Time: 0.582s, 1760.31/s  (2.599s,  394.00/s)  LR: 2.241e-05  Data: 0.019 (2.004)
Train: 195 [ 100/1171 (  9%)]  Loss:  2.745857 (2.8653)  Time: 0.583s, 1756.54/s  (2.551s,  401.47/s)  LR: 2.241e-05  Data: 0.018 (1.949)
Train: 195 [ 150/1171 ( 13%)]  Loss:  2.438399 (2.7586)  Time: 0.584s, 1753.52/s  (2.465s,  415.44/s)  LR: 2.241e-05  Data: 0.019 (1.868)
Train: 195 [ 200/1171 ( 17%)]  Loss:  3.211680 (2.8492)  Time: 0.588s, 1741.68/s  (2.451s,  417.74/s)  LR: 2.241e-05  Data: 0.017 (1.858)
Train: 195 [ 250/1171 ( 21%)]  Loss:  2.811298 (2.8429)  Time: 0.583s, 1756.75/s  (2.400s,  426.71/s)  LR: 2.241e-05  Data: 0.020 (1.804)
Train: 195 [ 300/1171 ( 26%)]  Loss:  3.123392 (2.8830)  Time: 0.597s, 1715.97/s  (2.385s,  429.39/s)  LR: 2.241e-05  Data: 0.032 (1.786)
Train: 195 [ 350/1171 ( 30%)]  Loss:  3.362775 (2.9429)  Time: 0.588s, 1740.80/s  (2.405s,  425.69/s)  LR: 2.241e-05  Data: 0.020 (1.809)
Train: 195 [ 400/1171 ( 34%)]  Loss:  2.537657 (2.8979)  Time: 0.588s, 1742.32/s  (2.420s,  423.23/s)  LR: 2.241e-05  Data: 0.019 (1.823)
Train: 195 [ 450/1171 ( 38%)]  Loss:  3.096517 (2.9178)  Time: 0.584s, 1754.85/s  (2.422s,  422.85/s)  LR: 2.241e-05  Data: 0.019 (1.827)
Train: 195 [ 500/1171 ( 43%)]  Loss:  2.993855 (2.9247)  Time: 0.587s, 1744.86/s  (2.432s,  421.14/s)  LR: 2.241e-05  Data: 0.018 (1.838)
Train: 195 [ 550/1171 ( 47%)]  Loss:  2.753706 (2.9104)  Time: 0.584s, 1752.67/s  (2.433s,  420.90/s)  LR: 2.241e-05  Data: 0.019 (1.838)
Train: 195 [ 600/1171 ( 51%)]  Loss:  2.967571 (2.9148)  Time: 0.587s, 1743.03/s  (2.445s,  418.77/s)  LR: 2.241e-05  Data: 0.018 (1.851)
Train: 195 [ 650/1171 ( 56%)]  Loss:  3.003865 (2.9212)  Time: 0.586s, 1747.04/s  (2.439s,  419.82/s)  LR: 2.241e-05  Data: 0.021 (1.846)
Train: 195 [ 700/1171 ( 60%)]  Loss:  2.814907 (2.9141)  Time: 0.585s, 1749.02/s  (2.456s,  416.91/s)  LR: 2.241e-05  Data: 0.022 (1.863)
Train: 195 [ 750/1171 ( 64%)]  Loss:  2.850452 (2.9101)  Time: 0.581s, 1763.48/s  (2.457s,  416.77/s)  LR: 2.241e-05  Data: 0.018 (1.864)
Train: 195 [ 800/1171 ( 68%)]  Loss:  2.968298 (2.9135)  Time: 0.585s, 1751.89/s  (2.464s,  415.55/s)  LR: 2.241e-05  Data: 0.021 (1.872)
Train: 195 [ 850/1171 ( 73%)]  Loss:  2.920020 (2.9139)  Time: 0.585s, 1751.32/s  (2.458s,  416.53/s)  LR: 2.241e-05  Data: 0.020 (1.867)
Train: 195 [ 900/1171 ( 77%)]  Loss:  2.696464 (2.9025)  Time: 0.586s, 1748.65/s  (2.467s,  415.15/s)  LR: 2.241e-05  Data: 0.021 (1.875)
Train: 195 [ 950/1171 ( 81%)]  Loss:  3.008875 (2.9078)  Time: 0.582s, 1760.44/s  (2.459s,  416.51/s)  LR: 2.241e-05  Data: 0.019 (1.868)
Train: 195 [1000/1171 ( 85%)]  Loss:  2.864349 (2.9057)  Time: 0.590s, 1736.02/s  (2.452s,  417.60/s)  LR: 2.241e-05  Data: 0.018 (1.862)
Train: 195 [1050/1171 ( 90%)]  Loss:  2.829859 (2.9023)  Time: 0.583s, 1756.26/s  (2.440s,  419.69/s)  LR: 2.241e-05  Data: 0.018 (1.850)
Train: 195 [1100/1171 ( 94%)]  Loss:  3.080004 (2.9100)  Time: 0.587s, 1744.81/s  (2.453s,  417.49/s)  LR: 2.241e-05  Data: 0.020 (1.863)
Train: 195 [1150/1171 ( 98%)]  Loss:  2.653655 (2.8993)  Time: 0.587s, 1744.58/s  (2.453s,  417.49/s)  LR: 2.241e-05  Data: 0.025 (1.863)
Train: 195 [1170/1171 (100%)]  Loss:  3.062114 (2.9058)  Time: 0.565s, 1811.39/s  (2.456s,  416.91/s)  LR: 2.241e-05  Data: 0.000 (1.867)
Test: [   0/97]  Time: 13.990 (13.990)  Loss:  0.2745 (0.2745)  Acc@1: 97.1680 (97.1680)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.335 (3.186)  Loss:  0.4442 (0.3553)  Acc@1: 93.0664 (95.4293)  Acc@5: 98.2422 (98.9871)
Test: [  97/97]  Time: 0.120 (3.126)  Loss:  0.3325 (0.3656)  Acc@1: 94.6429 (94.9610)  Acc@5: 99.4048 (98.8660)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-195.pth.tar', 94.96100002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-192.pth.tar', 94.92500001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-190.pth.tar', 94.85999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-194.pth.tar', 94.85899999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 196 [   0/1171 (  0%)]  Loss:  2.538856 (2.5389)  Time: 10.463s,   97.87/s  (10.463s,   97.87/s)  LR: 2.082e-05  Data: 9.777 (9.777)
Train: 196 [  50/1171 (  4%)]  Loss:  2.704439 (2.6216)  Time: 0.585s, 1751.63/s  (2.344s,  436.84/s)  LR: 2.082e-05  Data: 0.022 (1.749)
Train: 196 [ 100/1171 (  9%)]  Loss:  2.547021 (2.5968)  Time: 0.589s, 1738.36/s  (2.317s,  441.88/s)  LR: 2.082e-05  Data: 0.021 (1.718)
Train: 196 [ 150/1171 ( 13%)]  Loss:  2.795496 (2.6465)  Time: 0.589s, 1737.58/s  (2.401s,  426.57/s)  LR: 2.082e-05  Data: 0.021 (1.802)
Train: 196 [ 200/1171 ( 17%)]  Loss:  2.777804 (2.6727)  Time: 2.373s,  431.51/s  (2.443s,  419.24/s)  LR: 2.082e-05  Data: 1.696 (1.840)
Train: 196 [ 250/1171 ( 21%)]  Loss:  3.045993 (2.7349)  Time: 0.584s, 1754.11/s  (2.444s,  419.00/s)  LR: 2.082e-05  Data: 0.020 (1.843)
Train: 196 [ 300/1171 ( 26%)]  Loss:  2.688592 (2.7283)  Time: 0.582s, 1758.74/s  (2.466s,  415.23/s)  LR: 2.082e-05  Data: 0.019 (1.866)
Train: 196 [ 350/1171 ( 30%)]  Loss:  2.714349 (2.7266)  Time: 0.584s, 1754.04/s  (2.440s,  419.71/s)  LR: 2.082e-05  Data: 0.019 (1.842)
Train: 196 [ 400/1171 ( 34%)]  Loss:  2.351994 (2.6849)  Time: 0.587s, 1744.62/s  (2.431s,  421.22/s)  LR: 2.082e-05  Data: 0.021 (1.835)
Train: 196 [ 450/1171 ( 38%)]  Loss:  2.711137 (2.6876)  Time: 0.581s, 1761.26/s  (2.417s,  423.74/s)  LR: 2.082e-05  Data: 0.019 (1.823)
Train: 196 [ 500/1171 ( 43%)]  Loss:  3.008651 (2.7168)  Time: 0.584s, 1752.34/s  (2.435s,  420.55/s)  LR: 2.082e-05  Data: 0.017 (1.843)
Train: 196 [ 550/1171 ( 47%)]  Loss:  3.050252 (2.7445)  Time: 0.585s, 1750.62/s  (2.458s,  416.64/s)  LR: 2.082e-05  Data: 0.020 (1.867)
Train: 196 [ 600/1171 ( 51%)]  Loss:  2.733640 (2.7437)  Time: 2.975s,  344.24/s  (2.488s,  411.56/s)  LR: 2.082e-05  Data: 2.319 (1.897)
Train: 196 [ 650/1171 ( 56%)]  Loss:  3.104232 (2.7695)  Time: 0.583s, 1757.60/s  (2.481s,  412.66/s)  LR: 2.082e-05  Data: 0.020 (1.890)
Train: 196 [ 700/1171 ( 60%)]  Loss:  2.893685 (2.7777)  Time: 0.586s, 1747.16/s  (2.481s,  412.77/s)  LR: 2.082e-05  Data: 0.018 (1.887)
Train: 196 [ 750/1171 ( 64%)]  Loss:  2.830699 (2.7811)  Time: 0.589s, 1739.30/s  (2.466s,  415.24/s)  LR: 2.082e-05  Data: 0.017 (1.873)
Train: 196 [ 800/1171 ( 68%)]  Loss:  2.905937 (2.7884)  Time: 0.588s, 1740.15/s  (2.461s,  416.08/s)  LR: 2.082e-05  Data: 0.024 (1.869)
Train: 196 [ 850/1171 ( 73%)]  Loss:  3.152717 (2.8086)  Time: 0.585s, 1750.89/s  (2.447s,  418.54/s)  LR: 2.082e-05  Data: 0.018 (1.854)
Train: 196 [ 900/1171 ( 77%)]  Loss:  2.748855 (2.8055)  Time: 1.379s,  742.44/s  (2.463s,  415.73/s)  LR: 2.082e-05  Data: 0.818 (1.869)
Train: 196 [ 950/1171 ( 81%)]  Loss:  3.010503 (2.8157)  Time: 0.583s, 1755.84/s  (2.464s,  415.62/s)  LR: 2.082e-05  Data: 0.018 (1.869)
Train: 196 [1000/1171 ( 85%)]  Loss:  3.190553 (2.8336)  Time: 1.647s,  621.68/s  (2.465s,  415.48/s)  LR: 2.082e-05  Data: 0.994 (1.870)
Train: 196 [1050/1171 ( 90%)]  Loss:  2.439920 (2.8157)  Time: 0.586s, 1747.30/s  (2.460s,  416.26/s)  LR: 2.082e-05  Data: 0.019 (1.866)
Train: 196 [1100/1171 ( 94%)]  Loss:  2.898382 (2.8193)  Time: 3.992s,  256.48/s  (2.460s,  416.20/s)  LR: 2.082e-05  Data: 3.417 (1.864)
Train: 196 [1150/1171 ( 98%)]  Loss:  2.959733 (2.8251)  Time: 0.584s, 1752.35/s  (2.449s,  418.06/s)  LR: 2.082e-05  Data: 0.020 (1.853)
Train: 196 [1170/1171 (100%)]  Loss:  2.683006 (2.8195)  Time: 0.566s, 1810.30/s  (2.446s,  418.61/s)  LR: 2.082e-05  Data: 0.000 (1.850)
Test: [   0/97]  Time: 13.116 (13.116)  Loss:  0.2637 (0.2637)  Acc@1: 96.9727 (96.9727)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (3.259)  Loss:  0.4225 (0.3333)  Acc@1: 93.1641 (95.4389)  Acc@5: 98.4375 (98.9717)
Test: [  97/97]  Time: 0.120 (3.202)  Loss:  0.3084 (0.3442)  Acc@1: 95.0893 (94.9770)  Acc@5: 99.4048 (98.8590)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-196.pth.tar', 94.97700000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-195.pth.tar', 94.96100002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-192.pth.tar', 94.92500001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-190.pth.tar', 94.85999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-194.pth.tar', 94.85899999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 197 [   0/1171 (  0%)]  Loss:  2.592399 (2.5924)  Time: 11.015s,   92.96/s  (11.015s,   92.96/s)  LR: 1.933e-05  Data: 9.918 (9.918)
Train: 197 [  50/1171 (  4%)]  Loss:  2.624455 (2.6084)  Time: 0.585s, 1749.15/s  (2.494s,  410.53/s)  LR: 1.933e-05  Data: 0.019 (1.893)
Train: 197 [ 100/1171 (  9%)]  Loss:  2.969435 (2.7288)  Time: 0.586s, 1746.96/s  (2.442s,  419.37/s)  LR: 1.933e-05  Data: 0.019 (1.848)
Train: 197 [ 150/1171 ( 13%)]  Loss:  2.795016 (2.7453)  Time: 0.586s, 1747.31/s  (2.357s,  434.39/s)  LR: 1.933e-05  Data: 0.018 (1.758)
Train: 197 [ 200/1171 ( 17%)]  Loss:  2.671180 (2.7305)  Time: 1.724s,  594.14/s  (2.337s,  438.22/s)  LR: 1.933e-05  Data: 1.058 (1.738)
Train: 197 [ 250/1171 ( 21%)]  Loss:  2.590897 (2.7072)  Time: 0.590s, 1735.83/s  (2.306s,  444.12/s)  LR: 1.933e-05  Data: 0.018 (1.706)
Train: 197 [ 300/1171 ( 26%)]  Loss:  2.653860 (2.6996)  Time: 2.060s,  497.19/s  (2.299s,  445.44/s)  LR: 1.933e-05  Data: 1.352 (1.699)
Train: 197 [ 350/1171 ( 30%)]  Loss:  2.665323 (2.6953)  Time: 0.583s, 1755.51/s  (2.343s,  436.98/s)  LR: 1.933e-05  Data: 0.018 (1.741)
Train: 197 [ 400/1171 ( 34%)]  Loss:  2.700893 (2.6959)  Time: 1.592s,  643.24/s  (2.359s,  434.16/s)  LR: 1.933e-05  Data: 0.780 (1.755)
Train: 197 [ 450/1171 ( 38%)]  Loss:  2.818030 (2.7081)  Time: 0.584s, 1753.27/s  (2.362s,  433.56/s)  LR: 1.933e-05  Data: 0.019 (1.758)
Train: 197 [ 500/1171 ( 43%)]  Loss:  2.542093 (2.6931)  Time: 3.130s,  327.16/s  (2.381s,  430.14/s)  LR: 1.933e-05  Data: 2.483 (1.777)
Train: 197 [ 550/1171 ( 47%)]  Loss:  2.980069 (2.7170)  Time: 0.583s, 1755.66/s  (2.379s,  430.49/s)  LR: 1.933e-05  Data: 0.019 (1.775)
Train: 197 [ 600/1171 ( 51%)]  Loss:  3.146772 (2.7500)  Time: 3.142s,  325.91/s  (2.383s,  429.74/s)  LR: 1.933e-05  Data: 2.580 (1.780)
Train: 197 [ 650/1171 ( 56%)]  Loss:  2.940062 (2.7636)  Time: 0.584s, 1753.76/s  (2.381s,  430.03/s)  LR: 1.933e-05  Data: 0.022 (1.778)
Train: 197 [ 700/1171 ( 60%)]  Loss:  3.023951 (2.7810)  Time: 1.642s,  623.50/s  (2.407s,  425.44/s)  LR: 1.933e-05  Data: 1.021 (1.804)
Train: 197 [ 750/1171 ( 64%)]  Loss:  3.317405 (2.8145)  Time: 0.590s, 1736.42/s  (2.403s,  426.18/s)  LR: 1.933e-05  Data: 0.025 (1.799)
Train: 197 [ 800/1171 ( 68%)]  Loss:  2.663409 (2.8056)  Time: 3.388s,  302.25/s  (2.409s,  425.08/s)  LR: 1.933e-05  Data: 2.820 (1.806)
Train: 197 [ 850/1171 ( 73%)]  Loss:  2.883797 (2.8099)  Time: 0.589s, 1738.34/s  (2.403s,  426.07/s)  LR: 1.933e-05  Data: 0.025 (1.801)
Train: 197 [ 900/1171 ( 77%)]  Loss:  2.900966 (2.8147)  Time: 3.988s,  256.76/s  (2.407s,  425.39/s)  LR: 1.933e-05  Data: 3.329 (1.803)
Train: 197 [ 950/1171 ( 81%)]  Loss:  2.977452 (2.8229)  Time: 0.588s, 1741.04/s  (2.397s,  427.23/s)  LR: 1.933e-05  Data: 0.020 (1.793)
Train: 197 [1000/1171 ( 85%)]  Loss:  3.123089 (2.8372)  Time: 7.466s,  137.16/s  (2.393s,  427.85/s)  LR: 1.933e-05  Data: 6.887 (1.790)
Train: 197 [1050/1171 ( 90%)]  Loss:  2.735927 (2.8326)  Time: 0.587s, 1744.08/s  (2.384s,  429.55/s)  LR: 1.933e-05  Data: 0.023 (1.782)
Train: 197 [1100/1171 ( 94%)]  Loss:  3.299225 (2.8529)  Time: 6.495s,  157.65/s  (2.396s,  427.38/s)  LR: 1.933e-05  Data: 5.934 (1.794)
Train: 197 [1150/1171 ( 98%)]  Loss:  3.006122 (2.8592)  Time: 0.585s, 1749.42/s  (2.393s,  427.96/s)  LR: 1.933e-05  Data: 0.020 (1.791)
Train: 197 [1170/1171 (100%)]  Loss:  2.571126 (2.8477)  Time: 0.564s, 1815.23/s  (2.392s,  428.11/s)  LR: 1.933e-05  Data: 0.000 (1.790)
Test: [   0/97]  Time: 12.847 (12.847)  Loss:  0.2881 (0.2881)  Acc@1: 96.9727 (96.9727)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (3.074)  Loss:  0.4491 (0.3504)  Acc@1: 92.5781 (95.4561)  Acc@5: 98.4375 (98.9775)
Test: [  97/97]  Time: 0.120 (3.014)  Loss:  0.3492 (0.3622)  Acc@1: 93.7500 (95.0000)  Acc@5: 99.1071 (98.8600)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-197.pth.tar', 95.0)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-196.pth.tar', 94.97700000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-195.pth.tar', 94.96100002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-192.pth.tar', 94.92500001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-190.pth.tar', 94.85999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-194.pth.tar', 94.85899999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-188.pth.tar', 94.80599998291015)

Train: 198 [   0/1171 (  0%)]  Loss:  2.611213 (2.6112)  Time: 10.270s,   99.71/s  (10.270s,   99.71/s)  LR: 1.795e-05  Data: 9.405 (9.405)
Train: 198 [  50/1171 (  4%)]  Loss:  2.317712 (2.4645)  Time: 0.584s, 1752.07/s  (2.282s,  448.75/s)  LR: 1.795e-05  Data: 0.019 (1.685)
Train: 198 [ 100/1171 (  9%)]  Loss:  2.377376 (2.4354)  Time: 0.587s, 1744.24/s  (2.217s,  461.92/s)  LR: 1.795e-05  Data: 0.019 (1.620)
Train: 198 [ 150/1171 ( 13%)]  Loss:  2.704504 (2.5027)  Time: 0.583s, 1755.46/s  (2.247s,  455.73/s)  LR: 1.795e-05  Data: 0.021 (1.643)
Train: 198 [ 200/1171 ( 17%)]  Loss:  2.813316 (2.5648)  Time: 0.591s, 1732.78/s  (2.307s,  443.79/s)  LR: 1.795e-05  Data: 0.019 (1.703)
Train: 198 [ 250/1171 ( 21%)]  Loss:  2.995081 (2.6365)  Time: 0.584s, 1753.46/s  (2.329s,  439.60/s)  LR: 1.795e-05  Data: 0.021 (1.724)
Train: 198 [ 300/1171 ( 26%)]  Loss:  2.569782 (2.6270)  Time: 0.585s, 1749.24/s  (2.330s,  439.41/s)  LR: 1.795e-05  Data: 0.018 (1.722)
Train: 198 [ 350/1171 ( 30%)]  Loss:  2.962368 (2.6689)  Time: 0.588s, 1742.73/s  (2.316s,  442.11/s)  LR: 1.795e-05  Data: 0.022 (1.709)
Train: 198 [ 400/1171 ( 34%)]  Loss:  2.885176 (2.6929)  Time: 0.590s, 1735.01/s  (2.313s,  442.77/s)  LR: 1.795e-05  Data: 0.019 (1.706)
Train: 198 [ 450/1171 ( 38%)]  Loss:  2.594854 (2.6831)  Time: 0.583s, 1757.62/s  (2.303s,  444.56/s)  LR: 1.795e-05  Data: 0.021 (1.697)
Train: 198 [ 500/1171 ( 43%)]  Loss:  2.685844 (2.6834)  Time: 0.585s, 1751.10/s  (2.295s,  446.21/s)  LR: 1.795e-05  Data: 0.022 (1.690)
Train: 198 [ 550/1171 ( 47%)]  Loss:  2.472728 (2.6658)  Time: 0.584s, 1753.58/s  (2.341s,  437.39/s)  LR: 1.795e-05  Data: 0.018 (1.738)
Train: 198 [ 600/1171 ( 51%)]  Loss:  2.836690 (2.6790)  Time: 0.582s, 1758.81/s  (2.357s,  434.54/s)  LR: 1.795e-05  Data: 0.019 (1.755)
Train: 198 [ 650/1171 ( 56%)]  Loss:  2.748954 (2.6840)  Time: 0.590s, 1737.04/s  (2.373s,  431.43/s)  LR: 1.795e-05  Data: 0.019 (1.773)
Train: 198 [ 700/1171 ( 60%)]  Loss:  2.868241 (2.6963)  Time: 0.584s, 1753.72/s  (2.367s,  432.64/s)  LR: 1.795e-05  Data: 0.019 (1.768)
Train: 198 [ 750/1171 ( 64%)]  Loss:  2.815927 (2.7037)  Time: 0.588s, 1740.47/s  (2.368s,  432.42/s)  LR: 1.795e-05  Data: 0.020 (1.770)
Train: 198 [ 800/1171 ( 68%)]  Loss:  2.884455 (2.7144)  Time: 0.588s, 1742.68/s  (2.358s,  434.32/s)  LR: 1.795e-05  Data: 0.022 (1.760)
Train: 198 [ 850/1171 ( 73%)]  Loss:  2.868487 (2.7229)  Time: 0.584s, 1754.76/s  (2.355s,  434.81/s)  LR: 1.795e-05  Data: 0.019 (1.758)
Train: 198 [ 900/1171 ( 77%)]  Loss:  2.858419 (2.7301)  Time: 0.586s, 1747.75/s  (2.344s,  436.90/s)  LR: 1.795e-05  Data: 0.021 (1.748)
Train: 198 [ 950/1171 ( 81%)]  Loss:  2.312199 (2.7092)  Time: 0.584s, 1752.30/s  (2.363s,  433.41/s)  LR: 1.795e-05  Data: 0.021 (1.768)
Train: 198 [1000/1171 ( 85%)]  Loss:  3.150717 (2.7302)  Time: 3.033s,  337.57/s  (2.364s,  433.17/s)  LR: 1.795e-05  Data: 2.382 (1.769)
Train: 198 [1050/1171 ( 90%)]  Loss:  2.804540 (2.7336)  Time: 0.585s, 1750.12/s  (2.363s,  433.34/s)  LR: 1.795e-05  Data: 0.018 (1.768)
Train: 198 [1100/1171 ( 94%)]  Loss:  3.180715 (2.7530)  Time: 6.081s,  168.41/s  (2.362s,  433.57/s)  LR: 1.795e-05  Data: 5.424 (1.766)
Train: 198 [1150/1171 ( 98%)]  Loss:  2.414334 (2.7389)  Time: 0.588s, 1741.88/s  (2.372s,  431.73/s)  LR: 1.795e-05  Data: 0.019 (1.776)
Train: 198 [1170/1171 (100%)]  Loss:  2.674191 (2.7363)  Time: 0.565s, 1811.48/s  (2.371s,  431.86/s)  LR: 1.795e-05  Data: 0.000 (1.775)
Test: [   0/97]  Time: 12.375 (12.375)  Loss:  0.2718 (0.2718)  Acc@1: 97.0703 (97.0703)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.197 (2.988)  Loss:  0.4206 (0.3390)  Acc@1: 92.9688 (95.4925)  Acc@5: 98.3398 (98.9756)
Test: [  97/97]  Time: 0.120 (3.072)  Loss:  0.3165 (0.3524)  Acc@1: 94.4940 (94.9930)  Acc@5: 99.1071 (98.8530)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-197.pth.tar', 95.0)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-198.pth.tar', 94.99300000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-196.pth.tar', 94.97700000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-195.pth.tar', 94.96100002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-192.pth.tar', 94.92500001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-190.pth.tar', 94.85999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-194.pth.tar', 94.85899999511719)

Train: 199 [   0/1171 (  0%)]  Loss:  2.673727 (2.6737)  Time: 12.478s,   82.07/s  (12.478s,   82.07/s)  LR: 1.669e-05  Data: 11.623 (11.623)
Train: 199 [  50/1171 (  4%)]  Loss:  2.146018 (2.4099)  Time: 0.584s, 1754.57/s  (2.494s,  410.57/s)  LR: 1.669e-05  Data: 0.020 (1.895)
Train: 199 [ 100/1171 (  9%)]  Loss:  2.726191 (2.5153)  Time: 0.587s, 1744.82/s  (2.417s,  423.63/s)  LR: 1.669e-05  Data: 0.022 (1.823)
Train: 199 [ 150/1171 ( 13%)]  Loss:  2.612074 (2.5395)  Time: 0.583s, 1755.47/s  (2.370s,  432.06/s)  LR: 1.669e-05  Data: 0.022 (1.780)
Train: 199 [ 200/1171 ( 17%)]  Loss:  2.792028 (2.5900)  Time: 0.585s, 1748.98/s  (2.361s,  433.71/s)  LR: 1.669e-05  Data: 0.019 (1.766)
Train: 199 [ 250/1171 ( 21%)]  Loss:  2.397782 (2.5580)  Time: 0.585s, 1751.62/s  (2.323s,  440.79/s)  LR: 1.669e-05  Data: 0.021 (1.728)
Train: 199 [ 300/1171 ( 26%)]  Loss:  2.924757 (2.6104)  Time: 0.586s, 1748.33/s  (2.322s,  440.96/s)  LR: 1.669e-05  Data: 0.021 (1.725)
Train: 199 [ 350/1171 ( 30%)]  Loss:  2.782051 (2.6318)  Time: 2.321s,  441.27/s  (2.300s,  445.17/s)  LR: 1.669e-05  Data: 1.652 (1.698)
Train: 199 [ 400/1171 ( 34%)]  Loss:  2.208042 (2.5847)  Time: 0.589s, 1737.15/s  (2.351s,  435.50/s)  LR: 1.669e-05  Data: 0.026 (1.747)
Train: 199 [ 450/1171 ( 38%)]  Loss:  3.185899 (2.6449)  Time: 1.666s,  614.70/s  (2.353s,  435.20/s)  LR: 1.669e-05  Data: 1.070 (1.748)
Train: 199 [ 500/1171 ( 43%)]  Loss:  3.321429 (2.7064)  Time: 0.590s, 1735.31/s  (2.372s,  431.72/s)  LR: 1.669e-05  Data: 0.021 (1.768)
Train: 199 [ 550/1171 ( 47%)]  Loss:  2.353590 (2.6770)  Time: 0.586s, 1746.20/s  (2.358s,  434.35/s)  LR: 1.669e-05  Data: 0.022 (1.755)
Train: 199 [ 600/1171 ( 51%)]  Loss:  3.124726 (2.7114)  Time: 0.585s, 1751.75/s  (2.346s,  436.40/s)  LR: 1.669e-05  Data: 0.020 (1.744)
Train: 199 [ 650/1171 ( 56%)]  Loss:  2.579110 (2.7020)  Time: 2.816s,  363.68/s  (2.336s,  438.43/s)  LR: 1.669e-05  Data: 2.166 (1.733)
Train: 199 [ 700/1171 ( 60%)]  Loss:  3.291641 (2.7413)  Time: 0.590s, 1734.86/s  (2.335s,  438.56/s)  LR: 1.669e-05  Data: 0.019 (1.733)
Train: 199 [ 750/1171 ( 64%)]  Loss:  2.744449 (2.7415)  Time: 3.901s,  262.53/s  (2.372s,  431.69/s)  LR: 1.669e-05  Data: 3.246 (1.769)
Train: 199 [ 800/1171 ( 68%)]  Loss:  2.348467 (2.7184)  Time: 0.586s, 1748.11/s  (2.374s,  431.42/s)  LR: 1.669e-05  Data: 0.020 (1.770)
Train: 199 [ 850/1171 ( 73%)]  Loss:  2.650506 (2.7146)  Time: 5.222s,  196.10/s  (2.384s,  429.51/s)  LR: 1.669e-05  Data: 4.618 (1.780)
Train: 199 [ 900/1171 ( 77%)]  Loss:  2.803068 (2.7192)  Time: 0.586s, 1747.44/s  (2.385s,  429.33/s)  LR: 1.669e-05  Data: 0.020 (1.781)
Train: 199 [ 950/1171 ( 81%)]  Loss:  2.976591 (2.7321)  Time: 3.175s,  322.55/s  (2.386s,  429.24/s)  LR: 1.669e-05  Data: 2.472 (1.782)
Train: 199 [1000/1171 ( 85%)]  Loss:  2.911507 (2.7406)  Time: 0.582s, 1758.02/s  (2.383s,  429.73/s)  LR: 1.669e-05  Data: 0.019 (1.780)
Train: 199 [1050/1171 ( 90%)]  Loss:  2.643841 (2.7362)  Time: 4.980s,  205.61/s  (2.380s,  430.34/s)  LR: 1.669e-05  Data: 4.356 (1.777)
Train: 199 [1100/1171 ( 94%)]  Loss:  3.240180 (2.7582)  Time: 0.940s, 1089.06/s  (2.373s,  431.54/s)  LR: 1.669e-05  Data: 0.363 (1.770)
Train: 199 [1150/1171 ( 98%)]  Loss:  3.195984 (2.7764)  Time: 6.478s,  158.08/s  (2.391s,  428.23/s)  LR: 1.669e-05  Data: 5.894 (1.790)
Train: 199 [1170/1171 (100%)]  Loss:  2.682112 (2.7726)  Time: 0.563s, 1819.52/s  (2.387s,  428.93/s)  LR: 1.669e-05  Data: 0.000 (1.785)
Test: [   0/97]  Time: 14.757 (14.757)  Loss:  0.2942 (0.2942)  Acc@1: 96.8750 (96.8750)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (3.302)  Loss:  0.4233 (0.3545)  Acc@1: 93.3594 (95.3699)  Acc@5: 98.3398 (98.9602)
Test: [  97/97]  Time: 0.120 (3.258)  Loss:  0.3159 (0.3621)  Acc@1: 94.9405 (94.9860)  Acc@5: 99.4048 (98.8520)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-197.pth.tar', 95.0)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-198.pth.tar', 94.99300000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-199.pth.tar', 94.98600004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-196.pth.tar', 94.97700000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-195.pth.tar', 94.96100002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-192.pth.tar', 94.92500001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-190.pth.tar', 94.85999998535156)

Train: 200 [   0/1171 (  0%)]  Loss:  2.390730 (2.3907)  Time: 11.404s,   89.79/s  (11.404s,   89.79/s)  LR: 1.553e-05  Data: 10.741 (10.741)
Train: 200 [  50/1171 (  4%)]  Loss:  3.050379 (2.7206)  Time: 0.585s, 1751.12/s  (2.493s,  410.80/s)  LR: 1.553e-05  Data: 0.023 (1.911)
Train: 200 [ 100/1171 (  9%)]  Loss:  3.105497 (2.8489)  Time: 0.582s, 1758.95/s  (2.404s,  425.89/s)  LR: 1.553e-05  Data: 0.019 (1.820)
Train: 200 [ 150/1171 ( 13%)]  Loss:  2.509972 (2.7641)  Time: 0.584s, 1753.58/s  (2.331s,  439.21/s)  LR: 1.553e-05  Data: 0.021 (1.743)
Train: 200 [ 200/1171 ( 17%)]  Loss:  2.912623 (2.7938)  Time: 0.589s, 1739.16/s  (2.413s,  424.43/s)  LR: 1.553e-05  Data: 0.025 (1.824)
Train: 200 [ 250/1171 ( 21%)]  Loss:  3.113818 (2.8472)  Time: 0.583s, 1757.31/s  (2.394s,  427.67/s)  LR: 1.553e-05  Data: 0.019 (1.808)
Train: 200 [ 300/1171 ( 26%)]  Loss:  2.991687 (2.8678)  Time: 0.586s, 1746.92/s  (2.477s,  413.42/s)  LR: 1.553e-05  Data: 0.021 (1.889)
Train: 200 [ 350/1171 ( 30%)]  Loss:  2.603539 (2.8348)  Time: 0.583s, 1755.47/s  (2.457s,  416.69/s)  LR: 1.553e-05  Data: 0.018 (1.871)
Train: 200 [ 400/1171 ( 34%)]  Loss:  2.881740 (2.8400)  Time: 3.824s,  267.76/s  (2.551s,  401.33/s)  LR: 1.553e-05  Data: 3.249 (1.964)
Train: 200 [ 450/1171 ( 38%)]  Loss:  2.694100 (2.8254)  Time: 0.582s, 1759.35/s  (2.515s,  407.12/s)  LR: 1.553e-05  Data: 0.019 (1.928)
Train: 200 [ 500/1171 ( 43%)]  Loss:  2.997429 (2.8410)  Time: 4.479s,  228.62/s  (2.506s,  408.66/s)  LR: 1.553e-05  Data: 3.914 (1.917)
Train: 200 [ 550/1171 ( 47%)]  Loss:  2.746532 (2.8332)  Time: 0.581s, 1761.45/s  (2.525s,  405.50/s)  LR: 1.553e-05  Data: 0.018 (1.937)
Train: 200 [ 600/1171 ( 51%)]  Loss:  2.663436 (2.8201)  Time: 4.972s,  205.94/s  (2.539s,  403.37/s)  LR: 1.553e-05  Data: 4.089 (1.948)
Train: 200 [ 650/1171 ( 56%)]  Loss:  2.809142 (2.8193)  Time: 0.584s, 1753.83/s  (2.527s,  405.24/s)  LR: 1.553e-05  Data: 0.022 (1.937)
Train: 200 [ 700/1171 ( 60%)]  Loss:  2.904330 (2.8250)  Time: 4.137s,  247.50/s  (2.526s,  405.45/s)  LR: 1.553e-05  Data: 3.467 (1.935)
Train: 200 [ 750/1171 ( 64%)]  Loss:  3.048836 (2.8390)  Time: 0.586s, 1746.90/s  (2.511s,  407.78/s)  LR: 1.553e-05  Data: 0.020 (1.919)
Train: 200 [ 800/1171 ( 68%)]  Loss:  2.567142 (2.8230)  Time: 4.054s,  252.61/s  (2.503s,  409.06/s)  LR: 1.553e-05  Data: 3.453 (1.911)
Train: 200 [ 850/1171 ( 73%)]  Loss:  2.957050 (2.8304)  Time: 0.586s, 1747.11/s  (2.488s,  411.51/s)  LR: 1.553e-05  Data: 0.020 (1.895)
Train: 200 [ 900/1171 ( 77%)]  Loss:  3.156488 (2.8476)  Time: 4.716s,  217.12/s  (2.487s,  411.77/s)  LR: 1.553e-05  Data: 4.075 (1.893)
Train: 200 [ 950/1171 ( 81%)]  Loss:  3.170898 (2.8638)  Time: 0.584s, 1752.90/s  (2.499s,  409.78/s)  LR: 1.553e-05  Data: 0.019 (1.905)
Train: 200 [1000/1171 ( 85%)]  Loss:  2.731796 (2.8575)  Time: 1.828s,  560.29/s  (2.508s,  408.25/s)  LR: 1.553e-05  Data: 1.084 (1.914)
Train: 200 [1050/1171 ( 90%)]  Loss:  2.615597 (2.8465)  Time: 0.587s, 1745.31/s  (2.503s,  409.08/s)  LR: 1.553e-05  Data: 0.019 (1.909)
Train: 200 [1100/1171 ( 94%)]  Loss:  2.569525 (2.8344)  Time: 4.121s,  248.51/s  (2.503s,  409.16/s)  LR: 1.553e-05  Data: 3.555 (1.909)
Train: 200 [1150/1171 ( 98%)]  Loss:  2.875116 (2.8361)  Time: 0.589s, 1739.70/s  (2.509s,  408.15/s)  LR: 1.553e-05  Data: 0.018 (1.915)
Train: 200 [1170/1171 (100%)]  Loss:  2.287313 (2.8142)  Time: 0.565s, 1811.41/s  (2.507s,  408.46/s)  LR: 1.553e-05  Data: 0.000 (1.912)
Test: [   0/97]  Time: 12.086 (12.086)  Loss:  0.2943 (0.2943)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.197 (3.559)  Loss:  0.4309 (0.3559)  Acc@1: 93.6523 (95.5002)  Acc@5: 98.2422 (98.9775)
Test: [  97/97]  Time: 0.120 (3.542)  Loss:  0.3403 (0.3657)  Acc@1: 93.7500 (95.0250)  Acc@5: 99.2560 (98.8740)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-200.pth.tar', 95.02500005126953)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-197.pth.tar', 95.0)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-198.pth.tar', 94.99300000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-199.pth.tar', 94.98600004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-196.pth.tar', 94.97700000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-195.pth.tar', 94.96100002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-192.pth.tar', 94.92500001220704)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-189.pth.tar', 94.87399997314454)

Train: 201 [   0/1171 (  0%)]  Loss:  2.287796 (2.2878)  Time: 10.264s,   99.76/s  (10.264s,   99.76/s)  LR: 1.448e-05  Data: 9.513 (9.513)
Train: 201 [  50/1171 (  4%)]  Loss:  2.935076 (2.6114)  Time: 0.582s, 1759.00/s  (2.607s,  392.82/s)  LR: 1.448e-05  Data: 0.016 (2.006)
Train: 201 [ 100/1171 (  9%)]  Loss:  2.795208 (2.6727)  Time: 0.982s, 1042.60/s  (2.534s,  404.07/s)  LR: 1.448e-05  Data: 0.401 (1.935)
Train: 201 [ 150/1171 ( 13%)]  Loss:  2.232405 (2.5626)  Time: 0.588s, 1742.65/s  (2.442s,  419.37/s)  LR: 1.448e-05  Data: 0.024 (1.842)
Train: 201 [ 200/1171 ( 17%)]  Loss:  2.687118 (2.5875)  Time: 1.947s,  525.96/s  (2.416s,  423.77/s)  LR: 1.448e-05  Data: 1.282 (1.815)
Train: 201 [ 250/1171 ( 21%)]  Loss:  2.643481 (2.5968)  Time: 0.585s, 1749.97/s  (2.365s,  432.94/s)  LR: 1.448e-05  Data: 0.020 (1.766)
Train: 201 [ 300/1171 ( 26%)]  Loss:  2.973503 (2.6507)  Time: 0.588s, 1740.23/s  (2.342s,  437.21/s)  LR: 1.448e-05  Data: 0.023 (1.743)
Train: 201 [ 350/1171 ( 30%)]  Loss:  3.027799 (2.6978)  Time: 1.596s,  641.76/s  (2.372s,  431.66/s)  LR: 1.448e-05  Data: 1.018 (1.774)
Train: 201 [ 400/1171 ( 34%)]  Loss:  3.089323 (2.7413)  Time: 3.211s,  318.87/s  (2.384s,  429.54/s)  LR: 1.448e-05  Data: 2.577 (1.783)
Train: 201 [ 450/1171 ( 38%)]  Loss:  2.867688 (2.7539)  Time: 0.583s, 1757.37/s  (2.381s,  430.09/s)  LR: 1.448e-05  Data: 0.018 (1.778)
Train: 201 [ 500/1171 ( 43%)]  Loss:  2.458904 (2.7271)  Time: 4.647s,  220.34/s  (2.392s,  428.08/s)  LR: 1.448e-05  Data: 4.071 (1.790)
Train: 201 [ 550/1171 ( 47%)]  Loss:  2.909544 (2.7423)  Time: 0.584s, 1754.88/s  (2.397s,  427.20/s)  LR: 1.448e-05  Data: 0.019 (1.796)
Train: 201 [ 600/1171 ( 51%)]  Loss:  2.016378 (2.6865)  Time: 2.913s,  351.53/s  (2.410s,  424.86/s)  LR: 1.448e-05  Data: 2.352 (1.810)
Train: 201 [ 650/1171 ( 56%)]  Loss:  2.517306 (2.6744)  Time: 0.584s, 1754.74/s  (2.403s,  426.15/s)  LR: 1.448e-05  Data: 0.019 (1.803)
Train: 201 [ 700/1171 ( 60%)]  Loss:  2.339526 (2.6521)  Time: 5.343s,  191.66/s  (2.452s,  417.58/s)  LR: 1.448e-05  Data: 4.767 (1.850)
Train: 201 [ 750/1171 ( 64%)]  Loss:  2.397583 (2.6362)  Time: 0.585s, 1750.19/s  (2.494s,  410.55/s)  LR: 1.448e-05  Data: 0.019 (1.892)
Train: 201 [ 800/1171 ( 68%)]  Loss:  2.869091 (2.6499)  Time: 6.340s,  161.52/s  (2.513s,  407.47/s)  LR: 1.448e-05  Data: 5.722 (1.910)
Train: 201 [ 850/1171 ( 73%)]  Loss:  2.671756 (2.6511)  Time: 0.584s, 1752.03/s  (2.505s,  408.80/s)  LR: 1.448e-05  Data: 0.020 (1.902)
Train: 201 [ 900/1171 ( 77%)]  Loss:  3.341274 (2.6874)  Time: 7.353s,  139.26/s  (2.499s,  409.81/s)  LR: 1.448e-05  Data: 6.781 (1.896)
Train: 201 [ 950/1171 ( 81%)]  Loss:  3.021372 (2.7041)  Time: 0.586s, 1747.99/s  (2.486s,  411.97/s)  LR: 1.448e-05  Data: 0.021 (1.884)
Train: 201 [1000/1171 ( 85%)]  Loss:  2.705392 (2.7042)  Time: 3.841s,  266.60/s  (2.476s,  413.52/s)  LR: 1.448e-05  Data: 3.152 (1.875)
Train: 201 [1050/1171 ( 90%)]  Loss:  2.895364 (2.7129)  Time: 0.588s, 1740.20/s  (2.482s,  412.61/s)  LR: 1.448e-05  Data: 0.022 (1.880)
Train: 201 [1100/1171 ( 94%)]  Loss:  2.764372 (2.7151)  Time: 3.779s,  271.00/s  (2.481s,  412.71/s)  LR: 1.448e-05  Data: 3.118 (1.880)
Train: 201 [1150/1171 ( 98%)]  Loss:  2.646365 (2.7122)  Time: 0.586s, 1748.01/s  (2.473s,  414.10/s)  LR: 1.448e-05  Data: 0.021 (1.871)
Train: 201 [1170/1171 (100%)]  Loss:  3.034517 (2.7251)  Time: 0.564s, 1814.56/s  (2.474s,  413.96/s)  LR: 1.448e-05  Data: 0.000 (1.872)
Test: [   0/97]  Time: 14.067 (14.067)  Loss:  0.2798 (0.2798)  Acc@1: 97.2656 (97.2656)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (3.054)  Loss:  0.4269 (0.3471)  Acc@1: 92.7734 (95.4082)  Acc@5: 98.3398 (98.9622)
Test: [  97/97]  Time: 0.120 (2.959)  Loss:  0.3070 (0.3568)  Acc@1: 95.5357 (94.9950)  Acc@5: 99.4048 (98.8560)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-200.pth.tar', 95.02500005126953)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-197.pth.tar', 95.0)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-201.pth.tar', 94.99500004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-198.pth.tar', 94.99300000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-199.pth.tar', 94.98600004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-196.pth.tar', 94.97700000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-195.pth.tar', 94.96100002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-192.pth.tar', 94.92500001220704)

Train: 202 [   0/1171 (  0%)]  Loss:  3.066555 (3.0666)  Time: 10.701s,   95.70/s  (10.701s,   95.70/s)  LR: 1.354e-05  Data: 9.717 (9.717)
Train: 202 [  50/1171 (  4%)]  Loss:  2.262902 (2.6647)  Time: 0.584s, 1754.66/s  (2.277s,  449.79/s)  LR: 1.354e-05  Data: 0.020 (1.675)
Train: 202 [ 100/1171 (  9%)]  Loss:  2.488435 (2.6060)  Time: 0.587s, 1743.30/s  (2.251s,  454.82/s)  LR: 1.354e-05  Data: 0.019 (1.654)
Train: 202 [ 150/1171 ( 13%)]  Loss:  2.670505 (2.6221)  Time: 0.588s, 1740.86/s  (2.354s,  435.03/s)  LR: 1.354e-05  Data: 0.020 (1.755)
Train: 202 [ 200/1171 ( 17%)]  Loss:  2.571053 (2.6119)  Time: 3.760s,  272.32/s  (2.385s,  429.29/s)  LR: 1.354e-05  Data: 3.113 (1.784)
Train: 202 [ 250/1171 ( 21%)]  Loss:  2.516788 (2.5960)  Time: 0.585s, 1749.63/s  (2.364s,  433.13/s)  LR: 1.354e-05  Data: 0.021 (1.764)
Train: 202 [ 300/1171 ( 26%)]  Loss:  2.672192 (2.6069)  Time: 5.941s,  172.37/s  (2.451s,  417.77/s)  LR: 1.354e-05  Data: 5.279 (1.849)
Train: 202 [ 350/1171 ( 30%)]  Loss:  2.641339 (2.6112)  Time: 0.587s, 1743.11/s  (2.524s,  405.67/s)  LR: 1.354e-05  Data: 0.021 (1.921)
Train: 202 [ 400/1171 ( 34%)]  Loss:  2.395679 (2.5873)  Time: 6.431s,  159.24/s  (2.499s,  409.74/s)  LR: 1.354e-05  Data: 5.726 (1.895)
Train: 202 [ 450/1171 ( 38%)]  Loss:  2.780637 (2.6066)  Time: 0.586s, 1747.77/s  (2.462s,  415.88/s)  LR: 1.354e-05  Data: 0.021 (1.859)
Train: 202 [ 500/1171 ( 43%)]  Loss:  2.672388 (2.6126)  Time: 4.569s,  224.14/s  (2.496s,  410.24/s)  LR: 1.354e-05  Data: 3.973 (1.894)
Train: 202 [ 550/1171 ( 47%)]  Loss:  2.885827 (2.6354)  Time: 0.586s, 1748.12/s  (2.499s,  409.76/s)  LR: 1.354e-05  Data: 0.021 (1.897)
Train: 202 [ 600/1171 ( 51%)]  Loss:  2.545353 (2.6284)  Time: 6.170s,  165.96/s  (2.521s,  406.24/s)  LR: 1.354e-05  Data: 5.490 (1.919)
Train: 202 [ 650/1171 ( 56%)]  Loss:  2.712376 (2.6344)  Time: 0.582s, 1760.57/s  (2.515s,  407.15/s)  LR: 1.354e-05  Data: 0.020 (1.914)
Train: 202 [ 700/1171 ( 60%)]  Loss:  2.869134 (2.6501)  Time: 6.940s,  147.54/s  (2.513s,  407.49/s)  LR: 1.354e-05  Data: 6.253 (1.911)
Train: 202 [ 750/1171 ( 64%)]  Loss:  2.450371 (2.6376)  Time: 0.587s, 1744.90/s  (2.496s,  410.32/s)  LR: 1.354e-05  Data: 0.020 (1.894)
Train: 202 [ 800/1171 ( 68%)]  Loss:  2.451105 (2.6266)  Time: 7.227s,  141.69/s  (2.488s,  411.62/s)  LR: 1.354e-05  Data: 6.635 (1.887)
Train: 202 [ 850/1171 ( 73%)]  Loss:  2.591385 (2.6247)  Time: 0.588s, 1741.71/s  (2.497s,  410.05/s)  LR: 1.354e-05  Data: 0.022 (1.897)
Train: 202 [ 900/1171 ( 77%)]  Loss:  2.492945 (2.6177)  Time: 8.201s,  124.87/s  (2.501s,  409.50/s)  LR: 1.354e-05  Data: 7.614 (1.901)
Train: 202 [ 950/1171 ( 81%)]  Loss:  2.551402 (2.6144)  Time: 0.586s, 1745.97/s  (2.491s,  411.01/s)  LR: 1.354e-05  Data: 0.022 (1.893)
Train: 202 [1000/1171 ( 85%)]  Loss:  2.376009 (2.6031)  Time: 7.255s,  141.15/s  (2.491s,  411.05/s)  LR: 1.354e-05  Data: 6.559 (1.893)
Train: 202 [1050/1171 ( 90%)]  Loss:  2.624128 (2.6040)  Time: 0.589s, 1739.89/s  (2.483s,  412.42/s)  LR: 1.354e-05  Data: 0.018 (1.885)
Train: 202 [1100/1171 ( 94%)]  Loss:  3.044126 (2.6232)  Time: 6.995s,  146.38/s  (2.485s,  412.10/s)  LR: 1.354e-05  Data: 6.416 (1.888)
Train: 202 [1150/1171 ( 98%)]  Loss:  2.672635 (2.6252)  Time: 0.586s, 1746.73/s  (2.489s,  411.48/s)  LR: 1.354e-05  Data: 0.023 (1.892)
Train: 202 [1170/1171 (100%)]  Loss:  2.674887 (2.6272)  Time: 0.565s, 1812.31/s  (2.512s,  407.62/s)  LR: 1.354e-05  Data: 0.000 (1.916)
Test: [   0/97]  Time: 21.883 (21.883)  Loss:  0.2932 (0.2932)  Acc@1: 97.4609 (97.4609)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.197 (3.523)  Loss:  0.4465 (0.3629)  Acc@1: 93.6523 (95.5327)  Acc@5: 98.4375 (98.9871)
Test: [  97/97]  Time: 0.120 (3.321)  Loss:  0.3288 (0.3735)  Acc@1: 95.2381 (95.0530)  Acc@5: 99.2560 (98.8600)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-202.pth.tar', 95.05300001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-200.pth.tar', 95.02500005126953)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-197.pth.tar', 95.0)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-201.pth.tar', 94.99500004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-198.pth.tar', 94.99300000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-199.pth.tar', 94.98600004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-196.pth.tar', 94.97700000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-195.pth.tar', 94.96100002197265)

Train: 203 [   0/1171 (  0%)]  Loss:  2.546689 (2.5467)  Time: 11.352s,   90.20/s  (11.352s,   90.20/s)  LR: 1.271e-05  Data: 10.618 (10.618)
Train: 203 [  50/1171 (  4%)]  Loss:  2.969496 (2.7581)  Time: 0.586s, 1746.58/s  (2.422s,  422.74/s)  LR: 1.271e-05  Data: 0.022 (1.815)
Train: 203 [ 100/1171 (  9%)]  Loss:  2.731977 (2.7494)  Time: 0.585s, 1749.43/s  (2.382s,  429.90/s)  LR: 1.271e-05  Data: 0.020 (1.786)
Train: 203 [ 150/1171 ( 13%)]  Loss:  2.690611 (2.7347)  Time: 0.583s, 1757.00/s  (2.314s,  442.55/s)  LR: 1.271e-05  Data: 0.019 (1.718)
Train: 203 [ 200/1171 ( 17%)]  Loss:  2.869913 (2.7617)  Time: 2.211s,  463.23/s  (2.298s,  445.53/s)  LR: 1.271e-05  Data: 1.569 (1.707)
Train: 203 [ 250/1171 ( 21%)]  Loss:  3.023486 (2.8054)  Time: 0.586s, 1746.77/s  (2.262s,  452.67/s)  LR: 1.271e-05  Data: 0.022 (1.669)
Train: 203 [ 300/1171 ( 26%)]  Loss:  2.695560 (2.7897)  Time: 0.584s, 1754.60/s  (2.343s,  437.06/s)  LR: 1.271e-05  Data: 0.021 (1.750)
Train: 203 [ 350/1171 ( 30%)]  Loss:  3.078883 (2.8258)  Time: 0.585s, 1751.69/s  (2.349s,  435.96/s)  LR: 1.271e-05  Data: 0.017 (1.758)
Train: 203 [ 400/1171 ( 34%)]  Loss:  3.076649 (2.8537)  Time: 0.588s, 1740.81/s  (2.363s,  433.29/s)  LR: 1.271e-05  Data: 0.020 (1.771)
Train: 203 [ 450/1171 ( 38%)]  Loss:  2.891120 (2.8574)  Time: 0.584s, 1752.45/s  (2.348s,  436.11/s)  LR: 1.271e-05  Data: 0.021 (1.754)
Train: 203 [ 500/1171 ( 43%)]  Loss:  3.233040 (2.8916)  Time: 0.584s, 1752.37/s  (2.357s,  434.53/s)  LR: 1.271e-05  Data: 0.019 (1.763)
Train: 203 [ 550/1171 ( 47%)]  Loss:  2.629284 (2.8697)  Time: 0.593s, 1726.87/s  (2.352s,  435.41/s)  LR: 1.271e-05  Data: 0.020 (1.759)
Train: 203 [ 600/1171 ( 51%)]  Loss:  3.063642 (2.8846)  Time: 0.585s, 1750.50/s  (2.360s,  433.95/s)  LR: 1.271e-05  Data: 0.022 (1.768)
Train: 203 [ 650/1171 ( 56%)]  Loss:  2.952688 (2.8895)  Time: 0.587s, 1745.57/s  (2.378s,  430.59/s)  LR: 1.271e-05  Data: 0.018 (1.786)
Train: 203 [ 700/1171 ( 60%)]  Loss:  2.683286 (2.8758)  Time: 0.585s, 1751.35/s  (2.399s,  426.93/s)  LR: 1.271e-05  Data: 0.019 (1.807)
Train: 203 [ 750/1171 ( 64%)]  Loss:  2.817328 (2.8721)  Time: 0.588s, 1742.27/s  (2.471s,  414.34/s)  LR: 1.271e-05  Data: 0.021 (1.880)
Train: 203 [ 800/1171 ( 68%)]  Loss:  2.461380 (2.8479)  Time: 0.585s, 1750.92/s  (2.466s,  415.17/s)  LR: 1.271e-05  Data: 0.020 (1.875)
Train: 203 [ 850/1171 ( 73%)]  Loss:  2.380894 (2.8220)  Time: 0.584s, 1753.44/s  (2.454s,  417.22/s)  LR: 1.271e-05  Data: 0.018 (1.863)
Train: 203 [ 900/1171 ( 77%)]  Loss:  3.202708 (2.8420)  Time: 0.586s, 1748.89/s  (2.451s,  417.76/s)  LR: 1.271e-05  Data: 0.019 (1.860)
Train: 203 [ 950/1171 ( 81%)]  Loss:  2.775834 (2.8387)  Time: 0.585s, 1749.99/s  (2.436s,  420.37/s)  LR: 1.271e-05  Data: 0.019 (1.845)
Train: 203 [1000/1171 ( 85%)]  Loss:  2.907164 (2.8420)  Time: 0.588s, 1742.73/s  (2.452s,  417.64/s)  LR: 1.271e-05  Data: 0.018 (1.861)
Train: 203 [1050/1171 ( 90%)]  Loss:  3.011256 (2.8497)  Time: 0.585s, 1749.19/s  (2.442s,  419.32/s)  LR: 1.271e-05  Data: 0.020 (1.851)
Train: 203 [1100/1171 ( 94%)]  Loss:  3.112774 (2.8611)  Time: 0.583s, 1756.53/s  (2.441s,  419.52/s)  LR: 1.271e-05  Data: 0.018 (1.850)
Train: 203 [1150/1171 ( 98%)]  Loss:  2.804605 (2.8588)  Time: 0.583s, 1756.81/s  (2.437s,  420.14/s)  LR: 1.271e-05  Data: 0.019 (1.847)
Train: 203 [1170/1171 (100%)]  Loss:  3.127642 (2.8695)  Time: 0.562s, 1821.45/s  (2.434s,  420.67/s)  LR: 1.271e-05  Data: 0.000 (1.845)
Test: [   0/97]  Time: 12.624 (12.624)  Loss:  0.2733 (0.2733)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.195 (3.042)  Loss:  0.4236 (0.3361)  Acc@1: 92.7734 (95.5423)  Acc@5: 98.3398 (98.9851)
Test: [  97/97]  Time: 0.120 (2.916)  Loss:  0.3127 (0.3477)  Acc@1: 94.3452 (95.0270)  Acc@5: 99.4048 (98.8710)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-202.pth.tar', 95.05300001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-203.pth.tar', 95.02699999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-200.pth.tar', 95.02500005126953)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-197.pth.tar', 95.0)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-201.pth.tar', 94.99500004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-198.pth.tar', 94.99300000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-199.pth.tar', 94.98600004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-196.pth.tar', 94.97700000732422)

Train: 204 [   0/1171 (  0%)]  Loss:  2.944985 (2.9450)  Time: 9.527s,  107.48/s  (9.527s,  107.48/s)  LR: 1.199e-05  Data: 8.917 (8.917)
Train: 204 [  50/1171 (  4%)]  Loss:  2.981313 (2.9631)  Time: 0.584s, 1752.80/s  (2.252s,  454.66/s)  LR: 1.199e-05  Data: 0.021 (1.646)
Train: 204 [ 100/1171 (  9%)]  Loss:  2.708326 (2.8782)  Time: 1.645s,  622.36/s  (2.434s,  420.65/s)  LR: 1.199e-05  Data: 1.078 (1.831)
Train: 204 [ 150/1171 ( 13%)]  Loss:  2.475547 (2.7775)  Time: 0.585s, 1749.68/s  (2.379s,  430.47/s)  LR: 1.199e-05  Data: 0.020 (1.777)
Train: 204 [ 200/1171 ( 17%)]  Loss:  2.487340 (2.7195)  Time: 4.144s,  247.08/s  (2.375s,  431.14/s)  LR: 1.199e-05  Data: 3.583 (1.772)
Train: 204 [ 250/1171 ( 21%)]  Loss:  2.786568 (2.7307)  Time: 0.584s, 1754.73/s  (2.346s,  436.42/s)  LR: 1.199e-05  Data: 0.019 (1.744)
Train: 204 [ 300/1171 ( 26%)]  Loss:  2.799747 (2.7405)  Time: 0.693s, 1477.05/s  (2.438s,  420.10/s)  LR: 1.199e-05  Data: 0.131 (1.834)
Train: 204 [ 350/1171 ( 30%)]  Loss:  2.683152 (2.7334)  Time: 0.585s, 1751.25/s  (2.462s,  415.88/s)  LR: 1.199e-05  Data: 0.021 (1.860)
Train: 204 [ 400/1171 ( 34%)]  Loss:  2.860276 (2.7475)  Time: 0.585s, 1749.98/s  (2.439s,  419.84/s)  LR: 1.199e-05  Data: 0.019 (1.839)
Train: 204 [ 450/1171 ( 38%)]  Loss:  3.205430 (2.7933)  Time: 0.586s, 1746.05/s  (2.453s,  417.47/s)  LR: 1.199e-05  Data: 0.022 (1.852)
Train: 204 [ 500/1171 ( 43%)]  Loss:  2.689810 (2.7839)  Time: 0.585s, 1750.76/s  (2.446s,  418.64/s)  LR: 1.199e-05  Data: 0.020 (1.848)
Train: 204 [ 550/1171 ( 47%)]  Loss:  2.672494 (2.7746)  Time: 0.588s, 1742.65/s  (2.452s,  417.57/s)  LR: 1.199e-05  Data: 0.023 (1.855)
Train: 204 [ 600/1171 ( 51%)]  Loss:  2.673350 (2.7668)  Time: 0.589s, 1739.95/s  (2.466s,  415.19/s)  LR: 1.199e-05  Data: 0.024 (1.869)
Train: 204 [ 650/1171 ( 56%)]  Loss:  2.724432 (2.7638)  Time: 0.584s, 1754.17/s  (2.463s,  415.79/s)  LR: 1.199e-05  Data: 0.021 (1.867)
Train: 204 [ 700/1171 ( 60%)]  Loss:  2.811654 (2.7670)  Time: 0.586s, 1746.09/s  (2.453s,  417.40/s)  LR: 1.199e-05  Data: 0.022 (1.858)
Train: 204 [ 750/1171 ( 64%)]  Loss:  2.736522 (2.7651)  Time: 0.585s, 1750.13/s  (2.434s,  420.63/s)  LR: 1.199e-05  Data: 0.022 (1.839)
Train: 204 [ 800/1171 ( 68%)]  Loss:  3.027525 (2.7805)  Time: 0.586s, 1745.98/s  (2.423s,  422.60/s)  LR: 1.199e-05  Data: 0.021 (1.829)
Train: 204 [ 850/1171 ( 73%)]  Loss:  3.150849 (2.8011)  Time: 0.586s, 1746.14/s  (2.440s,  419.68/s)  LR: 1.199e-05  Data: 0.019 (1.846)
Train: 204 [ 900/1171 ( 77%)]  Loss:  2.835788 (2.8029)  Time: 0.583s, 1755.04/s  (2.426s,  422.07/s)  LR: 1.199e-05  Data: 0.021 (1.833)
Train: 204 [ 950/1171 ( 81%)]  Loss:  2.620779 (2.7938)  Time: 0.583s, 1756.46/s  (2.423s,  422.62/s)  LR: 1.199e-05  Data: 0.018 (1.830)
Train: 204 [1000/1171 ( 85%)]  Loss:  3.025995 (2.8049)  Time: 0.588s, 1740.76/s  (2.417s,  423.64/s)  LR: 1.199e-05  Data: 0.020 (1.825)
Train: 204 [1050/1171 ( 90%)]  Loss:  2.661846 (2.7984)  Time: 0.585s, 1750.11/s  (2.410s,  424.94/s)  LR: 1.199e-05  Data: 0.021 (1.818)
Train: 204 [1100/1171 ( 94%)]  Loss:  2.523519 (2.7864)  Time: 0.583s, 1757.35/s  (2.402s,  426.26/s)  LR: 1.199e-05  Data: 0.019 (1.811)
Train: 204 [1150/1171 ( 98%)]  Loss:  3.164982 (2.8022)  Time: 0.584s, 1752.69/s  (2.394s,  427.79/s)  LR: 1.199e-05  Data: 0.020 (1.803)
Train: 204 [1170/1171 (100%)]  Loss:  2.810025 (2.8025)  Time: 0.565s, 1813.28/s  (2.391s,  428.22/s)  LR: 1.199e-05  Data: 0.000 (1.799)
Test: [   0/97]  Time: 12.615 (12.615)  Loss:  0.2768 (0.2768)  Acc@1: 97.2656 (97.2656)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (4.343)  Loss:  0.4114 (0.3331)  Acc@1: 93.7500 (95.5902)  Acc@5: 98.5352 (98.9832)
Test: [  97/97]  Time: 0.120 (3.707)  Loss:  0.3180 (0.3435)  Acc@1: 94.3452 (95.1100)  Acc@5: 99.4048 (98.8730)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-204.pth.tar', 95.10999999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-202.pth.tar', 95.05300001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-203.pth.tar', 95.02699999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-200.pth.tar', 95.02500005126953)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-197.pth.tar', 95.0)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-201.pth.tar', 94.99500004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-198.pth.tar', 94.99300000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-199.pth.tar', 94.98600004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-191.pth.tar', 94.98000003417968)

Train: 205 [   0/1171 (  0%)]  Loss:  2.875467 (2.8755)  Time: 10.484s,   97.67/s  (10.484s,   97.67/s)  LR: 1.138e-05  Data: 9.778 (9.778)
Train: 205 [  50/1171 (  4%)]  Loss:  2.899040 (2.8873)  Time: 0.584s, 1754.13/s  (2.356s,  434.65/s)  LR: 1.138e-05  Data: 0.020 (1.760)
Train: 205 [ 100/1171 (  9%)]  Loss:  3.053731 (2.9427)  Time: 0.585s, 1750.12/s  (2.303s,  444.64/s)  LR: 1.138e-05  Data: 0.023 (1.708)
Train: 205 [ 150/1171 ( 13%)]  Loss:  2.772631 (2.9002)  Time: 0.590s, 1736.19/s  (2.257s,  453.74/s)  LR: 1.138e-05  Data: 0.020 (1.658)
Train: 205 [ 200/1171 ( 17%)]  Loss:  2.300178 (2.7802)  Time: 1.555s,  658.52/s  (2.230s,  459.16/s)  LR: 1.138e-05  Data: 0.973 (1.631)
Train: 205 [ 250/1171 ( 21%)]  Loss:  2.940870 (2.8070)  Time: 0.597s, 1715.32/s  (2.205s,  464.35/s)  LR: 1.138e-05  Data: 0.021 (1.605)
Train: 205 [ 300/1171 ( 26%)]  Loss:  2.501457 (2.7633)  Time: 0.588s, 1742.51/s  (2.275s,  450.05/s)  LR: 1.138e-05  Data: 0.022 (1.671)
Train: 205 [ 350/1171 ( 30%)]  Loss:  2.582635 (2.7408)  Time: 0.591s, 1733.29/s  (2.256s,  453.95/s)  LR: 1.138e-05  Data: 0.019 (1.654)
Train: 205 [ 400/1171 ( 34%)]  Loss:  3.057034 (2.7759)  Time: 0.587s, 1744.40/s  (2.271s,  450.88/s)  LR: 1.138e-05  Data: 0.020 (1.669)
Train: 205 [ 450/1171 ( 38%)]  Loss:  3.173589 (2.8157)  Time: 0.590s, 1735.41/s  (2.264s,  452.31/s)  LR: 1.138e-05  Data: 0.018 (1.663)
Train: 205 [ 500/1171 ( 43%)]  Loss:  2.392865 (2.7772)  Time: 0.585s, 1749.37/s  (2.264s,  452.23/s)  LR: 1.138e-05  Data: 0.018 (1.665)
Train: 205 [ 550/1171 ( 47%)]  Loss:  2.747141 (2.7747)  Time: 0.588s, 1741.88/s  (2.271s,  450.99/s)  LR: 1.138e-05  Data: 0.019 (1.672)
Train: 205 [ 600/1171 ( 51%)]  Loss:  2.996565 (2.7918)  Time: 0.585s, 1751.27/s  (2.276s,  449.96/s)  LR: 1.138e-05  Data: 0.021 (1.679)
Train: 205 [ 650/1171 ( 56%)]  Loss:  2.960327 (2.8038)  Time: 0.586s, 1746.55/s  (2.301s,  445.01/s)  LR: 1.138e-05  Data: 0.018 (1.704)
Train: 205 [ 700/1171 ( 60%)]  Loss:  3.095556 (2.8233)  Time: 0.586s, 1747.00/s  (2.315s,  442.40/s)  LR: 1.138e-05  Data: 0.019 (1.719)
Train: 205 [ 750/1171 ( 64%)]  Loss:  2.771689 (2.8200)  Time: 0.587s, 1745.86/s  (2.318s,  441.80/s)  LR: 1.138e-05  Data: 0.020 (1.723)
Train: 205 [ 800/1171 ( 68%)]  Loss:  2.665571 (2.8110)  Time: 0.580s, 1764.18/s  (2.366s,  432.80/s)  LR: 1.138e-05  Data: 0.018 (1.771)
Train: 205 [ 850/1171 ( 73%)]  Loss:  2.744066 (2.8072)  Time: 0.585s, 1750.76/s  (2.392s,  428.14/s)  LR: 1.138e-05  Data: 0.019 (1.797)
Train: 205 [ 900/1171 ( 77%)]  Loss:  3.092725 (2.8223)  Time: 0.586s, 1748.07/s  (2.378s,  430.60/s)  LR: 1.138e-05  Data: 0.019 (1.784)
Train: 205 [ 950/1171 ( 81%)]  Loss:  2.629743 (2.8126)  Time: 0.587s, 1743.13/s  (2.379s,  430.48/s)  LR: 1.138e-05  Data: 0.018 (1.786)
Train: 205 [1000/1171 ( 85%)]  Loss:  2.709959 (2.8078)  Time: 0.581s, 1760.97/s  (2.368s,  432.42/s)  LR: 1.138e-05  Data: 0.018 (1.775)
Train: 205 [1050/1171 ( 90%)]  Loss:  2.406103 (2.7895)  Time: 0.587s, 1745.38/s  (2.389s,  428.67/s)  LR: 1.138e-05  Data: 0.021 (1.796)
Train: 205 [1100/1171 ( 94%)]  Loss:  2.721285 (2.7865)  Time: 0.583s, 1755.18/s  (2.376s,  431.00/s)  LR: 1.138e-05  Data: 0.022 (1.784)
Train: 205 [1150/1171 ( 98%)]  Loss:  2.713351 (2.7835)  Time: 0.589s, 1737.21/s  (2.378s,  430.59/s)  LR: 1.138e-05  Data: 0.019 (1.787)
Train: 205 [1170/1171 (100%)]  Loss:  2.615751 (2.7768)  Time: 0.565s, 1813.62/s  (2.375s,  431.25/s)  LR: 1.138e-05  Data: 0.000 (1.784)
Test: [   0/97]  Time: 11.983 (11.983)  Loss:  0.2813 (0.2813)  Acc@1: 97.0703 (97.0703)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.196 (3.023)  Loss:  0.4108 (0.3432)  Acc@1: 93.4570 (95.5538)  Acc@5: 98.6328 (98.9813)
Test: [  97/97]  Time: 0.120 (2.922)  Loss:  0.3226 (0.3540)  Acc@1: 94.1964 (95.0580)  Acc@5: 99.4048 (98.8720)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-204.pth.tar', 95.10999999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-205.pth.tar', 95.0580000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-202.pth.tar', 95.05300001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-203.pth.tar', 95.02699999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-200.pth.tar', 95.02500005126953)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-197.pth.tar', 95.0)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-201.pth.tar', 94.99500004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-198.pth.tar', 94.99300000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-199.pth.tar', 94.98600004638672)

Train: 206 [   0/1171 (  0%)]  Loss:  2.597682 (2.5977)  Time: 10.538s,   97.17/s  (10.538s,   97.17/s)  LR: 1.089e-05  Data: 9.887 (9.887)
Train: 206 [  50/1171 (  4%)]  Loss:  3.006618 (2.8022)  Time: 0.588s, 1742.11/s  (2.277s,  449.76/s)  LR: 1.089e-05  Data: 0.019 (1.693)
Train: 206 [ 100/1171 (  9%)]  Loss:  2.723379 (2.7759)  Time: 0.585s, 1751.78/s  (2.321s,  441.27/s)  LR: 1.089e-05  Data: 0.017 (1.731)
Train: 206 [ 150/1171 ( 13%)]  Loss:  2.552747 (2.7201)  Time: 0.587s, 1744.89/s  (2.339s,  437.84/s)  LR: 1.089e-05  Data: 0.020 (1.750)
Train: 206 [ 200/1171 ( 17%)]  Loss:  2.539570 (2.6840)  Time: 1.556s,  658.21/s  (2.326s,  440.27/s)  LR: 1.089e-05  Data: 0.727 (1.734)
Train: 206 [ 250/1171 ( 21%)]  Loss:  2.613268 (2.6722)  Time: 0.807s, 1268.18/s  (2.311s,  443.07/s)  LR: 1.089e-05  Data: 0.215 (1.717)
Train: 206 [ 300/1171 ( 26%)]  Loss:  2.594833 (2.6612)  Time: 0.688s, 1488.97/s  (2.309s,  443.57/s)  LR: 1.089e-05  Data: 0.113 (1.714)
Train: 206 [ 350/1171 ( 30%)]  Loss:  3.094090 (2.7153)  Time: 1.849s,  553.72/s  (2.293s,  446.51/s)  LR: 1.089e-05  Data: 1.288 (1.698)
Train: 206 [ 400/1171 ( 34%)]  Loss:  2.633918 (2.7062)  Time: 0.584s, 1753.08/s  (2.362s,  433.59/s)  LR: 1.089e-05  Data: 0.019 (1.763)
Train: 206 [ 450/1171 ( 38%)]  Loss:  3.008398 (2.7365)  Time: 5.271s,  194.28/s  (2.379s,  430.45/s)  LR: 1.089e-05  Data: 4.599 (1.779)
Train: 206 [ 500/1171 ( 43%)]  Loss:  2.767858 (2.7393)  Time: 0.584s, 1754.59/s  (2.413s,  424.31/s)  LR: 1.089e-05  Data: 0.019 (1.814)
Train: 206 [ 550/1171 ( 47%)]  Loss:  2.909634 (2.7535)  Time: 7.476s,  136.97/s  (2.411s,  424.77/s)  LR: 1.089e-05  Data: 6.891 (1.813)
Train: 206 [ 600/1171 ( 51%)]  Loss:  3.241405 (2.7910)  Time: 0.585s, 1750.42/s  (2.415s,  423.98/s)  LR: 1.089e-05  Data: 0.018 (1.818)
Train: 206 [ 650/1171 ( 56%)]  Loss:  3.007427 (2.8065)  Time: 7.561s,  135.44/s  (2.423s,  422.64/s)  LR: 1.089e-05  Data: 6.965 (1.826)
Train: 206 [ 700/1171 ( 60%)]  Loss:  2.879648 (2.8114)  Time: 0.586s, 1746.67/s  (2.415s,  424.05/s)  LR: 1.089e-05  Data: 0.017 (1.817)
Train: 206 [ 750/1171 ( 64%)]  Loss:  2.993008 (2.8227)  Time: 2.707s,  378.22/s  (2.404s,  425.96/s)  LR: 1.089e-05  Data: 2.121 (1.807)
Train: 206 [ 800/1171 ( 68%)]  Loss:  2.512928 (2.8045)  Time: 0.593s, 1727.89/s  (2.395s,  427.58/s)  LR: 1.089e-05  Data: 0.020 (1.797)
Train: 206 [ 850/1171 ( 73%)]  Loss:  2.159763 (2.7687)  Time: 2.310s,  443.36/s  (2.391s,  428.27/s)  LR: 1.089e-05  Data: 1.495 (1.793)
Train: 206 [ 900/1171 ( 77%)]  Loss:  2.498498 (2.7545)  Time: 0.590s, 1735.36/s  (2.400s,  426.72/s)  LR: 1.089e-05  Data: 0.018 (1.801)
Train: 206 [ 950/1171 ( 81%)]  Loss:  3.237579 (2.7786)  Time: 0.706s, 1451.38/s  (2.387s,  428.98/s)  LR: 1.089e-05  Data: 0.028 (1.789)
Train: 206 [1000/1171 ( 85%)]  Loss:  2.644840 (2.7722)  Time: 0.590s, 1734.17/s  (2.392s,  428.16/s)  LR: 1.089e-05  Data: 0.024 (1.793)
Train: 206 [1050/1171 ( 90%)]  Loss:  2.367832 (2.7539)  Time: 0.588s, 1741.01/s  (2.383s,  429.72/s)  LR: 1.089e-05  Data: 0.020 (1.785)
Train: 206 [1100/1171 ( 94%)]  Loss:  2.865416 (2.7587)  Time: 0.587s, 1743.31/s  (2.380s,  430.23/s)  LR: 1.089e-05  Data: 0.019 (1.783)
Train: 206 [1150/1171 ( 98%)]  Loss:  2.681128 (2.7555)  Time: 0.586s, 1746.20/s  (2.368s,  432.51/s)  LR: 1.089e-05  Data: 0.017 (1.770)
Train: 206 [1170/1171 (100%)]  Loss:  3.108001 (2.7696)  Time: 0.562s, 1822.45/s  (2.364s,  433.12/s)  LR: 1.089e-05  Data: 0.000 (1.767)
Test: [   0/97]  Time: 13.081 (13.081)  Loss:  0.2810 (0.2810)  Acc@1: 97.2656 (97.2656)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (3.041)  Loss:  0.4213 (0.3476)  Acc@1: 92.7734 (95.5097)  Acc@5: 98.3398 (98.9794)
Test: [  97/97]  Time: 0.120 (3.599)  Loss:  0.3197 (0.3570)  Acc@1: 94.9405 (95.0710)  Acc@5: 99.4048 (98.8710)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-204.pth.tar', 95.10999999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-206.pth.tar', 95.07099999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-205.pth.tar', 95.0580000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-202.pth.tar', 95.05300001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-203.pth.tar', 95.02699999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-200.pth.tar', 95.02500005126953)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-197.pth.tar', 95.0)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-201.pth.tar', 94.99500004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-198.pth.tar', 94.99300000976562)

Train: 207 [   0/1171 (  0%)]  Loss:  3.024210 (3.0242)  Time: 11.579s,   88.43/s  (11.579s,   88.43/s)  LR: 1.050e-05  Data: 10.423 (10.423)
Train: 207 [  50/1171 (  4%)]  Loss:  2.175936 (2.6001)  Time: 0.583s, 1756.50/s  (2.393s,  427.89/s)  LR: 1.050e-05  Data: 0.019 (1.789)
Train: 207 [ 100/1171 (  9%)]  Loss:  2.569205 (2.5898)  Time: 1.713s,  597.63/s  (2.331s,  439.30/s)  LR: 1.050e-05  Data: 1.016 (1.733)
Train: 207 [ 150/1171 ( 13%)]  Loss:  2.842167 (2.6529)  Time: 0.587s, 1745.38/s  (2.289s,  447.41/s)  LR: 1.050e-05  Data: 0.021 (1.683)
Train: 207 [ 200/1171 ( 17%)]  Loss:  3.064548 (2.7352)  Time: 2.940s,  348.35/s  (2.283s,  448.48/s)  LR: 1.050e-05  Data: 2.378 (1.681)
Train: 207 [ 250/1171 ( 21%)]  Loss:  2.823925 (2.7500)  Time: 0.585s, 1749.09/s  (2.250s,  455.09/s)  LR: 1.050e-05  Data: 0.020 (1.647)
Train: 207 [ 300/1171 ( 26%)]  Loss:  2.802357 (2.7575)  Time: 2.008s,  510.04/s  (2.243s,  456.61/s)  LR: 1.050e-05  Data: 1.440 (1.642)
Train: 207 [ 350/1171 ( 30%)]  Loss:  2.980873 (2.7854)  Time: 0.586s, 1748.62/s  (2.295s,  446.22/s)  LR: 1.050e-05  Data: 0.019 (1.690)
Train: 207 [ 400/1171 ( 34%)]  Loss:  2.584998 (2.7631)  Time: 1.278s,  801.05/s  (2.274s,  450.39/s)  LR: 1.050e-05  Data: 0.690 (1.670)
Train: 207 [ 450/1171 ( 38%)]  Loss:  2.823444 (2.7692)  Time: 0.584s, 1753.59/s  (2.284s,  448.25/s)  LR: 1.050e-05  Data: 0.020 (1.684)
Train: 207 [ 500/1171 ( 43%)]  Loss:  2.418146 (2.7373)  Time: 0.586s, 1748.54/s  (2.287s,  447.78/s)  LR: 1.050e-05  Data: 0.020 (1.687)
Train: 207 [ 550/1171 ( 47%)]  Loss:  3.023397 (2.7611)  Time: 0.583s, 1756.98/s  (2.287s,  447.79/s)  LR: 1.050e-05  Data: 0.021 (1.687)
Train: 207 [ 600/1171 ( 51%)]  Loss:  2.947457 (2.7754)  Time: 0.587s, 1744.87/s  (2.293s,  446.55/s)  LR: 1.050e-05  Data: 0.023 (1.694)
Train: 207 [ 650/1171 ( 56%)]  Loss:  3.113966 (2.7996)  Time: 0.587s, 1743.33/s  (2.284s,  448.31/s)  LR: 1.050e-05  Data: 0.023 (1.685)
Train: 207 [ 700/1171 ( 60%)]  Loss:  2.676869 (2.7914)  Time: 0.588s, 1741.38/s  (2.305s,  444.20/s)  LR: 1.050e-05  Data: 0.019 (1.708)
Train: 207 [ 750/1171 ( 64%)]  Loss:  2.277704 (2.7593)  Time: 0.777s, 1317.41/s  (2.307s,  443.93/s)  LR: 1.050e-05  Data: 0.082 (1.710)
Train: 207 [ 800/1171 ( 68%)]  Loss:  3.160366 (2.7829)  Time: 0.917s, 1116.09/s  (2.310s,  443.38/s)  LR: 1.050e-05  Data: 0.140 (1.714)
Train: 207 [ 850/1171 ( 73%)]  Loss:  2.796266 (2.7837)  Time: 2.643s,  387.46/s  (2.328s,  439.92/s)  LR: 1.050e-05  Data: 1.989 (1.732)
Train: 207 [ 900/1171 ( 77%)]  Loss:  2.926544 (2.7912)  Time: 0.584s, 1752.50/s  (2.373s,  431.54/s)  LR: 1.050e-05  Data: 0.018 (1.777)
Train: 207 [ 950/1171 ( 81%)]  Loss:  3.077054 (2.8055)  Time: 4.099s,  249.79/s  (2.366s,  432.89/s)  LR: 1.050e-05  Data: 3.530 (1.768)
Train: 207 [1000/1171 ( 85%)]  Loss:  3.102107 (2.8196)  Time: 0.586s, 1748.51/s  (2.355s,  434.85/s)  LR: 1.050e-05  Data: 0.020 (1.757)
Train: 207 [1050/1171 ( 90%)]  Loss:  2.581060 (2.8088)  Time: 0.842s, 1215.91/s  (2.349s,  435.97/s)  LR: 1.050e-05  Data: 0.168 (1.751)
Train: 207 [1100/1171 ( 94%)]  Loss:  2.976405 (2.8160)  Time: 0.589s, 1739.64/s  (2.365s,  433.05/s)  LR: 1.050e-05  Data: 0.020 (1.768)
Train: 207 [1150/1171 ( 98%)]  Loss:  2.976549 (2.8227)  Time: 0.584s, 1754.05/s  (2.350s,  435.69/s)  LR: 1.050e-05  Data: 0.020 (1.754)
Train: 207 [1170/1171 (100%)]  Loss:  3.011850 (2.8303)  Time: 0.566s, 1809.72/s  (2.352s,  435.33/s)  LR: 1.050e-05  Data: 0.000 (1.756)
Test: [   0/97]  Time: 14.772 (14.772)  Loss:  0.2798 (0.2798)  Acc@1: 97.0703 (97.0703)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (3.068)  Loss:  0.4313 (0.3449)  Acc@1: 92.8711 (95.5002)  Acc@5: 98.2422 (98.9890)
Test: [  97/97]  Time: 0.120 (2.967)  Loss:  0.3180 (0.3546)  Acc@1: 94.7917 (95.0320)  Acc@5: 99.4048 (98.8660)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-204.pth.tar', 95.10999999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-206.pth.tar', 95.07099999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-205.pth.tar', 95.0580000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-202.pth.tar', 95.05300001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-207.pth.tar', 95.03200003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-203.pth.tar', 95.02699999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-200.pth.tar', 95.02500005126953)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-197.pth.tar', 95.0)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-201.pth.tar', 94.99500004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-193.pth.tar', 94.99499999511718)

Train: 208 [   0/1171 (  0%)]  Loss:  2.838156 (2.8382)  Time: 10.642s,   96.22/s  (10.642s,   96.22/s)  LR: 1.022e-05  Data: 9.594 (9.594)
Train: 208 [  50/1171 (  4%)]  Loss:  2.796408 (2.8173)  Time: 0.585s, 1751.05/s  (2.276s,  449.96/s)  LR: 1.022e-05  Data: 0.019 (1.681)
Train: 208 [ 100/1171 (  9%)]  Loss:  2.822893 (2.8192)  Time: 0.583s, 1755.05/s  (2.239s,  457.27/s)  LR: 1.022e-05  Data: 0.019 (1.644)
Train: 208 [ 150/1171 ( 13%)]  Loss:  2.655725 (2.7783)  Time: 0.586s, 1748.72/s  (2.173s,  471.34/s)  LR: 1.022e-05  Data: 0.018 (1.577)
Train: 208 [ 200/1171 ( 17%)]  Loss:  2.359164 (2.6945)  Time: 0.585s, 1750.64/s  (2.302s,  444.90/s)  LR: 1.022e-05  Data: 0.020 (1.706)
Train: 208 [ 250/1171 ( 21%)]  Loss:  3.035047 (2.7512)  Time: 0.583s, 1755.93/s  (2.242s,  456.80/s)  LR: 1.022e-05  Data: 0.021 (1.651)
Train: 208 [ 300/1171 ( 26%)]  Loss:  3.082911 (2.7986)  Time: 0.836s, 1224.92/s  (2.267s,  451.62/s)  LR: 1.022e-05  Data: 0.263 (1.678)
Train: 208 [ 350/1171 ( 30%)]  Loss:  3.152833 (2.8429)  Time: 0.596s, 1719.38/s  (2.246s,  455.96/s)  LR: 1.022e-05  Data: 0.030 (1.650)
Train: 208 [ 400/1171 ( 34%)]  Loss:  2.965824 (2.8566)  Time: 0.585s, 1751.15/s  (2.260s,  453.20/s)  LR: 1.022e-05  Data: 0.022 (1.664)
Train: 208 [ 450/1171 ( 38%)]  Loss:  2.829526 (2.8538)  Time: 2.878s,  355.86/s  (2.278s,  449.45/s)  LR: 1.022e-05  Data: 2.217 (1.684)
Train: 208 [ 500/1171 ( 43%)]  Loss:  3.089489 (2.8753)  Time: 1.497s,  684.10/s  (2.349s,  436.01/s)  LR: 1.022e-05  Data: 0.848 (1.753)
Train: 208 [ 550/1171 ( 47%)]  Loss:  2.472503 (2.8417)  Time: 3.414s,  299.93/s  (2.388s,  428.73/s)  LR: 1.022e-05  Data: 2.852 (1.792)
Train: 208 [ 600/1171 ( 51%)]  Loss:  2.569545 (2.8208)  Time: 1.662s,  616.15/s  (2.387s,  428.99/s)  LR: 1.022e-05  Data: 0.960 (1.790)
Train: 208 [ 650/1171 ( 56%)]  Loss:  2.192942 (2.7759)  Time: 2.032s,  503.87/s  (2.390s,  428.43/s)  LR: 1.022e-05  Data: 1.467 (1.793)
Train: 208 [ 700/1171 ( 60%)]  Loss:  2.772378 (2.7757)  Time: 3.451s,  296.75/s  (2.390s,  428.37/s)  LR: 1.022e-05  Data: 2.776 (1.792)
Train: 208 [ 750/1171 ( 64%)]  Loss:  3.140742 (2.7985)  Time: 1.974s,  518.62/s  (2.379s,  430.51/s)  LR: 1.022e-05  Data: 1.308 (1.780)
Train: 208 [ 800/1171 ( 68%)]  Loss:  3.217752 (2.8232)  Time: 0.586s, 1746.54/s  (2.364s,  433.10/s)  LR: 1.022e-05  Data: 0.022 (1.764)
Train: 208 [ 850/1171 ( 73%)]  Loss:  2.373928 (2.7982)  Time: 3.764s,  272.02/s  (2.355s,  434.86/s)  LR: 1.022e-05  Data: 3.073 (1.753)
Train: 208 [ 900/1171 ( 77%)]  Loss:  2.512344 (2.7832)  Time: 0.587s, 1743.66/s  (2.343s,  437.09/s)  LR: 1.022e-05  Data: 0.018 (1.741)
Train: 208 [ 950/1171 ( 81%)]  Loss:  3.189995 (2.8035)  Time: 4.672s,  219.16/s  (2.362s,  433.62/s)  LR: 1.022e-05  Data: 4.111 (1.760)
Train: 208 [1000/1171 ( 85%)]  Loss:  3.001004 (2.8129)  Time: 0.586s, 1746.29/s  (2.352s,  435.37/s)  LR: 1.022e-05  Data: 0.019 (1.751)
Train: 208 [1050/1171 ( 90%)]  Loss:  3.247341 (2.8327)  Time: 3.698s,  276.93/s  (2.354s,  434.96/s)  LR: 1.022e-05  Data: 3.133 (1.753)
Train: 208 [1100/1171 ( 94%)]  Loss:  2.495641 (2.8180)  Time: 0.590s, 1735.12/s  (2.350s,  435.76/s)  LR: 1.022e-05  Data: 0.020 (1.748)
Train: 208 [1150/1171 ( 98%)]  Loss:  3.117974 (2.8305)  Time: 3.499s,  292.67/s  (2.345s,  436.67/s)  LR: 1.022e-05  Data: 2.934 (1.744)
Train: 208 [1170/1171 (100%)]  Loss:  2.903634 (2.8334)  Time: 0.564s, 1816.58/s  (2.341s,  437.47/s)  LR: 1.022e-05  Data: 0.000 (1.740)
Test: [   0/97]  Time: 12.684 (12.684)  Loss:  0.2907 (0.2907)  Acc@1: 97.1680 (97.1680)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (2.929)  Loss:  0.4125 (0.3443)  Acc@1: 93.4570 (95.5595)  Acc@5: 98.5352 (98.9794)
Test: [  97/97]  Time: 0.119 (2.868)  Loss:  0.3191 (0.3541)  Acc@1: 95.0893 (95.0940)  Acc@5: 99.4048 (98.8810)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-204.pth.tar', 95.10999999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-208.pth.tar', 95.09400000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-206.pth.tar', 95.07099999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-205.pth.tar', 95.0580000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-202.pth.tar', 95.05300001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-207.pth.tar', 95.03200003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-203.pth.tar', 95.02699999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-200.pth.tar', 95.02500005126953)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-197.pth.tar', 95.0)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-201.pth.tar', 94.99500004394531)

Train: 209 [   0/1171 (  0%)]  Loss:  2.873749 (2.8737)  Time: 9.445s,  108.42/s  (9.445s,  108.42/s)  LR: 1.006e-05  Data: 8.859 (8.859)
Train: 209 [  50/1171 (  4%)]  Loss:  2.543671 (2.7087)  Time: 0.583s, 1757.05/s  (3.173s,  322.76/s)  LR: 1.006e-05  Data: 0.017 (2.569)
Train: 209 [ 100/1171 (  9%)]  Loss:  2.672473 (2.6966)  Time: 0.638s, 1605.03/s  (3.113s,  328.98/s)  LR: 1.006e-05  Data: 0.022 (2.495)
Train: 209 [ 150/1171 ( 13%)]  Loss:  3.158891 (2.8122)  Time: 0.585s, 1749.20/s  (2.831s,  361.65/s)  LR: 1.006e-05  Data: 0.019 (2.224)
Train: 209 [ 200/1171 ( 17%)]  Loss:  2.639618 (2.7777)  Time: 0.586s, 1746.01/s  (2.699s,  379.37/s)  LR: 1.006e-05  Data: 0.020 (2.096)
Train: 209 [ 250/1171 ( 21%)]  Loss:  2.844717 (2.7889)  Time: 0.583s, 1756.59/s  (2.599s,  394.03/s)  LR: 1.006e-05  Data: 0.018 (2.000)
Train: 209 [ 300/1171 ( 26%)]  Loss:  2.542672 (2.7537)  Time: 0.587s, 1745.53/s  (2.540s,  403.14/s)  LR: 1.006e-05  Data: 0.020 (1.940)
Train: 209 [ 350/1171 ( 30%)]  Loss:  2.255021 (2.6914)  Time: 0.584s, 1752.80/s  (2.479s,  413.07/s)  LR: 1.006e-05  Data: 0.020 (1.881)
Train: 209 [ 400/1171 ( 34%)]  Loss:  2.740600 (2.6968)  Time: 0.585s, 1749.86/s  (2.504s,  408.87/s)  LR: 1.006e-05  Data: 0.019 (1.907)
Train: 209 [ 450/1171 ( 38%)]  Loss:  2.864192 (2.7136)  Time: 0.585s, 1750.92/s  (2.459s,  416.49/s)  LR: 1.006e-05  Data: 0.018 (1.863)
Train: 209 [ 500/1171 ( 43%)]  Loss:  2.893327 (2.7299)  Time: 0.586s, 1746.86/s  (2.459s,  416.49/s)  LR: 1.006e-05  Data: 0.021 (1.865)
Train: 209 [ 550/1171 ( 47%)]  Loss:  2.530584 (2.7133)  Time: 0.584s, 1752.63/s  (2.447s,  418.52/s)  LR: 1.006e-05  Data: 0.020 (1.854)
Train: 209 [ 600/1171 ( 51%)]  Loss:  3.015434 (2.7365)  Time: 2.114s,  484.39/s  (2.446s,  418.67/s)  LR: 1.006e-05  Data: 1.446 (1.852)
Train: 209 [ 650/1171 ( 56%)]  Loss:  2.856278 (2.7451)  Time: 0.586s, 1747.08/s  (2.428s,  421.78/s)  LR: 1.006e-05  Data: 0.022 (1.834)
Train: 209 [ 700/1171 ( 60%)]  Loss:  2.812948 (2.7496)  Time: 3.498s,  292.73/s  (2.424s,  422.46/s)  LR: 1.006e-05  Data: 2.569 (1.828)
Train: 209 [ 750/1171 ( 64%)]  Loss:  2.853058 (2.7561)  Time: 0.587s, 1743.78/s  (2.407s,  425.40/s)  LR: 1.006e-05  Data: 0.020 (1.812)
Train: 209 [ 800/1171 ( 68%)]  Loss:  3.240745 (2.7846)  Time: 2.309s,  443.42/s  (2.428s,  421.73/s)  LR: 1.006e-05  Data: 1.645 (1.832)
Train: 209 [ 850/1171 ( 73%)]  Loss:  2.902339 (2.7911)  Time: 0.584s, 1754.07/s  (2.409s,  425.08/s)  LR: 1.006e-05  Data: 0.020 (1.812)
Train: 209 [ 900/1171 ( 77%)]  Loss:  2.867597 (2.7952)  Time: 2.976s,  344.10/s  (2.412s,  424.57/s)  LR: 1.006e-05  Data: 2.333 (1.813)
Train: 209 [ 950/1171 ( 81%)]  Loss:  2.498151 (2.7803)  Time: 0.585s, 1751.11/s  (2.429s,  421.56/s)  LR: 1.006e-05  Data: 0.020 (1.831)
Train: 209 [1000/1171 ( 85%)]  Loss:  2.873598 (2.7847)  Time: 4.196s,  244.05/s  (2.467s,  415.12/s)  LR: 1.006e-05  Data: 3.616 (1.866)
Train: 209 [1050/1171 ( 90%)]  Loss:  2.990679 (2.7941)  Time: 0.584s, 1752.75/s  (2.454s,  417.33/s)  LR: 1.006e-05  Data: 0.022 (1.853)
Train: 209 [1100/1171 ( 94%)]  Loss:  2.764781 (2.7928)  Time: 3.315s,  308.88/s  (2.446s,  418.59/s)  LR: 1.006e-05  Data: 2.753 (1.846)
Train: 209 [1150/1171 ( 98%)]  Loss:  2.610645 (2.7852)  Time: 0.587s, 1745.76/s  (2.455s,  417.14/s)  LR: 1.006e-05  Data: 0.020 (1.854)
Train: 209 [1170/1171 (100%)]  Loss:  3.043465 (2.7956)  Time: 0.563s, 1820.22/s  (2.449s,  418.16/s)  LR: 1.006e-05  Data: 0.000 (1.848)
Test: [   0/97]  Time: 11.552 (11.552)  Loss:  0.2769 (0.2769)  Acc@1: 97.1680 (97.1680)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.196 (3.101)  Loss:  0.4278 (0.3411)  Acc@1: 93.2617 (95.5576)  Acc@5: 98.2422 (98.9813)
Test: [  97/97]  Time: 0.120 (3.058)  Loss:  0.3270 (0.3523)  Acc@1: 94.1964 (95.0770)  Acc@5: 99.4048 (98.8850)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-204.pth.tar', 95.10999999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-208.pth.tar', 95.09400000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-209.pth.tar', 95.07699998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-206.pth.tar', 95.07099999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-205.pth.tar', 95.0580000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-202.pth.tar', 95.05300001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-207.pth.tar', 95.03200003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-203.pth.tar', 95.02699999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-200.pth.tar', 95.02500005126953)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-197.pth.tar', 95.0)

*** Best metric: 95.10999999755859 (epoch 204)

wandb: Waiting for W&B process to finish, PID 16220
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210606_135127-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1/logs/debug.log
wandb: Find internal logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210606_135127-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 209
wandb:     _runtime 657316
wandb:    eval_loss 0.35231
wandb:    eval_top1 95.077
wandb:    eval_top5 98.885
wandb:   _timestamp 1623023553
wandb:   train_loss 2.79557
wandb:        _step 209
wandb: Run history:
wandb:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   train_loss ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñà‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÅ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ
wandb:    eval_loss ‚ñÉ‚ñÇ‚ñà‚ñÅ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ
wandb:    eval_top1 ‚ñÅ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñà‚ñá
wandb:    eval_top5 ‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÜ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñà
wandb:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced PreTraining_vit_deit_tiny_patch16_224_fake_1k: https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
--End--
Mon Jun 7 08:52:44 JST 2021

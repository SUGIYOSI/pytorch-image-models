--Start--
Sun Jun 6 13:45:53 JST 2021
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 3
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
wandb: Currently logged in as: gyama_x (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/last.pth.tar'
wandb: Tracking run with wandb version 0.10.27
wandb: Resuming run PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gyama_x/pytorch-image-models
wandb: üöÄ View run at https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k
wandb: Run data is saved locally in /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210606_134658-PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k
wandb: Run `wandb offline` to turn off syncing.

Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/last.pth.tar'
Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/last.pth.tar' (epoch 187)
Using native Torch DistributedDataParallel.
Scheduled epochs: 210
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Train: 188 [   0/927 (  0%)]  Loss:  4.844906 (4.8449)  Time: 14.904s,   68.70/s  (14.904s,   68.70/s)  LR: 3.657e-05  Data: 12.944 (12.944)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 188 [  50/927 (  5%)]  Loss:  4.988967 (4.9169)  Time: 0.585s, 1749.30/s  (2.272s,  450.67/s)  LR: 3.657e-05  Data: 0.021 (1.667)
Train: 188 [ 100/927 ( 11%)]  Loss:  4.910842 (4.9149)  Time: 0.584s, 1753.18/s  (2.032s,  503.95/s)  LR: 3.657e-05  Data: 0.019 (1.434)
Train: 188 [ 150/927 ( 16%)]  Loss:  4.266286 (4.7528)  Time: 2.256s,  454.00/s  (1.906s,  537.29/s)  LR: 3.657e-05  Data: 1.684 (1.311)
Train: 188 [ 200/927 ( 22%)]  Loss:  4.407610 (4.6837)  Time: 0.591s, 1733.94/s  (1.951s,  524.85/s)  LR: 3.657e-05  Data: 0.018 (1.357)
Train: 188 [ 250/927 ( 27%)]  Loss:  4.016592 (4.5725)  Time: 3.635s,  281.71/s  (1.952s,  524.58/s)  LR: 3.657e-05  Data: 2.952 (1.355)
Train: 188 [ 300/927 ( 32%)]  Loss:  5.111973 (4.6496)  Time: 0.587s, 1745.54/s  (1.959s,  522.64/s)  LR: 3.657e-05  Data: 0.019 (1.361)
Train: 188 [ 350/927 ( 38%)]  Loss:  4.139459 (4.5858)  Time: 3.061s,  334.51/s  (1.958s,  522.96/s)  LR: 3.657e-05  Data: 2.435 (1.360)
Train: 188 [ 400/927 ( 43%)]  Loss:  4.921535 (4.6231)  Time: 0.588s, 1742.69/s  (1.988s,  515.08/s)  LR: 3.657e-05  Data: 0.018 (1.390)
Train: 188 [ 450/927 ( 49%)]  Loss:  4.340160 (4.5948)  Time: 2.151s,  476.11/s  (1.979s,  517.44/s)  LR: 3.657e-05  Data: 1.582 (1.382)
Train: 188 [ 500/927 ( 54%)]  Loss:  4.326757 (4.5705)  Time: 0.584s, 1754.34/s  (2.000s,  512.03/s)  LR: 3.657e-05  Data: 0.018 (1.402)
Train: 188 [ 550/927 ( 59%)]  Loss:  4.617469 (4.5744)  Time: 0.715s, 1432.37/s  (2.002s,  511.57/s)  LR: 3.657e-05  Data: 0.034 (1.404)
Train: 188 [ 600/927 ( 65%)]  Loss:  5.587525 (4.6523)  Time: 0.585s, 1750.28/s  (2.005s,  510.85/s)  LR: 3.657e-05  Data: 0.020 (1.407)
Train: 188 [ 650/927 ( 70%)]  Loss:  4.627577 (4.6505)  Time: 2.508s,  408.26/s  (2.009s,  509.67/s)  LR: 3.657e-05  Data: 1.932 (1.410)
Train: 188 [ 700/927 ( 76%)]  Loss:  4.801491 (4.6606)  Time: 0.585s, 1750.97/s  (2.020s,  507.00/s)  LR: 3.657e-05  Data: 0.020 (1.420)
Train: 188 [ 750/927 ( 81%)]  Loss:  5.276149 (4.6991)  Time: 4.435s,  230.88/s  (2.038s,  502.40/s)  LR: 3.657e-05  Data: 3.859 (1.435)
Train: 188 [ 800/927 ( 86%)]  Loss:  4.599344 (4.6932)  Time: 0.583s, 1755.65/s  (2.068s,  495.11/s)  LR: 3.657e-05  Data: 0.018 (1.467)
Train: 188 [ 850/927 ( 92%)]  Loss:  4.707723 (4.6940)  Time: 2.001s,  511.78/s  (2.071s,  494.45/s)  LR: 3.657e-05  Data: 1.417 (1.471)
Train: 188 [ 900/927 ( 97%)]  Loss:  5.177021 (4.7194)  Time: 0.585s, 1749.20/s  (2.087s,  490.65/s)  LR: 3.657e-05  Data: 0.018 (1.487)
Train: 188 [ 926/927 (100%)]  Loss:  4.604986 (4.7137)  Time: 0.567s, 1806.00/s  (2.085s,  491.16/s)  LR: 3.657e-05  Data: 0.000 (1.485)
Test: [   0/48]  Time: 14.829 (14.829)  Loss:  0.7731 (0.7731)  Acc@1: 87.1094 (87.1094)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.509 (3.146)  Loss:  0.6487 (0.7806)  Acc@1: 89.7406 (87.5420)  Acc@5: 97.4057 (96.8220)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-188.pth.tar', 87.54199993896485)

Train: 189 [   0/927 (  0%)]  Loss:  4.734385 (4.7344)  Time: 9.507s,  107.71/s  (9.507s,  107.71/s)  LR: 3.423e-05  Data: 8.844 (8.844)
Train: 189 [  50/927 (  5%)]  Loss:  4.601664 (4.6680)  Time: 0.586s, 1746.53/s  (2.108s,  485.76/s)  LR: 3.423e-05  Data: 0.018 (1.523)
Train: 189 [ 100/927 ( 11%)]  Loss:  3.952137 (4.4294)  Time: 0.583s, 1755.32/s  (2.112s,  484.92/s)  LR: 3.423e-05  Data: 0.018 (1.520)
Train: 189 [ 150/927 ( 16%)]  Loss:  5.089232 (4.5944)  Time: 0.584s, 1754.55/s  (2.058s,  497.52/s)  LR: 3.423e-05  Data: 0.020 (1.471)
Train: 189 [ 200/927 ( 22%)]  Loss:  4.823553 (4.6402)  Time: 0.585s, 1750.15/s  (2.056s,  497.97/s)  LR: 3.423e-05  Data: 0.020 (1.470)
Train: 189 [ 250/927 ( 27%)]  Loss:  5.290739 (4.7486)  Time: 0.586s, 1746.05/s  (2.123s,  482.24/s)  LR: 3.423e-05  Data: 0.023 (1.537)
Train: 189 [ 300/927 ( 32%)]  Loss:  3.422630 (4.5592)  Time: 0.585s, 1749.69/s  (2.162s,  473.63/s)  LR: 3.423e-05  Data: 0.020 (1.575)
Train: 189 [ 350/927 ( 38%)]  Loss:  5.464267 (4.6723)  Time: 0.587s, 1743.88/s  (2.167s,  472.49/s)  LR: 3.423e-05  Data: 0.023 (1.582)
Train: 189 [ 400/927 ( 43%)]  Loss:  4.832469 (4.6901)  Time: 0.591s, 1732.84/s  (2.196s,  466.32/s)  LR: 3.423e-05  Data: 0.020 (1.610)
Train: 189 [ 450/927 ( 49%)]  Loss:  4.230197 (4.6441)  Time: 3.520s,  290.88/s  (2.203s,  464.77/s)  LR: 3.423e-05  Data: 2.957 (1.617)
Train: 189 [ 500/927 ( 54%)]  Loss:  4.579770 (4.6383)  Time: 0.589s, 1738.64/s  (2.236s,  457.99/s)  LR: 3.423e-05  Data: 0.022 (1.647)
Train: 189 [ 550/927 ( 59%)]  Loss:  4.934496 (4.6630)  Time: 6.866s,  149.14/s  (2.243s,  456.47/s)  LR: 3.423e-05  Data: 6.168 (1.654)
Train: 189 [ 600/927 ( 65%)]  Loss:  5.161950 (4.7013)  Time: 0.588s, 1740.49/s  (2.254s,  454.26/s)  LR: 3.423e-05  Data: 0.023 (1.663)
Train: 189 [ 650/927 ( 70%)]  Loss:  4.996192 (4.7224)  Time: 7.218s,  141.86/s  (2.264s,  452.30/s)  LR: 3.423e-05  Data: 6.471 (1.673)
Train: 189 [ 700/927 ( 76%)]  Loss:  3.963020 (4.6718)  Time: 0.585s, 1751.59/s  (2.273s,  450.55/s)  LR: 3.423e-05  Data: 0.020 (1.682)
Train: 189 [ 750/927 ( 81%)]  Loss:  4.662718 (4.6712)  Time: 7.451s,  137.43/s  (2.280s,  449.12/s)  LR: 3.423e-05  Data: 6.888 (1.690)
Train: 189 [ 800/927 ( 86%)]  Loss:  5.055881 (4.6938)  Time: 0.586s, 1746.67/s  (2.275s,  450.08/s)  LR: 3.423e-05  Data: 0.022 (1.685)
Train: 189 [ 850/927 ( 92%)]  Loss:  4.253237 (4.6694)  Time: 7.284s,  140.58/s  (2.276s,  449.82/s)  LR: 3.423e-05  Data: 6.719 (1.687)
Train: 189 [ 900/927 ( 97%)]  Loss:  5.010812 (4.6873)  Time: 0.584s, 1752.91/s  (2.266s,  451.92/s)  LR: 3.423e-05  Data: 0.018 (1.678)
Train: 189 [ 926/927 (100%)]  Loss:  4.293904 (4.6677)  Time: 0.564s, 1815.07/s  (2.259s,  453.29/s)  LR: 3.423e-05  Data: 0.000 (1.671)
Test: [   0/48]  Time: 12.723 (12.723)  Loss:  0.7627 (0.7627)  Acc@1: 87.3047 (87.3047)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.149 (3.165)  Loss:  0.6663 (0.7708)  Acc@1: 88.9151 (87.7140)  Acc@5: 97.2877 (96.8600)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-189.pth.tar', 87.71399996826172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-188.pth.tar', 87.54199993896485)

Train: 190 [   0/927 (  0%)]  Loss:  4.459633 (4.4596)  Time: 14.144s,   72.40/s  (14.144s,   72.40/s)  LR: 3.199e-05  Data: 13.132 (13.132)
Train: 190 [  50/927 (  5%)]  Loss:  4.858956 (4.6593)  Time: 0.591s, 1733.89/s  (2.397s,  427.29/s)  LR: 3.199e-05  Data: 0.023 (1.808)
Train: 190 [ 100/927 ( 11%)]  Loss:  5.101551 (4.8067)  Time: 6.051s,  169.24/s  (2.394s,  427.81/s)  LR: 3.199e-05  Data: 5.449 (1.797)
Train: 190 [ 150/927 ( 16%)]  Loss:  5.283455 (4.9259)  Time: 0.589s, 1739.72/s  (2.320s,  441.38/s)  LR: 3.199e-05  Data: 0.022 (1.726)
Train: 190 [ 200/927 ( 22%)]  Loss:  4.630337 (4.8668)  Time: 7.306s,  140.16/s  (2.301s,  444.95/s)  LR: 3.199e-05  Data: 6.742 (1.709)
Train: 190 [ 250/927 ( 27%)]  Loss:  4.595291 (4.8215)  Time: 0.586s, 1747.77/s  (2.249s,  455.24/s)  LR: 3.199e-05  Data: 0.022 (1.656)
Train: 190 [ 300/927 ( 32%)]  Loss:  5.160115 (4.8699)  Time: 4.826s,  212.18/s  (2.251s,  454.98/s)  LR: 3.199e-05  Data: 4.173 (1.655)
Train: 190 [ 350/927 ( 38%)]  Loss:  5.065471 (4.8944)  Time: 0.584s, 1752.12/s  (2.222s,  460.93/s)  LR: 3.199e-05  Data: 0.021 (1.624)
Train: 190 [ 400/927 ( 43%)]  Loss:  4.763771 (4.8798)  Time: 10.394s,   98.52/s  (2.247s,  455.66/s)  LR: 3.199e-05  Data: 9.663 (1.651)
Train: 190 [ 450/927 ( 49%)]  Loss:  4.553245 (4.8472)  Time: 0.587s, 1744.30/s  (2.240s,  457.23/s)  LR: 3.199e-05  Data: 0.022 (1.644)
Train: 190 [ 500/927 ( 54%)]  Loss:  4.732840 (4.8368)  Time: 7.250s,  141.24/s  (2.258s,  453.57/s)  LR: 3.199e-05  Data: 6.686 (1.662)
Train: 190 [ 550/927 ( 59%)]  Loss:  4.984567 (4.8491)  Time: 0.588s, 1742.94/s  (2.263s,  452.58/s)  LR: 3.199e-05  Data: 0.017 (1.667)
Train: 190 [ 600/927 ( 65%)]  Loss:  5.055864 (4.8650)  Time: 7.274s,  140.78/s  (2.275s,  450.12/s)  LR: 3.199e-05  Data: 6.705 (1.681)
Train: 190 [ 650/927 ( 70%)]  Loss:  4.638774 (4.8488)  Time: 0.585s, 1749.21/s  (2.297s,  445.75/s)  LR: 3.199e-05  Data: 0.019 (1.704)
Train: 190 [ 700/927 ( 76%)]  Loss:  4.004061 (4.7925)  Time: 7.557s,  135.50/s  (2.309s,  443.56/s)  LR: 3.199e-05  Data: 6.881 (1.716)
Train: 190 [ 750/927 ( 81%)]  Loss:  5.058003 (4.8091)  Time: 0.589s, 1737.94/s  (2.301s,  445.10/s)  LR: 3.199e-05  Data: 0.020 (1.707)
Train: 190 [ 800/927 ( 86%)]  Loss:  5.141527 (4.8287)  Time: 6.872s,  149.01/s  (2.319s,  441.54/s)  LR: 3.199e-05  Data: 6.295 (1.726)
Train: 190 [ 850/927 ( 92%)]  Loss:  5.454377 (4.8634)  Time: 0.586s, 1748.18/s  (2.317s,  442.00/s)  LR: 3.199e-05  Data: 0.019 (1.724)
Train: 190 [ 900/927 ( 97%)]  Loss:  5.139619 (4.8780)  Time: 7.325s,  139.80/s  (2.318s,  441.72/s)  LR: 3.199e-05  Data: 6.743 (1.726)
Train: 190 [ 926/927 (100%)]  Loss:  5.166497 (4.8924)  Time: 0.564s, 1814.88/s  (2.311s,  443.11/s)  LR: 3.199e-05  Data: 0.000 (1.719)
Test: [   0/48]  Time: 12.622 (12.622)  Loss:  0.7647 (0.7647)  Acc@1: 87.7930 (87.7930)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.149 (3.175)  Loss:  0.6497 (0.7547)  Acc@1: 89.7406 (87.7320)  Acc@5: 97.6415 (96.9180)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-190.pth.tar', 87.73200006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-189.pth.tar', 87.71399996826172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-188.pth.tar', 87.54199993896485)

Train: 191 [   0/927 (  0%)]  Loss:  5.133933 (5.1339)  Time: 9.070s,  112.90/s  (9.070s,  112.90/s)  LR: 2.986e-05  Data: 8.489 (8.489)
Train: 191 [  50/927 (  5%)]  Loss:  4.872697 (5.0033)  Time: 0.585s, 1750.87/s  (2.234s,  458.42/s)  LR: 2.986e-05  Data: 0.021 (1.645)
Train: 191 [ 100/927 ( 11%)]  Loss:  5.127072 (5.0446)  Time: 0.590s, 1734.22/s  (2.191s,  467.35/s)  LR: 2.986e-05  Data: 0.023 (1.602)
Train: 191 [ 150/927 ( 16%)]  Loss:  4.414434 (4.8870)  Time: 0.585s, 1750.35/s  (2.123s,  482.31/s)  LR: 2.986e-05  Data: 0.021 (1.538)
Train: 191 [ 200/927 ( 22%)]  Loss:  5.436659 (4.9970)  Time: 1.192s,  859.01/s  (2.213s,  462.75/s)  LR: 2.986e-05  Data: 0.536 (1.622)
Train: 191 [ 250/927 ( 27%)]  Loss:  4.577692 (4.9271)  Time: 0.587s, 1743.70/s  (2.253s,  454.58/s)  LR: 2.986e-05  Data: 0.022 (1.659)
Train: 191 [ 300/927 ( 32%)]  Loss:  4.757858 (4.9029)  Time: 3.627s,  282.36/s  (2.281s,  448.98/s)  LR: 2.986e-05  Data: 2.938 (1.682)
Train: 191 [ 350/927 ( 38%)]  Loss:  4.954026 (4.9093)  Time: 0.590s, 1735.05/s  (2.282s,  448.69/s)  LR: 2.986e-05  Data: 0.025 (1.683)
Train: 191 [ 400/927 ( 43%)]  Loss:  5.124750 (4.9332)  Time: 5.993s,  170.85/s  (2.282s,  448.63/s)  LR: 2.986e-05  Data: 5.430 (1.682)
Train: 191 [ 450/927 ( 49%)]  Loss:  4.544441 (4.8944)  Time: 0.587s, 1743.97/s  (2.271s,  450.86/s)  LR: 2.986e-05  Data: 0.021 (1.668)
Train: 191 [ 500/927 ( 54%)]  Loss:  4.328108 (4.8429)  Time: 4.503s,  227.41/s  (2.267s,  451.67/s)  LR: 2.986e-05  Data: 3.939 (1.664)
Train: 191 [ 550/927 ( 59%)]  Loss:  4.942046 (4.8511)  Time: 0.585s, 1751.70/s  (2.260s,  453.19/s)  LR: 2.986e-05  Data: 0.019 (1.656)
Train: 191 [ 600/927 ( 65%)]  Loss:  4.706044 (4.8400)  Time: 5.653s,  181.14/s  (2.296s,  446.02/s)  LR: 2.986e-05  Data: 4.992 (1.693)
Train: 191 [ 650/927 ( 70%)]  Loss:  5.038661 (4.8542)  Time: 0.584s, 1753.55/s  (2.307s,  443.93/s)  LR: 2.986e-05  Data: 0.018 (1.704)
Train: 191 [ 700/927 ( 76%)]  Loss:  4.759260 (4.8478)  Time: 7.185s,  142.52/s  (2.316s,  442.13/s)  LR: 2.986e-05  Data: 6.584 (1.714)
Train: 191 [ 750/927 ( 81%)]  Loss:  4.601498 (4.8324)  Time: 0.587s, 1745.60/s  (2.310s,  443.38/s)  LR: 2.986e-05  Data: 0.020 (1.708)
Train: 191 [ 800/927 ( 86%)]  Loss:  4.401670 (4.8071)  Time: 6.749s,  151.72/s  (2.310s,  443.35/s)  LR: 2.986e-05  Data: 6.163 (1.708)
Train: 191 [ 850/927 ( 92%)]  Loss:  4.355223 (4.7820)  Time: 0.585s, 1749.52/s  (2.299s,  445.38/s)  LR: 2.986e-05  Data: 0.021 (1.697)
Train: 191 [ 900/927 ( 97%)]  Loss:  4.265388 (4.7548)  Time: 6.628s,  154.49/s  (2.296s,  445.90/s)  LR: 2.986e-05  Data: 6.047 (1.695)
Train: 191 [ 926/927 (100%)]  Loss:  4.573646 (4.7458)  Time: 0.567s, 1807.24/s  (2.287s,  447.74/s)  LR: 2.986e-05  Data: 0.000 (1.686)
Test: [   0/48]  Time: 12.172 (12.172)  Loss:  0.7580 (0.7580)  Acc@1: 87.5000 (87.5000)  Acc@5: 96.1914 (96.1914)
Test: [  48/48]  Time: 0.149 (3.460)  Loss:  0.6647 (0.7589)  Acc@1: 89.0330 (88.0580)  Acc@5: 97.2877 (96.9420)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-191.pth.tar', 88.05800001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-190.pth.tar', 87.73200006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-189.pth.tar', 87.71399996826172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-188.pth.tar', 87.54199993896485)

Train: 192 [   0/927 (  0%)]  Loss:  5.435021 (5.4350)  Time: 12.087s,   84.72/s  (12.087s,   84.72/s)  LR: 2.784e-05  Data: 10.925 (10.925)
Train: 192 [  50/927 (  5%)]  Loss:  5.172859 (5.3039)  Time: 0.587s, 1743.92/s  (2.531s,  404.53/s)  LR: 2.784e-05  Data: 0.022 (1.934)
Train: 192 [ 100/927 ( 11%)]  Loss:  4.840541 (5.1495)  Time: 0.589s, 1739.23/s  (2.482s,  412.53/s)  LR: 2.784e-05  Data: 0.020 (1.884)
Train: 192 [ 150/927 ( 16%)]  Loss:  5.246058 (5.1736)  Time: 0.587s, 1743.83/s  (2.392s,  428.03/s)  LR: 2.784e-05  Data: 0.020 (1.798)
Train: 192 [ 200/927 ( 22%)]  Loss:  4.964332 (5.1318)  Time: 0.588s, 1740.21/s  (2.367s,  432.59/s)  LR: 2.784e-05  Data: 0.021 (1.777)
Train: 192 [ 250/927 ( 27%)]  Loss:  4.415419 (5.0124)  Time: 0.588s, 1741.92/s  (2.321s,  441.27/s)  LR: 2.784e-05  Data: 0.024 (1.733)
Train: 192 [ 300/927 ( 32%)]  Loss:  4.083902 (4.8797)  Time: 0.587s, 1743.42/s  (2.303s,  444.60/s)  LR: 2.784e-05  Data: 0.019 (1.716)
Train: 192 [ 350/927 ( 38%)]  Loss:  5.434189 (4.9490)  Time: 0.587s, 1744.04/s  (2.282s,  448.73/s)  LR: 2.784e-05  Data: 0.019 (1.695)
Train: 192 [ 400/927 ( 43%)]  Loss:  4.753211 (4.9273)  Time: 0.700s, 1463.50/s  (2.327s,  439.98/s)  LR: 2.784e-05  Data: 0.107 (1.739)
Train: 192 [ 450/927 ( 49%)]  Loss:  4.130037 (4.8476)  Time: 0.586s, 1748.13/s  (2.330s,  439.48/s)  LR: 2.784e-05  Data: 0.019 (1.741)
Train: 192 [ 500/927 ( 54%)]  Loss:  4.212808 (4.7899)  Time: 0.591s, 1732.69/s  (2.334s,  438.71/s)  LR: 2.784e-05  Data: 0.023 (1.746)
Train: 192 [ 550/927 ( 59%)]  Loss:  4.534247 (4.7686)  Time: 0.585s, 1749.20/s  (2.325s,  440.48/s)  LR: 2.784e-05  Data: 0.020 (1.737)
Train: 192 [ 600/927 ( 65%)]  Loss:  3.945429 (4.7052)  Time: 1.206s,  849.32/s  (2.332s,  439.12/s)  LR: 2.784e-05  Data: 0.640 (1.744)
Train: 192 [ 650/927 ( 70%)]  Loss:  5.168895 (4.7384)  Time: 0.583s, 1755.89/s  (2.324s,  440.55/s)  LR: 2.784e-05  Data: 0.018 (1.736)
Train: 192 [ 700/927 ( 76%)]  Loss:  5.607888 (4.7963)  Time: 0.584s, 1752.40/s  (2.318s,  441.85/s)  LR: 2.784e-05  Data: 0.019 (1.729)
Train: 192 [ 750/927 ( 81%)]  Loss:  5.177181 (4.8201)  Time: 0.589s, 1738.15/s  (2.322s,  440.98/s)  LR: 2.784e-05  Data: 0.023 (1.734)
Train: 192 [ 800/927 ( 86%)]  Loss:  5.168198 (4.8406)  Time: 0.585s, 1750.04/s  (2.328s,  439.77/s)  LR: 2.784e-05  Data: 0.018 (1.740)
Train: 192 [ 850/927 ( 92%)]  Loss:  4.269763 (4.8089)  Time: 0.592s, 1731.15/s  (2.328s,  439.86/s)  LR: 2.784e-05  Data: 0.019 (1.739)
Train: 192 [ 900/927 ( 97%)]  Loss:  4.087756 (4.7709)  Time: 0.589s, 1737.33/s  (2.332s,  439.03/s)  LR: 2.784e-05  Data: 0.019 (1.744)
Train: 192 [ 926/927 (100%)]  Loss:  4.810195 (4.7729)  Time: 0.565s, 1812.49/s  (2.328s,  439.94/s)  LR: 2.784e-05  Data: 0.000 (1.739)
Test: [   0/48]  Time: 13.395 (13.395)  Loss:  0.7321 (0.7321)  Acc@1: 88.2812 (88.2812)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.149 (3.231)  Loss:  0.6631 (0.7470)  Acc@1: 89.1509 (88.2280)  Acc@5: 97.1698 (97.0160)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-192.pth.tar', 88.22800007080077)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-191.pth.tar', 88.05800001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-190.pth.tar', 87.73200006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-189.pth.tar', 87.71399996826172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-188.pth.tar', 87.54199993896485)

Train: 193 [   0/927 (  0%)]  Loss:  4.860399 (4.8604)  Time: 6.486s,  157.89/s  (6.486s,  157.89/s)  LR: 2.592e-05  Data: 5.646 (5.646)
Train: 193 [  50/927 (  5%)]  Loss:  4.647776 (4.7541)  Time: 0.588s, 1741.32/s  (2.046s,  500.51/s)  LR: 2.592e-05  Data: 0.021 (1.452)
Train: 193 [ 100/927 ( 11%)]  Loss:  4.670169 (4.7261)  Time: 2.116s,  483.89/s  (2.071s,  494.47/s)  LR: 2.592e-05  Data: 1.445 (1.478)
Train: 193 [ 150/927 ( 16%)]  Loss:  4.390645 (4.6422)  Time: 0.586s, 1746.61/s  (2.190s,  467.64/s)  LR: 2.592e-05  Data: 0.022 (1.589)
Train: 193 [ 200/927 ( 22%)]  Loss:  4.528977 (4.6196)  Time: 3.860s,  265.30/s  (2.251s,  455.00/s)  LR: 2.592e-05  Data: 3.296 (1.653)
Train: 193 [ 250/927 ( 27%)]  Loss:  5.480653 (4.7631)  Time: 0.585s, 1751.49/s  (2.248s,  455.45/s)  LR: 2.592e-05  Data: 0.020 (1.650)
Train: 193 [ 300/927 ( 32%)]  Loss:  4.530522 (4.7299)  Time: 4.492s,  227.96/s  (2.279s,  449.36/s)  LR: 2.592e-05  Data: 3.900 (1.681)
Train: 193 [ 350/927 ( 38%)]  Loss:  5.032070 (4.7677)  Time: 0.588s, 1740.19/s  (2.280s,  449.07/s)  LR: 2.592e-05  Data: 0.024 (1.683)
Train: 193 [ 400/927 ( 43%)]  Loss:  4.209942 (4.7057)  Time: 6.900s,  148.41/s  (2.297s,  445.75/s)  LR: 2.592e-05  Data: 6.184 (1.697)
Train: 193 [ 450/927 ( 49%)]  Loss:  5.140283 (4.7491)  Time: 0.584s, 1753.67/s  (2.286s,  448.02/s)  LR: 2.592e-05  Data: 0.019 (1.684)
Train: 193 [ 500/927 ( 54%)]  Loss:  4.725070 (4.7470)  Time: 4.398s,  232.86/s  (2.280s,  449.04/s)  LR: 2.592e-05  Data: 3.750 (1.678)
Train: 193 [ 550/927 ( 59%)]  Loss:  4.813932 (4.7525)  Time: 0.585s, 1749.63/s  (2.299s,  445.39/s)  LR: 2.592e-05  Data: 0.021 (1.696)
Train: 193 [ 600/927 ( 65%)]  Loss:  4.784873 (4.7550)  Time: 2.986s,  342.95/s  (2.321s,  441.14/s)  LR: 2.592e-05  Data: 2.332 (1.718)
Train: 193 [ 650/927 ( 70%)]  Loss:  5.225992 (4.7887)  Time: 0.586s, 1748.56/s  (2.324s,  440.71/s)  LR: 2.592e-05  Data: 0.021 (1.720)
Train: 193 [ 700/927 ( 76%)]  Loss:  5.333134 (4.8250)  Time: 8.060s,  127.04/s  (2.327s,  440.11/s)  LR: 2.592e-05  Data: 7.382 (1.724)
Train: 193 [ 750/927 ( 81%)]  Loss:  4.877162 (4.8282)  Time: 0.588s, 1740.93/s  (2.320s,  441.30/s)  LR: 2.592e-05  Data: 0.020 (1.719)
Train: 193 [ 800/927 ( 86%)]  Loss:  4.451862 (4.8061)  Time: 6.199s,  165.19/s  (2.323s,  440.81/s)  LR: 2.592e-05  Data: 5.635 (1.723)
Train: 193 [ 850/927 ( 92%)]  Loss:  4.876392 (4.8100)  Time: 0.587s, 1743.98/s  (2.318s,  441.81/s)  LR: 2.592e-05  Data: 0.021 (1.717)
Train: 193 [ 900/927 ( 97%)]  Loss:  4.637677 (4.8009)  Time: 9.286s,  110.27/s  (2.325s,  440.52/s)  LR: 2.592e-05  Data: 8.672 (1.723)
Train: 193 [ 926/927 (100%)]  Loss:  5.137228 (4.8177)  Time: 0.564s, 1816.38/s  (2.327s,  440.10/s)  LR: 2.592e-05  Data: 0.000 (1.725)
Test: [   0/48]  Time: 12.806 (12.806)  Loss:  0.7527 (0.7527)  Acc@1: 87.7930 (87.7930)  Acc@5: 95.9961 (95.9961)
Test: [  48/48]  Time: 0.149 (3.445)  Loss:  0.6393 (0.7382)  Acc@1: 90.2123 (88.4460)  Acc@5: 97.7594 (97.0300)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-193.pth.tar', 88.44600001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-192.pth.tar', 88.22800007080077)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-191.pth.tar', 88.05800001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-190.pth.tar', 87.73200006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-189.pth.tar', 87.71399996826172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-188.pth.tar', 87.54199993896485)

Train: 194 [   0/927 (  0%)]  Loss:  5.225031 (5.2250)  Time: 11.377s,   90.00/s  (11.377s,   90.00/s)  LR: 2.411e-05  Data: 10.610 (10.610)
Train: 194 [  50/927 (  5%)]  Loss:  4.744095 (4.9846)  Time: 0.591s, 1733.33/s  (2.400s,  426.64/s)  LR: 2.411e-05  Data: 0.025 (1.791)
Train: 194 [ 100/927 ( 11%)]  Loss:  5.035375 (5.0015)  Time: 1.772s,  577.78/s  (2.397s,  427.25/s)  LR: 2.411e-05  Data: 1.173 (1.791)
Train: 194 [ 150/927 ( 16%)]  Loss:  3.515052 (4.6299)  Time: 0.587s, 1743.85/s  (2.331s,  439.24/s)  LR: 2.411e-05  Data: 0.023 (1.730)
Train: 194 [ 200/927 ( 22%)]  Loss:  4.474069 (4.5987)  Time: 5.382s,  190.27/s  (2.308s,  443.76/s)  LR: 2.411e-05  Data: 4.790 (1.706)
Train: 194 [ 250/927 ( 27%)]  Loss:  4.674472 (4.6113)  Time: 0.586s, 1748.59/s  (2.273s,  450.54/s)  LR: 2.411e-05  Data: 0.019 (1.670)
Train: 194 [ 300/927 ( 32%)]  Loss:  5.003792 (4.6674)  Time: 8.213s,  124.67/s  (2.322s,  441.03/s)  LR: 2.411e-05  Data: 7.644 (1.722)
Train: 194 [ 350/927 ( 38%)]  Loss:  4.607581 (4.6599)  Time: 0.585s, 1751.26/s  (2.318s,  441.84/s)  LR: 2.411e-05  Data: 0.018 (1.719)
Train: 194 [ 400/927 ( 43%)]  Loss:  4.465806 (4.6384)  Time: 7.250s,  141.23/s  (2.326s,  440.18/s)  LR: 2.411e-05  Data: 6.552 (1.729)
Train: 194 [ 450/927 ( 49%)]  Loss:  4.677076 (4.6422)  Time: 0.591s, 1734.01/s  (2.321s,  441.18/s)  LR: 2.411e-05  Data: 0.019 (1.725)
Train: 194 [ 500/927 ( 54%)]  Loss:  4.764001 (4.6533)  Time: 5.677s,  180.38/s  (2.329s,  439.58/s)  LR: 2.411e-05  Data: 5.113 (1.734)
Train: 194 [ 550/927 ( 59%)]  Loss:  4.884109 (4.6725)  Time: 0.588s, 1740.08/s  (2.326s,  440.24/s)  LR: 2.411e-05  Data: 0.019 (1.731)
Train: 194 [ 600/927 ( 65%)]  Loss:  4.698983 (4.6746)  Time: 7.600s,  134.74/s  (2.343s,  437.11/s)  LR: 2.411e-05  Data: 7.011 (1.747)
Train: 194 [ 650/927 ( 70%)]  Loss:  4.916162 (4.6918)  Time: 0.585s, 1749.51/s  (2.340s,  437.65/s)  LR: 2.411e-05  Data: 0.019 (1.744)
Train: 194 [ 700/927 ( 76%)]  Loss:  4.307796 (4.6662)  Time: 8.594s,  119.16/s  (2.374s,  431.25/s)  LR: 2.411e-05  Data: 7.985 (1.779)
Train: 194 [ 750/927 ( 81%)]  Loss:  5.000579 (4.6871)  Time: 0.591s, 1731.78/s  (2.376s,  431.07/s)  LR: 2.411e-05  Data: 0.019 (1.781)
Train: 194 [ 800/927 ( 86%)]  Loss:  4.344094 (4.6669)  Time: 8.238s,  124.31/s  (2.383s,  429.69/s)  LR: 2.411e-05  Data: 7.583 (1.788)
Train: 194 [ 850/927 ( 92%)]  Loss:  4.888333 (4.6792)  Time: 0.588s, 1740.66/s  (2.379s,  430.42/s)  LR: 2.411e-05  Data: 0.019 (1.784)
Train: 194 [ 900/927 ( 97%)]  Loss:  4.944888 (4.6932)  Time: 7.361s,  139.11/s  (2.380s,  430.27/s)  LR: 2.411e-05  Data: 6.711 (1.785)
Train: 194 [ 926/927 (100%)]  Loss:  4.709168 (4.6940)  Time: 0.564s, 1815.71/s  (2.374s,  431.40/s)  LR: 2.411e-05  Data: 0.000 (1.779)
Test: [   0/48]  Time: 13.583 (13.583)  Loss:  0.7221 (0.7221)  Acc@1: 88.6719 (88.6719)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.150 (3.205)  Loss:  0.6487 (0.7298)  Acc@1: 89.7406 (88.4080)  Acc@5: 97.1698 (97.0320)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-193.pth.tar', 88.44600001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-194.pth.tar', 88.40800006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-192.pth.tar', 88.22800007080077)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-191.pth.tar', 88.05800001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-190.pth.tar', 87.73200006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-189.pth.tar', 87.71399996826172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-188.pth.tar', 87.54199993896485)

Train: 195 [   0/927 (  0%)]  Loss:  4.644964 (4.6450)  Time: 9.200s,  111.30/s  (9.200s,  111.30/s)  LR: 2.241e-05  Data: 8.250 (8.250)
Train: 195 [  50/927 (  5%)]  Loss:  5.221051 (4.9330)  Time: 0.588s, 1740.36/s  (2.504s,  408.99/s)  LR: 2.241e-05  Data: 0.019 (1.905)
Train: 195 [ 100/927 ( 11%)]  Loss:  4.380826 (4.7489)  Time: 0.585s, 1750.95/s  (2.497s,  410.03/s)  LR: 2.241e-05  Data: 0.019 (1.898)
Train: 195 [ 150/927 ( 16%)]  Loss:  3.594756 (4.4604)  Time: 0.584s, 1752.58/s  (2.455s,  417.18/s)  LR: 2.241e-05  Data: 0.018 (1.859)
Train: 195 [ 200/927 ( 22%)]  Loss:  3.835999 (4.3355)  Time: 0.585s, 1750.09/s  (2.442s,  419.28/s)  LR: 2.241e-05  Data: 0.019 (1.849)
Train: 195 [ 250/927 ( 27%)]  Loss:  4.364239 (4.3403)  Time: 0.585s, 1750.71/s  (2.409s,  425.11/s)  LR: 2.241e-05  Data: 0.019 (1.814)
Train: 195 [ 300/927 ( 32%)]  Loss:  4.831122 (4.4104)  Time: 1.697s,  603.54/s  (2.389s,  428.71/s)  LR: 2.241e-05  Data: 1.133 (1.794)
Train: 195 [ 350/927 ( 38%)]  Loss:  3.832123 (4.3381)  Time: 0.588s, 1741.15/s  (2.358s,  434.22/s)  LR: 2.241e-05  Data: 0.019 (1.762)
Train: 195 [ 400/927 ( 43%)]  Loss:  4.580415 (4.3651)  Time: 1.486s,  689.01/s  (2.349s,  435.94/s)  LR: 2.241e-05  Data: 0.837 (1.751)
Train: 195 [ 450/927 ( 49%)]  Loss:  4.433062 (4.3719)  Time: 0.584s, 1752.27/s  (2.370s,  432.15/s)  LR: 2.241e-05  Data: 0.019 (1.769)
Train: 195 [ 500/927 ( 54%)]  Loss:  5.459254 (4.4707)  Time: 1.288s,  794.90/s  (2.374s,  431.34/s)  LR: 2.241e-05  Data: 0.620 (1.774)
Train: 195 [ 550/927 ( 59%)]  Loss:  4.561568 (4.4783)  Time: 0.586s, 1748.69/s  (2.375s,  431.13/s)  LR: 2.241e-05  Data: 0.020 (1.773)
Train: 195 [ 600/927 ( 65%)]  Loss:  5.170518 (4.5315)  Time: 5.693s,  179.88/s  (2.388s,  428.79/s)  LR: 2.241e-05  Data: 5.127 (1.786)
Train: 195 [ 650/927 ( 70%)]  Loss:  4.884677 (4.5568)  Time: 0.587s, 1745.30/s  (2.384s,  429.45/s)  LR: 2.241e-05  Data: 0.023 (1.783)
Train: 195 [ 700/927 ( 76%)]  Loss:  4.290954 (4.5390)  Time: 3.316s,  308.82/s  (2.381s,  430.00/s)  LR: 2.241e-05  Data: 2.725 (1.779)
Train: 195 [ 750/927 ( 81%)]  Loss:  4.585500 (4.5419)  Time: 0.587s, 1743.05/s  (2.368s,  432.50/s)  LR: 2.241e-05  Data: 0.022 (1.764)
Train: 195 [ 800/927 ( 86%)]  Loss:  4.790400 (4.5566)  Time: 11.390s,   89.91/s  (2.378s,  430.66/s)  LR: 2.241e-05  Data: 10.783 (1.774)
Train: 195 [ 850/927 ( 92%)]  Loss:  5.088071 (4.5861)  Time: 0.591s, 1732.52/s  (2.370s,  432.06/s)  LR: 2.241e-05  Data: 0.027 (1.767)
Train: 195 [ 900/927 ( 97%)]  Loss:  5.085523 (4.6124)  Time: 4.146s,  246.98/s  (2.376s,  430.96/s)  LR: 2.241e-05  Data: 3.501 (1.773)
Train: 195 [ 926/927 (100%)]  Loss:  5.761405 (4.6698)  Time: 0.564s, 1815.37/s  (2.373s,  431.48/s)  LR: 2.241e-05  Data: 0.000 (1.770)
Test: [   0/48]  Time: 14.768 (14.768)  Loss:  0.7322 (0.7322)  Acc@1: 88.4766 (88.4766)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.149 (3.408)  Loss:  0.6324 (0.7463)  Acc@1: 90.3302 (88.1040)  Acc@5: 97.8774 (97.0420)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-193.pth.tar', 88.44600001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-194.pth.tar', 88.40800006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-192.pth.tar', 88.22800007080077)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-195.pth.tar', 88.10399993652344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-191.pth.tar', 88.05800001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-190.pth.tar', 87.73200006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-189.pth.tar', 87.71399996826172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-188.pth.tar', 87.54199993896485)

Train: 196 [   0/927 (  0%)]  Loss:  5.025454 (5.0255)  Time: 12.090s,   84.69/s  (12.090s,   84.69/s)  LR: 2.082e-05  Data: 10.679 (10.679)
Train: 196 [  50/927 (  5%)]  Loss:  4.418283 (4.7219)  Time: 0.585s, 1750.67/s  (2.406s,  425.57/s)  LR: 2.082e-05  Data: 0.021 (1.776)
Train: 196 [ 100/927 ( 11%)]  Loss:  4.115399 (4.5197)  Time: 3.184s,  321.60/s  (2.313s,  442.78/s)  LR: 2.082e-05  Data: 2.524 (1.699)
Train: 196 [ 150/927 ( 16%)]  Loss:  4.391811 (4.4877)  Time: 0.617s, 1658.41/s  (2.217s,  461.96/s)  LR: 2.082e-05  Data: 0.023 (1.606)
Train: 196 [ 200/927 ( 22%)]  Loss:  4.045568 (4.3993)  Time: 0.701s, 1459.93/s  (2.277s,  449.74/s)  LR: 2.082e-05  Data: 0.110 (1.665)
Train: 196 [ 250/927 ( 27%)]  Loss:  5.653892 (4.6084)  Time: 1.841s,  556.26/s  (2.285s,  448.06/s)  LR: 2.082e-05  Data: 1.269 (1.676)
Train: 196 [ 300/927 ( 32%)]  Loss:  4.540325 (4.5987)  Time: 4.767s,  214.79/s  (2.322s,  441.05/s)  LR: 2.082e-05  Data: 4.194 (1.715)
Train: 196 [ 350/927 ( 38%)]  Loss:  4.643924 (4.6043)  Time: 0.586s, 1748.25/s  (2.315s,  442.40/s)  LR: 2.082e-05  Data: 0.020 (1.710)
Train: 196 [ 400/927 ( 43%)]  Loss:  5.162098 (4.6663)  Time: 4.268s,  239.95/s  (2.328s,  439.95/s)  LR: 2.082e-05  Data: 3.703 (1.722)
Train: 196 [ 450/927 ( 49%)]  Loss:  4.703362 (4.6700)  Time: 0.586s, 1746.56/s  (2.317s,  441.98/s)  LR: 2.082e-05  Data: 0.021 (1.711)
Train: 196 [ 500/927 ( 54%)]  Loss:  4.795852 (4.6815)  Time: 5.094s,  201.01/s  (2.317s,  441.99/s)  LR: 2.082e-05  Data: 4.531 (1.712)
Train: 196 [ 550/927 ( 59%)]  Loss:  4.793430 (4.6908)  Time: 0.869s, 1178.77/s  (2.306s,  444.07/s)  LR: 2.082e-05  Data: 0.184 (1.698)
Train: 196 [ 600/927 ( 65%)]  Loss:  5.218123 (4.7313)  Time: 6.558s,  156.16/s  (2.337s,  438.09/s)  LR: 2.082e-05  Data: 5.945 (1.730)
Train: 196 [ 650/927 ( 70%)]  Loss:  5.359363 (4.7762)  Time: 0.584s, 1752.04/s  (2.344s,  436.88/s)  LR: 2.082e-05  Data: 0.020 (1.738)
Train: 196 [ 700/927 ( 76%)]  Loss:  5.357937 (4.8150)  Time: 2.834s,  361.37/s  (2.351s,  435.52/s)  LR: 2.082e-05  Data: 2.240 (1.745)
Train: 196 [ 750/927 ( 81%)]  Loss:  4.444575 (4.7918)  Time: 0.656s, 1560.84/s  (2.343s,  437.07/s)  LR: 2.082e-05  Data: 0.069 (1.737)
Train: 196 [ 800/927 ( 86%)]  Loss:  4.816103 (4.7933)  Time: 0.877s, 1167.78/s  (2.344s,  436.80/s)  LR: 2.082e-05  Data: 0.193 (1.738)
Train: 196 [ 850/927 ( 92%)]  Loss:  5.243741 (4.8183)  Time: 1.770s,  578.69/s  (2.338s,  438.06/s)  LR: 2.082e-05  Data: 1.093 (1.732)
Train: 196 [ 900/927 ( 97%)]  Loss:  4.434325 (4.7981)  Time: 0.591s, 1731.94/s  (2.334s,  438.72/s)  LR: 2.082e-05  Data: 0.023 (1.727)
Train: 196 [ 926/927 (100%)]  Loss:  4.777063 (4.7970)  Time: 0.565s, 1813.85/s  (2.327s,  439.99/s)  LR: 2.082e-05  Data: 0.000 (1.720)
Test: [   0/48]  Time: 12.313 (12.313)  Loss:  0.7493 (0.7493)  Acc@1: 88.1836 (88.1836)  Acc@5: 96.3867 (96.3867)
Test: [  48/48]  Time: 0.149 (3.473)  Loss:  0.6360 (0.7465)  Acc@1: 90.9198 (88.2780)  Acc@5: 97.4057 (96.9980)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-193.pth.tar', 88.44600001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-194.pth.tar', 88.40800006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-196.pth.tar', 88.27799993408203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-192.pth.tar', 88.22800007080077)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-195.pth.tar', 88.10399993652344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-191.pth.tar', 88.05800001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-190.pth.tar', 87.73200006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-189.pth.tar', 87.71399996826172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-188.pth.tar', 87.54199993896485)

Train: 197 [   0/927 (  0%)]  Loss:  4.804506 (4.8045)  Time: 13.058s,   78.42/s  (13.058s,   78.42/s)  LR: 1.933e-05  Data: 11.956 (11.956)
Train: 197 [  50/927 (  5%)]  Loss:  4.930933 (4.8677)  Time: 0.588s, 1742.90/s  (2.516s,  406.95/s)  LR: 1.933e-05  Data: 0.020 (1.915)
Train: 197 [ 100/927 ( 11%)]  Loss:  4.700310 (4.8119)  Time: 1.761s,  581.52/s  (2.448s,  418.31/s)  LR: 1.933e-05  Data: 1.158 (1.842)
Train: 197 [ 150/927 ( 16%)]  Loss:  4.585621 (4.7553)  Time: 0.585s, 1749.26/s  (2.373s,  431.44/s)  LR: 1.933e-05  Data: 0.022 (1.771)
Train: 197 [ 200/927 ( 22%)]  Loss:  4.943119 (4.7929)  Time: 0.589s, 1739.03/s  (2.371s,  431.85/s)  LR: 1.933e-05  Data: 0.022 (1.770)
Train: 197 [ 250/927 ( 27%)]  Loss:  5.140010 (4.8507)  Time: 0.585s, 1750.95/s  (2.334s,  438.71/s)  LR: 1.933e-05  Data: 0.019 (1.738)
Train: 197 [ 300/927 ( 32%)]  Loss:  5.348130 (4.9218)  Time: 0.587s, 1744.52/s  (2.329s,  439.76/s)  LR: 1.933e-05  Data: 0.023 (1.732)
Train: 197 [ 350/927 ( 38%)]  Loss:  4.195988 (4.8311)  Time: 0.589s, 1739.58/s  (2.343s,  437.07/s)  LR: 1.933e-05  Data: 0.024 (1.749)
Train: 197 [ 400/927 ( 43%)]  Loss:  4.561316 (4.8011)  Time: 0.587s, 1745.94/s  (2.369s,  432.22/s)  LR: 1.933e-05  Data: 0.021 (1.776)
Train: 197 [ 450/927 ( 49%)]  Loss:  4.265727 (4.7476)  Time: 0.585s, 1750.95/s  (2.361s,  433.64/s)  LR: 1.933e-05  Data: 0.019 (1.769)
Train: 197 [ 500/927 ( 54%)]  Loss:  4.618265 (4.7358)  Time: 0.659s, 1553.51/s  (2.369s,  432.20/s)  LR: 1.933e-05  Data: 0.090 (1.776)
Train: 197 [ 550/927 ( 59%)]  Loss:  4.103617 (4.6831)  Time: 0.585s, 1749.91/s  (2.365s,  433.01/s)  LR: 1.933e-05  Data: 0.019 (1.772)
Train: 197 [ 600/927 ( 65%)]  Loss:  4.712898 (4.6854)  Time: 2.194s,  466.73/s  (2.376s,  431.06/s)  LR: 1.933e-05  Data: 1.538 (1.783)
Train: 197 [ 650/927 ( 70%)]  Loss:  4.407499 (4.6656)  Time: 0.585s, 1749.07/s  (2.369s,  432.28/s)  LR: 1.933e-05  Data: 0.020 (1.776)
Train: 197 [ 700/927 ( 76%)]  Loss:  5.053176 (4.6914)  Time: 2.023s,  506.09/s  (2.367s,  432.69/s)  LR: 1.933e-05  Data: 1.422 (1.774)
Train: 197 [ 750/927 ( 81%)]  Loss:  3.605056 (4.6235)  Time: 1.971s,  519.60/s  (2.378s,  430.54/s)  LR: 1.933e-05  Data: 1.319 (1.786)
Train: 197 [ 800/927 ( 86%)]  Loss:  4.929843 (4.6415)  Time: 0.588s, 1742.51/s  (2.391s,  428.27/s)  LR: 1.933e-05  Data: 0.021 (1.798)
Train: 197 [ 850/927 ( 92%)]  Loss:  3.869618 (4.5986)  Time: 2.801s,  365.55/s  (2.393s,  427.87/s)  LR: 1.933e-05  Data: 2.234 (1.800)
Train: 197 [ 900/927 ( 97%)]  Loss:  4.759833 (4.6071)  Time: 0.588s, 1741.44/s  (2.397s,  427.19/s)  LR: 1.933e-05  Data: 0.020 (1.804)
Train: 197 [ 926/927 (100%)]  Loss:  4.589594 (4.6063)  Time: 0.564s, 1815.38/s  (2.392s,  428.17/s)  LR: 1.933e-05  Data: 0.000 (1.798)
Test: [   0/48]  Time: 13.434 (13.434)  Loss:  0.7252 (0.7252)  Acc@1: 88.5742 (88.5742)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.149 (3.282)  Loss:  0.6316 (0.7357)  Acc@1: 90.5660 (88.3620)  Acc@5: 97.6415 (97.1000)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-193.pth.tar', 88.44600001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-194.pth.tar', 88.40800006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-197.pth.tar', 88.3620000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-196.pth.tar', 88.27799993408203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-192.pth.tar', 88.22800007080077)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-195.pth.tar', 88.10399993652344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-191.pth.tar', 88.05800001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-190.pth.tar', 87.73200006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-189.pth.tar', 87.71399996826172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-188.pth.tar', 87.54199993896485)

Train: 198 [   0/927 (  0%)]  Loss:  4.565476 (4.5655)  Time: 9.504s,  107.74/s  (9.504s,  107.74/s)  LR: 1.795e-05  Data: 8.841 (8.841)
Train: 198 [  50/927 (  5%)]  Loss:  4.990484 (4.7780)  Time: 0.586s, 1747.57/s  (2.283s,  448.59/s)  LR: 1.795e-05  Data: 0.021 (1.690)
Train: 198 [ 100/927 ( 11%)]  Loss:  4.341368 (4.6324)  Time: 0.585s, 1749.22/s  (2.372s,  431.78/s)  LR: 1.795e-05  Data: 0.019 (1.783)
Train: 198 [ 150/927 ( 16%)]  Loss:  4.827776 (4.6813)  Time: 0.587s, 1745.29/s  (2.352s,  435.46/s)  LR: 1.795e-05  Data: 0.023 (1.762)
Train: 198 [ 200/927 ( 22%)]  Loss:  4.372041 (4.6194)  Time: 0.587s, 1744.49/s  (2.383s,  429.79/s)  LR: 1.795e-05  Data: 0.020 (1.793)
Train: 198 [ 250/927 ( 27%)]  Loss:  5.446769 (4.7573)  Time: 0.594s, 1725.01/s  (2.359s,  434.05/s)  LR: 1.795e-05  Data: 0.022 (1.766)
Train: 198 [ 300/927 ( 32%)]  Loss:  4.768448 (4.7589)  Time: 0.593s, 1728.24/s  (2.367s,  432.63/s)  LR: 1.795e-05  Data: 0.025 (1.774)
Train: 198 [ 350/927 ( 38%)]  Loss:  4.742719 (4.7569)  Time: 0.586s, 1746.50/s  (2.344s,  436.90/s)  LR: 1.795e-05  Data: 0.023 (1.750)
Train: 198 [ 400/927 ( 43%)]  Loss:  4.374420 (4.7144)  Time: 0.649s, 1577.86/s  (2.342s,  437.16/s)  LR: 1.795e-05  Data: 0.062 (1.748)
Train: 198 [ 450/927 ( 49%)]  Loss:  4.931424 (4.7361)  Time: 0.589s, 1739.69/s  (2.324s,  440.64/s)  LR: 1.795e-05  Data: 0.021 (1.729)
Train: 198 [ 500/927 ( 54%)]  Loss:  4.093517 (4.6777)  Time: 2.502s,  409.34/s  (2.360s,  433.89/s)  LR: 1.795e-05  Data: 1.937 (1.763)
Train: 198 [ 550/927 ( 59%)]  Loss:  4.748397 (4.6836)  Time: 0.589s, 1738.63/s  (2.361s,  433.78/s)  LR: 1.795e-05  Data: 0.025 (1.762)
Train: 198 [ 600/927 ( 65%)]  Loss:  5.089314 (4.7148)  Time: 3.535s,  289.69/s  (2.385s,  429.40/s)  LR: 1.795e-05  Data: 2.891 (1.786)
Train: 198 [ 650/927 ( 70%)]  Loss:  4.537400 (4.7021)  Time: 0.589s, 1737.41/s  (2.392s,  428.10/s)  LR: 1.795e-05  Data: 0.022 (1.793)
Train: 198 [ 700/927 ( 76%)]  Loss:  4.768003 (4.7065)  Time: 0.587s, 1743.59/s  (2.401s,  426.45/s)  LR: 1.795e-05  Data: 0.023 (1.802)
Train: 198 [ 750/927 ( 81%)]  Loss:  4.792850 (4.7119)  Time: 0.700s, 1461.93/s  (2.393s,  427.96/s)  LR: 1.795e-05  Data: 0.118 (1.794)
Train: 198 [ 800/927 ( 86%)]  Loss:  4.809807 (4.7177)  Time: 0.586s, 1746.43/s  (2.394s,  427.78/s)  LR: 1.795e-05  Data: 0.020 (1.796)
Train: 198 [ 850/927 ( 92%)]  Loss:  4.421564 (4.7012)  Time: 0.586s, 1748.10/s  (2.400s,  426.62/s)  LR: 1.795e-05  Data: 0.022 (1.804)
Train: 198 [ 900/927 ( 97%)]  Loss:  4.821395 (4.7075)  Time: 1.343s,  762.42/s  (2.407s,  425.38/s)  LR: 1.795e-05  Data: 0.765 (1.810)
Train: 198 [ 926/927 (100%)]  Loss:  4.898747 (4.7171)  Time: 0.564s, 1815.97/s  (2.404s,  425.96/s)  LR: 1.795e-05  Data: 0.000 (1.806)
Test: [   0/48]  Time: 14.741 (14.741)  Loss:  0.7245 (0.7245)  Acc@1: 88.3789 (88.3789)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.150 (3.454)  Loss:  0.6335 (0.7262)  Acc@1: 90.6840 (88.7200)  Acc@5: 97.5236 (97.1720)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-198.pth.tar', 88.7199999609375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-193.pth.tar', 88.44600001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-194.pth.tar', 88.40800006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-197.pth.tar', 88.3620000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-196.pth.tar', 88.27799993408203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-192.pth.tar', 88.22800007080077)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-195.pth.tar', 88.10399993652344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-191.pth.tar', 88.05800001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-190.pth.tar', 87.73200006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-189.pth.tar', 87.71399996826172)

Train: 199 [   0/927 (  0%)]  Loss:  4.887564 (4.8876)  Time: 10.419s,   98.28/s  (10.419s,   98.28/s)  LR: 1.669e-05  Data: 9.644 (9.644)
Train: 199 [  50/927 (  5%)]  Loss:  4.842774 (4.8652)  Time: 0.586s, 1747.45/s  (2.392s,  428.09/s)  LR: 1.669e-05  Data: 0.019 (1.801)
Train: 199 [ 100/927 ( 11%)]  Loss:  4.853682 (4.8613)  Time: 0.589s, 1738.48/s  (2.366s,  432.86/s)  LR: 1.669e-05  Data: 0.024 (1.768)
Train: 199 [ 150/927 ( 16%)]  Loss:  4.698492 (4.8206)  Time: 2.257s,  453.80/s  (2.287s,  447.77/s)  LR: 1.669e-05  Data: 1.590 (1.695)
Train: 199 [ 200/927 ( 22%)]  Loss:  5.143894 (4.8853)  Time: 0.589s, 1737.34/s  (2.274s,  450.37/s)  LR: 1.669e-05  Data: 0.024 (1.676)
Train: 199 [ 250/927 ( 27%)]  Loss:  4.655576 (4.8470)  Time: 3.890s,  263.23/s  (2.329s,  439.63/s)  LR: 1.669e-05  Data: 3.176 (1.727)
Train: 199 [ 300/927 ( 32%)]  Loss:  4.647948 (4.8186)  Time: 0.587s, 1743.18/s  (2.340s,  437.53/s)  LR: 1.669e-05  Data: 0.022 (1.737)
Train: 199 [ 350/927 ( 38%)]  Loss:  4.069886 (4.7250)  Time: 1.817s,  563.43/s  (2.343s,  436.95/s)  LR: 1.669e-05  Data: 1.164 (1.741)
Train: 199 [ 400/927 ( 43%)]  Loss:  4.384096 (4.6871)  Time: 0.588s, 1742.54/s  (2.350s,  435.69/s)  LR: 1.669e-05  Data: 0.022 (1.748)
Train: 199 [ 450/927 ( 49%)]  Loss:  4.080413 (4.6264)  Time: 0.587s, 1745.42/s  (2.338s,  438.01/s)  LR: 1.669e-05  Data: 0.021 (1.736)
Train: 199 [ 500/927 ( 54%)]  Loss:  4.586544 (4.6228)  Time: 0.591s, 1733.48/s  (2.339s,  437.75/s)  LR: 1.669e-05  Data: 0.025 (1.737)
Train: 199 [ 550/927 ( 59%)]  Loss:  4.432533 (4.6070)  Time: 0.589s, 1739.19/s  (2.329s,  439.59/s)  LR: 1.669e-05  Data: 0.023 (1.729)
Train: 199 [ 600/927 ( 65%)]  Loss:  4.943803 (4.6329)  Time: 0.586s, 1746.21/s  (2.333s,  438.87/s)  LR: 1.669e-05  Data: 0.021 (1.733)
Train: 199 [ 650/927 ( 70%)]  Loss:  4.875994 (4.6502)  Time: 0.584s, 1753.45/s  (2.364s,  433.18/s)  LR: 1.669e-05  Data: 0.019 (1.763)
Train: 199 [ 700/927 ( 76%)]  Loss:  5.366956 (4.6980)  Time: 0.586s, 1746.52/s  (2.372s,  431.77/s)  LR: 1.669e-05  Data: 0.019 (1.771)
Train: 199 [ 750/927 ( 81%)]  Loss:  4.909378 (4.7112)  Time: 0.588s, 1740.82/s  (2.372s,  431.73/s)  LR: 1.669e-05  Data: 0.023 (1.771)
Train: 199 [ 800/927 ( 86%)]  Loss:  4.895657 (4.7221)  Time: 0.587s, 1745.06/s  (2.372s,  431.78/s)  LR: 1.669e-05  Data: 0.021 (1.771)
Train: 199 [ 850/927 ( 92%)]  Loss:  4.705643 (4.7212)  Time: 0.586s, 1747.89/s  (2.366s,  432.73/s)  LR: 1.669e-05  Data: 0.020 (1.765)
Train: 199 [ 900/927 ( 97%)]  Loss:  4.274821 (4.6977)  Time: 0.588s, 1741.70/s  (2.366s,  432.88/s)  LR: 1.669e-05  Data: 0.018 (1.765)
Train: 199 [ 926/927 (100%)]  Loss:  5.423612 (4.7340)  Time: 0.565s, 1811.42/s  (2.361s,  433.71/s)  LR: 1.669e-05  Data: 0.000 (1.760)
Test: [   0/48]  Time: 13.608 (13.608)  Loss:  0.7376 (0.7376)  Acc@1: 88.0859 (88.0859)  Acc@5: 96.5820 (96.5820)
Test: [  48/48]  Time: 0.149 (3.514)  Loss:  0.6264 (0.7356)  Acc@1: 90.5660 (88.5440)  Acc@5: 97.6415 (97.1860)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-198.pth.tar', 88.7199999609375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-199.pth.tar', 88.5440000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-193.pth.tar', 88.44600001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-194.pth.tar', 88.40800006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-197.pth.tar', 88.3620000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-196.pth.tar', 88.27799993408203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-192.pth.tar', 88.22800007080077)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-195.pth.tar', 88.10399993652344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-191.pth.tar', 88.05800001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-190.pth.tar', 87.73200006835937)

Train: 200 [   0/927 (  0%)]  Loss:  3.545917 (3.5459)  Time: 12.227s,   83.75/s  (12.227s,   83.75/s)  LR: 1.553e-05  Data: 10.390 (10.390)
Train: 200 [  50/927 (  5%)]  Loss:  5.228152 (4.3870)  Time: 0.585s, 1750.26/s  (2.526s,  405.45/s)  LR: 1.553e-05  Data: 0.021 (1.922)
Train: 200 [ 100/927 ( 11%)]  Loss:  4.801004 (4.5250)  Time: 0.584s, 1752.06/s  (2.483s,  412.42/s)  LR: 1.553e-05  Data: 0.020 (1.882)
Train: 200 [ 150/927 ( 16%)]  Loss:  4.843588 (4.6047)  Time: 0.584s, 1753.01/s  (2.389s,  428.62/s)  LR: 1.553e-05  Data: 0.020 (1.793)
Train: 200 [ 200/927 ( 22%)]  Loss:  4.750718 (4.6339)  Time: 1.187s,  862.58/s  (2.367s,  432.69/s)  LR: 1.553e-05  Data: 0.592 (1.769)
Train: 200 [ 250/927 ( 27%)]  Loss:  3.809747 (4.4965)  Time: 0.586s, 1748.05/s  (2.331s,  439.25/s)  LR: 1.553e-05  Data: 0.022 (1.733)
Train: 200 [ 300/927 ( 32%)]  Loss:  5.050533 (4.5757)  Time: 0.589s, 1738.59/s  (2.327s,  439.96/s)  LR: 1.553e-05  Data: 0.019 (1.731)
Train: 200 [ 350/927 ( 38%)]  Loss:  4.022413 (4.5065)  Time: 0.587s, 1743.48/s  (2.302s,  444.93/s)  LR: 1.553e-05  Data: 0.020 (1.704)
Train: 200 [ 400/927 ( 43%)]  Loss:  5.408564 (4.6067)  Time: 0.586s, 1746.79/s  (2.344s,  436.83/s)  LR: 1.553e-05  Data: 0.021 (1.746)
Train: 200 [ 450/927 ( 49%)]  Loss:  5.134897 (4.6596)  Time: 0.591s, 1733.58/s  (2.338s,  438.04/s)  LR: 1.553e-05  Data: 0.019 (1.739)
Train: 200 [ 500/927 ( 54%)]  Loss:  5.624098 (4.7472)  Time: 1.840s,  556.59/s  (2.345s,  436.60/s)  LR: 1.553e-05  Data: 1.175 (1.747)
Train: 200 [ 550/927 ( 59%)]  Loss:  4.428258 (4.7207)  Time: 0.585s, 1749.46/s  (2.348s,  436.20/s)  LR: 1.553e-05  Data: 0.021 (1.747)
Train: 200 [ 600/927 ( 65%)]  Loss:  4.320097 (4.6898)  Time: 1.647s,  621.89/s  (2.357s,  434.51/s)  LR: 1.553e-05  Data: 0.924 (1.757)
Train: 200 [ 650/927 ( 70%)]  Loss:  4.433569 (4.6715)  Time: 0.703s, 1456.61/s  (2.351s,  435.61/s)  LR: 1.553e-05  Data: 0.033 (1.749)
Train: 200 [ 700/927 ( 76%)]  Loss:  4.448923 (4.6567)  Time: 4.264s,  240.13/s  (2.352s,  435.38/s)  LR: 1.553e-05  Data: 3.672 (1.750)
Train: 200 [ 750/927 ( 81%)]  Loss:  4.917111 (4.6730)  Time: 0.588s, 1740.65/s  (2.353s,  435.24/s)  LR: 1.553e-05  Data: 0.021 (1.750)
Train: 200 [ 800/927 ( 86%)]  Loss:  4.485950 (4.6620)  Time: 2.414s,  424.24/s  (2.363s,  433.41/s)  LR: 1.553e-05  Data: 1.847 (1.760)
Train: 200 [ 850/927 ( 92%)]  Loss:  5.254569 (4.6949)  Time: 0.586s, 1746.43/s  (2.363s,  433.35/s)  LR: 1.553e-05  Data: 0.022 (1.760)
Train: 200 [ 900/927 ( 97%)]  Loss:  5.466741 (4.7355)  Time: 0.589s, 1738.61/s  (2.366s,  432.79/s)  LR: 1.553e-05  Data: 0.024 (1.764)
Train: 200 [ 926/927 (100%)]  Loss:  4.052630 (4.7014)  Time: 0.566s, 1809.19/s  (2.363s,  433.39/s)  LR: 1.553e-05  Data: 0.000 (1.761)
Test: [   0/48]  Time: 13.868 (13.868)  Loss:  0.7276 (0.7276)  Acc@1: 88.4766 (88.4766)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.149 (3.244)  Loss:  0.6256 (0.7173)  Acc@1: 90.0943 (88.8020)  Acc@5: 97.2877 (97.2300)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-200.pth.tar', 88.80200009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-198.pth.tar', 88.7199999609375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-199.pth.tar', 88.5440000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-193.pth.tar', 88.44600001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-194.pth.tar', 88.40800006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-197.pth.tar', 88.3620000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-196.pth.tar', 88.27799993408203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-192.pth.tar', 88.22800007080077)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-195.pth.tar', 88.10399993652344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-191.pth.tar', 88.05800001953125)

Train: 201 [   0/927 (  0%)]  Loss:  4.046185 (4.0462)  Time: 9.788s,  104.61/s  (9.788s,  104.61/s)  LR: 1.448e-05  Data: 8.813 (8.813)
Train: 201 [  50/927 (  5%)]  Loss:  4.664392 (4.3553)  Time: 0.619s, 1653.98/s  (2.184s,  468.93/s)  LR: 1.448e-05  Data: 0.018 (1.569)
Train: 201 [ 100/927 ( 11%)]  Loss:  5.292141 (4.6676)  Time: 0.588s, 1741.06/s  (2.187s,  468.22/s)  LR: 1.448e-05  Data: 0.020 (1.586)
Train: 201 [ 150/927 ( 16%)]  Loss:  4.474653 (4.6193)  Time: 0.587s, 1745.40/s  (2.268s,  451.41/s)  LR: 1.448e-05  Data: 0.021 (1.661)
Train: 201 [ 200/927 ( 22%)]  Loss:  5.369455 (4.7694)  Time: 4.296s,  238.38/s  (2.247s,  455.81/s)  LR: 1.448e-05  Data: 3.732 (1.638)
Train: 201 [ 250/927 ( 27%)]  Loss:  5.165479 (4.8354)  Time: 0.590s, 1735.24/s  (2.281s,  448.99/s)  LR: 1.448e-05  Data: 0.024 (1.673)
Train: 201 [ 300/927 ( 32%)]  Loss:  4.834270 (4.8352)  Time: 7.530s,  135.98/s  (2.304s,  444.39/s)  LR: 1.448e-05  Data: 6.720 (1.700)
Train: 201 [ 350/927 ( 38%)]  Loss:  4.841058 (4.8360)  Time: 0.586s, 1747.69/s  (2.298s,  445.58/s)  LR: 1.448e-05  Data: 0.018 (1.696)
Train: 201 [ 400/927 ( 43%)]  Loss:  4.965490 (4.8503)  Time: 7.124s,  143.74/s  (2.303s,  444.60/s)  LR: 1.448e-05  Data: 6.372 (1.702)
Train: 201 [ 450/927 ( 49%)]  Loss:  4.733804 (4.8387)  Time: 0.586s, 1746.90/s  (2.280s,  449.19/s)  LR: 1.448e-05  Data: 0.019 (1.679)
Train: 201 [ 500/927 ( 54%)]  Loss:  4.294458 (4.7892)  Time: 7.555s,  135.54/s  (2.280s,  449.09/s)  LR: 1.448e-05  Data: 6.887 (1.680)
Train: 201 [ 550/927 ( 59%)]  Loss:  4.904914 (4.7989)  Time: 0.586s, 1747.43/s  (2.306s,  444.03/s)  LR: 1.448e-05  Data: 0.020 (1.708)
Train: 201 [ 600/927 ( 65%)]  Loss:  4.855149 (4.8032)  Time: 9.594s,  106.74/s  (2.310s,  443.20/s)  LR: 1.448e-05  Data: 9.028 (1.713)
Train: 201 [ 650/927 ( 70%)]  Loss:  5.272814 (4.8367)  Time: 0.587s, 1744.27/s  (2.316s,  442.12/s)  LR: 1.448e-05  Data: 0.020 (1.720)
Train: 201 [ 700/927 ( 76%)]  Loss:  5.200945 (4.8610)  Time: 7.516s,  136.24/s  (2.323s,  440.75/s)  LR: 1.448e-05  Data: 6.857 (1.728)
Train: 201 [ 750/927 ( 81%)]  Loss:  5.164520 (4.8800)  Time: 0.587s, 1743.27/s  (2.316s,  442.14/s)  LR: 1.448e-05  Data: 0.022 (1.722)
Train: 201 [ 800/927 ( 86%)]  Loss:  5.484448 (4.9155)  Time: 6.189s,  165.44/s  (2.315s,  442.34/s)  LR: 1.448e-05  Data: 5.568 (1.720)
Train: 201 [ 850/927 ( 92%)]  Loss:  4.312130 (4.8820)  Time: 0.587s, 1743.86/s  (2.308s,  443.58/s)  LR: 1.448e-05  Data: 0.020 (1.714)
Train: 201 [ 900/927 ( 97%)]  Loss:  4.859328 (4.8808)  Time: 6.382s,  160.45/s  (2.304s,  444.44/s)  LR: 1.448e-05  Data: 5.794 (1.710)
Train: 201 [ 926/927 (100%)]  Loss:  4.918244 (4.8827)  Time: 0.564s, 1814.90/s  (2.317s,  441.87/s)  LR: 1.448e-05  Data: 0.000 (1.724)
Test: [   0/48]  Time: 15.207 (15.207)  Loss:  0.7255 (0.7255)  Acc@1: 88.8672 (88.8672)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.149 (3.365)  Loss:  0.6207 (0.7176)  Acc@1: 91.3915 (88.9140)  Acc@5: 97.7594 (97.2660)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-201.pth.tar', 88.91400000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-200.pth.tar', 88.80200009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-198.pth.tar', 88.7199999609375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-199.pth.tar', 88.5440000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-193.pth.tar', 88.44600001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-194.pth.tar', 88.40800006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-197.pth.tar', 88.3620000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-196.pth.tar', 88.27799993408203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-192.pth.tar', 88.22800007080077)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-195.pth.tar', 88.10399993652344)

Train: 202 [   0/927 (  0%)]  Loss:  4.877916 (4.8779)  Time: 9.802s,  104.47/s  (9.802s,  104.47/s)  LR: 1.354e-05  Data: 9.151 (9.151)
Train: 202 [  50/927 (  5%)]  Loss:  4.471622 (4.6748)  Time: 0.585s, 1750.64/s  (2.346s,  436.42/s)  LR: 1.354e-05  Data: 0.021 (1.758)
Train: 202 [ 100/927 ( 11%)]  Loss:  4.265098 (4.5382)  Time: 0.587s, 1745.69/s  (2.296s,  445.99/s)  LR: 1.354e-05  Data: 0.021 (1.714)
Train: 202 [ 150/927 ( 16%)]  Loss:  4.172684 (4.4468)  Time: 0.587s, 1743.64/s  (2.262s,  452.61/s)  LR: 1.354e-05  Data: 0.022 (1.671)
Train: 202 [ 200/927 ( 22%)]  Loss:  4.982139 (4.5539)  Time: 2.355s,  434.82/s  (2.254s,  454.28/s)  LR: 1.354e-05  Data: 1.774 (1.660)
Train: 202 [ 250/927 ( 27%)]  Loss:  4.828287 (4.5996)  Time: 0.588s, 1741.79/s  (2.229s,  459.38/s)  LR: 1.354e-05  Data: 0.022 (1.630)
Train: 202 [ 300/927 ( 32%)]  Loss:  4.792379 (4.6272)  Time: 2.456s,  416.94/s  (2.219s,  461.55/s)  LR: 1.354e-05  Data: 1.882 (1.615)
Train: 202 [ 350/927 ( 38%)]  Loss:  5.320578 (4.7138)  Time: 0.588s, 1741.57/s  (2.266s,  451.83/s)  LR: 1.354e-05  Data: 0.022 (1.658)
Train: 202 [ 400/927 ( 43%)]  Loss:  4.445785 (4.6841)  Time: 2.014s,  508.56/s  (2.273s,  450.44/s)  LR: 1.354e-05  Data: 1.379 (1.664)
Train: 202 [ 450/927 ( 49%)]  Loss:  4.213265 (4.6370)  Time: 0.588s, 1740.15/s  (2.272s,  450.70/s)  LR: 1.354e-05  Data: 0.021 (1.665)
Train: 202 [ 500/927 ( 54%)]  Loss:  4.586860 (4.6324)  Time: 0.586s, 1746.71/s  (2.277s,  449.81/s)  LR: 1.354e-05  Data: 0.020 (1.671)
Train: 202 [ 550/927 ( 59%)]  Loss:  4.522511 (4.6233)  Time: 0.588s, 1742.29/s  (2.304s,  444.46/s)  LR: 1.354e-05  Data: 0.020 (1.698)
Train: 202 [ 600/927 ( 65%)]  Loss:  4.639851 (4.6245)  Time: 0.589s, 1737.95/s  (2.309s,  443.40/s)  LR: 1.354e-05  Data: 0.020 (1.704)
Train: 202 [ 650/927 ( 70%)]  Loss:  4.795450 (4.6367)  Time: 0.589s, 1737.43/s  (2.319s,  441.58/s)  LR: 1.354e-05  Data: 0.024 (1.714)
Train: 202 [ 700/927 ( 76%)]  Loss:  4.471385 (4.6257)  Time: 0.587s, 1744.37/s  (2.347s,  436.27/s)  LR: 1.354e-05  Data: 0.022 (1.742)
Train: 202 [ 750/927 ( 81%)]  Loss:  4.669734 (4.6285)  Time: 0.591s, 1732.64/s  (2.340s,  437.58/s)  LR: 1.354e-05  Data: 0.020 (1.735)
Train: 202 [ 800/927 ( 86%)]  Loss:  4.777320 (4.6372)  Time: 0.587s, 1745.02/s  (2.347s,  436.24/s)  LR: 1.354e-05  Data: 0.021 (1.742)
Train: 202 [ 850/927 ( 92%)]  Loss:  4.999650 (4.6574)  Time: 0.588s, 1741.91/s  (2.344s,  436.79/s)  LR: 1.354e-05  Data: 0.020 (1.740)
Train: 202 [ 900/927 ( 97%)]  Loss:  4.532172 (4.6508)  Time: 0.586s, 1747.76/s  (2.343s,  437.02/s)  LR: 1.354e-05  Data: 0.020 (1.738)
Train: 202 [ 926/927 (100%)]  Loss:  4.908791 (4.6637)  Time: 0.566s, 1809.87/s  (2.337s,  438.13/s)  LR: 1.354e-05  Data: 0.000 (1.732)
Test: [   0/48]  Time: 12.987 (12.987)  Loss:  0.7259 (0.7259)  Acc@1: 88.6719 (88.6719)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.150 (3.184)  Loss:  0.6305 (0.7310)  Acc@1: 90.6840 (88.7100)  Acc@5: 97.5236 (97.1980)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-201.pth.tar', 88.91400000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-200.pth.tar', 88.80200009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-198.pth.tar', 88.7199999609375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-202.pth.tar', 88.71000009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-199.pth.tar', 88.5440000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-193.pth.tar', 88.44600001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-194.pth.tar', 88.40800006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-197.pth.tar', 88.3620000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-196.pth.tar', 88.27799993408203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-192.pth.tar', 88.22800007080077)

Train: 203 [   0/927 (  0%)]  Loss:  4.955227 (4.9552)  Time: 9.436s,  108.52/s  (9.436s,  108.52/s)  LR: 1.271e-05  Data: 8.769 (8.769)
Train: 203 [  50/927 (  5%)]  Loss:  3.867163 (4.4112)  Time: 0.586s, 1748.47/s  (2.215s,  462.35/s)  LR: 1.271e-05  Data: 0.020 (1.621)
Train: 203 [ 100/927 ( 11%)]  Loss:  5.461228 (4.7612)  Time: 1.283s,  797.95/s  (2.418s,  423.40/s)  LR: 1.271e-05  Data: 0.709 (1.823)
Train: 203 [ 150/927 ( 16%)]  Loss:  4.192711 (4.6191)  Time: 0.589s, 1738.25/s  (2.322s,  440.97/s)  LR: 1.271e-05  Data: 0.020 (1.726)
Train: 203 [ 200/927 ( 22%)]  Loss:  5.037060 (4.7027)  Time: 0.590s, 1736.67/s  (2.352s,  435.44/s)  LR: 1.271e-05  Data: 0.024 (1.755)
Train: 203 [ 250/927 ( 27%)]  Loss:  5.294575 (4.8013)  Time: 2.492s,  410.93/s  (2.314s,  442.56/s)  LR: 1.271e-05  Data: 1.838 (1.714)
Train: 203 [ 300/927 ( 32%)]  Loss:  4.213773 (4.7174)  Time: 0.586s, 1745.98/s  (2.294s,  446.47/s)  LR: 1.271e-05  Data: 0.019 (1.695)
Train: 203 [ 350/927 ( 38%)]  Loss:  4.826643 (4.7310)  Time: 5.866s,  174.56/s  (2.274s,  450.28/s)  LR: 1.271e-05  Data: 5.260 (1.678)
Train: 203 [ 400/927 ( 43%)]  Loss:  4.627775 (4.7196)  Time: 0.586s, 1748.02/s  (2.254s,  454.34/s)  LR: 1.271e-05  Data: 0.020 (1.657)
Train: 203 [ 450/927 ( 49%)]  Loss:  5.045006 (4.7521)  Time: 3.249s,  315.16/s  (2.280s,  449.20/s)  LR: 1.271e-05  Data: 2.227 (1.679)
Train: 203 [ 500/927 ( 54%)]  Loss:  4.014606 (4.6851)  Time: 3.131s,  327.10/s  (2.321s,  441.12/s)  LR: 1.271e-05  Data: 2.567 (1.719)
Train: 203 [ 550/927 ( 59%)]  Loss:  4.678242 (4.6845)  Time: 0.588s, 1740.92/s  (2.325s,  440.38/s)  LR: 1.271e-05  Data: 0.024 (1.722)
Train: 203 [ 600/927 ( 65%)]  Loss:  5.233541 (4.7267)  Time: 5.190s,  197.29/s  (2.340s,  437.59/s)  LR: 1.271e-05  Data: 4.532 (1.736)
Train: 203 [ 650/927 ( 70%)]  Loss:  5.289936 (4.7670)  Time: 3.275s,  312.68/s  (2.345s,  436.58/s)  LR: 1.271e-05  Data: 2.709 (1.741)
Train: 203 [ 700/927 ( 76%)]  Loss:  5.204566 (4.7961)  Time: 1.408s,  727.04/s  (2.348s,  436.18/s)  LR: 1.271e-05  Data: 0.737 (1.743)
Train: 203 [ 750/927 ( 81%)]  Loss:  4.821902 (4.7977)  Time: 6.423s,  159.42/s  (2.345s,  436.59/s)  LR: 1.271e-05  Data: 5.843 (1.741)
Train: 203 [ 800/927 ( 86%)]  Loss:  4.939982 (4.8061)  Time: 1.023s, 1000.98/s  (2.338s,  438.01/s)  LR: 1.271e-05  Data: 0.455 (1.734)
Train: 203 [ 850/927 ( 92%)]  Loss:  4.877644 (4.8101)  Time: 9.111s,  112.39/s  (2.357s,  434.36/s)  LR: 1.271e-05  Data: 8.436 (1.755)
Train: 203 [ 900/927 ( 97%)]  Loss:  5.174772 (4.8293)  Time: 0.585s, 1749.07/s  (2.357s,  434.53/s)  LR: 1.271e-05  Data: 0.018 (1.754)
Train: 203 [ 926/927 (100%)]  Loss:  3.953427 (4.7855)  Time: 0.567s, 1805.92/s  (2.357s,  434.52/s)  LR: 1.271e-05  Data: 0.000 (1.755)
Test: [   0/48]  Time: 14.487 (14.487)  Loss:  0.7120 (0.7120)  Acc@1: 88.7695 (88.7695)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.149 (3.554)  Loss:  0.6122 (0.7107)  Acc@1: 91.7453 (89.0540)  Acc@5: 97.7594 (97.3020)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-203.pth.tar', 89.0540000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-201.pth.tar', 88.91400000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-200.pth.tar', 88.80200009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-198.pth.tar', 88.7199999609375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-202.pth.tar', 88.71000009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-199.pth.tar', 88.5440000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-193.pth.tar', 88.44600001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-194.pth.tar', 88.40800006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-197.pth.tar', 88.3620000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-196.pth.tar', 88.27799993408203)

Train: 204 [   0/927 (  0%)]  Loss:  5.194123 (5.1941)  Time: 11.904s,   86.02/s  (11.904s,   86.02/s)  LR: 1.199e-05  Data: 10.755 (10.755)
Train: 204 [  50/927 (  5%)]  Loss:  4.590514 (4.8923)  Time: 0.584s, 1753.93/s  (2.623s,  390.41/s)  LR: 1.199e-05  Data: 0.020 (2.031)
Train: 204 [ 100/927 ( 11%)]  Loss:  5.084235 (4.9563)  Time: 3.619s,  282.92/s  (2.489s,  411.33/s)  LR: 1.199e-05  Data: 2.951 (1.881)
Train: 204 [ 150/927 ( 16%)]  Loss:  5.037672 (4.9766)  Time: 0.584s, 1753.20/s  (2.384s,  429.48/s)  LR: 1.199e-05  Data: 0.018 (1.766)
Train: 204 [ 200/927 ( 22%)]  Loss:  4.891274 (4.9596)  Time: 0.588s, 1742.28/s  (2.349s,  435.98/s)  LR: 1.199e-05  Data: 0.022 (1.737)
Train: 204 [ 250/927 ( 27%)]  Loss:  4.187344 (4.8309)  Time: 0.589s, 1738.94/s  (2.384s,  429.52/s)  LR: 1.199e-05  Data: 0.021 (1.774)
Train: 204 [ 300/927 ( 32%)]  Loss:  4.421709 (4.7724)  Time: 0.584s, 1752.42/s  (2.392s,  428.16/s)  LR: 1.199e-05  Data: 0.018 (1.785)
Train: 204 [ 350/927 ( 38%)]  Loss:  4.829650 (4.7796)  Time: 0.585s, 1751.92/s  (2.438s,  420.09/s)  LR: 1.199e-05  Data: 0.019 (1.831)
Train: 204 [ 400/927 ( 43%)]  Loss:  5.415209 (4.8502)  Time: 2.029s,  504.80/s  (2.493s,  410.78/s)  LR: 1.199e-05  Data: 1.458 (1.882)
Train: 204 [ 450/927 ( 49%)]  Loss:  3.824468 (4.7476)  Time: 0.586s, 1746.35/s  (2.496s,  410.19/s)  LR: 1.199e-05  Data: 0.021 (1.886)
Train: 204 [ 500/927 ( 54%)]  Loss:  4.269992 (4.7042)  Time: 6.448s,  158.80/s  (2.477s,  413.43/s)  LR: 1.199e-05  Data: 5.869 (1.866)
Train: 204 [ 550/927 ( 59%)]  Loss:  4.623389 (4.6975)  Time: 1.249s,  820.17/s  (2.448s,  418.25/s)  LR: 1.199e-05  Data: 0.584 (1.838)
Train: 204 [ 600/927 ( 65%)]  Loss:  4.294758 (4.6665)  Time: 7.132s,  143.57/s  (2.475s,  413.71/s)  LR: 1.199e-05  Data: 6.484 (1.862)
Train: 204 [ 650/927 ( 70%)]  Loss:  3.930347 (4.6139)  Time: 0.592s, 1731.09/s  (2.465s,  415.50/s)  LR: 1.199e-05  Data: 0.025 (1.853)
Train: 204 [ 700/927 ( 76%)]  Loss:  4.607203 (4.6135)  Time: 3.970s,  257.95/s  (2.458s,  416.61/s)  LR: 1.199e-05  Data: 3.363 (1.846)
Train: 204 [ 750/927 ( 81%)]  Loss:  4.460149 (4.6039)  Time: 0.584s, 1752.11/s  (2.449s,  418.14/s)  LR: 1.199e-05  Data: 0.020 (1.838)
Train: 204 [ 800/927 ( 86%)]  Loss:  4.997945 (4.6271)  Time: 4.820s,  212.46/s  (2.445s,  418.87/s)  LR: 1.199e-05  Data: 4.248 (1.833)
Train: 204 [ 850/927 ( 92%)]  Loss:  4.299920 (4.6089)  Time: 0.588s, 1740.31/s  (2.433s,  420.95/s)  LR: 1.199e-05  Data: 0.021 (1.822)
Train: 204 [ 900/927 ( 97%)]  Loss:  4.085120 (4.5813)  Time: 4.498s,  227.63/s  (2.426s,  422.03/s)  LR: 1.199e-05  Data: 3.935 (1.816)
Train: 204 [ 926/927 (100%)]  Loss:  4.938248 (4.5992)  Time: 0.564s, 1814.86/s  (2.419s,  423.26/s)  LR: 1.199e-05  Data: 0.000 (1.810)
Test: [   0/48]  Time: 12.873 (12.873)  Loss:  0.7129 (0.7129)  Acc@1: 88.5742 (88.5742)  Acc@5: 96.4844 (96.4844)
Test: [  48/48]  Time: 0.149 (3.649)  Loss:  0.6273 (0.7179)  Acc@1: 90.5660 (88.7440)  Acc@5: 97.7594 (97.2900)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-203.pth.tar', 89.0540000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-201.pth.tar', 88.91400000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-200.pth.tar', 88.80200009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-204.pth.tar', 88.7440000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-198.pth.tar', 88.7199999609375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-202.pth.tar', 88.71000009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-199.pth.tar', 88.5440000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-193.pth.tar', 88.44600001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-194.pth.tar', 88.40800006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-197.pth.tar', 88.3620000390625)

Train: 205 [   0/927 (  0%)]  Loss:  4.902161 (4.9022)  Time: 9.438s,  108.49/s  (9.438s,  108.49/s)  LR: 1.138e-05  Data: 8.847 (8.847)
Train: 205 [  50/927 (  5%)]  Loss:  5.227356 (5.0648)  Time: 0.590s, 1735.56/s  (2.521s,  406.22/s)  LR: 1.138e-05  Data: 0.024 (1.944)
Train: 205 [ 100/927 ( 11%)]  Loss:  4.482165 (4.8706)  Time: 0.889s, 1152.48/s  (2.500s,  409.61/s)  LR: 1.138e-05  Data: 0.238 (1.897)
Train: 205 [ 150/927 ( 16%)]  Loss:  4.175423 (4.6968)  Time: 0.590s, 1736.97/s  (2.443s,  419.22/s)  LR: 1.138e-05  Data: 0.024 (1.838)
Train: 205 [ 200/927 ( 22%)]  Loss:  5.370193 (4.8315)  Time: 0.587s, 1743.76/s  (2.481s,  412.72/s)  LR: 1.138e-05  Data: 0.022 (1.876)
Train: 205 [ 250/927 ( 27%)]  Loss:  3.837725 (4.6658)  Time: 0.593s, 1727.50/s  (2.454s,  417.32/s)  LR: 1.138e-05  Data: 0.027 (1.848)
Train: 205 [ 300/927 ( 32%)]  Loss:  4.237085 (4.6046)  Time: 1.437s,  712.70/s  (2.511s,  407.80/s)  LR: 1.138e-05  Data: 0.870 (1.904)
Train: 205 [ 350/927 ( 38%)]  Loss:  5.003679 (4.6545)  Time: 0.592s, 1730.94/s  (2.549s,  401.71/s)  LR: 1.138e-05  Data: 0.026 (1.944)
Train: 205 [ 400/927 ( 43%)]  Loss:  5.290284 (4.7251)  Time: 1.490s,  687.36/s  (2.544s,  402.47/s)  LR: 1.138e-05  Data: 0.918 (1.939)
Train: 205 [ 450/927 ( 49%)]  Loss:  4.784923 (4.7311)  Time: 0.589s, 1737.68/s  (2.537s,  403.57/s)  LR: 1.138e-05  Data: 0.019 (1.932)
Train: 205 [ 500/927 ( 54%)]  Loss:  4.809349 (4.7382)  Time: 3.456s,  296.28/s  (2.518s,  406.72/s)  LR: 1.138e-05  Data: 2.885 (1.912)
Train: 205 [ 550/927 ( 59%)]  Loss:  4.741390 (4.7385)  Time: 0.585s, 1751.36/s  (2.494s,  410.54/s)  LR: 1.138e-05  Data: 0.020 (1.889)
Train: 205 [ 600/927 ( 65%)]  Loss:  5.254368 (4.7782)  Time: 5.194s,  197.16/s  (2.490s,  411.32/s)  LR: 1.138e-05  Data: 4.535 (1.884)
Train: 205 [ 650/927 ( 70%)]  Loss:  4.607330 (4.7660)  Time: 0.586s, 1748.41/s  (2.472s,  414.30/s)  LR: 1.138e-05  Data: 0.022 (1.867)
Train: 205 [ 700/927 ( 76%)]  Loss:  4.364418 (4.7392)  Time: 4.803s,  213.20/s  (2.486s,  411.87/s)  LR: 1.138e-05  Data: 4.239 (1.880)
Train: 205 [ 750/927 ( 81%)]  Loss:  4.489432 (4.7236)  Time: 2.073s,  493.99/s  (2.467s,  415.08/s)  LR: 1.138e-05  Data: 1.406 (1.859)
Train: 205 [ 800/927 ( 86%)]  Loss:  4.506237 (4.7108)  Time: 4.123s,  248.37/s  (2.466s,  415.22/s)  LR: 1.138e-05  Data: 3.559 (1.857)
Train: 205 [ 850/927 ( 92%)]  Loss:  4.607423 (4.7051)  Time: 0.586s, 1747.16/s  (2.456s,  416.96/s)  LR: 1.138e-05  Data: 0.022 (1.847)
Train: 205 [ 900/927 ( 97%)]  Loss:  4.729201 (4.7063)  Time: 6.714s,  152.53/s  (2.450s,  417.90/s)  LR: 1.138e-05  Data: 6.106 (1.842)
Train: 205 [ 926/927 (100%)]  Loss:  4.789919 (4.7105)  Time: 0.566s, 1808.94/s  (2.443s,  419.22/s)  LR: 1.138e-05  Data: 0.000 (1.834)
Test: [   0/48]  Time: 13.136 (13.136)  Loss:  0.6983 (0.6983)  Acc@1: 89.0625 (89.0625)  Acc@5: 96.6797 (96.6797)
Test: [  48/48]  Time: 0.150 (3.230)  Loss:  0.5995 (0.7057)  Acc@1: 91.5094 (89.1140)  Acc@5: 97.6415 (97.3180)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-205.pth.tar', 89.11400006103516)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-203.pth.tar', 89.0540000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-201.pth.tar', 88.91400000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-200.pth.tar', 88.80200009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-204.pth.tar', 88.7440000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-198.pth.tar', 88.7199999609375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-202.pth.tar', 88.71000009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-199.pth.tar', 88.5440000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-193.pth.tar', 88.44600001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-194.pth.tar', 88.40800006835937)

Train: 206 [   0/927 (  0%)]  Loss:  4.784579 (4.7846)  Time: 9.249s,  110.71/s  (9.249s,  110.71/s)  LR: 1.089e-05  Data: 8.613 (8.613)
Train: 206 [  50/927 (  5%)]  Loss:  5.573897 (5.1792)  Time: 0.589s, 1738.06/s  (2.179s,  469.95/s)  LR: 1.089e-05  Data: 0.021 (1.593)
Train: 206 [ 100/927 ( 11%)]  Loss:  4.876825 (5.0784)  Time: 0.589s, 1738.26/s  (2.632s,  388.99/s)  LR: 1.089e-05  Data: 0.020 (2.033)
Train: 206 [ 150/927 ( 16%)]  Loss:  4.425050 (4.9151)  Time: 0.587s, 1743.03/s  (2.787s,  367.37/s)  LR: 1.089e-05  Data: 0.018 (2.173)
Train: 206 [ 200/927 ( 22%)]  Loss:  4.397072 (4.8115)  Time: 7.545s,  135.73/s  (2.697s,  379.64/s)  LR: 1.089e-05  Data: 6.952 (2.085)
Train: 206 [ 250/927 ( 27%)]  Loss:  4.366360 (4.7373)  Time: 0.585s, 1750.62/s  (2.606s,  393.00/s)  LR: 1.089e-05  Data: 0.020 (2.000)
Train: 206 [ 300/927 ( 32%)]  Loss:  5.234558 (4.8083)  Time: 7.875s,  130.04/s  (2.558s,  400.35/s)  LR: 1.089e-05  Data: 7.284 (1.955)
Train: 206 [ 350/927 ( 38%)]  Loss:  4.655389 (4.7892)  Time: 0.589s, 1739.40/s  (2.493s,  410.69/s)  LR: 1.089e-05  Data: 0.025 (1.894)
Train: 206 [ 400/927 ( 43%)]  Loss:  4.598707 (4.7680)  Time: 6.978s,  146.75/s  (2.468s,  414.92/s)  LR: 1.089e-05  Data: 6.414 (1.869)
Train: 206 [ 450/927 ( 49%)]  Loss:  5.219989 (4.8132)  Time: 0.587s, 1743.87/s  (2.474s,  413.86/s)  LR: 1.089e-05  Data: 0.022 (1.875)
Train: 206 [ 500/927 ( 54%)]  Loss:  5.149797 (4.8438)  Time: 7.487s,  136.78/s  (2.449s,  418.08/s)  LR: 1.089e-05  Data: 6.846 (1.849)
Train: 206 [ 550/927 ( 59%)]  Loss:  4.151680 (4.7862)  Time: 0.698s, 1466.19/s  (2.434s,  420.75/s)  LR: 1.089e-05  Data: 0.112 (1.834)
Train: 206 [ 600/927 ( 65%)]  Loss:  5.119009 (4.8118)  Time: 7.498s,  136.57/s  (2.438s,  420.05/s)  LR: 1.089e-05  Data: 6.934 (1.837)
Train: 206 [ 650/927 ( 70%)]  Loss:  4.043811 (4.7569)  Time: 0.586s, 1746.82/s  (2.432s,  421.03/s)  LR: 1.089e-05  Data: 0.021 (1.833)
Train: 206 [ 700/927 ( 76%)]  Loss:  4.625095 (4.7481)  Time: 7.526s,  136.05/s  (2.430s,  421.42/s)  LR: 1.089e-05  Data: 6.864 (1.832)
Train: 206 [ 750/927 ( 81%)]  Loss:  4.709366 (4.7457)  Time: 0.590s, 1735.99/s  (2.410s,  424.96/s)  LR: 1.089e-05  Data: 0.026 (1.812)
Train: 206 [ 800/927 ( 86%)]  Loss:  4.676153 (4.7416)  Time: 6.824s,  150.05/s  (2.400s,  426.59/s)  LR: 1.089e-05  Data: 6.149 (1.803)
Train: 206 [ 850/927 ( 92%)]  Loss:  4.486583 (4.7274)  Time: 0.589s, 1739.44/s  (2.414s,  424.25/s)  LR: 1.089e-05  Data: 0.024 (1.817)
Train: 206 [ 900/927 ( 97%)]  Loss:  4.179948 (4.6986)  Time: 7.760s,  131.96/s  (2.415s,  423.98/s)  LR: 1.089e-05  Data: 7.074 (1.819)
Train: 206 [ 926/927 (100%)]  Loss:  5.001042 (4.7137)  Time: 0.566s, 1809.62/s  (2.410s,  424.91/s)  LR: 1.089e-05  Data: 0.000 (1.814)
Test: [   0/48]  Time: 12.914 (12.914)  Loss:  0.7132 (0.7132)  Acc@1: 88.7695 (88.7695)  Acc@5: 96.8750 (96.8750)
Test: [  48/48]  Time: 0.149 (3.814)  Loss:  0.6095 (0.7156)  Acc@1: 91.3915 (88.8680)  Acc@5: 97.8774 (97.2820)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-205.pth.tar', 89.11400006103516)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-203.pth.tar', 89.0540000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-201.pth.tar', 88.91400000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-206.pth.tar', 88.86799988037109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-200.pth.tar', 88.80200009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-204.pth.tar', 88.7440000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-198.pth.tar', 88.7199999609375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-202.pth.tar', 88.71000009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-199.pth.tar', 88.5440000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-193.pth.tar', 88.44600001464843)

Train: 207 [   0/927 (  0%)]  Loss:  5.016794 (5.0168)  Time: 10.055s,  101.84/s  (10.055s,  101.84/s)  LR: 1.050e-05  Data: 9.005 (9.005)
Train: 207 [  50/927 (  5%)]  Loss:  3.925051 (4.4709)  Time: 0.584s, 1753.00/s  (2.981s,  343.51/s)  LR: 1.050e-05  Data: 0.018 (2.378)
Train: 207 [ 100/927 ( 11%)]  Loss:  4.633442 (4.5251)  Time: 0.587s, 1744.72/s  (2.601s,  393.62/s)  LR: 1.050e-05  Data: 0.021 (2.007)
Train: 207 [ 150/927 ( 16%)]  Loss:  4.636437 (4.5529)  Time: 0.584s, 1754.68/s  (2.450s,  417.96/s)  LR: 1.050e-05  Data: 0.020 (1.864)
Train: 207 [ 200/927 ( 22%)]  Loss:  5.025868 (4.6475)  Time: 0.587s, 1745.25/s  (2.497s,  410.01/s)  LR: 1.050e-05  Data: 0.019 (1.909)
Train: 207 [ 250/927 ( 27%)]  Loss:  4.182564 (4.5700)  Time: 0.586s, 1746.71/s  (2.431s,  421.29/s)  LR: 1.050e-05  Data: 0.019 (1.843)
Train: 207 [ 300/927 ( 32%)]  Loss:  5.251290 (4.6673)  Time: 0.584s, 1752.06/s  (2.440s,  419.63/s)  LR: 1.050e-05  Data: 0.019 (1.853)
Train: 207 [ 350/927 ( 38%)]  Loss:  3.845381 (4.5646)  Time: 0.586s, 1746.61/s  (2.413s,  424.43/s)  LR: 1.050e-05  Data: 0.020 (1.826)
Train: 207 [ 400/927 ( 43%)]  Loss:  4.853217 (4.5967)  Time: 0.588s, 1742.23/s  (2.407s,  425.35/s)  LR: 1.050e-05  Data: 0.021 (1.822)
Train: 207 [ 450/927 ( 49%)]  Loss:  4.246540 (4.5617)  Time: 0.587s, 1744.43/s  (2.391s,  428.24/s)  LR: 1.050e-05  Data: 0.022 (1.803)
Train: 207 [ 500/927 ( 54%)]  Loss:  4.505136 (4.5565)  Time: 0.588s, 1742.79/s  (2.389s,  428.60/s)  LR: 1.050e-05  Data: 0.021 (1.802)
Train: 207 [ 550/927 ( 59%)]  Loss:  4.769160 (4.5742)  Time: 0.588s, 1740.99/s  (2.368s,  432.45/s)  LR: 1.050e-05  Data: 0.020 (1.781)
Train: 207 [ 600/927 ( 65%)]  Loss:  4.887699 (4.5984)  Time: 0.591s, 1733.40/s  (2.402s,  426.37/s)  LR: 1.050e-05  Data: 0.025 (1.815)
Train: 207 [ 650/927 ( 70%)]  Loss:  3.951073 (4.5521)  Time: 1.414s,  724.16/s  (2.408s,  425.18/s)  LR: 1.050e-05  Data: 0.848 (1.819)
Train: 207 [ 700/927 ( 76%)]  Loss:  4.989110 (4.5813)  Time: 0.589s, 1737.91/s  (2.412s,  424.57/s)  LR: 1.050e-05  Data: 0.025 (1.822)
Train: 207 [ 750/927 ( 81%)]  Loss:  4.815809 (4.5959)  Time: 0.591s, 1731.55/s  (2.405s,  425.81/s)  LR: 1.050e-05  Data: 0.024 (1.815)
Train: 207 [ 800/927 ( 86%)]  Loss:  4.188342 (4.5719)  Time: 0.588s, 1740.66/s  (2.409s,  425.10/s)  LR: 1.050e-05  Data: 0.024 (1.819)
Train: 207 [ 850/927 ( 92%)]  Loss:  4.953353 (4.5931)  Time: 0.587s, 1744.26/s  (2.413s,  424.38/s)  LR: 1.050e-05  Data: 0.019 (1.824)
Train: 207 [ 900/927 ( 97%)]  Loss:  4.682752 (4.5978)  Time: 0.584s, 1753.31/s  (2.462s,  415.98/s)  LR: 1.050e-05  Data: 0.019 (1.873)
Train: 207 [ 926/927 (100%)]  Loss:  4.474060 (4.5917)  Time: 0.564s, 1815.35/s  (2.475s,  413.81/s)  LR: 1.050e-05  Data: 0.000 (1.886)
Test: [   0/48]  Time: 13.551 (13.551)  Loss:  0.7091 (0.7091)  Acc@1: 88.6719 (88.6719)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.149 (3.574)  Loss:  0.6223 (0.7083)  Acc@1: 90.9198 (88.9840)  Acc@5: 97.6415 (97.3160)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-205.pth.tar', 89.11400006103516)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-203.pth.tar', 89.0540000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-207.pth.tar', 88.98400006347656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-201.pth.tar', 88.91400000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-206.pth.tar', 88.86799988037109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-200.pth.tar', 88.80200009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-204.pth.tar', 88.7440000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-198.pth.tar', 88.7199999609375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-202.pth.tar', 88.71000009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-199.pth.tar', 88.5440000390625)

Train: 208 [   0/927 (  0%)]  Loss:  4.599829 (4.5998)  Time: 11.190s,   91.51/s  (11.190s,   91.51/s)  LR: 1.022e-05  Data: 10.085 (10.085)
Train: 208 [  50/927 (  5%)]  Loss:  5.254207 (4.9270)  Time: 0.588s, 1741.74/s  (2.486s,  411.96/s)  LR: 1.022e-05  Data: 0.024 (1.893)
Train: 208 [ 100/927 ( 11%)]  Loss:  4.438712 (4.7642)  Time: 1.275s,  803.30/s  (2.411s,  424.69/s)  LR: 1.022e-05  Data: 0.584 (1.818)
Train: 208 [ 150/927 ( 16%)]  Loss:  4.927147 (4.8050)  Time: 0.585s, 1750.02/s  (2.336s,  438.40/s)  LR: 1.022e-05  Data: 0.021 (1.737)
Train: 208 [ 200/927 ( 22%)]  Loss:  5.011136 (4.8462)  Time: 2.288s,  447.47/s  (2.325s,  440.40/s)  LR: 1.022e-05  Data: 1.706 (1.729)
Train: 208 [ 250/927 ( 27%)]  Loss:  5.128325 (4.8932)  Time: 0.589s, 1737.32/s  (2.281s,  448.99/s)  LR: 1.022e-05  Data: 0.019 (1.681)
Train: 208 [ 300/927 ( 32%)]  Loss:  3.769296 (4.7327)  Time: 3.436s,  297.98/s  (2.345s,  436.74/s)  LR: 1.022e-05  Data: 2.853 (1.746)
Train: 208 [ 350/927 ( 38%)]  Loss:  4.629942 (4.7198)  Time: 0.585s, 1749.33/s  (2.325s,  440.35/s)  LR: 1.022e-05  Data: 0.019 (1.726)
Train: 208 [ 400/927 ( 43%)]  Loss:  5.362650 (4.7912)  Time: 0.592s, 1728.39/s  (2.341s,  437.43/s)  LR: 1.022e-05  Data: 0.024 (1.744)
Train: 208 [ 450/927 ( 49%)]  Loss:  5.297154 (4.8418)  Time: 0.584s, 1754.16/s  (2.331s,  439.25/s)  LR: 1.022e-05  Data: 0.019 (1.735)
Train: 208 [ 500/927 ( 54%)]  Loss:  3.869704 (4.7535)  Time: 0.586s, 1747.51/s  (2.333s,  438.85/s)  LR: 1.022e-05  Data: 0.020 (1.738)
Train: 208 [ 550/927 ( 59%)]  Loss:  5.440361 (4.8107)  Time: 0.587s, 1744.16/s  (2.322s,  441.01/s)  LR: 1.022e-05  Data: 0.018 (1.727)
Train: 208 [ 600/927 ( 65%)]  Loss:  4.786253 (4.8088)  Time: 3.260s,  314.14/s  (2.330s,  439.40/s)  LR: 1.022e-05  Data: 2.693 (1.736)
Train: 208 [ 650/927 ( 70%)]  Loss:  4.016801 (4.7523)  Time: 0.589s, 1739.89/s  (2.327s,  440.14/s)  LR: 1.022e-05  Data: 0.022 (1.732)
Train: 208 [ 700/927 ( 76%)]  Loss:  4.320881 (4.7235)  Time: 4.862s,  210.61/s  (2.353s,  435.21/s)  LR: 1.022e-05  Data: 4.298 (1.756)
Train: 208 [ 750/927 ( 81%)]  Loss:  5.013186 (4.7416)  Time: 0.586s, 1747.88/s  (2.376s,  430.89/s)  LR: 1.022e-05  Data: 0.022 (1.780)
Train: 208 [ 800/927 ( 86%)]  Loss:  4.572733 (4.7317)  Time: 4.809s,  212.93/s  (2.432s,  421.01/s)  LR: 1.022e-05  Data: 4.220 (1.834)
Train: 208 [ 850/927 ( 92%)]  Loss:  4.870492 (4.7394)  Time: 0.587s, 1744.04/s  (2.418s,  423.50/s)  LR: 1.022e-05  Data: 0.021 (1.819)
Train: 208 [ 900/927 ( 97%)]  Loss:  4.583800 (4.7312)  Time: 1.837s,  557.56/s  (2.415s,  423.97/s)  LR: 1.022e-05  Data: 1.184 (1.816)
Train: 208 [ 926/927 (100%)]  Loss:  4.490915 (4.7192)  Time: 0.565s, 1811.05/s  (2.409s,  425.01/s)  LR: 1.022e-05  Data: 0.000 (1.810)
Test: [   0/48]  Time: 13.445 (13.445)  Loss:  0.7086 (0.7086)  Acc@1: 88.8672 (88.8672)  Acc@5: 96.9727 (96.9727)
Test: [  48/48]  Time: 0.149 (3.136)  Loss:  0.5986 (0.6988)  Acc@1: 91.2736 (89.2720)  Acc@5: 97.6415 (97.4000)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-208.pth.tar', 89.27200008789063)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-205.pth.tar', 89.11400006103516)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-203.pth.tar', 89.0540000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-207.pth.tar', 88.98400006347656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-201.pth.tar', 88.91400000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-206.pth.tar', 88.86799988037109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-200.pth.tar', 88.80200009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-204.pth.tar', 88.7440000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-198.pth.tar', 88.7199999609375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-202.pth.tar', 88.71000009033203)

Train: 209 [   0/927 (  0%)]  Loss:  4.485658 (4.4857)  Time: 8.940s,  114.55/s  (8.940s,  114.55/s)  LR: 1.006e-05  Data: 8.294 (8.294)
Train: 209 [  50/927 (  5%)]  Loss:  4.407446 (4.4466)  Time: 0.650s, 1574.20/s  (2.597s,  394.25/s)  LR: 1.006e-05  Data: 0.020 (1.996)
Train: 209 [ 100/927 ( 11%)]  Loss:  4.585007 (4.4927)  Time: 1.060s,  966.13/s  (2.429s,  421.54/s)  LR: 1.006e-05  Data: 0.329 (1.833)
Train: 209 [ 150/927 ( 16%)]  Loss:  4.676396 (4.5386)  Time: 0.587s, 1743.75/s  (2.357s,  434.51/s)  LR: 1.006e-05  Data: 0.021 (1.761)
Train: 209 [ 200/927 ( 22%)]  Loss:  4.718727 (4.5746)  Time: 0.589s, 1737.58/s  (2.360s,  433.83/s)  LR: 1.006e-05  Data: 0.022 (1.767)
Train: 209 [ 250/927 ( 27%)]  Loss:  4.433568 (4.5511)  Time: 0.587s, 1745.27/s  (2.316s,  442.12/s)  LR: 1.006e-05  Data: 0.023 (1.722)
Train: 209 [ 300/927 ( 32%)]  Loss:  5.103455 (4.6300)  Time: 2.594s,  394.69/s  (2.313s,  442.69/s)  LR: 1.006e-05  Data: 1.359 (1.716)
Train: 209 [ 350/927 ( 38%)]  Loss:  5.009256 (4.6774)  Time: 0.591s, 1731.96/s  (2.286s,  447.94/s)  LR: 1.006e-05  Data: 0.023 (1.690)
Train: 209 [ 400/927 ( 43%)]  Loss:  4.461614 (4.6535)  Time: 0.589s, 1739.76/s  (2.274s,  450.40/s)  LR: 1.006e-05  Data: 0.023 (1.678)
Train: 209 [ 450/927 ( 49%)]  Loss:  5.149549 (4.7031)  Time: 1.070s,  957.40/s  (2.296s,  445.94/s)  LR: 1.006e-05  Data: 0.506 (1.700)
Train: 209 [ 500/927 ( 54%)]  Loss:  4.418432 (4.6772)  Time: 0.587s, 1744.62/s  (2.290s,  447.15/s)  LR: 1.006e-05  Data: 0.020 (1.695)
Train: 209 [ 550/927 ( 59%)]  Loss:  3.632187 (4.5901)  Time: 0.587s, 1745.04/s  (2.288s,  447.61/s)  LR: 1.006e-05  Data: 0.023 (1.693)
Train: 209 [ 600/927 ( 65%)]  Loss:  4.178322 (4.5584)  Time: 0.589s, 1738.15/s  (2.296s,  445.95/s)  LR: 1.006e-05  Data: 0.023 (1.703)
Train: 209 [ 650/927 ( 70%)]  Loss:  4.940904 (4.5858)  Time: 1.096s,  934.67/s  (2.294s,  446.43/s)  LR: 1.006e-05  Data: 0.532 (1.701)
Train: 209 [ 700/927 ( 76%)]  Loss:  4.814181 (4.6010)  Time: 0.591s, 1733.77/s  (2.368s,  432.37/s)  LR: 1.006e-05  Data: 0.025 (1.776)
Train: 209 [ 750/927 ( 81%)]  Loss:  4.316547 (4.5832)  Time: 0.590s, 1736.26/s  (2.349s,  436.01/s)  LR: 1.006e-05  Data: 0.021 (1.756)
Train: 209 [ 800/927 ( 86%)]  Loss:  4.401939 (4.5725)  Time: 0.592s, 1728.44/s  (2.357s,  434.40/s)  LR: 1.006e-05  Data: 0.023 (1.764)
Train: 209 [ 850/927 ( 92%)]  Loss:  5.058975 (4.5996)  Time: 0.586s, 1748.39/s  (2.352s,  435.32/s)  LR: 1.006e-05  Data: 0.020 (1.759)
Train: 209 [ 900/927 ( 97%)]  Loss:  5.024321 (4.6219)  Time: 0.589s, 1739.28/s  (2.359s,  434.14/s)  LR: 1.006e-05  Data: 0.023 (1.766)
Train: 209 [ 926/927 (100%)]  Loss:  4.646414 (4.6231)  Time: 0.567s, 1806.89/s  (2.351s,  435.61/s)  LR: 1.006e-05  Data: 0.000 (1.759)
Test: [   0/48]  Time: 12.598 (12.598)  Loss:  0.7088 (0.7088)  Acc@1: 88.8672 (88.8672)  Acc@5: 96.7773 (96.7773)
Test: [  48/48]  Time: 0.149 (3.248)  Loss:  0.6136 (0.7063)  Acc@1: 90.8019 (88.9860)  Acc@5: 97.7594 (97.3380)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-208.pth.tar', 89.27200008789063)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-205.pth.tar', 89.11400006103516)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-203.pth.tar', 89.0540000341797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-209.pth.tar', 88.98600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-207.pth.tar', 88.98400006347656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-201.pth.tar', 88.91400000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-206.pth.tar', 88.86799988037109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-200.pth.tar', 88.80200009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-204.pth.tar', 88.7440000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/checkpoint-198.pth.tar', 88.7199999609375)

*** Best metric: 89.27200008789063 (epoch 208)

wandb: Waiting for W&B process to finish, PID 52330
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210606_134658-PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/logs/debug.log
wandb: Find internal logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210606_134658-PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 209
wandb:     _runtime 505033
wandb:    eval_loss 0.70628
wandb:    eval_top1 88.986
wandb:    eval_top5 97.338
wandb:   _timestamp 1623006479
wandb:   train_loss 4.62314
wandb:        _step 209
wandb: Run history:
wandb:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   train_loss ‚ñÑ‚ñÉ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÜ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñÜ‚ñÅ‚ñÑ‚ñÑ‚ñÅ‚ñÑ‚ñÇ
wandb:    eval_loss ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ
wandb:    eval_top1 ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñà‚ñá
wandb:    eval_top5 ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá
wandb:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k: https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
--End--
Mon Jun 7 04:08:09 JST 2021

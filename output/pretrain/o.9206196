--Start--
Sun Jun 6 13:59:02 JST 2021
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 3
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
wandb: Currently logged in as: gyama_x (use `wandb login --relogin` to force relogin)
wandb: WARNING Tried to auto resume run with id PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1 but id PreTraining_vit_deit_tiny_patch16_224_1k is set.
wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
wandb: Tracking run with wandb version 0.10.27
wandb: Resuming run PreTraining_vit_deit_tiny_patch16_224_1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gyama_x/pytorch-image-models
wandb: üöÄ View run at https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_1k
wandb: Run data is saved locally in /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210606_135942-PreTraining_vit_deit_tiny_patch16_224_1k
wandb: Run `wandb offline` to turn off syncing.

Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar' (epoch 187)
Using native Torch DistributedDataParallel.
Scheduled epochs: 210
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Train: 188 [   0/1251 (  0%)]  Loss:  4.586035 (4.5860)  Time: 17.578s,   58.26/s  (17.578s,   58.26/s)  LR: 3.657e-05  Data: 16.245 (16.245)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 188 [  50/1251 (  4%)]  Loss:  4.754755 (4.6704)  Time: 0.584s, 1752.42/s  (2.545s,  402.43/s)  LR: 3.657e-05  Data: 0.021 (1.933)
Train: 188 [ 100/1251 (  8%)]  Loss:  4.629257 (4.6567)  Time: 0.589s, 1738.17/s  (2.420s,  423.05/s)  LR: 3.657e-05  Data: 0.021 (1.819)
Train: 188 [ 150/1251 ( 12%)]  Loss:  4.147996 (4.5295)  Time: 0.583s, 1756.23/s  (2.292s,  446.70/s)  LR: 3.657e-05  Data: 0.020 (1.698)
Train: 188 [ 200/1251 ( 16%)]  Loss:  4.216698 (4.4669)  Time: 0.586s, 1748.55/s  (2.264s,  452.34/s)  LR: 3.657e-05  Data: 0.021 (1.670)
Train: 188 [ 250/1251 ( 20%)]  Loss:  3.972712 (4.3846)  Time: 0.586s, 1746.44/s  (2.224s,  460.50/s)  LR: 3.657e-05  Data: 0.022 (1.630)
Train: 188 [ 300/1251 ( 24%)]  Loss:  4.728921 (4.4338)  Time: 0.584s, 1754.25/s  (2.204s,  464.64/s)  LR: 3.657e-05  Data: 0.021 (1.612)
Train: 188 [ 350/1251 ( 28%)]  Loss:  4.268163 (4.4131)  Time: 0.586s, 1746.11/s  (2.172s,  471.40/s)  LR: 3.657e-05  Data: 0.023 (1.578)
Train: 188 [ 400/1251 ( 32%)]  Loss:  4.672135 (4.4419)  Time: 0.587s, 1743.65/s  (2.172s,  471.42/s)  LR: 3.657e-05  Data: 0.022 (1.580)
Train: 188 [ 450/1251 ( 36%)]  Loss:  4.323913 (4.4301)  Time: 0.814s, 1258.28/s  (2.155s,  475.17/s)  LR: 3.657e-05  Data: 0.246 (1.564)
Train: 188 [ 500/1251 ( 40%)]  Loss:  4.222942 (4.4112)  Time: 0.583s, 1755.51/s  (2.154s,  475.31/s)  LR: 3.657e-05  Data: 0.020 (1.562)
Train: 188 [ 550/1251 ( 44%)]  Loss:  4.443089 (4.4139)  Time: 0.908s, 1127.25/s  (2.133s,  480.11/s)  LR: 3.657e-05  Data: 0.234 (1.539)
Train: 188 [ 600/1251 ( 48%)]  Loss:  5.168708 (4.4719)  Time: 1.964s,  521.32/s  (2.133s,  480.13/s)  LR: 3.657e-05  Data: 1.298 (1.537)
Train: 188 [ 650/1251 ( 52%)]  Loss:  4.352101 (4.4634)  Time: 0.585s, 1750.37/s  (2.120s,  483.04/s)  LR: 3.657e-05  Data: 0.022 (1.525)
Train: 188 [ 700/1251 ( 56%)]  Loss:  4.648986 (4.4758)  Time: 0.583s, 1755.35/s  (2.122s,  482.63/s)  LR: 3.657e-05  Data: 0.020 (1.528)
Train: 188 [ 750/1251 ( 60%)]  Loss:  4.975553 (4.5070)  Time: 0.587s, 1745.53/s  (2.118s,  483.56/s)  LR: 3.657e-05  Data: 0.020 (1.524)
Train: 188 [ 800/1251 ( 64%)]  Loss:  4.306664 (4.4952)  Time: 0.585s, 1751.91/s  (2.126s,  481.66/s)  LR: 3.657e-05  Data: 0.022 (1.533)
Train: 188 [ 850/1251 ( 68%)]  Loss:  4.462167 (4.4934)  Time: 0.589s, 1738.09/s  (2.146s,  477.22/s)  LR: 3.657e-05  Data: 0.019 (1.553)
Train: 188 [ 900/1251 ( 72%)]  Loss:  4.813373 (4.5102)  Time: 0.670s, 1527.51/s  (2.157s,  474.84/s)  LR: 3.657e-05  Data: 0.088 (1.564)
Train: 188 [ 950/1251 ( 76%)]  Loss:  4.250782 (4.4972)  Time: 0.584s, 1752.11/s  (2.162s,  473.60/s)  LR: 3.657e-05  Data: 0.021 (1.570)
Train: 188 [1000/1251 ( 80%)]  Loss:  4.106246 (4.4786)  Time: 1.090s,  939.78/s  (2.175s,  470.70/s)  LR: 3.657e-05  Data: 0.395 (1.583)
Train: 188 [1050/1251 ( 84%)]  Loss:  4.341173 (4.4724)  Time: 0.584s, 1752.93/s  (2.179s,  469.94/s)  LR: 3.657e-05  Data: 0.021 (1.585)
Train: 188 [1100/1251 ( 88%)]  Loss:  4.480535 (4.4727)  Time: 1.451s,  705.80/s  (2.198s,  465.96/s)  LR: 3.657e-05  Data: 0.856 (1.604)
Train: 188 [1150/1251 ( 92%)]  Loss:  4.122288 (4.4581)  Time: 0.585s, 1750.84/s  (2.203s,  464.81/s)  LR: 3.657e-05  Data: 0.023 (1.609)
Train: 188 [1200/1251 ( 96%)]  Loss:  4.220811 (4.4486)  Time: 10.361s,   98.83/s  (2.210s,  463.38/s)  LR: 3.657e-05  Data: 9.693 (1.616)
Train: 188 [1250/1251 (100%)]  Loss:  4.423550 (4.4477)  Time: 0.563s, 1819.61/s  (2.213s,  462.66/s)  LR: 3.657e-05  Data: 0.000 (1.619)
Test: [   0/48]  Time: 14.693 (14.693)  Loss:  1.0179 (1.0179)  Acc@1: 79.0039 (79.0039)  Acc@5: 93.4570 (93.4570)
Test: [  48/48]  Time: 0.569 (3.500)  Loss:  1.0706 (1.8938)  Acc@1: 77.5943 (58.0180)  Acc@5: 90.9198 (81.7720)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-188.pth.tar', 58.018000092773434)

Train: 189 [   0/1251 (  0%)]  Loss:  4.354164 (4.3542)  Time: 6.743s,  151.87/s  (6.743s,  151.87/s)  LR: 3.423e-05  Data: 6.107 (6.107)
Train: 189 [  50/1251 (  4%)]  Loss:  4.395271 (4.3747)  Time: 0.586s, 1746.12/s  (2.390s,  428.41/s)  LR: 3.423e-05  Data: 0.019 (1.799)
Train: 189 [ 100/1251 (  8%)]  Loss:  4.784398 (4.5113)  Time: 0.583s, 1755.12/s  (2.372s,  431.76/s)  LR: 3.423e-05  Data: 0.020 (1.778)
Train: 189 [ 150/1251 ( 12%)]  Loss:  4.127767 (4.4154)  Time: 0.586s, 1746.74/s  (2.306s,  444.13/s)  LR: 3.423e-05  Data: 0.019 (1.713)
Train: 189 [ 200/1251 ( 16%)]  Loss:  4.427083 (4.4177)  Time: 2.072s,  494.18/s  (2.294s,  446.41/s)  LR: 3.423e-05  Data: 1.485 (1.697)
Train: 189 [ 250/1251 ( 20%)]  Loss:  3.863183 (4.3253)  Time: 0.585s, 1750.00/s  (2.258s,  453.58/s)  LR: 3.423e-05  Data: 0.021 (1.658)
Train: 189 [ 300/1251 ( 24%)]  Loss:  5.015461 (4.4239)  Time: 3.657s,  280.03/s  (2.316s,  442.06/s)  LR: 3.423e-05  Data: 3.074 (1.716)
Train: 189 [ 350/1251 ( 28%)]  Loss:  4.167601 (4.3919)  Time: 0.589s, 1737.79/s  (2.310s,  443.33/s)  LR: 3.423e-05  Data: 0.020 (1.711)
Train: 189 [ 400/1251 ( 32%)]  Loss:  4.507111 (4.4047)  Time: 1.673s,  612.09/s  (2.313s,  442.70/s)  LR: 3.423e-05  Data: 1.020 (1.712)
Train: 189 [ 450/1251 ( 36%)]  Loss:  4.843587 (4.4486)  Time: 0.590s, 1735.95/s  (2.300s,  445.18/s)  LR: 3.423e-05  Data: 0.019 (1.700)
Train: 189 [ 500/1251 ( 40%)]  Loss:  4.103628 (4.4172)  Time: 3.069s,  333.62/s  (2.302s,  444.77/s)  LR: 3.423e-05  Data: 2.497 (1.701)
Train: 189 [ 550/1251 ( 44%)]  Loss:  4.763990 (4.4461)  Time: 0.588s, 1742.14/s  (2.290s,  447.20/s)  LR: 3.423e-05  Data: 0.019 (1.689)
Train: 189 [ 600/1251 ( 48%)]  Loss:  4.783237 (4.4720)  Time: 1.584s,  646.59/s  (2.292s,  446.82/s)  LR: 3.423e-05  Data: 1.015 (1.692)
Train: 189 [ 650/1251 ( 52%)]  Loss:  4.780589 (4.4941)  Time: 0.584s, 1753.12/s  (2.285s,  448.20/s)  LR: 3.423e-05  Data: 0.022 (1.686)
Train: 189 [ 700/1251 ( 56%)]  Loss:  4.250509 (4.4778)  Time: 0.587s, 1743.29/s  (2.314s,  442.44/s)  LR: 3.423e-05  Data: 0.019 (1.713)
Train: 189 [ 750/1251 ( 60%)]  Loss:  4.775510 (4.4964)  Time: 0.585s, 1749.20/s  (2.319s,  441.59/s)  LR: 3.423e-05  Data: 0.019 (1.719)
Train: 189 [ 800/1251 ( 64%)]  Loss:  5.007891 (4.5265)  Time: 0.587s, 1744.91/s  (2.328s,  439.87/s)  LR: 3.423e-05  Data: 0.019 (1.728)
Train: 189 [ 850/1251 ( 68%)]  Loss:  4.430472 (4.5212)  Time: 1.172s,  873.86/s  (2.325s,  440.39/s)  LR: 3.423e-05  Data: 0.580 (1.726)
Train: 189 [ 900/1251 ( 72%)]  Loss:  4.606414 (4.5257)  Time: 0.585s, 1750.63/s  (2.329s,  439.59/s)  LR: 3.423e-05  Data: 0.021 (1.731)
Train: 189 [ 950/1251 ( 76%)]  Loss:  4.347428 (4.5168)  Time: 0.587s, 1745.19/s  (2.327s,  439.99/s)  LR: 3.423e-05  Data: 0.020 (1.729)
Train: 189 [1000/1251 ( 80%)]  Loss:  4.507707 (4.5163)  Time: 0.658s, 1556.36/s  (2.327s,  439.97/s)  LR: 3.423e-05  Data: 0.020 (1.729)
Train: 189 [1050/1251 ( 84%)]  Loss:  4.679149 (4.5237)  Time: 0.582s, 1759.18/s  (2.333s,  438.95/s)  LR: 3.423e-05  Data: 0.020 (1.735)
Train: 189 [1100/1251 ( 88%)]  Loss:  4.939844 (4.5418)  Time: 0.587s, 1745.41/s  (2.337s,  438.14/s)  LR: 3.423e-05  Data: 0.021 (1.740)
Train: 189 [1150/1251 ( 92%)]  Loss:  4.981184 (4.5601)  Time: 0.585s, 1750.71/s  (2.336s,  438.36/s)  LR: 3.423e-05  Data: 0.021 (1.740)
Train: 189 [1200/1251 ( 96%)]  Loss:  4.604986 (4.5619)  Time: 0.589s, 1737.22/s  (2.337s,  438.17/s)  LR: 3.423e-05  Data: 0.019 (1.742)
Train: 189 [1250/1251 (100%)]  Loss:  4.661406 (4.5658)  Time: 0.563s, 1819.24/s  (2.334s,  438.73/s)  LR: 3.423e-05  Data: 0.000 (1.738)
Test: [   0/48]  Time: 14.419 (14.419)  Loss:  1.0287 (1.0287)  Acc@1: 78.4180 (78.4180)  Acc@5: 93.6523 (93.6523)
Test: [  48/48]  Time: 0.150 (3.288)  Loss:  1.0596 (1.8794)  Acc@1: 77.7123 (58.1920)  Acc@5: 91.3915 (81.8360)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-189.pth.tar', 58.192000014648436)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-188.pth.tar', 58.018000092773434)

Train: 190 [   0/1251 (  0%)]  Loss:  4.804276 (4.8043)  Time: 10.770s,   95.08/s  (10.770s,   95.08/s)  LR: 3.199e-05  Data: 10.208 (10.208)
Train: 190 [  50/1251 (  4%)]  Loss:  4.106849 (4.4556)  Time: 0.587s, 1744.09/s  (2.323s,  440.75/s)  LR: 3.199e-05  Data: 0.024 (1.725)
Train: 190 [ 100/1251 (  8%)]  Loss:  4.242973 (4.3847)  Time: 0.791s, 1295.34/s  (2.359s,  434.10/s)  LR: 3.199e-05  Data: 0.205 (1.763)
Train: 190 [ 150/1251 ( 12%)]  Loss:  4.777153 (4.4828)  Time: 0.586s, 1748.29/s  (2.404s,  426.01/s)  LR: 3.199e-05  Data: 0.019 (1.812)
Train: 190 [ 200/1251 ( 16%)]  Loss:  4.778717 (4.5420)  Time: 3.115s,  328.73/s  (2.439s,  419.83/s)  LR: 3.199e-05  Data: 2.464 (1.844)
Train: 190 [ 250/1251 ( 20%)]  Loss:  4.781758 (4.5820)  Time: 0.583s, 1757.17/s  (2.412s,  424.48/s)  LR: 3.199e-05  Data: 0.019 (1.817)
Train: 190 [ 300/1251 ( 24%)]  Loss:  4.773544 (4.6093)  Time: 3.216s,  318.36/s  (2.395s,  427.49/s)  LR: 3.199e-05  Data: 2.540 (1.796)
Train: 190 [ 350/1251 ( 28%)]  Loss:  4.500309 (4.5957)  Time: 0.843s, 1214.32/s  (2.364s,  433.16/s)  LR: 3.199e-05  Data: 0.192 (1.765)
Train: 190 [ 400/1251 ( 32%)]  Loss:  4.459534 (4.5806)  Time: 4.590s,  223.08/s  (2.357s,  434.51/s)  LR: 3.199e-05  Data: 3.991 (1.758)
Train: 190 [ 450/1251 ( 36%)]  Loss:  4.536391 (4.5762)  Time: 0.586s, 1746.44/s  (2.330s,  439.42/s)  LR: 3.199e-05  Data: 0.022 (1.733)
Train: 190 [ 500/1251 ( 40%)]  Loss:  4.172258 (4.5394)  Time: 0.989s, 1035.15/s  (2.357s,  434.46/s)  LR: 3.199e-05  Data: 0.318 (1.759)
Train: 190 [ 550/1251 ( 44%)]  Loss:  4.166704 (4.5084)  Time: 0.589s, 1737.92/s  (2.356s,  434.72/s)  LR: 3.199e-05  Data: 0.018 (1.756)
Train: 190 [ 600/1251 ( 48%)]  Loss:  4.125256 (4.4789)  Time: 2.711s,  377.67/s  (2.378s,  430.60/s)  LR: 3.199e-05  Data: 2.030 (1.780)
Train: 190 [ 650/1251 ( 52%)]  Loss:  3.875462 (4.4358)  Time: 0.587s, 1744.55/s  (2.384s,  429.49/s)  LR: 3.199e-05  Data: 0.020 (1.785)
Train: 190 [ 700/1251 ( 56%)]  Loss:  4.214982 (4.4211)  Time: 2.368s,  432.41/s  (2.391s,  428.30/s)  LR: 3.199e-05  Data: 1.805 (1.792)
Train: 190 [ 750/1251 ( 60%)]  Loss:  4.851133 (4.4480)  Time: 0.583s, 1754.93/s  (2.382s,  429.84/s)  LR: 3.199e-05  Data: 0.021 (1.783)
Train: 190 [ 800/1251 ( 64%)]  Loss:  4.375456 (4.4437)  Time: 1.799s,  569.32/s  (2.377s,  430.71/s)  LR: 3.199e-05  Data: 1.234 (1.778)
Train: 190 [ 850/1251 ( 68%)]  Loss:  3.585431 (4.3960)  Time: 0.589s, 1739.46/s  (2.365s,  432.93/s)  LR: 3.199e-05  Data: 0.023 (1.766)
Train: 190 [ 900/1251 ( 72%)]  Loss:  4.617115 (4.4076)  Time: 0.585s, 1750.28/s  (2.389s,  428.65/s)  LR: 3.199e-05  Data: 0.020 (1.791)
Train: 190 [ 950/1251 ( 76%)]  Loss:  4.773388 (4.4259)  Time: 0.585s, 1751.76/s  (2.390s,  428.45/s)  LR: 3.199e-05  Data: 0.023 (1.792)
Train: 190 [1000/1251 ( 80%)]  Loss:  4.393237 (4.4244)  Time: 0.587s, 1743.42/s  (2.396s,  427.40/s)  LR: 3.199e-05  Data: 0.020 (1.799)
Train: 190 [1050/1251 ( 84%)]  Loss:  4.260044 (4.4169)  Time: 0.583s, 1755.44/s  (2.390s,  428.37/s)  LR: 3.199e-05  Data: 0.019 (1.794)
Train: 190 [1100/1251 ( 88%)]  Loss:  4.594455 (4.4246)  Time: 1.917s,  534.14/s  (2.390s,  428.45/s)  LR: 3.199e-05  Data: 1.240 (1.794)
Train: 190 [1150/1251 ( 92%)]  Loss:  4.165186 (4.4138)  Time: 1.724s,  593.94/s  (2.382s,  429.91/s)  LR: 3.199e-05  Data: 1.161 (1.785)
Train: 190 [1200/1251 ( 96%)]  Loss:  4.459810 (4.4157)  Time: 0.589s, 1738.03/s  (2.380s,  430.32/s)  LR: 3.199e-05  Data: 0.021 (1.781)
Train: 190 [1250/1251 (100%)]  Loss:  4.625083 (4.4237)  Time: 0.564s, 1816.20/s  (2.383s,  429.71/s)  LR: 3.199e-05  Data: 0.000 (1.785)
Test: [   0/48]  Time: 13.703 (13.703)  Loss:  1.0239 (1.0239)  Acc@1: 78.6133 (78.6133)  Acc@5: 92.3828 (92.3828)
Test: [  48/48]  Time: 0.149 (3.448)  Loss:  1.0505 (1.8737)  Acc@1: 77.4764 (58.3860)  Acc@5: 91.2736 (81.9420)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-190.pth.tar', 58.38599991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-189.pth.tar', 58.192000014648436)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-188.pth.tar', 58.018000092773434)

Train: 191 [   0/1251 (  0%)]  Loss:  4.531277 (4.5313)  Time: 12.236s,   83.69/s  (12.236s,   83.69/s)  LR: 2.986e-05  Data: 11.526 (11.526)
Train: 191 [  50/1251 (  4%)]  Loss:  4.210066 (4.3707)  Time: 0.585s, 1749.11/s  (2.499s,  409.81/s)  LR: 2.986e-05  Data: 0.023 (1.909)
Train: 191 [ 100/1251 (  8%)]  Loss:  4.594501 (4.4453)  Time: 1.768s,  579.28/s  (2.444s,  418.96/s)  LR: 2.986e-05  Data: 1.201 (1.843)
Train: 191 [ 150/1251 ( 12%)]  Loss:  4.726914 (4.5157)  Time: 0.589s, 1738.85/s  (2.373s,  431.50/s)  LR: 2.986e-05  Data: 0.024 (1.778)
Train: 191 [ 200/1251 ( 16%)]  Loss:  4.229507 (4.4585)  Time: 2.428s,  421.70/s  (2.344s,  436.92/s)  LR: 2.986e-05  Data: 1.785 (1.749)
Train: 191 [ 250/1251 ( 20%)]  Loss:  4.725254 (4.5029)  Time: 0.582s, 1760.50/s  (2.303s,  444.69/s)  LR: 2.986e-05  Data: 0.020 (1.706)
Train: 191 [ 300/1251 ( 24%)]  Loss:  4.703099 (4.5315)  Time: 4.572s,  223.96/s  (2.339s,  437.84/s)  LR: 2.986e-05  Data: 3.907 (1.739)
Train: 191 [ 350/1251 ( 28%)]  Loss:  4.620875 (4.5427)  Time: 0.585s, 1750.86/s  (2.342s,  437.31/s)  LR: 2.986e-05  Data: 0.020 (1.742)
Train: 191 [ 400/1251 ( 32%)]  Loss:  4.395501 (4.5263)  Time: 5.605s,  182.69/s  (2.354s,  435.03/s)  LR: 2.986e-05  Data: 5.018 (1.754)
Train: 191 [ 450/1251 ( 36%)]  Loss:  3.878950 (4.4616)  Time: 0.583s, 1755.12/s  (2.350s,  435.76/s)  LR: 2.986e-05  Data: 0.021 (1.751)
Train: 191 [ 500/1251 ( 40%)]  Loss:  5.124939 (4.5219)  Time: 6.769s,  151.28/s  (2.356s,  434.59/s)  LR: 2.986e-05  Data: 6.085 (1.758)
Train: 191 [ 550/1251 ( 44%)]  Loss:  4.609525 (4.5292)  Time: 0.584s, 1752.01/s  (2.351s,  435.50/s)  LR: 2.986e-05  Data: 0.019 (1.752)
Train: 191 [ 600/1251 ( 48%)]  Loss:  4.670357 (4.5401)  Time: 2.026s,  505.52/s  (2.348s,  436.16/s)  LR: 2.986e-05  Data: 1.425 (1.748)
Train: 191 [ 650/1251 ( 52%)]  Loss:  4.696961 (4.5513)  Time: 0.587s, 1745.91/s  (2.348s,  436.09/s)  LR: 2.986e-05  Data: 0.020 (1.749)
Train: 191 [ 700/1251 ( 56%)]  Loss:  4.726434 (4.5629)  Time: 0.586s, 1746.27/s  (2.364s,  433.17/s)  LR: 2.986e-05  Data: 0.020 (1.763)
Train: 191 [ 750/1251 ( 60%)]  Loss:  4.615806 (4.5662)  Time: 0.584s, 1753.32/s  (2.375s,  431.24/s)  LR: 2.986e-05  Data: 0.020 (1.773)
Train: 191 [ 800/1251 ( 64%)]  Loss:  4.609824 (4.5688)  Time: 0.586s, 1748.59/s  (2.375s,  431.24/s)  LR: 2.986e-05  Data: 0.020 (1.773)
Train: 191 [ 850/1251 ( 68%)]  Loss:  4.004054 (4.5374)  Time: 0.586s, 1746.85/s  (2.376s,  430.98/s)  LR: 2.986e-05  Data: 0.022 (1.774)
Train: 191 [ 900/1251 ( 72%)]  Loss:  4.082178 (4.5135)  Time: 0.589s, 1739.16/s  (2.374s,  431.42/s)  LR: 2.986e-05  Data: 0.020 (1.772)
Train: 191 [ 950/1251 ( 76%)]  Loss:  4.140582 (4.4948)  Time: 0.584s, 1752.50/s  (2.374s,  431.26/s)  LR: 2.986e-05  Data: 0.020 (1.773)
Train: 191 [1000/1251 ( 80%)]  Loss:  4.035374 (4.4730)  Time: 0.592s, 1728.97/s  (2.366s,  432.77/s)  LR: 2.986e-05  Data: 0.019 (1.765)
Train: 191 [1050/1251 ( 84%)]  Loss:  4.731863 (4.4847)  Time: 0.587s, 1743.85/s  (2.372s,  431.70/s)  LR: 2.986e-05  Data: 0.025 (1.771)
Train: 191 [1100/1251 ( 88%)]  Loss:  4.635331 (4.4913)  Time: 0.817s, 1253.34/s  (2.371s,  431.86/s)  LR: 2.986e-05  Data: 0.136 (1.770)
Train: 191 [1150/1251 ( 92%)]  Loss:  4.802713 (4.5042)  Time: 0.585s, 1749.39/s  (2.375s,  431.21/s)  LR: 2.986e-05  Data: 0.020 (1.774)
Train: 191 [1200/1251 ( 96%)]  Loss:  4.157796 (4.4904)  Time: 0.587s, 1745.81/s  (2.372s,  431.65/s)  LR: 2.986e-05  Data: 0.021 (1.772)
Train: 191 [1250/1251 (100%)]  Loss:  4.399914 (4.4869)  Time: 0.562s, 1820.88/s  (2.372s,  431.74/s)  LR: 2.986e-05  Data: 0.000 (1.772)
Test: [   0/48]  Time: 14.161 (14.161)  Loss:  1.0458 (1.0458)  Acc@1: 78.2227 (78.2227)  Acc@5: 92.2852 (92.2852)
Test: [  48/48]  Time: 0.149 (3.325)  Loss:  1.0427 (1.8827)  Acc@1: 77.9481 (58.4480)  Acc@5: 91.6274 (81.9900)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-191.pth.tar', 58.44799998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-190.pth.tar', 58.38599991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-189.pth.tar', 58.192000014648436)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-188.pth.tar', 58.018000092773434)

Train: 192 [   0/1251 (  0%)]  Loss:  4.316085 (4.3161)  Time: 10.864s,   94.25/s  (10.864s,   94.25/s)  LR: 2.784e-05  Data: 9.721 (9.721)
Train: 192 [  50/1251 (  4%)]  Loss:  4.166614 (4.2413)  Time: 0.583s, 1757.07/s  (2.343s,  437.12/s)  LR: 2.784e-05  Data: 0.020 (1.734)
Train: 192 [ 100/1251 (  8%)]  Loss:  4.886542 (4.4564)  Time: 3.762s,  272.22/s  (2.308s,  443.64/s)  LR: 2.784e-05  Data: 2.829 (1.697)
Train: 192 [ 150/1251 ( 12%)]  Loss:  3.710536 (4.2699)  Time: 0.587s, 1743.98/s  (2.380s,  430.19/s)  LR: 2.784e-05  Data: 0.024 (1.770)
Train: 192 [ 200/1251 ( 16%)]  Loss:  4.854259 (4.3868)  Time: 0.587s, 1744.48/s  (2.432s,  421.01/s)  LR: 2.784e-05  Data: 0.022 (1.829)
Train: 192 [ 250/1251 ( 20%)]  Loss:  4.793570 (4.4546)  Time: 0.586s, 1746.11/s  (2.408s,  425.19/s)  LR: 2.784e-05  Data: 0.023 (1.808)
Train: 192 [ 300/1251 ( 24%)]  Loss:  4.531711 (4.4656)  Time: 0.584s, 1752.83/s  (2.414s,  424.11/s)  LR: 2.784e-05  Data: 0.020 (1.816)
Train: 192 [ 350/1251 ( 28%)]  Loss:  4.184174 (4.4304)  Time: 0.585s, 1751.20/s  (2.392s,  428.03/s)  LR: 2.784e-05  Data: 0.022 (1.795)
Train: 192 [ 400/1251 ( 32%)]  Loss:  5.112367 (4.5062)  Time: 0.586s, 1746.28/s  (2.385s,  429.40/s)  LR: 2.784e-05  Data: 0.020 (1.789)
Train: 192 [ 450/1251 ( 36%)]  Loss:  4.130925 (4.4687)  Time: 0.588s, 1741.14/s  (2.357s,  434.42/s)  LR: 2.784e-05  Data: 0.024 (1.762)
Train: 192 [ 500/1251 ( 40%)]  Loss:  4.322913 (4.4554)  Time: 0.584s, 1752.13/s  (2.380s,  430.18/s)  LR: 2.784e-05  Data: 0.021 (1.787)
Train: 192 [ 550/1251 ( 44%)]  Loss:  4.138466 (4.4290)  Time: 0.589s, 1738.71/s  (2.375s,  431.07/s)  LR: 2.784e-05  Data: 0.024 (1.782)
Train: 192 [ 600/1251 ( 48%)]  Loss:  4.788383 (4.4567)  Time: 0.586s, 1746.31/s  (2.390s,  428.36/s)  LR: 2.784e-05  Data: 0.020 (1.797)
Train: 192 [ 650/1251 ( 52%)]  Loss:  4.063184 (4.4286)  Time: 0.584s, 1753.10/s  (2.399s,  426.92/s)  LR: 2.784e-05  Data: 0.020 (1.804)
Train: 192 [ 700/1251 ( 56%)]  Loss:  4.215404 (4.4143)  Time: 0.592s, 1729.37/s  (2.403s,  426.22/s)  LR: 2.784e-05  Data: 0.029 (1.807)
Train: 192 [ 750/1251 ( 60%)]  Loss:  4.536048 (4.4219)  Time: 0.587s, 1744.55/s  (2.403s,  426.18/s)  LR: 2.784e-05  Data: 0.021 (1.806)
Train: 192 [ 800/1251 ( 64%)]  Loss:  4.261592 (4.4125)  Time: 0.590s, 1735.21/s  (2.400s,  426.65/s)  LR: 2.784e-05  Data: 0.024 (1.802)
Train: 192 [ 850/1251 ( 68%)]  Loss:  4.301887 (4.4064)  Time: 0.586s, 1747.47/s  (2.390s,  428.45/s)  LR: 2.784e-05  Data: 0.021 (1.793)
Train: 192 [ 900/1251 ( 72%)]  Loss:  4.194608 (4.3952)  Time: 0.586s, 1746.32/s  (2.410s,  424.86/s)  LR: 2.784e-05  Data: 0.023 (1.813)
Train: 192 [ 950/1251 ( 76%)]  Loss:  4.635395 (4.4072)  Time: 0.585s, 1751.21/s  (2.411s,  424.79/s)  LR: 2.784e-05  Data: 0.020 (1.813)
Train: 192 [1000/1251 ( 80%)]  Loss:  4.713213 (4.4218)  Time: 0.587s, 1745.85/s  (2.414s,  424.22/s)  LR: 2.784e-05  Data: 0.018 (1.816)
Train: 192 [1050/1251 ( 84%)]  Loss:  4.106476 (4.4075)  Time: 0.585s, 1751.47/s  (2.407s,  425.50/s)  LR: 2.784e-05  Data: 0.020 (1.809)
Train: 192 [1100/1251 ( 88%)]  Loss:  4.295608 (4.4026)  Time: 0.586s, 1747.27/s  (2.404s,  426.01/s)  LR: 2.784e-05  Data: 0.019 (1.806)
Train: 192 [1150/1251 ( 92%)]  Loss:  4.931422 (4.4246)  Time: 0.589s, 1737.54/s  (2.397s,  427.25/s)  LR: 2.784e-05  Data: 0.023 (1.799)
Train: 192 [1200/1251 ( 96%)]  Loss:  4.995110 (4.4475)  Time: 0.587s, 1745.73/s  (2.394s,  427.77/s)  LR: 2.784e-05  Data: 0.024 (1.797)
Train: 192 [1250/1251 (100%)]  Loss:  4.554426 (4.4516)  Time: 0.564s, 1815.58/s  (2.398s,  426.96/s)  LR: 2.784e-05  Data: 0.000 (1.801)
Test: [   0/48]  Time: 14.295 (14.295)  Loss:  1.0080 (1.0080)  Acc@1: 78.8086 (78.8086)  Acc@5: 93.4570 (93.4570)
Test: [  48/48]  Time: 0.149 (3.426)  Loss:  1.0460 (1.8632)  Acc@1: 77.7123 (58.6580)  Acc@5: 91.7453 (81.9840)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-192.pth.tar', 58.65800001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-191.pth.tar', 58.44799998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-190.pth.tar', 58.38599991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-189.pth.tar', 58.192000014648436)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-188.pth.tar', 58.018000092773434)

Train: 193 [   0/1251 (  0%)]  Loss:  4.622831 (4.6228)  Time: 12.014s,   85.23/s  (12.014s,   85.23/s)  LR: 2.592e-05  Data: 11.028 (11.028)
Train: 193 [  50/1251 (  4%)]  Loss:  4.893466 (4.7581)  Time: 0.585s, 1749.88/s  (2.556s,  400.60/s)  LR: 2.592e-05  Data: 0.021 (1.950)
Train: 193 [ 100/1251 (  8%)]  Loss:  4.395596 (4.6373)  Time: 3.400s,  301.19/s  (2.508s,  408.23/s)  LR: 2.592e-05  Data: 2.676 (1.902)
Train: 193 [ 150/1251 ( 12%)]  Loss:  4.453973 (4.5915)  Time: 0.583s, 1755.65/s  (2.405s,  425.80/s)  LR: 2.592e-05  Data: 0.020 (1.796)
Train: 193 [ 200/1251 ( 16%)]  Loss:  4.633640 (4.5999)  Time: 2.080s,  492.19/s  (2.381s,  430.14/s)  LR: 2.592e-05  Data: 1.405 (1.771)
Train: 193 [ 250/1251 ( 20%)]  Loss:  4.582274 (4.5970)  Time: 0.587s, 1745.63/s  (2.340s,  437.67/s)  LR: 2.592e-05  Data: 0.019 (1.730)
Train: 193 [ 300/1251 ( 24%)]  Loss:  4.178832 (4.5372)  Time: 3.011s,  340.12/s  (2.383s,  429.63/s)  LR: 2.592e-05  Data: 2.348 (1.772)
Train: 193 [ 350/1251 ( 28%)]  Loss:  4.336124 (4.5121)  Time: 0.583s, 1755.96/s  (2.378s,  430.64/s)  LR: 2.592e-05  Data: 0.021 (1.768)
Train: 193 [ 400/1251 ( 32%)]  Loss:  4.770631 (4.5408)  Time: 3.507s,  291.95/s  (2.390s,  428.38/s)  LR: 2.592e-05  Data: 2.750 (1.779)
Train: 193 [ 450/1251 ( 36%)]  Loss:  4.793549 (4.5661)  Time: 0.585s, 1750.59/s  (2.377s,  430.82/s)  LR: 2.592e-05  Data: 0.020 (1.766)
Train: 193 [ 500/1251 ( 40%)]  Loss:  4.244920 (4.5369)  Time: 2.550s,  401.62/s  (2.382s,  429.98/s)  LR: 2.592e-05  Data: 1.848 (1.768)
Train: 193 [ 550/1251 ( 44%)]  Loss:  4.760405 (4.5555)  Time: 1.968s,  520.32/s  (2.367s,  432.58/s)  LR: 2.592e-05  Data: 1.222 (1.753)
Train: 193 [ 600/1251 ( 48%)]  Loss:  4.905854 (4.5825)  Time: 1.958s,  522.91/s  (2.369s,  432.21/s)  LR: 2.592e-05  Data: 1.396 (1.756)
Train: 193 [ 650/1251 ( 52%)]  Loss:  4.976933 (4.6106)  Time: 1.798s,  569.40/s  (2.370s,  432.13/s)  LR: 2.592e-05  Data: 1.236 (1.757)
Train: 193 [ 700/1251 ( 56%)]  Loss:  4.866261 (4.6277)  Time: 2.264s,  452.22/s  (2.389s,  428.69/s)  LR: 2.592e-05  Data: 1.591 (1.773)
Train: 193 [ 750/1251 ( 60%)]  Loss:  4.750881 (4.6354)  Time: 0.585s, 1749.21/s  (2.395s,  427.48/s)  LR: 2.592e-05  Data: 0.021 (1.780)
Train: 193 [ 800/1251 ( 64%)]  Loss:  4.624938 (4.6348)  Time: 5.000s,  204.78/s  (2.405s,  425.83/s)  LR: 2.592e-05  Data: 4.426 (1.791)
Train: 193 [ 850/1251 ( 68%)]  Loss:  4.572494 (4.6313)  Time: 0.821s, 1247.72/s  (2.404s,  426.04/s)  LR: 2.592e-05  Data: 0.125 (1.791)
Train: 193 [ 900/1251 ( 72%)]  Loss:  4.468230 (4.6227)  Time: 3.538s,  289.41/s  (2.407s,  425.43/s)  LR: 2.592e-05  Data: 2.950 (1.794)
Train: 193 [ 950/1251 ( 76%)]  Loss:  4.801957 (4.6317)  Time: 1.496s,  684.69/s  (2.398s,  426.99/s)  LR: 2.592e-05  Data: 0.851 (1.784)
Train: 193 [1000/1251 ( 80%)]  Loss:  3.955364 (4.5995)  Time: 2.641s,  387.73/s  (2.394s,  427.74/s)  LR: 2.592e-05  Data: 1.949 (1.780)
Train: 193 [1050/1251 ( 84%)]  Loss:  4.123720 (4.5779)  Time: 0.585s, 1750.09/s  (2.402s,  426.34/s)  LR: 2.592e-05  Data: 0.020 (1.788)
Train: 193 [1100/1251 ( 88%)]  Loss:  4.949507 (4.5940)  Time: 1.585s,  646.12/s  (2.404s,  425.96/s)  LR: 2.592e-05  Data: 1.015 (1.791)
Train: 193 [1150/1251 ( 92%)]  Loss:  4.397597 (4.5858)  Time: 1.472s,  695.69/s  (2.405s,  425.74/s)  LR: 2.592e-05  Data: 0.822 (1.790)
Train: 193 [1200/1251 ( 96%)]  Loss:  4.280828 (4.5736)  Time: 2.786s,  367.55/s  (2.406s,  425.57/s)  LR: 2.592e-05  Data: 2.213 (1.790)
Train: 193 [1250/1251 (100%)]  Loss:  4.142360 (4.5570)  Time: 0.565s, 1812.40/s  (2.402s,  426.24/s)  LR: 2.592e-05  Data: 0.000 (1.785)
Test: [   0/48]  Time: 14.391 (14.391)  Loss:  1.0395 (1.0395)  Acc@1: 77.8320 (77.8320)  Acc@5: 92.9688 (92.9688)
Test: [  48/48]  Time: 0.149 (3.297)  Loss:  1.0480 (1.8786)  Acc@1: 77.7123 (58.6320)  Acc@5: 91.9811 (82.0660)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-192.pth.tar', 58.65800001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-193.pth.tar', 58.63200001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-191.pth.tar', 58.44799998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-190.pth.tar', 58.38599991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-189.pth.tar', 58.192000014648436)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-188.pth.tar', 58.018000092773434)

Train: 194 [   0/1251 (  0%)]  Loss:  4.177585 (4.1776)  Time: 11.024s,   92.88/s  (11.024s,   92.88/s)  LR: 2.411e-05  Data: 10.060 (10.060)
Train: 194 [  50/1251 (  4%)]  Loss:  4.405370 (4.2915)  Time: 0.584s, 1754.46/s  (2.362s,  433.58/s)  LR: 2.411e-05  Data: 0.020 (1.770)
Train: 194 [ 100/1251 (  8%)]  Loss:  4.429262 (4.3374)  Time: 1.139s,  899.13/s  (2.447s,  418.52/s)  LR: 2.411e-05  Data: 0.379 (1.841)
Train: 194 [ 150/1251 ( 12%)]  Loss:  4.675632 (4.4220)  Time: 0.587s, 1744.80/s  (2.393s,  427.91/s)  LR: 2.411e-05  Data: 0.020 (1.792)
Train: 194 [ 200/1251 ( 16%)]  Loss:  4.826262 (4.5028)  Time: 0.586s, 1747.05/s  (2.396s,  427.31/s)  LR: 2.411e-05  Data: 0.021 (1.797)
Train: 194 [ 250/1251 ( 20%)]  Loss:  4.795754 (4.5516)  Time: 0.588s, 1742.73/s  (2.378s,  430.70/s)  LR: 2.411e-05  Data: 0.021 (1.781)
Train: 194 [ 300/1251 ( 24%)]  Loss:  4.879535 (4.5985)  Time: 0.585s, 1749.39/s  (2.378s,  430.60/s)  LR: 2.411e-05  Data: 0.019 (1.783)
Train: 194 [ 350/1251 ( 28%)]  Loss:  4.425842 (4.5769)  Time: 1.524s,  671.90/s  (2.360s,  433.96/s)  LR: 2.411e-05  Data: 0.840 (1.763)
Train: 194 [ 400/1251 ( 32%)]  Loss:  4.443061 (4.5620)  Time: 1.842s,  555.93/s  (2.353s,  435.17/s)  LR: 2.411e-05  Data: 1.202 (1.753)
Train: 194 [ 450/1251 ( 36%)]  Loss:  4.872879 (4.5931)  Time: 0.589s, 1738.82/s  (2.329s,  439.67/s)  LR: 2.411e-05  Data: 0.021 (1.729)
Train: 194 [ 500/1251 ( 40%)]  Loss:  4.375103 (4.5733)  Time: 1.926s,  531.71/s  (2.357s,  434.51/s)  LR: 2.411e-05  Data: 1.364 (1.754)
Train: 194 [ 550/1251 ( 44%)]  Loss:  4.064561 (4.5309)  Time: 0.586s, 1746.16/s  (2.368s,  432.48/s)  LR: 2.411e-05  Data: 0.019 (1.767)
Train: 194 [ 600/1251 ( 48%)]  Loss:  4.871181 (4.5571)  Time: 0.585s, 1751.01/s  (2.397s,  427.28/s)  LR: 2.411e-05  Data: 0.022 (1.797)
Train: 194 [ 650/1251 ( 52%)]  Loss:  4.309814 (4.5394)  Time: 0.587s, 1744.70/s  (2.405s,  425.80/s)  LR: 2.411e-05  Data: 0.021 (1.806)
Train: 194 [ 700/1251 ( 56%)]  Loss:  4.723186 (4.5517)  Time: 0.586s, 1746.13/s  (2.412s,  424.54/s)  LR: 2.411e-05  Data: 0.024 (1.814)
Train: 194 [ 750/1251 ( 60%)]  Loss:  3.746465 (4.5013)  Time: 0.582s, 1760.31/s  (2.403s,  426.21/s)  LR: 2.411e-05  Data: 0.020 (1.806)
Train: 194 [ 800/1251 ( 64%)]  Loss:  4.192259 (4.4832)  Time: 1.900s,  539.03/s  (2.406s,  425.62/s)  LR: 2.411e-05  Data: 1.247 (1.809)
Train: 194 [ 850/1251 ( 68%)]  Loss:  4.122390 (4.4631)  Time: 0.587s, 1743.33/s  (2.416s,  423.78/s)  LR: 2.411e-05  Data: 0.020 (1.819)
Train: 194 [ 900/1251 ( 72%)]  Loss:  4.941455 (4.4883)  Time: 2.049s,  499.68/s  (2.423s,  422.61/s)  LR: 2.411e-05  Data: 1.398 (1.826)
Train: 194 [ 950/1251 ( 76%)]  Loss:  4.606595 (4.4942)  Time: 0.586s, 1748.06/s  (2.419s,  423.30/s)  LR: 2.411e-05  Data: 0.023 (1.822)
Train: 194 [1000/1251 ( 80%)]  Loss:  4.708686 (4.5044)  Time: 0.585s, 1751.08/s  (2.422s,  422.83/s)  LR: 2.411e-05  Data: 0.021 (1.824)
Train: 194 [1050/1251 ( 84%)]  Loss:  4.190519 (4.4902)  Time: 0.586s, 1748.67/s  (2.417s,  423.63/s)  LR: 2.411e-05  Data: 0.018 (1.819)
Train: 194 [1100/1251 ( 88%)]  Loss:  4.271152 (4.4806)  Time: 1.430s,  716.21/s  (2.416s,  423.79/s)  LR: 2.411e-05  Data: 0.867 (1.818)
Train: 194 [1150/1251 ( 92%)]  Loss:  3.762492 (4.4507)  Time: 0.586s, 1746.61/s  (2.408s,  425.33/s)  LR: 2.411e-05  Data: 0.019 (1.810)
Train: 194 [1200/1251 ( 96%)]  Loss:  5.117457 (4.4774)  Time: 1.909s,  536.29/s  (2.404s,  425.98/s)  LR: 2.411e-05  Data: 1.328 (1.807)
Train: 194 [1250/1251 (100%)]  Loss:  4.786767 (4.4893)  Time: 0.563s, 1818.38/s  (2.410s,  424.97/s)  LR: 2.411e-05  Data: 0.000 (1.812)
Test: [   0/48]  Time: 15.568 (15.568)  Loss:  1.0305 (1.0305)  Acc@1: 78.4180 (78.4180)  Acc@5: 93.0664 (93.0664)
Test: [  48/48]  Time: 0.149 (3.569)  Loss:  1.0878 (1.8750)  Acc@1: 77.5943 (58.6880)  Acc@5: 91.7453 (81.9900)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-194.pth.tar', 58.68799996337891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-192.pth.tar', 58.65800001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-193.pth.tar', 58.63200001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-191.pth.tar', 58.44799998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-190.pth.tar', 58.38599991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-189.pth.tar', 58.192000014648436)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-188.pth.tar', 58.018000092773434)

Train: 195 [   0/1251 (  0%)]  Loss:  4.669272 (4.6693)  Time: 12.106s,   84.59/s  (12.106s,   84.59/s)  LR: 2.241e-05  Data: 11.212 (11.212)
Train: 195 [  50/1251 (  4%)]  Loss:  4.436946 (4.5531)  Time: 0.585s, 1751.53/s  (2.558s,  400.28/s)  LR: 2.241e-05  Data: 0.021 (1.929)
Train: 195 [ 100/1251 (  8%)]  Loss:  4.965331 (4.6905)  Time: 2.355s,  434.79/s  (2.462s,  415.93/s)  LR: 2.241e-05  Data: 1.693 (1.828)
Train: 195 [ 150/1251 ( 12%)]  Loss:  4.387365 (4.6147)  Time: 0.588s, 1740.48/s  (2.422s,  422.85/s)  LR: 2.241e-05  Data: 0.020 (1.794)
Train: 195 [ 200/1251 ( 16%)]  Loss:  4.748105 (4.6414)  Time: 1.924s,  532.34/s  (2.391s,  428.20/s)  LR: 2.241e-05  Data: 1.249 (1.772)
Train: 195 [ 250/1251 ( 20%)]  Loss:  4.220135 (4.5712)  Time: 0.585s, 1750.01/s  (2.363s,  433.39/s)  LR: 2.241e-05  Data: 0.020 (1.747)
Train: 195 [ 300/1251 ( 24%)]  Loss:  4.457029 (4.5549)  Time: 5.026s,  203.73/s  (2.413s,  424.39/s)  LR: 2.241e-05  Data: 4.445 (1.798)
Train: 195 [ 350/1251 ( 28%)]  Loss:  4.335586 (4.5275)  Time: 0.586s, 1747.10/s  (2.410s,  424.93/s)  LR: 2.241e-05  Data: 0.021 (1.798)
Train: 195 [ 400/1251 ( 32%)]  Loss:  4.607138 (4.5363)  Time: 7.945s,  128.89/s  (2.422s,  422.73/s)  LR: 2.241e-05  Data: 7.287 (1.813)
Train: 195 [ 450/1251 ( 36%)]  Loss:  4.886180 (4.5713)  Time: 1.041s,  983.21/s  (2.414s,  424.24/s)  LR: 2.241e-05  Data: 0.356 (1.806)
Train: 195 [ 500/1251 ( 40%)]  Loss:  3.966943 (4.5164)  Time: 5.764s,  177.67/s  (2.414s,  424.24/s)  LR: 2.241e-05  Data: 5.190 (1.807)
Train: 195 [ 550/1251 ( 44%)]  Loss:  4.076345 (4.4797)  Time: 1.025s,  998.98/s  (2.402s,  426.33/s)  LR: 2.241e-05  Data: 0.361 (1.795)
Train: 195 [ 600/1251 ( 48%)]  Loss:  4.261988 (4.4630)  Time: 5.196s,  197.08/s  (2.407s,  425.42/s)  LR: 2.241e-05  Data: 4.378 (1.800)
Train: 195 [ 650/1251 ( 52%)]  Loss:  4.456843 (4.4625)  Time: 1.145s,  894.38/s  (2.430s,  421.45/s)  LR: 2.241e-05  Data: 0.489 (1.821)
Train: 195 [ 700/1251 ( 56%)]  Loss:  4.811317 (4.4858)  Time: 0.590s, 1735.51/s  (2.442s,  419.39/s)  LR: 2.241e-05  Data: 0.022 (1.835)
Train: 195 [ 750/1251 ( 60%)]  Loss:  4.799860 (4.5054)  Time: 5.688s,  180.03/s  (2.446s,  418.68/s)  LR: 2.241e-05  Data: 5.126 (1.840)
Train: 195 [ 800/1251 ( 64%)]  Loss:  4.291391 (4.4928)  Time: 2.945s,  347.70/s  (2.451s,  417.81/s)  LR: 2.241e-05  Data: 2.290 (1.843)
Train: 195 [ 850/1251 ( 68%)]  Loss:  4.713931 (4.5051)  Time: 0.585s, 1750.15/s  (2.446s,  418.56/s)  LR: 2.241e-05  Data: 0.021 (1.838)
Train: 195 [ 900/1251 ( 72%)]  Loss:  4.531805 (4.5065)  Time: 0.587s, 1744.45/s  (2.449s,  418.16/s)  LR: 2.241e-05  Data: 0.019 (1.841)
Train: 195 [ 950/1251 ( 76%)]  Loss:  3.899356 (4.4761)  Time: 0.583s, 1755.73/s  (2.439s,  419.89/s)  LR: 2.241e-05  Data: 0.020 (1.832)
Train: 195 [1000/1251 ( 80%)]  Loss:  4.181912 (4.4621)  Time: 0.586s, 1748.30/s  (2.456s,  416.96/s)  LR: 2.241e-05  Data: 0.023 (1.851)
Train: 195 [1050/1251 ( 84%)]  Loss:  4.748990 (4.4752)  Time: 0.590s, 1734.96/s  (2.456s,  416.94/s)  LR: 2.241e-05  Data: 0.020 (1.851)
Train: 195 [1100/1251 ( 88%)]  Loss:  4.400437 (4.4719)  Time: 0.586s, 1748.49/s  (2.462s,  415.93/s)  LR: 2.241e-05  Data: 0.023 (1.859)
Train: 195 [1150/1251 ( 92%)]  Loss:  4.535798 (4.4746)  Time: 0.586s, 1748.06/s  (2.463s,  415.78/s)  LR: 2.241e-05  Data: 0.018 (1.860)
Train: 195 [1200/1251 ( 96%)]  Loss:  4.617111 (4.4803)  Time: 0.585s, 1751.56/s  (2.465s,  415.45/s)  LR: 2.241e-05  Data: 0.022 (1.863)
Train: 195 [1250/1251 (100%)]  Loss:  5.067697 (4.5029)  Time: 0.566s, 1808.59/s  (2.459s,  416.42/s)  LR: 2.241e-05  Data: 0.000 (1.858)
Test: [   0/48]  Time: 14.219 (14.219)  Loss:  1.0311 (1.0311)  Acc@1: 78.9062 (78.9062)  Acc@5: 93.1641 (93.1641)
Test: [  48/48]  Time: 0.150 (3.302)  Loss:  1.0770 (1.8788)  Acc@1: 77.3585 (58.6880)  Acc@5: 92.0991 (82.2260)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-195.pth.tar', 58.687999990234374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-194.pth.tar', 58.68799996337891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-192.pth.tar', 58.65800001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-193.pth.tar', 58.63200001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-191.pth.tar', 58.44799998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-190.pth.tar', 58.38599991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-189.pth.tar', 58.192000014648436)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-188.pth.tar', 58.018000092773434)

Train: 196 [   0/1251 (  0%)]  Loss:  4.544106 (4.5441)  Time: 10.982s,   93.24/s  (10.982s,   93.24/s)  LR: 2.082e-05  Data: 10.328 (10.328)
Train: 196 [  50/1251 (  4%)]  Loss:  4.956874 (4.7505)  Time: 0.588s, 1742.29/s  (2.711s,  377.72/s)  LR: 2.082e-05  Data: 0.020 (2.117)
Train: 196 [ 100/1251 (  8%)]  Loss:  4.674858 (4.7253)  Time: 0.589s, 1738.12/s  (2.602s,  393.57/s)  LR: 2.082e-05  Data: 0.027 (2.007)
Train: 196 [ 150/1251 ( 12%)]  Loss:  4.365401 (4.6353)  Time: 0.589s, 1739.99/s  (2.514s,  407.36/s)  LR: 2.082e-05  Data: 0.021 (1.918)
Train: 196 [ 200/1251 ( 16%)]  Loss:  3.890563 (4.4864)  Time: 0.585s, 1749.99/s  (2.495s,  410.46/s)  LR: 2.082e-05  Data: 0.019 (1.902)
Train: 196 [ 250/1251 ( 20%)]  Loss:  4.910513 (4.5571)  Time: 0.584s, 1752.62/s  (2.450s,  417.99/s)  LR: 2.082e-05  Data: 0.019 (1.859)
Train: 196 [ 300/1251 ( 24%)]  Loss:  4.872570 (4.6021)  Time: 0.585s, 1750.65/s  (2.438s,  420.06/s)  LR: 2.082e-05  Data: 0.020 (1.847)
Train: 196 [ 350/1251 ( 28%)]  Loss:  4.561511 (4.5970)  Time: 0.584s, 1754.41/s  (2.397s,  427.14/s)  LR: 2.082e-05  Data: 0.020 (1.808)
Train: 196 [ 400/1251 ( 32%)]  Loss:  4.332646 (4.5677)  Time: 0.586s, 1745.96/s  (2.387s,  428.99/s)  LR: 2.082e-05  Data: 0.022 (1.797)
Train: 196 [ 450/1251 ( 36%)]  Loss:  4.601188 (4.5710)  Time: 0.587s, 1745.57/s  (2.405s,  425.78/s)  LR: 2.082e-05  Data: 0.024 (1.814)
Train: 196 [ 500/1251 ( 40%)]  Loss:  5.125401 (4.6214)  Time: 0.585s, 1750.42/s  (2.416s,  423.81/s)  LR: 2.082e-05  Data: 0.020 (1.825)
Train: 196 [ 550/1251 ( 44%)]  Loss:  4.879731 (4.6429)  Time: 0.585s, 1749.12/s  (2.413s,  424.46/s)  LR: 2.082e-05  Data: 0.020 (1.820)
Train: 196 [ 600/1251 ( 48%)]  Loss:  3.905216 (4.5862)  Time: 0.587s, 1745.20/s  (2.428s,  421.67/s)  LR: 2.082e-05  Data: 0.019 (1.836)
Train: 196 [ 650/1251 ( 52%)]  Loss:  4.483413 (4.5789)  Time: 0.584s, 1752.02/s  (2.430s,  421.40/s)  LR: 2.082e-05  Data: 0.019 (1.838)
Train: 196 [ 700/1251 ( 56%)]  Loss:  4.299430 (4.5602)  Time: 0.586s, 1746.04/s  (2.431s,  421.22/s)  LR: 2.082e-05  Data: 0.020 (1.839)
Train: 196 [ 750/1251 ( 60%)]  Loss:  4.268106 (4.5420)  Time: 0.588s, 1740.39/s  (2.420s,  423.14/s)  LR: 2.082e-05  Data: 0.019 (1.828)
Train: 196 [ 800/1251 ( 64%)]  Loss:  4.904047 (4.5633)  Time: 0.587s, 1743.36/s  (2.436s,  420.31/s)  LR: 2.082e-05  Data: 0.020 (1.844)
Train: 196 [ 850/1251 ( 68%)]  Loss:  4.565503 (4.5634)  Time: 0.596s, 1717.31/s  (2.437s,  420.27/s)  LR: 2.082e-05  Data: 0.022 (1.845)
Train: 196 [ 900/1251 ( 72%)]  Loss:  4.548016 (4.5626)  Time: 0.589s, 1739.26/s  (2.439s,  419.88/s)  LR: 2.082e-05  Data: 0.020 (1.847)
Train: 196 [ 950/1251 ( 76%)]  Loss:  4.244594 (4.5467)  Time: 0.587s, 1744.16/s  (2.431s,  421.30/s)  LR: 2.082e-05  Data: 0.022 (1.839)
Train: 196 [1000/1251 ( 80%)]  Loss:  4.510756 (4.5450)  Time: 0.584s, 1754.66/s  (2.430s,  421.37/s)  LR: 2.082e-05  Data: 0.017 (1.837)
Train: 196 [1050/1251 ( 84%)]  Loss:  4.314067 (4.5345)  Time: 0.584s, 1752.32/s  (2.420s,  423.15/s)  LR: 2.082e-05  Data: 0.021 (1.827)
Train: 196 [1100/1251 ( 88%)]  Loss:  5.103968 (4.5592)  Time: 0.586s, 1748.85/s  (2.417s,  423.64/s)  LR: 2.082e-05  Data: 0.022 (1.824)
Train: 196 [1150/1251 ( 92%)]  Loss:  4.423320 (4.5536)  Time: 5.169s,  198.09/s  (2.413s,  424.34/s)  LR: 2.082e-05  Data: 4.527 (1.820)
Train: 196 [1200/1251 ( 96%)]  Loss:  4.907610 (4.5677)  Time: 0.586s, 1747.31/s  (2.415s,  423.96/s)  LR: 2.082e-05  Data: 0.023 (1.822)
Train: 196 [1250/1251 (100%)]  Loss:  4.445550 (4.5630)  Time: 0.563s, 1818.09/s  (2.415s,  424.03/s)  LR: 2.082e-05  Data: 0.000 (1.821)
Test: [   0/48]  Time: 14.080 (14.080)  Loss:  1.0244 (1.0244)  Acc@1: 79.3945 (79.3945)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.149 (3.390)  Loss:  1.0563 (1.8646)  Acc@1: 78.3019 (58.7560)  Acc@5: 91.9811 (82.3440)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-196.pth.tar', 58.75600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-195.pth.tar', 58.687999990234374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-194.pth.tar', 58.68799996337891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-192.pth.tar', 58.65800001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-193.pth.tar', 58.63200001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-191.pth.tar', 58.44799998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-190.pth.tar', 58.38599991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-189.pth.tar', 58.192000014648436)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-188.pth.tar', 58.018000092773434)

Train: 197 [   0/1251 (  0%)]  Loss:  4.602982 (4.6030)  Time: 11.174s,   91.64/s  (11.174s,   91.64/s)  LR: 1.933e-05  Data: 9.995 (9.995)
Train: 197 [  50/1251 (  4%)]  Loss:  4.318383 (4.4607)  Time: 0.604s, 1695.36/s  (2.368s,  432.36/s)  LR: 1.933e-05  Data: 0.042 (1.773)
Train: 197 [ 100/1251 (  8%)]  Loss:  4.004170 (4.3085)  Time: 2.691s,  380.51/s  (2.336s,  438.27/s)  LR: 1.933e-05  Data: 2.096 (1.737)
Train: 197 [ 150/1251 ( 12%)]  Loss:  4.469972 (4.3489)  Time: 0.590s, 1735.98/s  (2.278s,  449.59/s)  LR: 1.933e-05  Data: 0.022 (1.678)
Train: 197 [ 200/1251 ( 16%)]  Loss:  4.204671 (4.3200)  Time: 2.261s,  452.96/s  (2.289s,  447.37/s)  LR: 1.933e-05  Data: 1.659 (1.689)
Train: 197 [ 250/1251 ( 20%)]  Loss:  4.758266 (4.3931)  Time: 0.585s, 1749.77/s  (2.337s,  438.18/s)  LR: 1.933e-05  Data: 0.019 (1.738)
Train: 197 [ 300/1251 ( 24%)]  Loss:  4.213103 (4.3674)  Time: 5.133s,  199.49/s  (2.360s,  433.83/s)  LR: 1.933e-05  Data: 4.544 (1.762)
Train: 197 [ 350/1251 ( 28%)]  Loss:  4.927292 (4.4374)  Time: 0.586s, 1746.99/s  (2.341s,  437.40/s)  LR: 1.933e-05  Data: 0.019 (1.740)
Train: 197 [ 400/1251 ( 32%)]  Loss:  4.890728 (4.4877)  Time: 0.584s, 1753.82/s  (2.342s,  437.32/s)  LR: 1.933e-05  Data: 0.020 (1.741)
Train: 197 [ 450/1251 ( 36%)]  Loss:  4.147519 (4.4537)  Time: 0.583s, 1755.36/s  (2.330s,  439.53/s)  LR: 1.933e-05  Data: 0.021 (1.732)
Train: 197 [ 500/1251 ( 40%)]  Loss:  4.712166 (4.4772)  Time: 0.585s, 1750.99/s  (2.331s,  439.32/s)  LR: 1.933e-05  Data: 0.022 (1.733)
Train: 197 [ 550/1251 ( 44%)]  Loss:  4.017554 (4.4389)  Time: 0.691s, 1482.86/s  (2.316s,  442.05/s)  LR: 1.933e-05  Data: 0.053 (1.718)
Train: 197 [ 600/1251 ( 48%)]  Loss:  5.051827 (4.4860)  Time: 0.582s, 1758.97/s  (2.355s,  434.87/s)  LR: 1.933e-05  Data: 0.019 (1.756)
Train: 197 [ 650/1251 ( 52%)]  Loss:  4.708031 (4.5019)  Time: 0.586s, 1748.81/s  (2.355s,  434.76/s)  LR: 1.933e-05  Data: 0.020 (1.758)
Train: 197 [ 700/1251 ( 56%)]  Loss:  4.889729 (4.5278)  Time: 0.586s, 1747.85/s  (2.371s,  431.96/s)  LR: 1.933e-05  Data: 0.019 (1.774)
Train: 197 [ 750/1251 ( 60%)]  Loss:  4.272861 (4.5118)  Time: 0.585s, 1751.40/s  (2.370s,  432.00/s)  LR: 1.933e-05  Data: 0.019 (1.773)
Train: 197 [ 800/1251 ( 64%)]  Loss:  4.625785 (4.5185)  Time: 0.587s, 1743.14/s  (2.376s,  430.96/s)  LR: 1.933e-05  Data: 0.020 (1.780)
Train: 197 [ 850/1251 ( 68%)]  Loss:  4.608451 (4.5235)  Time: 0.970s, 1055.18/s  (2.368s,  432.36/s)  LR: 1.933e-05  Data: 0.339 (1.772)
Train: 197 [ 900/1251 ( 72%)]  Loss:  3.749852 (4.4828)  Time: 0.589s, 1739.89/s  (2.365s,  433.03/s)  LR: 1.933e-05  Data: 0.023 (1.769)
Train: 197 [ 950/1251 ( 76%)]  Loss:  4.413461 (4.4793)  Time: 0.584s, 1754.04/s  (2.354s,  435.06/s)  LR: 1.933e-05  Data: 0.021 (1.758)
Train: 197 [1000/1251 ( 80%)]  Loss:  4.215403 (4.4668)  Time: 0.586s, 1746.10/s  (2.370s,  432.00/s)  LR: 1.933e-05  Data: 0.020 (1.776)
Train: 197 [1050/1251 ( 84%)]  Loss:  4.522836 (4.4693)  Time: 0.582s, 1758.38/s  (2.363s,  433.28/s)  LR: 1.933e-05  Data: 0.020 (1.769)
Train: 197 [1100/1251 ( 88%)]  Loss:  4.641446 (4.4768)  Time: 0.587s, 1743.20/s  (2.364s,  433.13/s)  LR: 1.933e-05  Data: 0.021 (1.771)
Train: 197 [1150/1251 ( 92%)]  Loss:  4.650491 (4.4840)  Time: 0.585s, 1750.57/s  (2.361s,  433.80/s)  LR: 1.933e-05  Data: 0.022 (1.767)
Train: 197 [1200/1251 ( 96%)]  Loss:  4.403529 (4.4808)  Time: 0.587s, 1743.91/s  (2.362s,  433.60/s)  LR: 1.933e-05  Data: 0.020 (1.768)
Train: 197 [1250/1251 (100%)]  Loss:  4.406873 (4.4780)  Time: 0.563s, 1818.71/s  (2.353s,  435.26/s)  LR: 1.933e-05  Data: 0.000 (1.759)
Test: [   0/48]  Time: 13.535 (13.535)  Loss:  0.9816 (0.9816)  Acc@1: 79.3945 (79.3945)  Acc@5: 93.4570 (93.4570)
Test: [  48/48]  Time: 0.149 (3.240)  Loss:  1.0375 (1.8563)  Acc@1: 78.1840 (58.8200)  Acc@5: 91.9811 (82.3360)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-197.pth.tar', 58.82000009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-196.pth.tar', 58.75600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-195.pth.tar', 58.687999990234374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-194.pth.tar', 58.68799996337891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-192.pth.tar', 58.65800001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-193.pth.tar', 58.63200001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-191.pth.tar', 58.44799998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-190.pth.tar', 58.38599991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-189.pth.tar', 58.192000014648436)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-188.pth.tar', 58.018000092773434)

Train: 198 [   0/1251 (  0%)]  Loss:  4.564864 (4.5649)  Time: 10.858s,   94.31/s  (10.858s,   94.31/s)  LR: 1.795e-05  Data: 9.664 (9.664)
Train: 198 [  50/1251 (  4%)]  Loss:  4.460246 (4.5126)  Time: 0.590s, 1735.17/s  (2.625s,  390.04/s)  LR: 1.795e-05  Data: 0.021 (2.012)
Train: 198 [ 100/1251 (  8%)]  Loss:  4.635802 (4.5536)  Time: 2.008s,  509.97/s  (2.537s,  403.59/s)  LR: 1.795e-05  Data: 1.387 (1.923)
Train: 198 [ 150/1251 ( 12%)]  Loss:  3.869678 (4.3826)  Time: 0.588s, 1741.60/s  (2.425s,  422.31/s)  LR: 1.795e-05  Data: 0.023 (1.817)
Train: 198 [ 200/1251 ( 16%)]  Loss:  4.751894 (4.4565)  Time: 2.929s,  349.55/s  (2.403s,  426.17/s)  LR: 1.795e-05  Data: 2.356 (1.798)
Train: 198 [ 250/1251 ( 20%)]  Loss:  4.551965 (4.4724)  Time: 0.585s, 1749.06/s  (2.365s,  433.00/s)  LR: 1.795e-05  Data: 0.019 (1.761)
Train: 198 [ 300/1251 ( 24%)]  Loss:  4.920548 (4.5364)  Time: 3.245s,  315.58/s  (2.357s,  434.44/s)  LR: 1.795e-05  Data: 2.594 (1.750)
Train: 198 [ 350/1251 ( 28%)]  Loss:  3.942929 (4.4622)  Time: 0.586s, 1748.86/s  (2.333s,  438.88/s)  LR: 1.795e-05  Data: 0.019 (1.728)
Train: 198 [ 400/1251 ( 32%)]  Loss:  4.453678 (4.4613)  Time: 4.387s,  233.43/s  (2.323s,  440.78/s)  LR: 1.795e-05  Data: 3.704 (1.717)
Train: 198 [ 450/1251 ( 36%)]  Loss:  3.861247 (4.4013)  Time: 0.584s, 1753.96/s  (2.345s,  436.68/s)  LR: 1.795e-05  Data: 0.020 (1.741)
Train: 198 [ 500/1251 ( 40%)]  Loss:  4.693487 (4.4278)  Time: 2.570s,  398.41/s  (2.345s,  436.60/s)  LR: 1.795e-05  Data: 1.876 (1.741)
Train: 198 [ 550/1251 ( 44%)]  Loss:  4.332880 (4.4199)  Time: 0.589s, 1739.20/s  (2.344s,  436.77/s)  LR: 1.795e-05  Data: 0.024 (1.739)
Train: 198 [ 600/1251 ( 48%)]  Loss:  4.948192 (4.4606)  Time: 0.898s, 1140.19/s  (2.358s,  434.20/s)  LR: 1.795e-05  Data: 0.235 (1.753)
Train: 198 [ 650/1251 ( 52%)]  Loss:  4.618518 (4.4719)  Time: 0.590s, 1734.63/s  (2.394s,  427.78/s)  LR: 1.795e-05  Data: 0.025 (1.788)
Train: 198 [ 700/1251 ( 56%)]  Loss:  4.334465 (4.4627)  Time: 0.682s, 1502.26/s  (2.386s,  429.16/s)  LR: 1.795e-05  Data: 0.098 (1.781)
Train: 198 [ 750/1251 ( 60%)]  Loss:  4.558052 (4.4687)  Time: 0.584s, 1752.05/s  (2.385s,  429.26/s)  LR: 1.795e-05  Data: 0.019 (1.780)
Train: 198 [ 800/1251 ( 64%)]  Loss:  4.560597 (4.4741)  Time: 1.007s, 1016.98/s  (2.397s,  427.13/s)  LR: 1.795e-05  Data: 0.302 (1.793)
Train: 198 [ 850/1251 ( 68%)]  Loss:  4.845023 (4.4947)  Time: 0.587s, 1745.44/s  (2.394s,  427.82/s)  LR: 1.795e-05  Data: 0.020 (1.791)
Train: 198 [ 900/1251 ( 72%)]  Loss:  4.669390 (4.5039)  Time: 0.594s, 1723.07/s  (2.392s,  428.11/s)  LR: 1.795e-05  Data: 0.021 (1.790)
Train: 198 [ 950/1251 ( 76%)]  Loss:  4.158321 (4.4866)  Time: 0.588s, 1741.84/s  (2.396s,  427.46/s)  LR: 1.795e-05  Data: 0.025 (1.794)
Train: 198 [1000/1251 ( 80%)]  Loss:  4.520033 (4.4882)  Time: 3.377s,  303.21/s  (2.389s,  428.71/s)  LR: 1.795e-05  Data: 2.728 (1.786)
Train: 198 [1050/1251 ( 84%)]  Loss:  4.421923 (4.4852)  Time: 0.585s, 1750.89/s  (2.383s,  429.66/s)  LR: 1.795e-05  Data: 0.019 (1.781)
Train: 198 [1100/1251 ( 88%)]  Loss:  4.348483 (4.4792)  Time: 4.910s,  208.54/s  (2.380s,  430.18/s)  LR: 1.795e-05  Data: 4.062 (1.778)
Train: 198 [1150/1251 ( 92%)]  Loss:  4.166095 (4.4662)  Time: 0.586s, 1746.62/s  (2.374s,  431.42/s)  LR: 1.795e-05  Data: 0.020 (1.771)
Train: 198 [1200/1251 ( 96%)]  Loss:  4.599919 (4.4715)  Time: 5.562s,  184.09/s  (2.384s,  429.49/s)  LR: 1.795e-05  Data: 4.899 (1.781)
Train: 198 [1250/1251 (100%)]  Loss:  4.669174 (4.4791)  Time: 0.562s, 1821.15/s  (2.382s,  429.96/s)  LR: 1.795e-05  Data: 0.000 (1.779)
Test: [   0/48]  Time: 14.865 (14.865)  Loss:  1.0092 (1.0092)  Acc@1: 79.1992 (79.1992)  Acc@5: 93.1641 (93.1641)
Test: [  48/48]  Time: 0.149 (3.425)  Loss:  1.0609 (1.8689)  Acc@1: 77.4764 (58.7840)  Acc@5: 92.2170 (82.2180)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-197.pth.tar', 58.82000009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-198.pth.tar', 58.78400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-196.pth.tar', 58.75600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-195.pth.tar', 58.687999990234374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-194.pth.tar', 58.68799996337891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-192.pth.tar', 58.65800001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-193.pth.tar', 58.63200001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-191.pth.tar', 58.44799998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-190.pth.tar', 58.38599991210938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-189.pth.tar', 58.192000014648436)

Train: 199 [   0/1251 (  0%)]  Loss:  4.661397 (4.6614)  Time: 10.613s,   96.49/s  (10.613s,   96.49/s)  LR: 1.669e-05  Data: 9.679 (9.679)
Train: 199 [  50/1251 (  4%)]  Loss:  4.393533 (4.5275)  Time: 0.588s, 1741.49/s  (2.443s,  419.20/s)  LR: 1.669e-05  Data: 0.019 (1.826)
Train: 199 [ 100/1251 (  8%)]  Loss:  4.497206 (4.5174)  Time: 5.893s,  173.76/s  (2.373s,  431.58/s)  LR: 1.669e-05  Data: 5.265 (1.764)
Train: 199 [ 150/1251 ( 12%)]  Loss:  4.896559 (4.6122)  Time: 0.589s, 1739.51/s  (2.311s,  443.15/s)  LR: 1.669e-05  Data: 0.018 (1.704)
Train: 199 [ 200/1251 ( 16%)]  Loss:  4.246403 (4.5390)  Time: 0.593s, 1727.29/s  (2.335s,  438.55/s)  LR: 1.669e-05  Data: 0.023 (1.704)
Train: 199 [ 250/1251 ( 20%)]  Loss:  4.272558 (4.4946)  Time: 0.585s, 1751.33/s  (2.414s,  424.25/s)  LR: 1.669e-05  Data: 0.020 (1.793)
Train: 199 [ 300/1251 ( 24%)]  Loss:  3.880265 (4.4068)  Time: 0.591s, 1731.88/s  (2.422s,  422.81/s)  LR: 1.669e-05  Data: 0.025 (1.807)
Train: 199 [ 350/1251 ( 28%)]  Loss:  4.682230 (4.4413)  Time: 0.586s, 1746.88/s  (2.408s,  425.20/s)  LR: 1.669e-05  Data: 0.020 (1.796)
Train: 199 [ 400/1251 ( 32%)]  Loss:  4.643129 (4.4637)  Time: 0.588s, 1742.15/s  (2.404s,  425.90/s)  LR: 1.669e-05  Data: 0.021 (1.797)
Train: 199 [ 450/1251 ( 36%)]  Loss:  4.745880 (4.4919)  Time: 0.583s, 1755.31/s  (2.387s,  429.00/s)  LR: 1.669e-05  Data: 0.021 (1.781)
Train: 199 [ 500/1251 ( 40%)]  Loss:  4.590145 (4.5008)  Time: 0.583s, 1756.62/s  (2.384s,  429.61/s)  LR: 1.669e-05  Data: 0.019 (1.781)
Train: 199 [ 550/1251 ( 44%)]  Loss:  4.441057 (4.4959)  Time: 0.589s, 1739.51/s  (2.374s,  431.40/s)  LR: 1.669e-05  Data: 0.021 (1.772)
Train: 199 [ 600/1251 ( 48%)]  Loss:  4.303977 (4.4811)  Time: 0.588s, 1742.50/s  (2.402s,  426.31/s)  LR: 1.669e-05  Data: 0.022 (1.802)
Train: 199 [ 650/1251 ( 52%)]  Loss:  4.665954 (4.4943)  Time: 0.585s, 1750.63/s  (2.424s,  422.47/s)  LR: 1.669e-05  Data: 0.022 (1.825)
Train: 199 [ 700/1251 ( 56%)]  Loss:  4.311997 (4.4822)  Time: 0.589s, 1738.00/s  (2.448s,  418.36/s)  LR: 1.669e-05  Data: 0.021 (1.849)
Train: 199 [ 750/1251 ( 60%)]  Loss:  4.251050 (4.4677)  Time: 0.586s, 1746.97/s  (2.451s,  417.71/s)  LR: 1.669e-05  Data: 0.021 (1.854)
Train: 199 [ 800/1251 ( 64%)]  Loss:  4.342186 (4.4603)  Time: 0.583s, 1755.06/s  (2.466s,  415.18/s)  LR: 1.669e-05  Data: 0.018 (1.870)
Train: 199 [ 850/1251 ( 68%)]  Loss:  4.111201 (4.4409)  Time: 0.587s, 1743.97/s  (2.461s,  416.16/s)  LR: 1.669e-05  Data: 0.020 (1.864)
Train: 199 [ 900/1251 ( 72%)]  Loss:  3.897994 (4.4124)  Time: 0.586s, 1747.41/s  (2.460s,  416.30/s)  LR: 1.669e-05  Data: 0.019 (1.864)
Train: 199 [ 950/1251 ( 76%)]  Loss:  4.528415 (4.4182)  Time: 0.585s, 1749.01/s  (2.457s,  416.85/s)  LR: 1.669e-05  Data: 0.017 (1.861)
Train: 199 [1000/1251 ( 80%)]  Loss:  4.026877 (4.3995)  Time: 0.583s, 1756.43/s  (2.462s,  415.92/s)  LR: 1.669e-05  Data: 0.019 (1.867)
Train: 199 [1050/1251 ( 84%)]  Loss:  4.751360 (4.4155)  Time: 0.584s, 1753.26/s  (2.458s,  416.63/s)  LR: 1.669e-05  Data: 0.019 (1.863)
Train: 199 [1100/1251 ( 88%)]  Loss:  4.688755 (4.4274)  Time: 0.588s, 1740.73/s  (2.478s,  413.30/s)  LR: 1.669e-05  Data: 0.018 (1.883)
Train: 199 [1150/1251 ( 92%)]  Loss:  4.534044 (4.4318)  Time: 1.461s,  700.92/s  (2.504s,  409.01/s)  LR: 1.669e-05  Data: 0.874 (1.909)
Train: 199 [1200/1251 ( 96%)]  Loss:  4.491704 (4.4342)  Time: 1.038s,  986.54/s  (2.497s,  410.10/s)  LR: 1.669e-05  Data: 0.377 (1.902)
Train: 199 [1250/1251 (100%)]  Loss:  4.037838 (4.4190)  Time: 0.564s, 1814.00/s  (2.486s,  411.86/s)  LR: 1.669e-05  Data: 0.000 (1.891)
Test: [   0/48]  Time: 14.933 (14.933)  Loss:  1.0105 (1.0105)  Acc@1: 79.1992 (79.1992)  Acc@5: 93.0664 (93.0664)
Test: [  48/48]  Time: 0.149 (3.456)  Loss:  1.0498 (1.8660)  Acc@1: 77.5943 (58.8380)  Acc@5: 91.5094 (82.3360)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-199.pth.tar', 58.837999963378905)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-197.pth.tar', 58.82000009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-198.pth.tar', 58.78400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-196.pth.tar', 58.75600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-195.pth.tar', 58.687999990234374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-194.pth.tar', 58.68799996337891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-192.pth.tar', 58.65800001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-193.pth.tar', 58.63200001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-191.pth.tar', 58.44799998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-190.pth.tar', 58.38599991210938)

Train: 200 [   0/1251 (  0%)]  Loss:  4.637046 (4.6370)  Time: 12.294s,   83.29/s  (12.294s,   83.29/s)  LR: 1.553e-05  Data: 11.175 (11.175)
Train: 200 [  50/1251 (  4%)]  Loss:  4.192095 (4.4146)  Time: 0.585s, 1750.24/s  (2.463s,  415.71/s)  LR: 1.553e-05  Data: 0.018 (1.864)
Train: 200 [ 100/1251 (  8%)]  Loss:  4.582027 (4.4704)  Time: 0.588s, 1742.82/s  (2.440s,  419.74/s)  LR: 1.553e-05  Data: 0.022 (1.843)
Train: 200 [ 150/1251 ( 12%)]  Loss:  4.216414 (4.4069)  Time: 0.589s, 1737.31/s  (2.406s,  425.56/s)  LR: 1.553e-05  Data: 0.021 (1.800)
Train: 200 [ 200/1251 ( 16%)]  Loss:  4.828517 (4.4912)  Time: 1.301s,  786.98/s  (2.404s,  425.94/s)  LR: 1.553e-05  Data: 0.726 (1.805)
Train: 200 [ 250/1251 ( 20%)]  Loss:  4.581300 (4.5062)  Time: 0.586s, 1747.94/s  (2.378s,  430.61/s)  LR: 1.553e-05  Data: 0.022 (1.781)
Train: 200 [ 300/1251 ( 24%)]  Loss:  4.970257 (4.5725)  Time: 0.594s, 1724.88/s  (2.371s,  431.90/s)  LR: 1.553e-05  Data: 0.019 (1.776)
Train: 200 [ 350/1251 ( 28%)]  Loss:  4.617503 (4.5781)  Time: 0.586s, 1748.89/s  (2.346s,  436.50/s)  LR: 1.553e-05  Data: 0.021 (1.752)
Train: 200 [ 400/1251 ( 32%)]  Loss:  4.760074 (4.5984)  Time: 1.710s,  598.79/s  (2.406s,  425.59/s)  LR: 1.553e-05  Data: 1.147 (1.810)
Train: 200 [ 450/1251 ( 36%)]  Loss:  4.572036 (4.5957)  Time: 0.590s, 1734.82/s  (2.418s,  423.53/s)  LR: 1.553e-05  Data: 0.020 (1.821)
Train: 200 [ 500/1251 ( 40%)]  Loss:  4.497539 (4.5868)  Time: 0.583s, 1757.62/s  (2.438s,  420.09/s)  LR: 1.553e-05  Data: 0.020 (1.842)
Train: 200 [ 550/1251 ( 44%)]  Loss:  4.220843 (4.5563)  Time: 0.587s, 1744.25/s  (2.434s,  420.70/s)  LR: 1.553e-05  Data: 0.022 (1.840)
Train: 200 [ 600/1251 ( 48%)]  Loss:  4.658254 (4.5641)  Time: 0.589s, 1739.88/s  (2.477s,  413.37/s)  LR: 1.553e-05  Data: 0.019 (1.884)
Train: 200 [ 650/1251 ( 52%)]  Loss:  4.446476 (4.5557)  Time: 1.271s,  805.77/s  (2.477s,  413.34/s)  LR: 1.553e-05  Data: 0.706 (1.884)
Train: 200 [ 700/1251 ( 56%)]  Loss:  4.401247 (4.5454)  Time: 0.583s, 1756.74/s  (2.533s,  404.21/s)  LR: 1.553e-05  Data: 0.019 (1.940)
Train: 200 [ 750/1251 ( 60%)]  Loss:  4.797658 (4.5612)  Time: 3.841s,  266.62/s  (2.545s,  402.37/s)  LR: 1.553e-05  Data: 3.277 (1.951)
Train: 200 [ 800/1251 ( 64%)]  Loss:  4.072364 (4.5325)  Time: 0.584s, 1752.73/s  (2.544s,  402.56/s)  LR: 1.553e-05  Data: 0.020 (1.950)
Train: 200 [ 850/1251 ( 68%)]  Loss:  4.455029 (4.5281)  Time: 8.030s,  127.52/s  (2.545s,  402.38/s)  LR: 1.553e-05  Data: 7.356 (1.950)
Train: 200 [ 900/1251 ( 72%)]  Loss:  3.971024 (4.4988)  Time: 0.586s, 1747.24/s  (2.532s,  404.36/s)  LR: 1.553e-05  Data: 0.022 (1.938)
Train: 200 [ 950/1251 ( 76%)]  Loss:  4.438560 (4.4958)  Time: 6.824s,  150.06/s  (2.525s,  405.60/s)  LR: 1.553e-05  Data: 6.234 (1.930)
Train: 200 [1000/1251 ( 80%)]  Loss:  4.398141 (4.4912)  Time: 0.587s, 1744.19/s  (2.511s,  407.87/s)  LR: 1.553e-05  Data: 0.020 (1.916)
Train: 200 [1050/1251 ( 84%)]  Loss:  4.420521 (4.4880)  Time: 7.508s,  136.39/s  (2.502s,  409.20/s)  LR: 1.553e-05  Data: 6.945 (1.909)
Train: 200 [1100/1251 ( 88%)]  Loss:  4.700583 (4.4972)  Time: 0.584s, 1753.65/s  (2.505s,  408.86/s)  LR: 1.553e-05  Data: 0.020 (1.910)
Train: 200 [1150/1251 ( 92%)]  Loss:  4.501153 (4.4974)  Time: 6.785s,  150.93/s  (2.504s,  408.89/s)  LR: 1.553e-05  Data: 6.222 (1.911)
Train: 200 [1200/1251 ( 96%)]  Loss:  4.279456 (4.4886)  Time: 0.584s, 1753.70/s  (2.497s,  410.09/s)  LR: 1.553e-05  Data: 0.017 (1.904)
Train: 200 [1250/1251 (100%)]  Loss:  4.711236 (4.4972)  Time: 0.566s, 1809.86/s  (2.488s,  411.54/s)  LR: 1.553e-05  Data: 0.000 (1.895)
Test: [   0/48]  Time: 13.966 (13.966)  Loss:  1.0157 (1.0157)  Acc@1: 78.7109 (78.7109)  Acc@5: 93.1641 (93.1641)
Test: [  48/48]  Time: 0.152 (3.302)  Loss:  1.0616 (1.8575)  Acc@1: 77.9481 (59.0060)  Acc@5: 91.3915 (82.3320)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-200.pth.tar', 59.0060001171875)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-199.pth.tar', 58.837999963378905)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-197.pth.tar', 58.82000009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-198.pth.tar', 58.78400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-196.pth.tar', 58.75600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-195.pth.tar', 58.687999990234374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-194.pth.tar', 58.68799996337891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-192.pth.tar', 58.65800001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-193.pth.tar', 58.63200001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-191.pth.tar', 58.44799998779297)

Train: 201 [   0/1251 (  0%)]  Loss:  4.852775 (4.8528)  Time: 10.865s,   94.25/s  (10.865s,   94.25/s)  LR: 1.448e-05  Data: 10.209 (10.209)
Train: 201 [  50/1251 (  4%)]  Loss:  4.753616 (4.8032)  Time: 0.588s, 1740.39/s  (2.410s,  424.84/s)  LR: 1.448e-05  Data: 0.026 (1.819)
Train: 201 [ 100/1251 (  8%)]  Loss:  4.212664 (4.6064)  Time: 2.608s,  392.64/s  (2.350s,  435.66/s)  LR: 1.448e-05  Data: 2.011 (1.747)
Train: 201 [ 150/1251 ( 12%)]  Loss:  4.428401 (4.5619)  Time: 0.583s, 1755.70/s  (2.564s,  399.43/s)  LR: 1.448e-05  Data: 0.019 (1.960)
Train: 201 [ 200/1251 ( 16%)]  Loss:  4.140927 (4.4777)  Time: 6.346s,  161.36/s  (2.740s,  373.71/s)  LR: 1.448e-05  Data: 5.652 (2.123)
Train: 201 [ 250/1251 ( 20%)]  Loss:  4.159834 (4.4247)  Time: 0.587s, 1744.54/s  (2.652s,  386.10/s)  LR: 1.448e-05  Data: 0.022 (2.036)
Train: 201 [ 300/1251 ( 24%)]  Loss:  3.886087 (4.3478)  Time: 3.201s,  319.91/s  (2.605s,  393.14/s)  LR: 1.448e-05  Data: 2.527 (1.991)
Train: 201 [ 350/1251 ( 28%)]  Loss:  4.423651 (4.3572)  Time: 0.587s, 1743.79/s  (2.554s,  400.89/s)  LR: 1.448e-05  Data: 0.022 (1.940)
Train: 201 [ 400/1251 ( 32%)]  Loss:  4.937231 (4.4217)  Time: 0.931s, 1100.14/s  (2.526s,  405.44/s)  LR: 1.448e-05  Data: 0.366 (1.912)
Train: 201 [ 450/1251 ( 36%)]  Loss:  5.036869 (4.4832)  Time: 0.586s, 1746.59/s  (2.491s,  411.00/s)  LR: 1.448e-05  Data: 0.024 (1.880)
Train: 201 [ 500/1251 ( 40%)]  Loss:  4.572379 (4.4913)  Time: 0.585s, 1749.97/s  (2.511s,  407.78/s)  LR: 1.448e-05  Data: 0.020 (1.902)
Train: 201 [ 550/1251 ( 44%)]  Loss:  4.404786 (4.4841)  Time: 0.587s, 1743.65/s  (2.500s,  409.54/s)  LR: 1.448e-05  Data: 0.022 (1.892)
Train: 201 [ 600/1251 ( 48%)]  Loss:  4.045623 (4.4504)  Time: 2.916s,  351.17/s  (2.508s,  408.34/s)  LR: 1.448e-05  Data: 2.212 (1.900)
Train: 201 [ 650/1251 ( 52%)]  Loss:  4.613868 (4.4621)  Time: 0.586s, 1746.80/s  (2.505s,  408.77/s)  LR: 1.448e-05  Data: 0.023 (1.898)
Train: 201 [ 700/1251 ( 56%)]  Loss:  4.102556 (4.4381)  Time: 4.023s,  254.51/s  (2.505s,  408.83/s)  LR: 1.448e-05  Data: 3.461 (1.898)
Train: 201 [ 750/1251 ( 60%)]  Loss:  4.972294 (4.4715)  Time: 0.590s, 1734.97/s  (2.491s,  411.09/s)  LR: 1.448e-05  Data: 0.022 (1.884)
Train: 201 [ 800/1251 ( 64%)]  Loss:  4.574268 (4.4775)  Time: 5.437s,  188.33/s  (2.481s,  412.72/s)  LR: 1.448e-05  Data: 4.783 (1.875)
Train: 201 [ 850/1251 ( 68%)]  Loss:  4.677925 (4.4887)  Time: 0.588s, 1742.89/s  (2.486s,  411.92/s)  LR: 1.448e-05  Data: 0.024 (1.880)
Train: 201 [ 900/1251 ( 72%)]  Loss:  4.488102 (4.4886)  Time: 8.732s,  117.27/s  (2.480s,  412.89/s)  LR: 1.448e-05  Data: 8.170 (1.875)
Train: 201 [ 950/1251 ( 76%)]  Loss:  5.150857 (4.5217)  Time: 0.589s, 1738.44/s  (2.477s,  413.33/s)  LR: 1.448e-05  Data: 0.022 (1.872)
Train: 201 [1000/1251 ( 80%)]  Loss:  4.442880 (4.5180)  Time: 7.117s,  143.88/s  (2.498s,  409.95/s)  LR: 1.448e-05  Data: 6.427 (1.890)
Train: 201 [1050/1251 ( 84%)]  Loss:  4.653563 (4.5241)  Time: 0.588s, 1742.47/s  (2.519s,  406.45/s)  LR: 1.448e-05  Data: 0.020 (1.911)
Train: 201 [1100/1251 ( 88%)]  Loss:  4.889455 (4.5400)  Time: 6.396s,  160.10/s  (2.515s,  407.09/s)  LR: 1.448e-05  Data: 5.833 (1.908)
Train: 201 [1150/1251 ( 92%)]  Loss:  4.625367 (4.5436)  Time: 0.587s, 1745.62/s  (2.503s,  409.03/s)  LR: 1.448e-05  Data: 0.019 (1.896)
Train: 201 [1200/1251 ( 96%)]  Loss:  4.958938 (4.5602)  Time: 4.098s,  249.88/s  (2.507s,  408.50/s)  LR: 1.448e-05  Data: 3.519 (1.899)
Train: 201 [1250/1251 (100%)]  Loss:  4.741641 (4.5672)  Time: 0.566s, 1810.34/s  (2.501s,  409.47/s)  LR: 1.448e-05  Data: 0.000 (1.894)
Test: [   0/48]  Time: 15.774 (15.774)  Loss:  0.9977 (0.9977)  Acc@1: 79.0039 (79.0039)  Acc@5: 93.5547 (93.5547)
Test: [  48/48]  Time: 0.149 (3.490)  Loss:  1.0389 (1.8458)  Acc@1: 77.8302 (59.0900)  Acc@5: 91.6274 (82.4720)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-201.pth.tar', 59.08999993652344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-200.pth.tar', 59.0060001171875)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-199.pth.tar', 58.837999963378905)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-197.pth.tar', 58.82000009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-198.pth.tar', 58.78400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-196.pth.tar', 58.75600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-195.pth.tar', 58.687999990234374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-194.pth.tar', 58.68799996337891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-192.pth.tar', 58.65800001464844)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-193.pth.tar', 58.63200001464844)

Train: 202 [   0/1251 (  0%)]  Loss:  4.644999 (4.6450)  Time: 10.634s,   96.29/s  (10.634s,   96.29/s)  LR: 1.354e-05  Data: 9.693 (9.693)
Train: 202 [  50/1251 (  4%)]  Loss:  4.235993 (4.4405)  Time: 0.585s, 1749.44/s  (2.460s,  416.30/s)  LR: 1.354e-05  Data: 0.019 (1.871)
Train: 202 [ 100/1251 (  8%)]  Loss:  4.746196 (4.5424)  Time: 0.590s, 1734.16/s  (2.470s,  414.57/s)  LR: 1.354e-05  Data: 0.019 (1.875)
Train: 202 [ 150/1251 ( 12%)]  Loss:  4.500852 (4.5320)  Time: 0.584s, 1753.51/s  (2.393s,  427.93/s)  LR: 1.354e-05  Data: 0.020 (1.796)
Train: 202 [ 200/1251 ( 16%)]  Loss:  4.425936 (4.5108)  Time: 1.072s,  954.88/s  (2.368s,  432.50/s)  LR: 1.354e-05  Data: 0.499 (1.770)
Train: 202 [ 250/1251 ( 20%)]  Loss:  4.575835 (4.5216)  Time: 0.585s, 1750.39/s  (2.378s,  430.63/s)  LR: 1.354e-05  Data: 0.020 (1.783)
Train: 202 [ 300/1251 ( 24%)]  Loss:  4.795964 (4.5608)  Time: 1.926s,  531.71/s  (2.392s,  428.04/s)  LR: 1.354e-05  Data: 1.270 (1.798)
Train: 202 [ 350/1251 ( 28%)]  Loss:  4.364488 (4.5363)  Time: 0.586s, 1747.28/s  (2.386s,  429.19/s)  LR: 1.354e-05  Data: 0.021 (1.791)
Train: 202 [ 400/1251 ( 32%)]  Loss:  4.880796 (4.5746)  Time: 1.588s,  645.04/s  (2.387s,  428.91/s)  LR: 1.354e-05  Data: 0.992 (1.794)
Train: 202 [ 450/1251 ( 36%)]  Loss:  5.002619 (4.6174)  Time: 0.587s, 1744.25/s  (2.375s,  431.10/s)  LR: 1.354e-05  Data: 0.020 (1.783)
Train: 202 [ 500/1251 ( 40%)]  Loss:  4.738297 (4.6284)  Time: 1.165s,  878.80/s  (2.379s,  430.36/s)  LR: 1.354e-05  Data: 0.513 (1.786)
Train: 202 [ 550/1251 ( 44%)]  Loss:  4.920595 (4.6527)  Time: 0.588s, 1742.62/s  (2.395s,  427.61/s)  LR: 1.354e-05  Data: 0.025 (1.799)
Train: 202 [ 600/1251 ( 48%)]  Loss:  4.259842 (4.6225)  Time: 1.550s,  660.74/s  (2.475s,  413.75/s)  LR: 1.354e-05  Data: 0.961 (1.870)
Train: 202 [ 650/1251 ( 52%)]  Loss:  4.715769 (4.6292)  Time: 0.668s, 1533.69/s  (2.499s,  409.76/s)  LR: 1.354e-05  Data: 0.054 (1.893)
Train: 202 [ 700/1251 ( 56%)]  Loss:  4.750260 (4.6372)  Time: 4.547s,  225.20/s  (2.516s,  406.97/s)  LR: 1.354e-05  Data: 3.328 (1.909)
Train: 202 [ 750/1251 ( 60%)]  Loss:  4.362466 (4.6201)  Time: 0.582s, 1758.47/s  (2.503s,  409.07/s)  LR: 1.354e-05  Data: 0.019 (1.897)
Train: 202 [ 800/1251 ( 64%)]  Loss:  4.666171 (4.6228)  Time: 3.777s,  271.14/s  (2.501s,  409.51/s)  LR: 1.354e-05  Data: 3.199 (1.892)
Train: 202 [ 850/1251 ( 68%)]  Loss:  4.105671 (4.5940)  Time: 0.585s, 1751.37/s  (2.489s,  411.39/s)  LR: 1.354e-05  Data: 0.019 (1.880)
Train: 202 [ 900/1251 ( 72%)]  Loss:  4.140444 (4.5702)  Time: 1.116s,  917.62/s  (2.478s,  413.22/s)  LR: 1.354e-05  Data: 0.548 (1.869)
Train: 202 [ 950/1251 ( 76%)]  Loss:  4.900147 (4.5867)  Time: 0.587s, 1744.94/s  (2.464s,  415.52/s)  LR: 1.354e-05  Data: 0.019 (1.856)
Train: 202 [1000/1251 ( 80%)]  Loss:  4.406485 (4.5781)  Time: 1.811s,  565.36/s  (2.476s,  413.64/s)  LR: 1.354e-05  Data: 1.188 (1.867)
Train: 202 [1050/1251 ( 84%)]  Loss:  4.593213 (4.5788)  Time: 0.587s, 1744.53/s  (2.473s,  414.08/s)  LR: 1.354e-05  Data: 0.023 (1.864)
Train: 202 [1100/1251 ( 88%)]  Loss:  4.704474 (4.5842)  Time: 0.588s, 1741.88/s  (2.471s,  414.40/s)  LR: 1.354e-05  Data: 0.024 (1.863)
Train: 202 [1150/1251 ( 92%)]  Loss:  4.869130 (4.5961)  Time: 0.583s, 1755.09/s  (2.464s,  415.52/s)  LR: 1.354e-05  Data: 0.021 (1.857)
Train: 202 [1200/1251 ( 96%)]  Loss:  4.451669 (4.5903)  Time: 0.591s, 1734.09/s  (2.460s,  416.21/s)  LR: 1.354e-05  Data: 0.026 (1.854)
Train: 202 [1250/1251 (100%)]  Loss:  4.565996 (4.5894)  Time: 0.564s, 1816.62/s  (2.450s,  417.94/s)  LR: 1.354e-05  Data: 0.000 (1.844)
Test: [   0/48]  Time: 13.071 (13.071)  Loss:  0.9901 (0.9901)  Acc@1: 79.1992 (79.1992)  Acc@5: 93.5547 (93.5547)
Test: [  48/48]  Time: 0.149 (3.168)  Loss:  1.0235 (1.8392)  Acc@1: 79.1274 (59.1500)  Acc@5: 91.9811 (82.4400)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-202.pth.tar', 59.15000011230469)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-201.pth.tar', 59.08999993652344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-200.pth.tar', 59.0060001171875)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-199.pth.tar', 58.837999963378905)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-197.pth.tar', 58.82000009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-198.pth.tar', 58.78400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-196.pth.tar', 58.75600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-195.pth.tar', 58.687999990234374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-194.pth.tar', 58.68799996337891)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-192.pth.tar', 58.65800001464844)

Train: 203 [   0/1251 (  0%)]  Loss:  4.529176 (4.5292)  Time: 10.360s,   98.85/s  (10.360s,   98.85/s)  LR: 1.271e-05  Data: 9.425 (9.425)
Train: 203 [  50/1251 (  4%)]  Loss:  4.828778 (4.6790)  Time: 0.584s, 1752.87/s  (2.613s,  391.83/s)  LR: 1.271e-05  Data: 0.021 (2.024)
Train: 203 [ 100/1251 (  8%)]  Loss:  4.467487 (4.6085)  Time: 0.587s, 1744.44/s  (2.719s,  376.60/s)  LR: 1.271e-05  Data: 0.025 (2.134)
Train: 203 [ 150/1251 ( 12%)]  Loss:  4.811204 (4.6592)  Time: 0.587s, 1743.68/s  (2.837s,  361.00/s)  LR: 1.271e-05  Data: 0.024 (2.240)
Train: 203 [ 200/1251 ( 16%)]  Loss:  4.926421 (4.7126)  Time: 0.586s, 1746.48/s  (2.712s,  377.53/s)  LR: 1.271e-05  Data: 0.021 (2.117)
Train: 203 [ 250/1251 ( 20%)]  Loss:  4.264509 (4.6379)  Time: 0.587s, 1743.03/s  (2.606s,  392.87/s)  LR: 1.271e-05  Data: 0.024 (2.010)
Train: 203 [ 300/1251 ( 24%)]  Loss:  4.578316 (4.6294)  Time: 0.586s, 1747.62/s  (2.558s,  400.25/s)  LR: 1.271e-05  Data: 0.017 (1.965)
Train: 203 [ 350/1251 ( 28%)]  Loss:  4.003178 (4.5511)  Time: 1.113s,  920.29/s  (2.497s,  410.09/s)  LR: 1.271e-05  Data: 0.510 (1.905)
Train: 203 [ 400/1251 ( 32%)]  Loss:  4.934814 (4.5938)  Time: 0.587s, 1744.99/s  (2.517s,  406.76/s)  LR: 1.271e-05  Data: 0.021 (1.922)
Train: 203 [ 450/1251 ( 36%)]  Loss:  3.963387 (4.5307)  Time: 2.862s,  357.83/s  (2.484s,  412.22/s)  LR: 1.271e-05  Data: 2.299 (1.888)
Train: 203 [ 500/1251 ( 40%)]  Loss:  4.154836 (4.4966)  Time: 0.583s, 1755.28/s  (2.480s,  412.90/s)  LR: 1.271e-05  Data: 0.020 (1.882)
Train: 203 [ 550/1251 ( 44%)]  Loss:  4.263964 (4.4772)  Time: 0.587s, 1745.92/s  (2.474s,  413.94/s)  LR: 1.271e-05  Data: 0.020 (1.875)
Train: 203 [ 600/1251 ( 48%)]  Loss:  4.732766 (4.4968)  Time: 0.585s, 1751.18/s  (2.478s,  413.29/s)  LR: 1.271e-05  Data: 0.021 (1.879)
Train: 203 [ 650/1251 ( 52%)]  Loss:  4.614353 (4.5052)  Time: 0.586s, 1747.36/s  (2.469s,  414.78/s)  LR: 1.271e-05  Data: 0.020 (1.872)
Train: 203 [ 700/1251 ( 56%)]  Loss:  4.364103 (4.4958)  Time: 0.585s, 1751.20/s  (2.460s,  416.29/s)  LR: 1.271e-05  Data: 0.019 (1.863)
Train: 203 [ 750/1251 ( 60%)]  Loss:  3.835917 (4.4546)  Time: 0.591s, 1733.29/s  (2.439s,  419.79/s)  LR: 1.271e-05  Data: 0.025 (1.843)
Train: 203 [ 800/1251 ( 64%)]  Loss:  4.457029 (4.4547)  Time: 0.585s, 1751.54/s  (2.454s,  417.34/s)  LR: 1.271e-05  Data: 0.020 (1.856)
Train: 203 [ 850/1251 ( 68%)]  Loss:  4.918368 (4.4805)  Time: 1.246s,  821.54/s  (2.449s,  418.11/s)  LR: 1.271e-05  Data: 0.551 (1.852)
Train: 203 [ 900/1251 ( 72%)]  Loss:  4.366529 (4.4745)  Time: 0.588s, 1741.55/s  (2.444s,  419.04/s)  LR: 1.271e-05  Data: 0.022 (1.847)
Train: 203 [ 950/1251 ( 76%)]  Loss:  4.947440 (4.4981)  Time: 4.871s,  210.20/s  (2.435s,  420.46/s)  LR: 1.271e-05  Data: 4.303 (1.839)
Train: 203 [1000/1251 ( 80%)]  Loss:  4.692761 (4.5074)  Time: 0.585s, 1751.65/s  (2.475s,  413.69/s)  LR: 1.271e-05  Data: 0.021 (1.856)
Train: 203 [1050/1251 ( 84%)]  Loss:  4.186259 (4.4928)  Time: 2.355s,  434.84/s  (2.465s,  415.44/s)  LR: 1.271e-05  Data: 1.773 (1.846)
Train: 203 [1100/1251 ( 88%)]  Loss:  4.303673 (4.4846)  Time: 0.590s, 1734.91/s  (2.456s,  416.95/s)  LR: 1.271e-05  Data: 0.023 (1.838)
Train: 203 [1150/1251 ( 92%)]  Loss:  4.683017 (4.4928)  Time: 0.586s, 1747.66/s  (2.462s,  415.99/s)  LR: 1.271e-05  Data: 0.019 (1.843)
Train: 203 [1200/1251 ( 96%)]  Loss:  4.420898 (4.4900)  Time: 1.356s,  754.97/s  (2.456s,  416.91/s)  LR: 1.271e-05  Data: 0.779 (1.839)
Train: 203 [1250/1251 (100%)]  Loss:  4.249577 (4.4807)  Time: 0.566s, 1808.14/s  (2.449s,  418.12/s)  LR: 1.271e-05  Data: 0.000 (1.831)
Test: [   0/48]  Time: 13.675 (13.675)  Loss:  1.0057 (1.0057)  Acc@1: 79.3945 (79.3945)  Acc@5: 93.3594 (93.3594)
Test: [  48/48]  Time: 0.150 (3.337)  Loss:  1.0193 (1.8502)  Acc@1: 78.8915 (59.0960)  Acc@5: 92.0991 (82.4400)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-202.pth.tar', 59.15000011230469)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-203.pth.tar', 59.096000009765625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-201.pth.tar', 59.08999993652344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-200.pth.tar', 59.0060001171875)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-199.pth.tar', 58.837999963378905)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-197.pth.tar', 58.82000009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-198.pth.tar', 58.78400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-196.pth.tar', 58.75600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-195.pth.tar', 58.687999990234374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-194.pth.tar', 58.68799996337891)

Train: 204 [   0/1251 (  0%)]  Loss:  3.781648 (3.7816)  Time: 10.914s,   93.82/s  (10.914s,   93.82/s)  LR: 1.199e-05  Data: 10.150 (10.150)
Train: 204 [  50/1251 (  4%)]  Loss:  4.020838 (3.9012)  Time: 0.590s, 1735.97/s  (2.391s,  428.26/s)  LR: 1.199e-05  Data: 0.026 (1.796)
Train: 204 [ 100/1251 (  8%)]  Loss:  4.480776 (4.0944)  Time: 1.669s,  613.44/s  (2.314s,  442.54/s)  LR: 1.199e-05  Data: 1.072 (1.719)
Train: 204 [ 150/1251 ( 12%)]  Loss:  4.615659 (4.2247)  Time: 2.915s,  351.28/s  (2.267s,  451.70/s)  LR: 1.199e-05  Data: 2.348 (1.668)
Train: 204 [ 200/1251 ( 16%)]  Loss:  4.159953 (4.2118)  Time: 0.591s, 1732.19/s  (2.351s,  435.55/s)  LR: 1.199e-05  Data: 0.021 (1.739)
Train: 204 [ 250/1251 ( 20%)]  Loss:  4.276758 (4.2226)  Time: 1.910s,  536.13/s  (2.318s,  441.80/s)  LR: 1.199e-05  Data: 1.228 (1.707)
Train: 204 [ 300/1251 ( 24%)]  Loss:  4.727097 (4.2947)  Time: 2.409s,  425.06/s  (2.313s,  442.73/s)  LR: 1.199e-05  Data: 1.847 (1.699)
Train: 204 [ 350/1251 ( 28%)]  Loss:  4.853660 (4.3645)  Time: 3.825s,  267.70/s  (2.311s,  443.19/s)  LR: 1.199e-05  Data: 3.157 (1.691)
Train: 204 [ 400/1251 ( 32%)]  Loss:  3.955199 (4.3191)  Time: 1.535s,  666.92/s  (2.297s,  445.78/s)  LR: 1.199e-05  Data: 0.969 (1.679)
Train: 204 [ 450/1251 ( 36%)]  Loss:  4.426335 (4.3298)  Time: 2.565s,  399.21/s  (2.295s,  446.09/s)  LR: 1.199e-05  Data: 2.002 (1.676)
Train: 204 [ 500/1251 ( 40%)]  Loss:  4.302297 (4.3273)  Time: 3.545s,  288.87/s  (2.285s,  448.09/s)  LR: 1.199e-05  Data: 2.860 (1.667)
Train: 204 [ 550/1251 ( 44%)]  Loss:  4.720954 (4.3601)  Time: 3.094s,  330.97/s  (2.283s,  448.45/s)  LR: 1.199e-05  Data: 2.419 (1.664)
Train: 204 [ 600/1251 ( 48%)]  Loss:  4.471581 (4.3687)  Time: 1.296s,  790.00/s  (2.403s,  426.11/s)  LR: 1.199e-05  Data: 0.641 (1.783)
Train: 204 [ 650/1251 ( 52%)]  Loss:  4.464340 (4.3755)  Time: 5.648s,  181.29/s  (2.423s,  422.68/s)  LR: 1.199e-05  Data: 4.959 (1.801)
Train: 204 [ 700/1251 ( 56%)]  Loss:  4.451406 (4.3806)  Time: 0.586s, 1747.83/s  (2.425s,  422.21/s)  LR: 1.199e-05  Data: 0.022 (1.806)
Train: 204 [ 750/1251 ( 60%)]  Loss:  4.308982 (4.3761)  Time: 4.945s,  207.09/s  (2.417s,  423.66/s)  LR: 1.199e-05  Data: 4.382 (1.799)
Train: 204 [ 800/1251 ( 64%)]  Loss:  4.234036 (4.3677)  Time: 3.072s,  333.37/s  (2.411s,  424.71/s)  LR: 1.199e-05  Data: 2.480 (1.792)
Train: 204 [ 850/1251 ( 68%)]  Loss:  4.571004 (4.3790)  Time: 0.586s, 1748.65/s  (2.396s,  427.36/s)  LR: 1.199e-05  Data: 0.022 (1.778)
Train: 204 [ 900/1251 ( 72%)]  Loss:  4.350821 (4.3775)  Time: 1.863s,  549.64/s  (2.394s,  427.77/s)  LR: 1.199e-05  Data: 1.300 (1.778)
Train: 204 [ 950/1251 ( 76%)]  Loss:  4.409364 (4.3791)  Time: 0.585s, 1749.26/s  (2.403s,  426.12/s)  LR: 1.199e-05  Data: 0.023 (1.787)
Train: 204 [1000/1251 ( 80%)]  Loss:  4.157128 (4.3686)  Time: 1.453s,  704.60/s  (2.394s,  427.73/s)  LR: 1.199e-05  Data: 0.888 (1.779)
Train: 204 [1050/1251 ( 84%)]  Loss:  4.864931 (4.3911)  Time: 0.587s, 1743.60/s  (2.392s,  428.12/s)  LR: 1.199e-05  Data: 0.021 (1.778)
Train: 204 [1100/1251 ( 88%)]  Loss:  4.621126 (4.4011)  Time: 0.661s, 1550.01/s  (2.387s,  429.08/s)  LR: 1.199e-05  Data: 0.097 (1.774)
Train: 204 [1150/1251 ( 92%)]  Loss:  4.123669 (4.3896)  Time: 0.586s, 1746.25/s  (2.379s,  430.49/s)  LR: 1.199e-05  Data: 0.024 (1.768)
Train: 204 [1200/1251 ( 96%)]  Loss:  4.552065 (4.3961)  Time: 1.467s,  698.10/s  (2.375s,  431.20/s)  LR: 1.199e-05  Data: 0.824 (1.764)
Train: 204 [1250/1251 (100%)]  Loss:  4.539184 (4.4016)  Time: 0.563s, 1817.22/s  (2.364s,  433.10/s)  LR: 1.199e-05  Data: 0.000 (1.754)
Test: [   0/48]  Time: 14.492 (14.492)  Loss:  0.9895 (0.9895)  Acc@1: 78.8086 (78.8086)  Acc@5: 93.2617 (93.2617)
Test: [  48/48]  Time: 0.149 (3.189)  Loss:  1.0359 (1.8379)  Acc@1: 78.5377 (59.2180)  Acc@5: 91.9811 (82.4280)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-204.pth.tar', 59.21799998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-202.pth.tar', 59.15000011230469)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-203.pth.tar', 59.096000009765625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-201.pth.tar', 59.08999993652344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-200.pth.tar', 59.0060001171875)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-199.pth.tar', 58.837999963378905)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-197.pth.tar', 58.82000009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-198.pth.tar', 58.78400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-196.pth.tar', 58.75600001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-195.pth.tar', 58.687999990234374)

Train: 205 [   0/1251 (  0%)]  Loss:  4.452913 (4.4529)  Time: 19.543s,   52.40/s  (19.543s,   52.40/s)  LR: 1.138e-05  Data: 18.223 (18.223)
Train: 205 [  50/1251 (  4%)]  Loss:  4.412880 (4.4329)  Time: 0.590s, 1735.41/s  (2.643s,  387.37/s)  LR: 1.138e-05  Data: 0.021 (2.035)
Train: 205 [ 100/1251 (  8%)]  Loss:  4.689427 (4.5184)  Time: 2.628s,  389.69/s  (2.454s,  417.32/s)  LR: 1.138e-05  Data: 2.065 (1.847)
Train: 205 [ 150/1251 ( 12%)]  Loss:  4.302773 (4.4645)  Time: 0.587s, 1744.16/s  (2.648s,  386.67/s)  LR: 1.138e-05  Data: 0.019 (2.028)
Train: 205 [ 200/1251 ( 16%)]  Loss:  4.919116 (4.5554)  Time: 0.617s, 1660.46/s  (2.668s,  383.85/s)  LR: 1.138e-05  Data: 0.021 (2.045)
Train: 205 [ 250/1251 ( 20%)]  Loss:  3.989504 (4.4611)  Time: 0.586s, 1746.26/s  (2.564s,  399.37/s)  LR: 1.138e-05  Data: 0.019 (1.941)
Train: 205 [ 300/1251 ( 24%)]  Loss:  4.547126 (4.4734)  Time: 3.665s,  279.42/s  (2.529s,  404.95/s)  LR: 1.138e-05  Data: 2.812 (1.906)
Train: 205 [ 350/1251 ( 28%)]  Loss:  4.418539 (4.4665)  Time: 0.589s, 1739.38/s  (2.481s,  412.74/s)  LR: 1.138e-05  Data: 0.020 (1.859)
Train: 205 [ 400/1251 ( 32%)]  Loss:  4.950930 (4.5204)  Time: 3.871s,  264.52/s  (2.506s,  408.59/s)  LR: 1.138e-05  Data: 3.162 (1.883)
Train: 205 [ 450/1251 ( 36%)]  Loss:  3.965216 (4.4648)  Time: 0.585s, 1749.43/s  (2.463s,  415.68/s)  LR: 1.138e-05  Data: 0.020 (1.842)
Train: 205 [ 500/1251 ( 40%)]  Loss:  4.342282 (4.4537)  Time: 3.332s,  307.31/s  (2.459s,  416.48/s)  LR: 1.138e-05  Data: 2.599 (1.839)
Train: 205 [ 550/1251 ( 44%)]  Loss:  4.716558 (4.4756)  Time: 0.583s, 1756.53/s  (2.440s,  419.72/s)  LR: 1.138e-05  Data: 0.020 (1.820)
Train: 205 [ 600/1251 ( 48%)]  Loss:  4.603324 (4.4854)  Time: 0.932s, 1098.21/s  (2.439s,  419.76/s)  LR: 1.138e-05  Data: 0.356 (1.819)
Train: 205 [ 650/1251 ( 52%)]  Loss:  4.206757 (4.4655)  Time: 0.592s, 1730.46/s  (2.445s,  418.76/s)  LR: 1.138e-05  Data: 0.021 (1.828)
Train: 205 [ 700/1251 ( 56%)]  Loss:  3.935545 (4.4302)  Time: 0.760s, 1347.88/s  (2.431s,  421.27/s)  LR: 1.138e-05  Data: 0.149 (1.816)
Train: 205 [ 750/1251 ( 60%)]  Loss:  4.373381 (4.4266)  Time: 0.585s, 1749.41/s  (2.439s,  419.79/s)  LR: 1.138e-05  Data: 0.021 (1.826)
Train: 205 [ 800/1251 ( 64%)]  Loss:  4.837715 (4.4508)  Time: 0.592s, 1729.45/s  (2.430s,  421.36/s)  LR: 1.138e-05  Data: 0.020 (1.818)
Train: 205 [ 850/1251 ( 68%)]  Loss:  4.452249 (4.4509)  Time: 0.588s, 1741.28/s  (2.423s,  422.63/s)  LR: 1.138e-05  Data: 0.018 (1.812)
Train: 205 [ 900/1251 ( 72%)]  Loss:  4.952000 (4.4773)  Time: 0.587s, 1743.81/s  (2.413s,  424.28/s)  LR: 1.138e-05  Data: 0.020 (1.804)
Train: 205 [ 950/1251 ( 76%)]  Loss:  4.444400 (4.4756)  Time: 0.586s, 1746.84/s  (2.410s,  424.98/s)  LR: 1.138e-05  Data: 0.019 (1.801)
Train: 205 [1000/1251 ( 80%)]  Loss:  5.026054 (4.5018)  Time: 0.585s, 1751.34/s  (2.399s,  426.91/s)  LR: 1.138e-05  Data: 0.021 (1.791)
Train: 205 [1050/1251 ( 84%)]  Loss:  4.161123 (4.4864)  Time: 0.586s, 1748.86/s  (2.437s,  420.27/s)  LR: 1.138e-05  Data: 0.021 (1.831)
Train: 205 [1100/1251 ( 88%)]  Loss:  4.125565 (4.4707)  Time: 0.591s, 1731.22/s  (2.428s,  421.71/s)  LR: 1.138e-05  Data: 0.022 (1.823)
Train: 205 [1150/1251 ( 92%)]  Loss:  4.332767 (4.4649)  Time: 0.590s, 1735.90/s  (2.444s,  418.92/s)  LR: 1.138e-05  Data: 0.023 (1.840)
Train: 205 [1200/1251 ( 96%)]  Loss:  4.127210 (4.4514)  Time: 0.587s, 1745.71/s  (2.431s,  421.30/s)  LR: 1.138e-05  Data: 0.023 (1.827)
Train: 205 [1250/1251 (100%)]  Loss:  4.924053 (4.4696)  Time: 0.564s, 1816.49/s  (2.430s,  421.42/s)  LR: 1.138e-05  Data: 0.000 (1.827)
Test: [   0/48]  Time: 13.216 (13.216)  Loss:  1.0075 (1.0075)  Acc@1: 78.7109 (78.7109)  Acc@5: 93.4570 (93.4570)
Test: [  48/48]  Time: 0.152 (3.286)  Loss:  1.0257 (1.8393)  Acc@1: 78.3019 (59.1900)  Acc@5: 91.8632 (82.6140)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-204.pth.tar', 59.21799998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-205.pth.tar', 59.19000001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-202.pth.tar', 59.15000011230469)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-203.pth.tar', 59.096000009765625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-201.pth.tar', 59.08999993652344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-200.pth.tar', 59.0060001171875)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-199.pth.tar', 58.837999963378905)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-197.pth.tar', 58.82000009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-198.pth.tar', 58.78400004150391)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-196.pth.tar', 58.75600001220703)

Train: 206 [   0/1251 (  0%)]  Loss:  4.810788 (4.8108)  Time: 11.447s,   89.46/s  (11.447s,   89.46/s)  LR: 1.089e-05  Data: 10.475 (10.475)
Train: 206 [  50/1251 (  4%)]  Loss:  4.768975 (4.7899)  Time: 0.584s, 1752.48/s  (2.333s,  438.99/s)  LR: 1.089e-05  Data: 0.021 (1.731)
Train: 206 [ 100/1251 (  8%)]  Loss:  5.034325 (4.8714)  Time: 1.115s,  918.25/s  (2.298s,  445.63/s)  LR: 1.089e-05  Data: 0.532 (1.696)
Train: 206 [ 150/1251 ( 12%)]  Loss:  4.900505 (4.8786)  Time: 0.585s, 1750.03/s  (2.248s,  455.43/s)  LR: 1.089e-05  Data: 0.021 (1.640)
Train: 206 [ 200/1251 ( 16%)]  Loss:  4.884873 (4.8799)  Time: 0.954s, 1073.36/s  (2.338s,  437.90/s)  LR: 1.089e-05  Data: 0.362 (1.730)
Train: 206 [ 250/1251 ( 20%)]  Loss:  4.255952 (4.7759)  Time: 0.590s, 1736.21/s  (2.289s,  447.36/s)  LR: 1.089e-05  Data: 0.025 (1.682)
Train: 206 [ 300/1251 ( 24%)]  Loss:  4.734992 (4.7701)  Time: 3.575s,  286.40/s  (2.306s,  443.98/s)  LR: 1.089e-05  Data: 2.999 (1.700)
Train: 206 [ 350/1251 ( 28%)]  Loss:  4.850725 (4.7801)  Time: 0.590s, 1735.63/s  (2.286s,  447.96/s)  LR: 1.089e-05  Data: 0.021 (1.681)
Train: 206 [ 400/1251 ( 32%)]  Loss:  4.521149 (4.7514)  Time: 1.350s,  758.24/s  (2.287s,  447.69/s)  LR: 1.089e-05  Data: 0.676 (1.681)
Train: 206 [ 450/1251 ( 36%)]  Loss:  4.770665 (4.7533)  Time: 0.586s, 1746.18/s  (2.273s,  450.45/s)  LR: 1.089e-05  Data: 0.024 (1.670)
Train: 206 [ 500/1251 ( 40%)]  Loss:  4.780320 (4.7558)  Time: 0.589s, 1739.15/s  (2.272s,  450.74/s)  LR: 1.089e-05  Data: 0.020 (1.666)
Train: 206 [ 550/1251 ( 44%)]  Loss:  4.348651 (4.7218)  Time: 0.587s, 1743.00/s  (2.261s,  452.90/s)  LR: 1.089e-05  Data: 0.021 (1.657)
Train: 206 [ 600/1251 ( 48%)]  Loss:  4.463265 (4.7019)  Time: 0.586s, 1746.79/s  (2.367s,  432.60/s)  LR: 1.089e-05  Data: 0.020 (1.765)
Train: 206 [ 650/1251 ( 52%)]  Loss:  3.587939 (4.6224)  Time: 0.589s, 1739.87/s  (2.408s,  425.17/s)  LR: 1.089e-05  Data: 0.018 (1.807)
Train: 206 [ 700/1251 ( 56%)]  Loss:  4.765637 (4.6319)  Time: 0.589s, 1739.53/s  (2.406s,  425.63/s)  LR: 1.089e-05  Data: 0.021 (1.806)
Train: 206 [ 750/1251 ( 60%)]  Loss:  4.345586 (4.6140)  Time: 0.584s, 1752.40/s  (2.401s,  426.51/s)  LR: 1.089e-05  Data: 0.019 (1.800)
Train: 206 [ 800/1251 ( 64%)]  Loss:  4.768073 (4.6231)  Time: 0.586s, 1748.33/s  (2.396s,  427.38/s)  LR: 1.089e-05  Data: 0.023 (1.797)
Train: 206 [ 850/1251 ( 68%)]  Loss:  5.010040 (4.6446)  Time: 0.585s, 1750.31/s  (2.387s,  428.96/s)  LR: 1.089e-05  Data: 0.020 (1.789)
Train: 206 [ 900/1251 ( 72%)]  Loss:  4.718110 (4.6485)  Time: 0.586s, 1747.23/s  (2.383s,  429.78/s)  LR: 1.089e-05  Data: 0.022 (1.785)
Train: 206 [ 950/1251 ( 76%)]  Loss:  4.313355 (4.6317)  Time: 0.585s, 1751.79/s  (2.393s,  427.87/s)  LR: 1.089e-05  Data: 0.022 (1.796)
Train: 206 [1000/1251 ( 80%)]  Loss:  4.087235 (4.6058)  Time: 0.590s, 1736.62/s  (2.385s,  429.34/s)  LR: 1.089e-05  Data: 0.023 (1.789)
Train: 206 [1050/1251 ( 84%)]  Loss:  4.308707 (4.5923)  Time: 0.586s, 1747.99/s  (2.384s,  429.54/s)  LR: 1.089e-05  Data: 0.020 (1.788)
Train: 206 [1100/1251 ( 88%)]  Loss:  4.958617 (4.6082)  Time: 0.587s, 1743.21/s  (2.382s,  429.83/s)  LR: 1.089e-05  Data: 0.024 (1.787)
Train: 206 [1150/1251 ( 92%)]  Loss:  4.693712 (4.6118)  Time: 0.585s, 1749.48/s  (2.374s,  431.26/s)  LR: 1.089e-05  Data: 0.020 (1.780)
Train: 206 [1200/1251 ( 96%)]  Loss:  4.559230 (4.6097)  Time: 0.591s, 1733.16/s  (2.366s,  432.75/s)  LR: 1.089e-05  Data: 0.023 (1.772)
Train: 206 [1250/1251 (100%)]  Loss:  4.869680 (4.6197)  Time: 0.563s, 1817.24/s  (2.353s,  435.15/s)  LR: 1.089e-05  Data: 0.000 (1.760)
Test: [   0/48]  Time: 13.802 (13.802)  Loss:  0.9848 (0.9848)  Acc@1: 78.5156 (78.5156)  Acc@5: 93.2617 (93.2617)
Test: [  48/48]  Time: 0.149 (3.105)  Loss:  1.0341 (1.8400)  Acc@1: 77.5943 (59.1080)  Acc@5: 92.3349 (82.5880)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-204.pth.tar', 59.21799998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-205.pth.tar', 59.19000001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-202.pth.tar', 59.15000011230469)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-206.pth.tar', 59.10800009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-203.pth.tar', 59.096000009765625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-201.pth.tar', 59.08999993652344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-200.pth.tar', 59.0060001171875)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-199.pth.tar', 58.837999963378905)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-197.pth.tar', 58.82000009033203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-198.pth.tar', 58.78400004150391)

Train: 207 [   0/1251 (  0%)]  Loss:  4.925424 (4.9254)  Time: 18.937s,   54.07/s  (18.937s,   54.07/s)  LR: 1.050e-05  Data: 17.333 (17.333)
Train: 207 [  50/1251 (  4%)]  Loss:  4.736429 (4.8309)  Time: 0.589s, 1739.12/s  (2.633s,  388.88/s)  LR: 1.050e-05  Data: 0.020 (2.022)
Train: 207 [ 100/1251 (  8%)]  Loss:  4.457186 (4.7063)  Time: 1.032s,  992.64/s  (2.430s,  421.48/s)  LR: 1.050e-05  Data: 0.337 (1.818)
Train: 207 [ 150/1251 ( 12%)]  Loss:  4.598520 (4.6794)  Time: 0.751s, 1363.07/s  (2.448s,  418.33/s)  LR: 1.050e-05  Data: 0.120 (1.827)
Train: 207 [ 200/1251 ( 16%)]  Loss:  4.219751 (4.5875)  Time: 0.584s, 1752.17/s  (2.631s,  389.20/s)  LR: 1.050e-05  Data: 0.021 (2.013)
Train: 207 [ 250/1251 ( 20%)]  Loss:  4.540681 (4.5797)  Time: 0.589s, 1737.42/s  (2.549s,  401.74/s)  LR: 1.050e-05  Data: 0.022 (1.938)
Train: 207 [ 300/1251 ( 24%)]  Loss:  3.876359 (4.4792)  Time: 0.583s, 1756.35/s  (2.480s,  412.84/s)  LR: 1.050e-05  Data: 0.019 (1.873)
Train: 207 [ 350/1251 ( 28%)]  Loss:  4.870184 (4.5281)  Time: 0.590s, 1734.28/s  (2.445s,  418.78/s)  LR: 1.050e-05  Data: 0.021 (1.839)
Train: 207 [ 400/1251 ( 32%)]  Loss:  4.830867 (4.5617)  Time: 0.583s, 1755.11/s  (2.464s,  415.55/s)  LR: 1.050e-05  Data: 0.019 (1.859)
Train: 207 [ 450/1251 ( 36%)]  Loss:  4.108502 (4.5164)  Time: 0.585s, 1750.43/s  (2.431s,  421.22/s)  LR: 1.050e-05  Data: 0.021 (1.825)
Train: 207 [ 500/1251 ( 40%)]  Loss:  4.018054 (4.4711)  Time: 0.586s, 1748.85/s  (2.430s,  421.48/s)  LR: 1.050e-05  Data: 0.020 (1.823)
Train: 207 [ 550/1251 ( 44%)]  Loss:  4.592175 (4.4812)  Time: 0.583s, 1755.37/s  (2.426s,  422.16/s)  LR: 1.050e-05  Data: 0.021 (1.820)
Train: 207 [ 600/1251 ( 48%)]  Loss:  5.015993 (4.5223)  Time: 0.585s, 1751.12/s  (2.411s,  424.66/s)  LR: 1.050e-05  Data: 0.021 (1.806)
Train: 207 [ 650/1251 ( 52%)]  Loss:  4.629892 (4.5300)  Time: 0.584s, 1753.85/s  (2.416s,  423.78/s)  LR: 1.050e-05  Data: 0.020 (1.813)
Train: 207 [ 700/1251 ( 56%)]  Loss:  5.098988 (4.5679)  Time: 0.584s, 1753.19/s  (2.403s,  426.11/s)  LR: 1.050e-05  Data: 0.020 (1.802)
Train: 207 [ 750/1251 ( 60%)]  Loss:  4.331249 (4.5531)  Time: 0.584s, 1754.12/s  (2.405s,  425.78/s)  LR: 1.050e-05  Data: 0.021 (1.804)
Train: 207 [ 800/1251 ( 64%)]  Loss:  4.859775 (4.5712)  Time: 0.589s, 1739.04/s  (2.406s,  425.61/s)  LR: 1.050e-05  Data: 0.024 (1.805)
Train: 207 [ 850/1251 ( 68%)]  Loss:  4.640539 (4.5750)  Time: 0.588s, 1741.87/s  (2.397s,  427.23/s)  LR: 1.050e-05  Data: 0.019 (1.797)
Train: 207 [ 900/1251 ( 72%)]  Loss:  4.239405 (4.5574)  Time: 0.587s, 1743.90/s  (2.390s,  428.38/s)  LR: 1.050e-05  Data: 0.024 (1.791)
Train: 207 [ 950/1251 ( 76%)]  Loss:  4.625073 (4.5608)  Time: 0.586s, 1747.00/s  (2.383s,  429.65/s)  LR: 1.050e-05  Data: 0.020 (1.785)
Train: 207 [1000/1251 ( 80%)]  Loss:  5.008404 (4.5821)  Time: 0.588s, 1741.42/s  (2.377s,  430.79/s)  LR: 1.050e-05  Data: 0.019 (1.779)
Train: 207 [1050/1251 ( 84%)]  Loss:  4.084804 (4.5595)  Time: 0.587s, 1743.62/s  (2.390s,  428.46/s)  LR: 1.050e-05  Data: 0.024 (1.792)
Train: 207 [1100/1251 ( 88%)]  Loss:  4.572272 (4.5600)  Time: 0.595s, 1721.86/s  (2.410s,  424.94/s)  LR: 1.050e-05  Data: 0.021 (1.811)
Train: 207 [1150/1251 ( 92%)]  Loss:  3.879440 (4.5317)  Time: 0.585s, 1749.05/s  (2.422s,  422.82/s)  LR: 1.050e-05  Data: 0.019 (1.822)
Train: 207 [1200/1251 ( 96%)]  Loss:  4.241576 (4.5201)  Time: 0.587s, 1745.58/s  (2.406s,  425.54/s)  LR: 1.050e-05  Data: 0.022 (1.807)
Train: 207 [1250/1251 (100%)]  Loss:  4.739076 (4.5285)  Time: 0.565s, 1813.71/s  (2.407s,  425.44/s)  LR: 1.050e-05  Data: 0.000 (1.809)
Test: [   0/48]  Time: 14.291 (14.291)  Loss:  1.0125 (1.0125)  Acc@1: 78.8086 (78.8086)  Acc@5: 93.1641 (93.1641)
Test: [  48/48]  Time: 0.149 (3.306)  Loss:  1.0517 (1.8442)  Acc@1: 77.9481 (59.2200)  Acc@5: 91.5094 (82.4800)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-207.pth.tar', 59.21999998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-204.pth.tar', 59.21799998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-205.pth.tar', 59.19000001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-202.pth.tar', 59.15000011230469)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-206.pth.tar', 59.10800009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-203.pth.tar', 59.096000009765625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-201.pth.tar', 59.08999993652344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-200.pth.tar', 59.0060001171875)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-199.pth.tar', 58.837999963378905)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-197.pth.tar', 58.82000009033203)

Train: 208 [   0/1251 (  0%)]  Loss:  4.326087 (4.3261)  Time: 11.510s,   88.97/s  (11.510s,   88.97/s)  LR: 1.022e-05  Data: 10.328 (10.328)
Train: 208 [  50/1251 (  4%)]  Loss:  4.623587 (4.4748)  Time: 0.584s, 1754.68/s  (2.342s,  437.32/s)  LR: 1.022e-05  Data: 0.021 (1.752)
Train: 208 [ 100/1251 (  8%)]  Loss:  4.421082 (4.4569)  Time: 0.585s, 1749.27/s  (2.289s,  447.27/s)  LR: 1.022e-05  Data: 0.022 (1.704)
Train: 208 [ 150/1251 ( 12%)]  Loss:  4.899509 (4.5676)  Time: 0.584s, 1753.12/s  (2.230s,  459.23/s)  LR: 1.022e-05  Data: 0.021 (1.647)
Train: 208 [ 200/1251 ( 16%)]  Loss:  4.943701 (4.6428)  Time: 0.586s, 1748.84/s  (2.305s,  444.20/s)  LR: 1.022e-05  Data: 0.018 (1.721)
Train: 208 [ 250/1251 ( 20%)]  Loss:  4.429093 (4.6072)  Time: 0.585s, 1749.49/s  (2.283s,  448.59/s)  LR: 1.022e-05  Data: 0.021 (1.698)
Train: 208 [ 300/1251 ( 24%)]  Loss:  4.269109 (4.5589)  Time: 0.584s, 1754.52/s  (2.287s,  447.68/s)  LR: 1.022e-05  Data: 0.020 (1.703)
Train: 208 [ 350/1251 ( 28%)]  Loss:  4.825350 (4.5922)  Time: 0.588s, 1740.98/s  (2.283s,  448.62/s)  LR: 1.022e-05  Data: 0.020 (1.698)
Train: 208 [ 400/1251 ( 32%)]  Loss:  4.570958 (4.5898)  Time: 0.583s, 1755.94/s  (2.282s,  448.80/s)  LR: 1.022e-05  Data: 0.020 (1.697)
Train: 208 [ 450/1251 ( 36%)]  Loss:  3.972713 (4.5281)  Time: 0.584s, 1754.49/s  (2.270s,  451.07/s)  LR: 1.022e-05  Data: 0.020 (1.685)
Train: 208 [ 500/1251 ( 40%)]  Loss:  4.503165 (4.5259)  Time: 0.586s, 1748.80/s  (2.271s,  450.95/s)  LR: 1.022e-05  Data: 0.019 (1.686)
Train: 208 [ 550/1251 ( 44%)]  Loss:  4.640984 (4.5354)  Time: 0.589s, 1739.71/s  (2.262s,  452.71/s)  LR: 1.022e-05  Data: 0.020 (1.676)
Train: 208 [ 600/1251 ( 48%)]  Loss:  4.804226 (4.5561)  Time: 0.588s, 1742.66/s  (2.322s,  441.06/s)  LR: 1.022e-05  Data: 0.021 (1.735)
Train: 208 [ 650/1251 ( 52%)]  Loss:  4.306926 (4.5383)  Time: 0.585s, 1751.43/s  (2.354s,  434.93/s)  LR: 1.022e-05  Data: 0.020 (1.767)
Train: 208 [ 700/1251 ( 56%)]  Loss:  4.569304 (4.5404)  Time: 0.588s, 1740.73/s  (2.412s,  424.59/s)  LR: 1.022e-05  Data: 0.022 (1.823)
Train: 208 [ 750/1251 ( 60%)]  Loss:  4.561818 (4.5417)  Time: 0.586s, 1748.72/s  (2.400s,  426.62/s)  LR: 1.022e-05  Data: 0.019 (1.811)
Train: 208 [ 800/1251 ( 64%)]  Loss:  4.909485 (4.5634)  Time: 0.586s, 1747.15/s  (2.394s,  427.66/s)  LR: 1.022e-05  Data: 0.019 (1.803)
Train: 208 [ 850/1251 ( 68%)]  Loss:  4.552187 (4.5627)  Time: 0.585s, 1751.04/s  (2.381s,  430.15/s)  LR: 1.022e-05  Data: 0.020 (1.789)
Train: 208 [ 900/1251 ( 72%)]  Loss:  3.951383 (4.5306)  Time: 0.585s, 1749.95/s  (2.373s,  431.46/s)  LR: 1.022e-05  Data: 0.019 (1.780)
Train: 208 [ 950/1251 ( 76%)]  Loss:  4.344562 (4.5213)  Time: 0.586s, 1746.75/s  (2.377s,  430.76/s)  LR: 1.022e-05  Data: 0.022 (1.784)
Train: 208 [1000/1251 ( 80%)]  Loss:  4.661479 (4.5279)  Time: 0.588s, 1742.60/s  (2.372s,  431.74/s)  LR: 1.022e-05  Data: 0.023 (1.778)
Train: 208 [1050/1251 ( 84%)]  Loss:  4.242064 (4.5149)  Time: 0.587s, 1745.41/s  (2.367s,  432.61/s)  LR: 1.022e-05  Data: 0.021 (1.772)
Train: 208 [1100/1251 ( 88%)]  Loss:  4.414667 (4.5106)  Time: 0.583s, 1756.42/s  (2.363s,  433.33/s)  LR: 1.022e-05  Data: 0.018 (1.768)
Train: 208 [1150/1251 ( 92%)]  Loss:  4.286739 (4.5013)  Time: 0.591s, 1733.12/s  (2.354s,  435.03/s)  LR: 1.022e-05  Data: 0.025 (1.759)
Train: 208 [1200/1251 ( 96%)]  Loss:  4.803578 (4.5134)  Time: 0.584s, 1754.21/s  (2.349s,  435.94/s)  LR: 1.022e-05  Data: 0.019 (1.754)
Train: 208 [1250/1251 (100%)]  Loss:  4.849358 (4.5263)  Time: 0.566s, 1809.04/s  (2.339s,  437.77/s)  LR: 1.022e-05  Data: 0.000 (1.744)
Test: [   0/48]  Time: 13.697 (13.697)  Loss:  0.9841 (0.9841)  Acc@1: 78.9062 (78.9062)  Acc@5: 93.5547 (93.5547)
Test: [  48/48]  Time: 0.149 (3.150)  Loss:  1.0364 (1.8379)  Acc@1: 78.4198 (59.3060)  Acc@5: 91.9811 (82.5560)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-208.pth.tar', 59.30599993408203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-207.pth.tar', 59.21999998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-204.pth.tar', 59.21799998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-205.pth.tar', 59.19000001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-202.pth.tar', 59.15000011230469)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-206.pth.tar', 59.10800009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-203.pth.tar', 59.096000009765625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-201.pth.tar', 59.08999993652344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-200.pth.tar', 59.0060001171875)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-199.pth.tar', 58.837999963378905)

Train: 209 [   0/1251 (  0%)]  Loss:  4.777935 (4.7779)  Time: 9.826s,  104.22/s  (9.826s,  104.22/s)  LR: 1.006e-05  Data: 9.165 (9.165)
Train: 209 [  50/1251 (  4%)]  Loss:  4.672323 (4.7251)  Time: 0.586s, 1746.06/s  (2.677s,  382.57/s)  LR: 1.006e-05  Data: 0.021 (2.075)
Train: 209 [ 100/1251 (  8%)]  Loss:  4.584249 (4.6782)  Time: 0.588s, 1741.96/s  (2.456s,  417.02/s)  LR: 1.006e-05  Data: 0.024 (1.853)
Train: 209 [ 150/1251 ( 12%)]  Loss:  4.590307 (4.6562)  Time: 0.585s, 1749.09/s  (2.397s,  427.19/s)  LR: 1.006e-05  Data: 0.023 (1.800)
Train: 209 [ 200/1251 ( 16%)]  Loss:  4.117893 (4.5485)  Time: 1.541s,  664.36/s  (2.547s,  402.02/s)  LR: 1.006e-05  Data: 0.879 (1.946)
Train: 209 [ 250/1251 ( 20%)]  Loss:  4.495168 (4.5396)  Time: 0.588s, 1741.17/s  (2.611s,  392.21/s)  LR: 1.006e-05  Data: 0.018 (2.008)
Train: 209 [ 300/1251 ( 24%)]  Loss:  4.553172 (4.5416)  Time: 3.435s,  298.15/s  (2.572s,  398.13/s)  LR: 1.006e-05  Data: 2.757 (1.968)
Train: 209 [ 350/1251 ( 28%)]  Loss:  4.413553 (4.5256)  Time: 0.589s, 1739.59/s  (2.512s,  407.58/s)  LR: 1.006e-05  Data: 0.024 (1.909)
Train: 209 [ 400/1251 ( 32%)]  Loss:  4.667212 (4.5413)  Time: 2.261s,  452.97/s  (2.543s,  402.60/s)  LR: 1.006e-05  Data: 1.592 (1.941)
Train: 209 [ 450/1251 ( 36%)]  Loss:  4.808818 (4.5681)  Time: 0.584s, 1752.38/s  (2.493s,  410.80/s)  LR: 1.006e-05  Data: 0.021 (1.891)
Train: 209 [ 500/1251 ( 40%)]  Loss:  4.250319 (4.5392)  Time: 2.593s,  394.93/s  (2.499s,  409.71/s)  LR: 1.006e-05  Data: 1.920 (1.899)
Train: 209 [ 550/1251 ( 44%)]  Loss:  4.686126 (4.5514)  Time: 0.584s, 1752.95/s  (2.489s,  411.47/s)  LR: 1.006e-05  Data: 0.019 (1.889)
Train: 209 [ 600/1251 ( 48%)]  Loss:  4.891600 (4.5776)  Time: 1.566s,  653.85/s  (2.488s,  411.58/s)  LR: 1.006e-05  Data: 0.889 (1.888)
Train: 209 [ 650/1251 ( 52%)]  Loss:  4.139234 (4.5463)  Time: 0.585s, 1751.38/s  (2.473s,  414.15/s)  LR: 1.006e-05  Data: 0.018 (1.872)
Train: 209 [ 700/1251 ( 56%)]  Loss:  4.382489 (4.5354)  Time: 3.761s,  272.27/s  (2.460s,  416.33/s)  LR: 1.006e-05  Data: 3.198 (1.858)
Train: 209 [ 750/1251 ( 60%)]  Loss:  4.426695 (4.5286)  Time: 0.587s, 1743.27/s  (2.441s,  419.57/s)  LR: 1.006e-05  Data: 0.023 (1.840)
Train: 209 [ 800/1251 ( 64%)]  Loss:  4.598019 (4.5327)  Time: 2.531s,  404.56/s  (2.458s,  416.58/s)  LR: 1.006e-05  Data: 1.968 (1.858)
Train: 209 [ 850/1251 ( 68%)]  Loss:  3.808670 (4.4924)  Time: 0.588s, 1740.96/s  (2.440s,  419.67/s)  LR: 1.006e-05  Data: 0.024 (1.839)
Train: 209 [ 900/1251 ( 72%)]  Loss:  3.833443 (4.4577)  Time: 0.584s, 1752.68/s  (2.431s,  421.28/s)  LR: 1.006e-05  Data: 0.020 (1.830)
Train: 209 [ 950/1251 ( 76%)]  Loss:  4.467269 (4.4582)  Time: 0.586s, 1748.60/s  (2.427s,  421.88/s)  LR: 1.006e-05  Data: 0.023 (1.826)
Train: 209 [1000/1251 ( 80%)]  Loss:  4.802515 (4.4746)  Time: 2.980s,  343.64/s  (2.420s,  423.19/s)  LR: 1.006e-05  Data: 2.394 (1.818)
Train: 209 [1050/1251 ( 84%)]  Loss:  4.734140 (4.4864)  Time: 0.584s, 1752.50/s  (2.407s,  425.40/s)  LR: 1.006e-05  Data: 0.021 (1.805)
Train: 209 [1100/1251 ( 88%)]  Loss:  4.469793 (4.4857)  Time: 0.587s, 1744.78/s  (2.423s,  422.65/s)  LR: 1.006e-05  Data: 0.024 (1.818)
Train: 209 [1150/1251 ( 92%)]  Loss:  4.653916 (4.4927)  Time: 0.586s, 1747.21/s  (2.461s,  416.13/s)  LR: 1.006e-05  Data: 0.021 (1.856)
Train: 209 [1200/1251 ( 96%)]  Loss:  4.706671 (4.5013)  Time: 1.175s,  871.33/s  (2.460s,  416.33/s)  LR: 1.006e-05  Data: 0.505 (1.854)
Train: 209 [1250/1251 (100%)]  Loss:  4.646462 (4.5068)  Time: 0.563s, 1817.75/s  (2.449s,  418.20/s)  LR: 1.006e-05  Data: 0.000 (1.844)
Test: [   0/48]  Time: 13.713 (13.713)  Loss:  0.9935 (0.9935)  Acc@1: 79.1016 (79.1016)  Acc@5: 93.3594 (93.3594)
Test: [  48/48]  Time: 0.149 (3.310)  Loss:  1.0299 (1.8367)  Acc@1: 79.1274 (59.2840)  Acc@5: 91.9811 (82.6060)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-208.pth.tar', 59.30599993408203)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-209.pth.tar', 59.28400011230469)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-207.pth.tar', 59.21999998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-204.pth.tar', 59.21799998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-205.pth.tar', 59.19000001220703)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-202.pth.tar', 59.15000011230469)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-206.pth.tar', 59.10800009277344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-203.pth.tar', 59.096000009765625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-201.pth.tar', 59.08999993652344)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-200.pth.tar', 59.0060001171875)

*** Best metric: 59.30599993408203 (epoch 208)

wandb: Waiting for W&B process to finish, PID 19111
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210606_135942-PreTraining_vit_deit_tiny_patch16_224_1k/logs/debug.log
wandb: Find internal logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210606_135942-PreTraining_vit_deit_tiny_patch16_224_1k/logs/debug-internal.log
wandb: Run summary:
wandb:    eval_top1 59.284
wandb:    eval_top5 82.606
wandb:   _timestamp 1623025393
wandb:   train_loss 4.50685
wandb:        _step 209
wandb:        epoch 209
wandb:     _runtime 660309
wandb:    eval_loss 1.83667
wandb: Run history:
wandb:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   train_loss ‚ñÇ‚ñÜ‚ñÇ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÜ‚ñá‚ñÑ‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñÖ‚ñÑ
wandb:    eval_loss ‚ñà‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ
wandb:    eval_top1 ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:    eval_top5 ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñà‚ñà
wandb:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced PreTraining_vit_deit_tiny_patch16_224_1k: https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_1k
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
--End--
Mon Jun 7 09:23:26 JST 2021

--Start--
Thu Jun 3 12:29:31 JST 2021
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 1
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
wandb: Currently logged in as: gyama_x (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
wandb: Tracking run with wandb version 0.10.27
wandb: Resuming run PreTraining_vit_deit_tiny_patch16_224_fake_1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gyama_x/pytorch-image-models
wandb: üöÄ View run at https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run data is saved locally in /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210603_123050-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run `wandb offline` to turn off syncing.

Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar' (epoch 143)
Using native Torch DistributedDataParallel.
Scheduled epochs: 166
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Train: 144 [   0/1171 (  0%)]  Loss:  2.829560 (2.8296)  Time: 16.611s,   61.65/s  (16.611s,   61.65/s)  LR: 5.229e-05  Data: 14.326 (14.326)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 144 [  50/1171 (  4%)]  Loss:  3.132512 (2.9810)  Time: 0.585s, 1749.44/s  (2.304s,  444.39/s)  LR: 5.229e-05  Data: 0.020 (1.690)
Train: 144 [ 100/1171 (  9%)]  Loss:  3.104427 (3.0222)  Time: 0.584s, 1754.41/s  (2.199s,  465.70/s)  LR: 5.229e-05  Data: 0.019 (1.590)
Train: 144 [ 150/1171 ( 13%)]  Loss:  2.478782 (2.8863)  Time: 0.587s, 1745.01/s  (2.126s,  481.66/s)  LR: 5.229e-05  Data: 0.020 (1.524)
Train: 144 [ 200/1171 ( 17%)]  Loss:  2.589476 (2.8270)  Time: 2.348s,  436.13/s  (2.124s,  482.08/s)  LR: 5.229e-05  Data: 1.785 (1.524)
Train: 144 [ 250/1171 ( 21%)]  Loss:  2.311714 (2.7411)  Time: 0.588s, 1741.57/s  (2.088s,  490.44/s)  LR: 5.229e-05  Data: 0.020 (1.488)
Train: 144 [ 300/1171 ( 26%)]  Loss:  3.006548 (2.7790)  Time: 5.111s,  200.36/s  (2.079s,  492.54/s)  LR: 5.229e-05  Data: 4.523 (1.480)
Train: 144 [ 350/1171 ( 30%)]  Loss:  2.526578 (2.7474)  Time: 0.587s, 1743.16/s  (2.051s,  499.20/s)  LR: 5.229e-05  Data: 0.024 (1.454)
Train: 144 [ 400/1171 ( 34%)]  Loss:  3.058934 (2.7821)  Time: 8.370s,  122.34/s  (2.086s,  490.88/s)  LR: 5.229e-05  Data: 7.793 (1.485)
Train: 144 [ 450/1171 ( 38%)]  Loss:  2.547497 (2.7586)  Time: 0.583s, 1755.72/s  (2.069s,  494.90/s)  LR: 5.229e-05  Data: 0.018 (1.469)
Train: 144 [ 500/1171 ( 43%)]  Loss:  2.671859 (2.7507)  Time: 7.170s,  142.82/s  (2.079s,  492.48/s)  LR: 5.229e-05  Data: 6.536 (1.480)
Train: 144 [ 550/1171 ( 47%)]  Loss:  2.705577 (2.7470)  Time: 0.586s, 1748.66/s  (2.068s,  495.07/s)  LR: 5.229e-05  Data: 0.020 (1.470)
Train: 144 [ 600/1171 ( 51%)]  Loss:  3.293288 (2.7890)  Time: 5.752s,  178.02/s  (2.070s,  494.79/s)  LR: 5.229e-05  Data: 5.098 (1.472)
Train: 144 [ 650/1171 ( 56%)]  Loss:  2.765706 (2.7873)  Time: 0.583s, 1755.95/s  (2.056s,  498.15/s)  LR: 5.229e-05  Data: 0.019 (1.459)
Train: 144 [ 700/1171 ( 60%)]  Loss:  2.958050 (2.7987)  Time: 6.792s,  150.76/s  (2.062s,  496.56/s)  LR: 5.229e-05  Data: 6.228 (1.467)
Train: 144 [ 750/1171 ( 64%)]  Loss:  3.207607 (2.8243)  Time: 0.583s, 1755.52/s  (2.066s,  495.71/s)  LR: 5.229e-05  Data: 0.020 (1.470)
Train: 144 [ 800/1171 ( 68%)]  Loss:  2.766838 (2.8209)  Time: 6.905s,  148.29/s  (2.075s,  493.41/s)  LR: 5.229e-05  Data: 6.330 (1.481)
Train: 144 [ 850/1171 ( 73%)]  Loss:  2.798823 (2.8197)  Time: 0.586s, 1748.37/s  (2.097s,  488.32/s)  LR: 5.229e-05  Data: 0.019 (1.504)
Train: 144 [ 900/1171 ( 77%)]  Loss:  3.136015 (2.8363)  Time: 9.509s,  107.69/s  (2.101s,  487.45/s)  LR: 5.229e-05  Data: 8.927 (1.507)
Train: 144 [ 950/1171 ( 81%)]  Loss:  2.611622 (2.8251)  Time: 0.583s, 1757.56/s  (2.110s,  485.38/s)  LR: 5.229e-05  Data: 0.018 (1.517)
Train: 144 [1000/1171 ( 85%)]  Loss:  2.415932 (2.8056)  Time: 7.306s,  140.16/s  (2.124s,  482.03/s)  LR: 5.229e-05  Data: 6.609 (1.532)
Train: 144 [1050/1171 ( 90%)]  Loss:  2.876172 (2.8088)  Time: 0.587s, 1744.33/s  (2.123s,  482.38/s)  LR: 5.229e-05  Data: 0.015 (1.530)
Train: 144 [1100/1171 ( 94%)]  Loss:  2.819599 (2.8093)  Time: 6.537s,  156.66/s  (2.127s,  481.40/s)  LR: 5.229e-05  Data: 5.950 (1.535)
Train: 144 [1150/1171 ( 98%)]  Loss:  2.454344 (2.7945)  Time: 0.586s, 1747.53/s  (2.123s,  482.32/s)  LR: 5.229e-05  Data: 0.023 (1.531)
Train: 144 [1170/1171 (100%)]  Loss:  2.884448 (2.7981)  Time: 0.563s, 1818.10/s  (2.124s,  482.16/s)  LR: 5.229e-05  Data: 0.000 (1.532)
Test: [   0/97]  Time: 12.800 (12.800)  Loss:  0.2789 (0.2789)  Acc@1: 96.5820 (96.5820)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.197 (3.067)  Loss:  0.4590 (0.3517)  Acc@1: 92.3828 (95.1057)  Acc@5: 98.3398 (98.9162)
Test: [  97/97]  Time: 0.541 (2.981)  Loss:  0.3274 (0.3650)  Acc@1: 94.1964 (94.5780)  Acc@5: 99.1071 (98.7650)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-144.pth.tar', 94.57800003662109)

Train: 145 [   0/1171 (  0%)]  Loss:  2.851218 (2.8512)  Time: 10.099s,  101.39/s  (10.099s,  101.39/s)  LR: 4.858e-05  Data: 9.236 (9.236)
Train: 145 [  50/1171 (  4%)]  Loss:  3.171960 (3.0116)  Time: 0.585s, 1749.78/s  (2.208s,  463.79/s)  LR: 4.858e-05  Data: 0.017 (1.609)
Train: 145 [ 100/1171 (  9%)]  Loss:  2.973062 (2.9987)  Time: 2.820s,  363.17/s  (2.169s,  472.05/s)  LR: 4.858e-05  Data: 2.184 (1.572)
Train: 145 [ 150/1171 ( 13%)]  Loss:  2.760999 (2.9393)  Time: 0.586s, 1746.14/s  (2.099s,  487.96/s)  LR: 4.858e-05  Data: 0.021 (1.498)
Train: 145 [ 200/1171 ( 17%)]  Loss:  2.705147 (2.8925)  Time: 3.080s,  332.43/s  (2.111s,  485.04/s)  LR: 4.858e-05  Data: 2.506 (1.513)
Train: 145 [ 250/1171 ( 21%)]  Loss:  2.557424 (2.8366)  Time: 0.586s, 1747.00/s  (2.083s,  491.51/s)  LR: 4.858e-05  Data: 0.023 (1.486)
Train: 145 [ 300/1171 ( 26%)]  Loss:  2.727164 (2.8210)  Time: 3.741s,  273.76/s  (2.077s,  493.13/s)  LR: 4.858e-05  Data: 2.720 (1.478)
Train: 145 [ 350/1171 ( 30%)]  Loss:  2.813906 (2.8201)  Time: 0.586s, 1747.51/s  (2.093s,  489.31/s)  LR: 4.858e-05  Data: 0.021 (1.493)
Train: 145 [ 400/1171 ( 34%)]  Loss:  3.204526 (2.8628)  Time: 5.773s,  177.39/s  (2.116s,  483.92/s)  LR: 4.858e-05  Data: 5.210 (1.518)
Train: 145 [ 450/1171 ( 38%)]  Loss:  2.696246 (2.8462)  Time: 0.586s, 1747.57/s  (2.115s,  484.25/s)  LR: 4.858e-05  Data: 0.019 (1.518)
Train: 145 [ 500/1171 ( 43%)]  Loss:  2.911738 (2.8521)  Time: 6.454s,  158.67/s  (2.128s,  481.31/s)  LR: 4.858e-05  Data: 5.796 (1.531)
Train: 145 [ 550/1171 ( 47%)]  Loss:  2.686153 (2.8383)  Time: 0.583s, 1757.83/s  (2.143s,  477.86/s)  LR: 4.858e-05  Data: 0.019 (1.547)
Train: 145 [ 600/1171 ( 51%)]  Loss:  3.009478 (2.8515)  Time: 1.810s,  565.79/s  (2.157s,  474.63/s)  LR: 4.858e-05  Data: 1.220 (1.561)
Train: 145 [ 650/1171 ( 56%)]  Loss:  2.983488 (2.8609)  Time: 0.585s, 1749.89/s  (2.159s,  474.33/s)  LR: 4.858e-05  Data: 0.022 (1.561)
Train: 145 [ 700/1171 ( 60%)]  Loss:  2.738207 (2.8527)  Time: 4.180s,  244.98/s  (2.164s,  473.27/s)  LR: 4.858e-05  Data: 3.597 (1.566)
Train: 145 [ 750/1171 ( 64%)]  Loss:  2.715787 (2.8442)  Time: 0.584s, 1752.42/s  (2.164s,  473.29/s)  LR: 4.858e-05  Data: 0.020 (1.565)
Train: 145 [ 800/1171 ( 68%)]  Loss:  2.746595 (2.8384)  Time: 5.806s,  176.38/s  (2.179s,  469.94/s)  LR: 4.858e-05  Data: 5.242 (1.581)
Train: 145 [ 850/1171 ( 73%)]  Loss:  2.844924 (2.8388)  Time: 0.591s, 1731.43/s  (2.179s,  469.86/s)  LR: 4.858e-05  Data: 0.021 (1.582)
Train: 145 [ 900/1171 ( 77%)]  Loss:  3.015319 (2.8481)  Time: 7.238s,  141.48/s  (2.179s,  469.86/s)  LR: 4.858e-05  Data: 6.675 (1.583)
Train: 145 [ 950/1171 ( 81%)]  Loss:  2.966767 (2.8540)  Time: 0.585s, 1751.53/s  (2.177s,  470.37/s)  LR: 4.858e-05  Data: 0.021 (1.581)
Train: 145 [1000/1171 ( 85%)]  Loss:  3.348829 (2.8776)  Time: 7.140s,  143.41/s  (2.176s,  470.59/s)  LR: 4.858e-05  Data: 6.380 (1.580)
Train: 145 [1050/1171 ( 90%)]  Loss:  3.344797 (2.8988)  Time: 0.587s, 1745.28/s  (2.168s,  472.24/s)  LR: 4.858e-05  Data: 0.021 (1.573)
Train: 145 [1100/1171 ( 94%)]  Loss:  3.002480 (2.9033)  Time: 6.986s,  146.58/s  (2.169s,  472.21/s)  LR: 4.858e-05  Data: 6.387 (1.574)
Train: 145 [1150/1171 ( 98%)]  Loss:  2.738332 (2.8964)  Time: 0.581s, 1761.92/s  (2.160s,  474.00/s)  LR: 4.858e-05  Data: 0.018 (1.567)
Train: 145 [1170/1171 (100%)]  Loss:  2.777712 (2.8917)  Time: 0.564s, 1816.62/s  (2.175s,  470.90/s)  LR: 4.858e-05  Data: 0.000 (1.581)
Test: [   0/97]  Time: 14.857 (14.857)  Loss:  0.2616 (0.2616)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (2.973)  Loss:  0.4047 (0.3275)  Acc@1: 92.7734 (95.0080)  Acc@5: 98.5352 (98.9411)
Test: [  97/97]  Time: 0.119 (2.900)  Loss:  0.2799 (0.3409)  Acc@1: 95.0893 (94.5710)  Acc@5: 99.2560 (98.7810)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-144.pth.tar', 94.57800003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-145.pth.tar', 94.57100000732422)

Train: 146 [   0/1171 (  0%)]  Loss:  2.807636 (2.8076)  Time: 11.214s,   91.32/s  (11.214s,   91.32/s)  LR: 4.504e-05  Data: 10.272 (10.272)
Train: 146 [  50/1171 (  4%)]  Loss:  2.657349 (2.7325)  Time: 0.581s, 1762.33/s  (2.204s,  464.59/s)  LR: 4.504e-05  Data: 0.019 (1.623)
Train: 146 [ 100/1171 (  9%)]  Loss:  3.363613 (2.9429)  Time: 0.583s, 1755.78/s  (2.127s,  481.45/s)  LR: 4.504e-05  Data: 0.020 (1.546)
Train: 146 [ 150/1171 ( 13%)]  Loss:  2.350075 (2.7947)  Time: 0.583s, 1756.18/s  (2.075s,  493.41/s)  LR: 4.504e-05  Data: 0.021 (1.492)
Train: 146 [ 200/1171 ( 17%)]  Loss:  2.595695 (2.7549)  Time: 0.584s, 1754.34/s  (2.094s,  489.08/s)  LR: 4.504e-05  Data: 0.020 (1.503)
Train: 146 [ 250/1171 ( 21%)]  Loss:  2.675915 (2.7417)  Time: 0.584s, 1752.18/s  (2.058s,  497.47/s)  LR: 4.504e-05  Data: 0.022 (1.469)
Train: 146 [ 300/1171 ( 26%)]  Loss:  2.479478 (2.7043)  Time: 1.209s,  846.91/s  (2.123s,  482.34/s)  LR: 4.504e-05  Data: 0.612 (1.531)
Train: 146 [ 350/1171 ( 30%)]  Loss:  2.737059 (2.7084)  Time: 2.436s,  420.44/s  (2.118s,  483.44/s)  LR: 4.504e-05  Data: 1.863 (1.522)
Train: 146 [ 400/1171 ( 34%)]  Loss:  2.890058 (2.7285)  Time: 2.792s,  366.72/s  (2.131s,  480.49/s)  LR: 4.504e-05  Data: 2.229 (1.537)
Train: 146 [ 450/1171 ( 38%)]  Loss:  2.764518 (2.7321)  Time: 0.588s, 1740.41/s  (2.119s,  483.19/s)  LR: 4.504e-05  Data: 0.024 (1.519)
Train: 146 [ 500/1171 ( 43%)]  Loss:  2.909965 (2.7483)  Time: 1.420s,  721.18/s  (2.124s,  482.06/s)  LR: 4.504e-05  Data: 0.761 (1.523)
Train: 146 [ 550/1171 ( 47%)]  Loss:  2.796741 (2.7523)  Time: 1.822s,  562.05/s  (2.124s,  482.08/s)  LR: 4.504e-05  Data: 1.157 (1.523)
Train: 146 [ 600/1171 ( 51%)]  Loss:  2.670622 (2.7461)  Time: 3.232s,  316.82/s  (2.136s,  479.40/s)  LR: 4.504e-05  Data: 2.545 (1.533)
Train: 146 [ 650/1171 ( 56%)]  Loss:  2.949991 (2.7606)  Time: 3.598s,  284.61/s  (2.134s,  479.82/s)  LR: 4.504e-05  Data: 2.577 (1.530)
Train: 146 [ 700/1171 ( 60%)]  Loss:  2.994673 (2.7762)  Time: 3.538s,  289.40/s  (2.147s,  476.86/s)  LR: 4.504e-05  Data: 2.856 (1.542)
Train: 146 [ 750/1171 ( 64%)]  Loss:  2.883160 (2.7829)  Time: 4.183s,  244.79/s  (2.153s,  475.71/s)  LR: 4.504e-05  Data: 3.576 (1.546)
Train: 146 [ 800/1171 ( 68%)]  Loss:  2.892867 (2.7894)  Time: 1.968s,  520.24/s  (2.154s,  475.45/s)  LR: 4.504e-05  Data: 1.393 (1.548)
Train: 146 [ 850/1171 ( 73%)]  Loss:  2.372376 (2.7662)  Time: 0.585s, 1751.08/s  (2.152s,  475.83/s)  LR: 4.504e-05  Data: 0.021 (1.547)
Train: 146 [ 900/1171 ( 77%)]  Loss:  2.671963 (2.7613)  Time: 1.988s,  515.00/s  (2.151s,  475.96/s)  LR: 4.504e-05  Data: 1.355 (1.546)
Train: 146 [ 950/1171 ( 81%)]  Loss:  3.293726 (2.7879)  Time: 1.945s,  526.41/s  (2.147s,  476.84/s)  LR: 4.504e-05  Data: 1.359 (1.542)
Train: 146 [1000/1171 ( 85%)]  Loss:  2.697679 (2.7836)  Time: 2.487s,  411.72/s  (2.145s,  477.48/s)  LR: 4.504e-05  Data: 1.924 (1.538)
Train: 146 [1050/1171 ( 90%)]  Loss:  2.987327 (2.7928)  Time: 0.585s, 1750.69/s  (2.140s,  478.44/s)  LR: 4.504e-05  Data: 0.018 (1.534)
Train: 146 [1100/1171 ( 94%)]  Loss:  3.280259 (2.8140)  Time: 6.661s,  153.72/s  (2.137s,  479.14/s)  LR: 4.504e-05  Data: 6.011 (1.532)
Train: 146 [1150/1171 ( 98%)]  Loss:  2.871574 (2.8164)  Time: 0.586s, 1746.70/s  (2.151s,  476.10/s)  LR: 4.504e-05  Data: 0.021 (1.545)
Train: 146 [1170/1171 (100%)]  Loss:  3.067575 (2.8265)  Time: 0.565s, 1813.88/s  (2.149s,  476.53/s)  LR: 4.504e-05  Data: 0.000 (1.543)
Test: [   0/97]  Time: 10.968 (10.968)  Loss:  0.3085 (0.3085)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.196 (2.947)  Loss:  0.4771 (0.3809)  Acc@1: 92.4805 (95.1651)  Acc@5: 98.2422 (98.9258)
Test: [  97/97]  Time: 0.119 (2.907)  Loss:  0.3385 (0.3915)  Acc@1: 94.4940 (94.6080)  Acc@5: 99.2560 (98.7940)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-146.pth.tar', 94.60800000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-144.pth.tar', 94.57800003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-145.pth.tar', 94.57100000732422)

Train: 147 [   0/1171 (  0%)]  Loss:  3.071548 (3.0715)  Time: 9.363s,  109.37/s  (9.363s,  109.37/s)  LR: 4.166e-05  Data: 8.709 (8.709)
Train: 147 [  50/1171 (  4%)]  Loss:  2.937704 (3.0046)  Time: 0.584s, 1754.84/s  (2.187s,  468.29/s)  LR: 4.166e-05  Data: 0.018 (1.588)
Train: 147 [ 100/1171 (  9%)]  Loss:  2.749764 (2.9197)  Time: 0.585s, 1750.78/s  (2.129s,  481.05/s)  LR: 4.166e-05  Data: 0.021 (1.524)
Train: 147 [ 150/1171 ( 13%)]  Loss:  2.847606 (2.9017)  Time: 0.586s, 1745.96/s  (2.071s,  494.51/s)  LR: 4.166e-05  Data: 0.019 (1.470)
Train: 147 [ 200/1171 ( 17%)]  Loss:  2.466428 (2.8146)  Time: 0.587s, 1743.80/s  (2.068s,  495.16/s)  LR: 4.166e-05  Data: 0.021 (1.472)
Train: 147 [ 250/1171 ( 21%)]  Loss:  2.824181 (2.8162)  Time: 0.585s, 1749.01/s  (2.115s,  484.14/s)  LR: 4.166e-05  Data: 0.020 (1.521)
Train: 147 [ 300/1171 ( 26%)]  Loss:  3.133362 (2.8615)  Time: 0.587s, 1743.85/s  (2.135s,  479.62/s)  LR: 4.166e-05  Data: 0.018 (1.542)
Train: 147 [ 350/1171 ( 30%)]  Loss:  3.254874 (2.9107)  Time: 0.584s, 1752.98/s  (2.138s,  478.84/s)  LR: 4.166e-05  Data: 0.020 (1.548)
Train: 147 [ 400/1171 ( 34%)]  Loss:  2.809912 (2.8995)  Time: 0.585s, 1749.38/s  (2.128s,  481.12/s)  LR: 4.166e-05  Data: 0.019 (1.538)
Train: 147 [ 450/1171 ( 38%)]  Loss:  3.094403 (2.9190)  Time: 3.248s,  315.32/s  (2.131s,  480.53/s)  LR: 4.166e-05  Data: 2.684 (1.540)
Train: 147 [ 500/1171 ( 43%)]  Loss:  2.755781 (2.9041)  Time: 0.620s, 1652.35/s  (2.128s,  481.14/s)  LR: 4.166e-05  Data: 0.053 (1.536)
Train: 147 [ 550/1171 ( 47%)]  Loss:  2.537225 (2.8736)  Time: 3.268s,  313.37/s  (2.137s,  479.11/s)  LR: 4.166e-05  Data: 2.675 (1.541)
Train: 147 [ 600/1171 ( 51%)]  Loss:  2.422979 (2.8389)  Time: 1.011s, 1013.18/s  (2.147s,  476.86/s)  LR: 4.166e-05  Data: 0.445 (1.550)
Train: 147 [ 650/1171 ( 56%)]  Loss:  3.055302 (2.8544)  Time: 0.882s, 1161.17/s  (2.165s,  472.87/s)  LR: 4.166e-05  Data: 0.314 (1.568)
Train: 147 [ 700/1171 ( 60%)]  Loss:  2.641883 (2.8402)  Time: 0.583s, 1755.05/s  (2.175s,  470.87/s)  LR: 4.166e-05  Data: 0.020 (1.577)
Train: 147 [ 750/1171 ( 64%)]  Loss:  3.037664 (2.8525)  Time: 0.751s, 1363.44/s  (2.172s,  471.36/s)  LR: 4.166e-05  Data: 0.166 (1.575)
Train: 147 [ 800/1171 ( 68%)]  Loss:  2.972059 (2.8596)  Time: 1.392s,  735.49/s  (2.169s,  472.19/s)  LR: 4.166e-05  Data: 0.729 (1.570)
Train: 147 [ 850/1171 ( 73%)]  Loss:  3.079823 (2.8718)  Time: 1.996s,  513.12/s  (2.169s,  472.20/s)  LR: 4.166e-05  Data: 1.405 (1.569)
Train: 147 [ 900/1171 ( 77%)]  Loss:  2.828149 (2.8695)  Time: 3.251s,  314.97/s  (2.166s,  472.86/s)  LR: 4.166e-05  Data: 2.584 (1.565)
Train: 147 [ 950/1171 ( 81%)]  Loss:  2.719337 (2.8620)  Time: 1.372s,  746.35/s  (2.163s,  473.36/s)  LR: 4.166e-05  Data: 0.244 (1.562)
Train: 147 [1000/1171 ( 85%)]  Loss:  2.704201 (2.8545)  Time: 5.225s,  195.99/s  (2.162s,  473.60/s)  LR: 4.166e-05  Data: 4.649 (1.561)
Train: 147 [1050/1171 ( 90%)]  Loss:  3.042007 (2.8630)  Time: 1.812s,  565.00/s  (2.156s,  474.85/s)  LR: 4.166e-05  Data: 1.249 (1.555)
Train: 147 [1100/1171 ( 94%)]  Loss:  2.922063 (2.8656)  Time: 5.436s,  188.39/s  (2.173s,  471.34/s)  LR: 4.166e-05  Data: 4.781 (1.570)
Train: 147 [1150/1171 ( 98%)]  Loss:  3.086858 (2.8748)  Time: 0.609s, 1680.26/s  (2.166s,  472.68/s)  LR: 4.166e-05  Data: 0.017 (1.564)
Train: 147 [1170/1171 (100%)]  Loss:  3.064665 (2.8824)  Time: 0.568s, 1804.22/s  (2.169s,  472.05/s)  LR: 4.166e-05  Data: 0.000 (1.567)
Test: [   0/97]  Time: 12.610 (12.610)  Loss:  0.2787 (0.2787)  Acc@1: 96.9727 (96.9727)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (2.843)  Loss:  0.4170 (0.3430)  Acc@1: 92.8711 (95.1938)  Acc@5: 98.4375 (98.9564)
Test: [  97/97]  Time: 0.119 (2.767)  Loss:  0.3024 (0.3552)  Acc@1: 94.3452 (94.6410)  Acc@5: 99.4048 (98.7990)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-147.pth.tar', 94.6409999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-146.pth.tar', 94.60800000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-144.pth.tar', 94.57800003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-145.pth.tar', 94.57100000732422)

Train: 148 [   0/1171 (  0%)]  Loss:  2.738010 (2.7380)  Time: 9.420s,  108.70/s  (9.420s,  108.70/s)  LR: 3.844e-05  Data: 8.650 (8.650)
Train: 148 [  50/1171 (  4%)]  Loss:  2.752069 (2.7450)  Time: 0.589s, 1739.63/s  (2.141s,  478.19/s)  LR: 3.844e-05  Data: 0.021 (1.558)
Train: 148 [ 100/1171 (  9%)]  Loss:  2.674723 (2.7216)  Time: 0.586s, 1747.03/s  (2.084s,  491.41/s)  LR: 3.844e-05  Data: 0.019 (1.501)
Train: 148 [ 150/1171 ( 13%)]  Loss:  2.804639 (2.7424)  Time: 0.583s, 1755.61/s  (2.026s,  505.36/s)  LR: 3.844e-05  Data: 0.020 (1.445)
Train: 148 [ 200/1171 ( 17%)]  Loss:  3.321733 (2.8582)  Time: 0.584s, 1752.29/s  (2.119s,  483.27/s)  LR: 3.844e-05  Data: 0.019 (1.538)
Train: 148 [ 250/1171 ( 21%)]  Loss:  2.715165 (2.8344)  Time: 0.587s, 1743.68/s  (2.090s,  489.87/s)  LR: 3.844e-05  Data: 0.020 (1.505)
Train: 148 [ 300/1171 ( 26%)]  Loss:  3.106713 (2.8733)  Time: 0.583s, 1755.49/s  (2.110s,  485.33/s)  LR: 3.844e-05  Data: 0.020 (1.523)
Train: 148 [ 350/1171 ( 30%)]  Loss:  2.609761 (2.8404)  Time: 0.585s, 1750.94/s  (2.100s,  487.64/s)  LR: 3.844e-05  Data: 0.021 (1.509)
Train: 148 [ 400/1171 ( 34%)]  Loss:  3.118913 (2.8713)  Time: 0.841s, 1217.95/s  (2.101s,  487.46/s)  LR: 3.844e-05  Data: 0.107 (1.506)
Train: 148 [ 450/1171 ( 38%)]  Loss:  2.680945 (2.8523)  Time: 0.584s, 1754.77/s  (2.111s,  484.97/s)  LR: 3.844e-05  Data: 0.018 (1.516)
Train: 148 [ 500/1171 ( 43%)]  Loss:  2.915410 (2.8580)  Time: 0.584s, 1752.75/s  (2.109s,  485.45/s)  LR: 3.844e-05  Data: 0.019 (1.514)
Train: 148 [ 550/1171 ( 47%)]  Loss:  2.913193 (2.8626)  Time: 0.589s, 1738.26/s  (2.118s,  483.48/s)  LR: 3.844e-05  Data: 0.019 (1.522)
Train: 148 [ 600/1171 ( 51%)]  Loss:  3.235300 (2.8913)  Time: 0.586s, 1746.20/s  (2.137s,  479.16/s)  LR: 3.844e-05  Data: 0.020 (1.540)
Train: 148 [ 650/1171 ( 56%)]  Loss:  3.217801 (2.9146)  Time: 0.587s, 1745.17/s  (2.170s,  471.90/s)  LR: 3.844e-05  Data: 0.021 (1.572)
Train: 148 [ 700/1171 ( 60%)]  Loss:  2.992627 (2.9198)  Time: 0.584s, 1752.72/s  (2.184s,  468.94/s)  LR: 3.844e-05  Data: 0.020 (1.586)
Train: 148 [ 750/1171 ( 64%)]  Loss:  2.639148 (2.9023)  Time: 0.584s, 1753.50/s  (2.192s,  467.17/s)  LR: 3.844e-05  Data: 0.019 (1.594)
Train: 148 [ 800/1171 ( 68%)]  Loss:  3.051951 (2.9111)  Time: 0.588s, 1742.87/s  (2.190s,  467.59/s)  LR: 3.844e-05  Data: 0.019 (1.593)
Train: 148 [ 850/1171 ( 73%)]  Loss:  2.810871 (2.9055)  Time: 0.586s, 1748.03/s  (2.188s,  467.92/s)  LR: 3.844e-05  Data: 0.017 (1.592)
Train: 148 [ 900/1171 ( 77%)]  Loss:  2.915871 (2.9060)  Time: 0.585s, 1751.09/s  (2.185s,  468.73/s)  LR: 3.844e-05  Data: 0.020 (1.589)
Train: 148 [ 950/1171 ( 81%)]  Loss:  3.252361 (2.9234)  Time: 0.586s, 1746.69/s  (2.182s,  469.28/s)  LR: 3.844e-05  Data: 0.021 (1.586)
Train: 148 [1000/1171 ( 85%)]  Loss:  2.596368 (2.9078)  Time: 0.581s, 1762.40/s  (2.196s,  466.31/s)  LR: 3.844e-05  Data: 0.018 (1.600)
Train: 148 [1050/1171 ( 90%)]  Loss:  2.253499 (2.8780)  Time: 2.081s,  492.12/s  (2.196s,  466.30/s)  LR: 3.844e-05  Data: 1.517 (1.600)
Train: 148 [1100/1171 ( 94%)]  Loss:  3.040044 (2.8851)  Time: 0.589s, 1738.75/s  (2.201s,  465.27/s)  LR: 3.844e-05  Data: 0.020 (1.604)
Train: 148 [1150/1171 ( 98%)]  Loss:  3.040067 (2.8915)  Time: 4.592s,  222.99/s  (2.200s,  465.51/s)  LR: 3.844e-05  Data: 4.029 (1.602)
Train: 148 [1170/1171 (100%)]  Loss:  2.666240 (2.8825)  Time: 0.563s, 1817.86/s  (2.196s,  466.21/s)  LR: 3.844e-05  Data: 0.000 (1.598)
Test: [   0/97]  Time: 12.406 (12.406)  Loss:  0.3039 (0.3039)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.198 (2.884)  Loss:  0.4619 (0.3707)  Acc@1: 92.8711 (95.0961)  Acc@5: 98.3398 (98.9315)
Test: [  97/97]  Time: 0.119 (2.822)  Loss:  0.3577 (0.3805)  Acc@1: 94.0476 (94.6440)  Acc@5: 99.1071 (98.7970)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-148.pth.tar', 94.64399997314453)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-147.pth.tar', 94.6409999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-146.pth.tar', 94.60800000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-144.pth.tar', 94.57800003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-145.pth.tar', 94.57100000732422)

Train: 149 [   0/1171 (  0%)]  Loss:  2.724394 (2.7244)  Time: 9.358s,  109.43/s  (9.358s,  109.43/s)  LR: 3.540e-05  Data: 8.663 (8.663)
Train: 149 [  50/1171 (  4%)]  Loss:  2.528878 (2.6266)  Time: 0.583s, 1755.75/s  (2.126s,  481.57/s)  LR: 3.540e-05  Data: 0.020 (1.534)
Train: 149 [ 100/1171 (  9%)]  Loss:  2.913908 (2.7224)  Time: 0.587s, 1744.62/s  (2.123s,  482.42/s)  LR: 3.540e-05  Data: 0.024 (1.529)
Train: 149 [ 150/1171 ( 13%)]  Loss:  2.941758 (2.7772)  Time: 0.585s, 1749.92/s  (2.295s,  446.26/s)  LR: 3.540e-05  Data: 0.023 (1.707)
Train: 149 [ 200/1171 ( 17%)]  Loss:  2.500944 (2.7220)  Time: 0.587s, 1745.22/s  (2.321s,  441.22/s)  LR: 3.540e-05  Data: 0.023 (1.735)
Train: 149 [ 250/1171 ( 21%)]  Loss:  2.742350 (2.7254)  Time: 0.585s, 1751.49/s  (2.300s,  445.27/s)  LR: 3.540e-05  Data: 0.020 (1.714)
Train: 149 [ 300/1171 ( 26%)]  Loss:  3.262288 (2.8021)  Time: 0.584s, 1754.75/s  (2.318s,  441.76/s)  LR: 3.540e-05  Data: 0.019 (1.732)
Train: 149 [ 350/1171 ( 30%)]  Loss:  3.369593 (2.8730)  Time: 0.584s, 1753.93/s  (2.300s,  445.23/s)  LR: 3.540e-05  Data: 0.020 (1.712)
Train: 149 [ 400/1171 ( 34%)]  Loss:  2.922425 (2.8785)  Time: 0.588s, 1740.16/s  (2.293s,  446.63/s)  LR: 3.540e-05  Data: 0.020 (1.707)
Train: 149 [ 450/1171 ( 38%)]  Loss:  3.018450 (2.8925)  Time: 0.586s, 1747.59/s  (2.269s,  451.27/s)  LR: 3.540e-05  Data: 0.022 (1.683)
Train: 149 [ 500/1171 ( 43%)]  Loss:  2.636819 (2.8693)  Time: 0.586s, 1747.36/s  (2.298s,  445.58/s)  LR: 3.540e-05  Data: 0.020 (1.713)
Train: 149 [ 550/1171 ( 47%)]  Loss:  2.815101 (2.8647)  Time: 0.587s, 1744.54/s  (2.302s,  444.78/s)  LR: 3.540e-05  Data: 0.020 (1.717)
Train: 149 [ 600/1171 ( 51%)]  Loss:  2.979003 (2.8735)  Time: 0.589s, 1737.57/s  (2.331s,  439.33/s)  LR: 3.540e-05  Data: 0.018 (1.745)
Train: 149 [ 650/1171 ( 56%)]  Loss:  2.983349 (2.8814)  Time: 0.585s, 1751.87/s  (2.326s,  440.17/s)  LR: 3.540e-05  Data: 0.019 (1.740)
Train: 149 [ 700/1171 ( 60%)]  Loss:  2.723527 (2.8709)  Time: 0.587s, 1743.69/s  (2.323s,  440.80/s)  LR: 3.540e-05  Data: 0.018 (1.737)
Train: 149 [ 750/1171 ( 64%)]  Loss:  2.719621 (2.8614)  Time: 0.585s, 1751.23/s  (2.307s,  443.92/s)  LR: 3.540e-05  Data: 0.021 (1.720)
Train: 149 [ 800/1171 ( 68%)]  Loss:  3.076463 (2.8741)  Time: 0.589s, 1738.25/s  (2.300s,  445.12/s)  LR: 3.540e-05  Data: 0.019 (1.714)
Train: 149 [ 850/1171 ( 73%)]  Loss:  3.123729 (2.8879)  Time: 0.585s, 1751.27/s  (2.285s,  448.18/s)  LR: 3.540e-05  Data: 0.019 (1.698)
Train: 149 [ 900/1171 ( 77%)]  Loss:  2.583705 (2.8719)  Time: 0.585s, 1749.00/s  (2.300s,  445.28/s)  LR: 3.540e-05  Data: 0.021 (1.713)
Train: 149 [ 950/1171 ( 81%)]  Loss:  3.205583 (2.8886)  Time: 0.586s, 1748.40/s  (2.287s,  447.81/s)  LR: 3.540e-05  Data: 0.020 (1.698)
Train: 149 [1000/1171 ( 85%)]  Loss:  3.170210 (2.9020)  Time: 0.833s, 1229.12/s  (2.291s,  446.89/s)  LR: 3.540e-05  Data: 0.202 (1.703)
Train: 149 [1050/1171 ( 90%)]  Loss:  3.204140 (2.9157)  Time: 0.590s, 1734.17/s  (2.280s,  449.21/s)  LR: 3.540e-05  Data: 0.020 (1.691)
Train: 149 [1100/1171 ( 94%)]  Loss:  3.161229 (2.9264)  Time: 0.924s, 1108.17/s  (2.278s,  449.45/s)  LR: 3.540e-05  Data: 0.264 (1.690)
Train: 149 [1150/1171 ( 98%)]  Loss:  3.063700 (2.9321)  Time: 0.586s, 1747.38/s  (2.268s,  451.44/s)  LR: 3.540e-05  Data: 0.023 (1.680)
Train: 149 [1170/1171 (100%)]  Loss:  3.309103 (2.9472)  Time: 0.564s, 1814.17/s  (2.266s,  451.94/s)  LR: 3.540e-05  Data: 0.000 (1.677)
Test: [   0/97]  Time: 12.017 (12.017)  Loss:  0.2880 (0.2880)  Acc@1: 96.2891 (96.2891)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (2.981)  Loss:  0.4475 (0.3602)  Acc@1: 92.4805 (95.2110)  Acc@5: 98.5352 (98.9277)
Test: [  97/97]  Time: 0.119 (3.040)  Loss:  0.3337 (0.3703)  Acc@1: 94.3452 (94.7010)  Acc@5: 99.2560 (98.8020)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-149.pth.tar', 94.7009999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-148.pth.tar', 94.64399997314453)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-147.pth.tar', 94.6409999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-146.pth.tar', 94.60800000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-144.pth.tar', 94.57800003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-145.pth.tar', 94.57100000732422)

Train: 150 [   0/1171 (  0%)]  Loss:  3.331294 (3.3313)  Time: 13.567s,   75.48/s  (13.567s,   75.48/s)  LR: 3.252e-05  Data: 12.055 (12.055)
Train: 150 [  50/1171 (  4%)]  Loss:  2.747045 (3.0392)  Time: 0.585s, 1750.26/s  (2.362s,  433.56/s)  LR: 3.252e-05  Data: 0.023 (1.757)
Train: 150 [ 100/1171 (  9%)]  Loss:  3.063015 (3.0471)  Time: 0.584s, 1753.34/s  (2.376s,  430.97/s)  LR: 3.252e-05  Data: 0.021 (1.769)
Train: 150 [ 150/1171 ( 13%)]  Loss:  3.179441 (3.0802)  Time: 2.807s,  364.82/s  (2.318s,  441.79/s)  LR: 3.252e-05  Data: 2.137 (1.702)
Train: 150 [ 200/1171 ( 17%)]  Loss:  2.986253 (3.0614)  Time: 3.066s,  333.99/s  (2.295s,  446.28/s)  LR: 3.252e-05  Data: 2.459 (1.675)
Train: 150 [ 250/1171 ( 21%)]  Loss:  2.978262 (3.0476)  Time: 0.588s, 1742.11/s  (2.251s,  454.84/s)  LR: 3.252e-05  Data: 0.024 (1.632)
Train: 150 [ 300/1171 ( 26%)]  Loss:  2.457167 (2.9632)  Time: 3.860s,  265.26/s  (2.238s,  457.61/s)  LR: 3.252e-05  Data: 3.297 (1.621)
Train: 150 [ 350/1171 ( 30%)]  Loss:  2.382759 (2.8907)  Time: 1.083s,  945.85/s  (2.201s,  465.18/s)  LR: 3.252e-05  Data: 0.408 (1.585)
Train: 150 [ 400/1171 ( 34%)]  Loss:  2.808543 (2.8815)  Time: 7.400s,  138.37/s  (2.213s,  462.62/s)  LR: 3.252e-05  Data: 6.698 (1.594)
Train: 150 [ 450/1171 ( 38%)]  Loss:  3.023433 (2.8957)  Time: 0.587s, 1745.89/s  (2.214s,  462.48/s)  LR: 3.252e-05  Data: 0.023 (1.594)
Train: 150 [ 500/1171 ( 43%)]  Loss:  2.945142 (2.9002)  Time: 7.820s,  130.95/s  (2.226s,  460.02/s)  LR: 3.252e-05  Data: 7.160 (1.607)
Train: 150 [ 550/1171 ( 47%)]  Loss:  2.793928 (2.8914)  Time: 0.588s, 1740.73/s  (2.227s,  459.71/s)  LR: 3.252e-05  Data: 0.020 (1.610)
Train: 150 [ 600/1171 ( 51%)]  Loss:  3.246250 (2.9187)  Time: 5.951s,  172.08/s  (2.235s,  458.22/s)  LR: 3.252e-05  Data: 5.363 (1.618)
Train: 150 [ 650/1171 ( 56%)]  Loss:  2.853166 (2.9140)  Time: 0.589s, 1737.81/s  (2.227s,  459.79/s)  LR: 3.252e-05  Data: 0.022 (1.612)
Train: 150 [ 700/1171 ( 60%)]  Loss:  2.693020 (2.8992)  Time: 6.016s,  170.23/s  (2.227s,  459.88/s)  LR: 3.252e-05  Data: 5.397 (1.614)
Train: 150 [ 750/1171 ( 64%)]  Loss:  2.871532 (2.8975)  Time: 0.587s, 1744.96/s  (2.217s,  461.95/s)  LR: 3.252e-05  Data: 0.019 (1.605)
Train: 150 [ 800/1171 ( 68%)]  Loss:  2.938257 (2.8999)  Time: 2.932s,  349.27/s  (2.230s,  459.20/s)  LR: 3.252e-05  Data: 2.266 (1.620)
Train: 150 [ 850/1171 ( 73%)]  Loss:  2.804076 (2.8946)  Time: 0.588s, 1740.05/s  (2.229s,  459.45/s)  LR: 3.252e-05  Data: 0.022 (1.617)
Train: 150 [ 900/1171 ( 77%)]  Loss:  3.079928 (2.9043)  Time: 5.447s,  187.99/s  (2.242s,  456.71/s)  LR: 3.252e-05  Data: 4.870 (1.630)
Train: 150 [ 950/1171 ( 81%)]  Loss:  3.193025 (2.9188)  Time: 0.588s, 1741.55/s  (2.237s,  457.77/s)  LR: 3.252e-05  Data: 0.019 (1.625)
Train: 150 [1000/1171 ( 85%)]  Loss:  2.429132 (2.8955)  Time: 5.132s,  199.52/s  (2.238s,  457.52/s)  LR: 3.252e-05  Data: 4.525 (1.625)
Train: 150 [1050/1171 ( 90%)]  Loss:  2.955773 (2.8982)  Time: 0.587s, 1743.40/s  (2.228s,  459.52/s)  LR: 3.252e-05  Data: 0.021 (1.617)
Train: 150 [1100/1171 ( 94%)]  Loss:  2.826530 (2.8951)  Time: 6.147s,  166.60/s  (2.228s,  459.65/s)  LR: 3.252e-05  Data: 5.545 (1.617)
Train: 150 [1150/1171 ( 98%)]  Loss:  2.808312 (2.8915)  Time: 0.584s, 1753.23/s  (2.218s,  461.58/s)  LR: 3.252e-05  Data: 0.018 (1.609)
Train: 150 [1170/1171 (100%)]  Loss:  3.200445 (2.9038)  Time: 0.565s, 1811.37/s  (2.216s,  462.10/s)  LR: 3.252e-05  Data: 0.000 (1.607)
Test: [   0/97]  Time: 11.981 (11.981)  Loss:  0.3031 (0.3031)  Acc@1: 96.3867 (96.3867)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.198 (3.092)  Loss:  0.4645 (0.3722)  Acc@1: 91.8945 (95.0827)  Acc@5: 98.2422 (98.9373)
Test: [  97/97]  Time: 0.119 (2.958)  Loss:  0.3460 (0.3807)  Acc@1: 94.7917 (94.6460)  Acc@5: 99.2560 (98.8190)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-149.pth.tar', 94.7009999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-150.pth.tar', 94.64600003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-148.pth.tar', 94.64399997314453)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-147.pth.tar', 94.6409999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-146.pth.tar', 94.60800000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-144.pth.tar', 94.57800003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-145.pth.tar', 94.57100000732422)

Train: 151 [   0/1171 (  0%)]  Loss:  3.253525 (3.2535)  Time: 10.346s,   98.97/s  (10.346s,   98.97/s)  LR: 2.981e-05  Data: 9.694 (9.694)
Train: 151 [  50/1171 (  4%)]  Loss:  2.908972 (3.0812)  Time: 0.581s, 1762.86/s  (2.265s,  452.00/s)  LR: 2.981e-05  Data: 0.017 (1.673)
Train: 151 [ 100/1171 (  9%)]  Loss:  2.669841 (2.9441)  Time: 0.587s, 1744.77/s  (2.212s,  462.98/s)  LR: 2.981e-05  Data: 0.019 (1.621)
Train: 151 [ 150/1171 ( 13%)]  Loss:  2.402610 (2.8087)  Time: 0.584s, 1752.23/s  (2.138s,  479.01/s)  LR: 2.981e-05  Data: 0.020 (1.542)
Train: 151 [ 200/1171 ( 17%)]  Loss:  3.267942 (2.9006)  Time: 1.533s,  667.99/s  (2.125s,  481.84/s)  LR: 2.981e-05  Data: 0.944 (1.532)
Train: 151 [ 250/1171 ( 21%)]  Loss:  2.809887 (2.8855)  Time: 0.589s, 1739.15/s  (2.100s,  487.53/s)  LR: 2.981e-05  Data: 0.020 (1.506)
Train: 151 [ 300/1171 ( 26%)]  Loss:  3.289557 (2.9432)  Time: 5.069s,  202.00/s  (2.094s,  488.96/s)  LR: 2.981e-05  Data: 4.506 (1.496)
Train: 151 [ 350/1171 ( 30%)]  Loss:  3.263188 (2.9832)  Time: 0.783s, 1307.88/s  (2.141s,  478.28/s)  LR: 2.981e-05  Data: 0.219 (1.545)
Train: 151 [ 400/1171 ( 34%)]  Loss:  2.543173 (2.9343)  Time: 3.491s,  293.33/s  (2.138s,  478.99/s)  LR: 2.981e-05  Data: 2.850 (1.539)
Train: 151 [ 450/1171 ( 38%)]  Loss:  2.999091 (2.9408)  Time: 4.702s,  217.76/s  (2.160s,  473.99/s)  LR: 2.981e-05  Data: 4.132 (1.561)
Train: 151 [ 500/1171 ( 43%)]  Loss:  3.027296 (2.9486)  Time: 4.518s,  226.67/s  (2.168s,  472.37/s)  LR: 2.981e-05  Data: 3.939 (1.569)
Train: 151 [ 550/1171 ( 47%)]  Loss:  2.850389 (2.9405)  Time: 5.520s,  185.49/s  (2.182s,  469.40/s)  LR: 2.981e-05  Data: 4.917 (1.580)
Train: 151 [ 600/1171 ( 51%)]  Loss:  2.986130 (2.9440)  Time: 2.897s,  353.52/s  (2.190s,  467.54/s)  LR: 2.981e-05  Data: 2.219 (1.588)
Train: 151 [ 650/1171 ( 56%)]  Loss:  3.127357 (2.9571)  Time: 5.400s,  189.63/s  (2.191s,  467.27/s)  LR: 2.981e-05  Data: 4.819 (1.589)
Train: 151 [ 700/1171 ( 60%)]  Loss:  2.856360 (2.9504)  Time: 1.175s,  871.74/s  (2.183s,  469.14/s)  LR: 2.981e-05  Data: 0.566 (1.581)
Train: 151 [ 750/1171 ( 64%)]  Loss:  2.882502 (2.9461)  Time: 6.364s,  160.91/s  (2.211s,  463.13/s)  LR: 2.981e-05  Data: 5.696 (1.608)
Train: 151 [ 800/1171 ( 68%)]  Loss:  2.957582 (2.9468)  Time: 1.523s,  672.55/s  (2.198s,  465.88/s)  LR: 2.981e-05  Data: 0.956 (1.596)
Train: 151 [ 850/1171 ( 73%)]  Loss:  2.959374 (2.9475)  Time: 5.816s,  176.06/s  (2.209s,  463.45/s)  LR: 2.981e-05  Data: 5.197 (1.608)
Train: 151 [ 900/1171 ( 77%)]  Loss:  2.742406 (2.9367)  Time: 0.963s, 1063.67/s  (2.199s,  465.57/s)  LR: 2.981e-05  Data: 0.357 (1.597)
Train: 151 [ 950/1171 ( 81%)]  Loss:  3.116331 (2.9457)  Time: 5.879s,  174.17/s  (2.199s,  465.59/s)  LR: 2.981e-05  Data: 5.305 (1.598)
Train: 151 [1000/1171 ( 85%)]  Loss:  2.930837 (2.9450)  Time: 2.263s,  452.48/s  (2.189s,  467.76/s)  LR: 2.981e-05  Data: 1.640 (1.588)
Train: 151 [1050/1171 ( 90%)]  Loss:  2.816876 (2.9391)  Time: 5.732s,  178.65/s  (2.186s,  468.43/s)  LR: 2.981e-05  Data: 5.169 (1.585)
Train: 151 [1100/1171 ( 94%)]  Loss:  3.133539 (2.9476)  Time: 0.587s, 1744.76/s  (2.176s,  470.50/s)  LR: 2.981e-05  Data: 0.019 (1.576)
Train: 151 [1150/1171 ( 98%)]  Loss:  2.669717 (2.9360)  Time: 7.834s,  130.72/s  (2.189s,  467.75/s)  LR: 2.981e-05  Data: 6.929 (1.589)
Train: 151 [1170/1171 (100%)]  Loss:  3.091529 (2.9422)  Time: 0.564s, 1814.11/s  (2.186s,  468.33/s)  LR: 2.981e-05  Data: 0.000 (1.586)
Test: [   0/97]  Time: 11.260 (11.260)  Loss:  0.2951 (0.2951)  Acc@1: 97.0703 (97.0703)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.202 (2.960)  Loss:  0.4478 (0.3597)  Acc@1: 93.1641 (95.2397)  Acc@5: 98.3398 (98.9622)
Test: [  97/97]  Time: 0.120 (2.904)  Loss:  0.3292 (0.3703)  Acc@1: 94.1964 (94.7700)  Acc@5: 99.4048 (98.8310)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-151.pth.tar', 94.76999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-149.pth.tar', 94.7009999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-150.pth.tar', 94.64600003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-148.pth.tar', 94.64399997314453)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-147.pth.tar', 94.6409999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-146.pth.tar', 94.60800000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-144.pth.tar', 94.57800003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-145.pth.tar', 94.57100000732422)

Train: 152 [   0/1171 (  0%)]  Loss:  2.656269 (2.6563)  Time: 10.653s,   96.13/s  (10.653s,   96.13/s)  LR: 2.727e-05  Data: 9.806 (9.806)
Train: 152 [  50/1171 (  4%)]  Loss:  2.803475 (2.7299)  Time: 0.586s, 1746.88/s  (2.253s,  454.53/s)  LR: 2.727e-05  Data: 0.018 (1.668)
Train: 152 [ 100/1171 (  9%)]  Loss:  2.665997 (2.7086)  Time: 0.587s, 1744.91/s  (2.232s,  458.72/s)  LR: 2.727e-05  Data: 0.019 (1.647)
Train: 152 [ 150/1171 ( 13%)]  Loss:  2.987731 (2.7784)  Time: 0.582s, 1758.97/s  (2.162s,  473.61/s)  LR: 2.727e-05  Data: 0.018 (1.575)
Train: 152 [ 200/1171 ( 17%)]  Loss:  2.911195 (2.8049)  Time: 3.270s,  313.17/s  (2.146s,  477.23/s)  LR: 2.727e-05  Data: 2.608 (1.554)
Train: 152 [ 250/1171 ( 21%)]  Loss:  2.992889 (2.8363)  Time: 0.584s, 1752.57/s  (2.164s,  473.10/s)  LR: 2.727e-05  Data: 0.019 (1.566)
Train: 152 [ 300/1171 ( 26%)]  Loss:  2.674504 (2.8132)  Time: 6.075s,  168.55/s  (2.206s,  464.13/s)  LR: 2.727e-05  Data: 5.511 (1.606)
Train: 152 [ 350/1171 ( 30%)]  Loss:  2.691579 (2.7980)  Time: 0.586s, 1747.86/s  (2.194s,  466.69/s)  LR: 2.727e-05  Data: 0.022 (1.596)
Train: 152 [ 400/1171 ( 34%)]  Loss:  2.490271 (2.7638)  Time: 5.389s,  190.01/s  (2.200s,  465.51/s)  LR: 2.727e-05  Data: 4.786 (1.599)
Train: 152 [ 450/1171 ( 38%)]  Loss:  2.698870 (2.7573)  Time: 0.588s, 1742.04/s  (2.187s,  468.18/s)  LR: 2.727e-05  Data: 0.021 (1.586)
Train: 152 [ 500/1171 ( 43%)]  Loss:  3.093430 (2.7878)  Time: 1.827s,  560.34/s  (2.186s,  468.41/s)  LR: 2.727e-05  Data: 1.177 (1.586)
Train: 152 [ 550/1171 ( 47%)]  Loss:  3.073815 (2.8117)  Time: 0.588s, 1741.68/s  (2.191s,  467.28/s)  LR: 2.727e-05  Data: 0.020 (1.592)
Train: 152 [ 600/1171 ( 51%)]  Loss:  2.802854 (2.8110)  Time: 2.604s,  393.19/s  (2.190s,  467.62/s)  LR: 2.727e-05  Data: 1.970 (1.590)
Train: 152 [ 650/1171 ( 56%)]  Loss:  3.099538 (2.8316)  Time: 0.588s, 1741.10/s  (2.192s,  467.16/s)  LR: 2.727e-05  Data: 0.024 (1.592)
Train: 152 [ 700/1171 ( 60%)]  Loss:  2.957777 (2.8400)  Time: 3.738s,  273.98/s  (2.218s,  461.71/s)  LR: 2.727e-05  Data: 3.053 (1.617)
Train: 152 [ 750/1171 ( 64%)]  Loss:  2.802033 (2.8376)  Time: 0.587s, 1744.08/s  (2.218s,  461.65/s)  LR: 2.727e-05  Data: 0.021 (1.617)
Train: 152 [ 800/1171 ( 68%)]  Loss:  2.849503 (2.8383)  Time: 0.790s, 1296.65/s  (2.215s,  462.25/s)  LR: 2.727e-05  Data: 0.151 (1.615)
Train: 152 [ 850/1171 ( 73%)]  Loss:  3.286178 (2.8632)  Time: 0.585s, 1751.44/s  (2.218s,  461.59/s)  LR: 2.727e-05  Data: 0.019 (1.618)
Train: 152 [ 900/1171 ( 77%)]  Loss:  2.787144 (2.8592)  Time: 3.588s,  285.38/s  (2.216s,  462.03/s)  LR: 2.727e-05  Data: 2.991 (1.615)
Train: 152 [ 950/1171 ( 81%)]  Loss:  3.026764 (2.8676)  Time: 0.586s, 1748.09/s  (2.212s,  462.91/s)  LR: 2.727e-05  Data: 0.020 (1.610)
Train: 152 [1000/1171 ( 85%)]  Loss:  3.233414 (2.8850)  Time: 3.555s,  288.07/s  (2.208s,  463.73/s)  LR: 2.727e-05  Data: 2.911 (1.607)
Train: 152 [1050/1171 ( 90%)]  Loss:  2.463045 (2.8658)  Time: 0.583s, 1755.04/s  (2.204s,  464.66/s)  LR: 2.727e-05  Data: 0.020 (1.603)
Train: 152 [1100/1171 ( 94%)]  Loss:  2.864563 (2.8658)  Time: 1.409s,  726.62/s  (2.217s,  461.97/s)  LR: 2.727e-05  Data: 0.799 (1.615)
Train: 152 [1150/1171 ( 98%)]  Loss:  2.980693 (2.8706)  Time: 0.585s, 1750.92/s  (2.221s,  461.00/s)  LR: 2.727e-05  Data: 0.021 (1.620)
Train: 152 [1170/1171 (100%)]  Loss:  2.749123 (2.8657)  Time: 0.564s, 1814.98/s  (2.223s,  460.68/s)  LR: 2.727e-05  Data: 0.000 (1.622)
Test: [   0/97]  Time: 12.488 (12.488)  Loss:  0.2741 (0.2741)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.200 (3.028)  Loss:  0.4172 (0.3463)  Acc@1: 93.7500 (95.2225)  Acc@5: 98.5352 (98.9411)
Test: [  97/97]  Time: 0.119 (2.936)  Loss:  0.3138 (0.3583)  Acc@1: 94.6429 (94.7380)  Acc@5: 99.2560 (98.8170)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-151.pth.tar', 94.76999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-152.pth.tar', 94.73800002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-149.pth.tar', 94.7009999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-150.pth.tar', 94.64600003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-148.pth.tar', 94.64399997314453)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-147.pth.tar', 94.6409999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-146.pth.tar', 94.60800000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-144.pth.tar', 94.57800003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-145.pth.tar', 94.57100000732422)

Train: 153 [   0/1171 (  0%)]  Loss:  2.658036 (2.6580)  Time: 11.244s,   91.07/s  (11.244s,   91.07/s)  LR: 2.491e-05  Data: 10.247 (10.247)
Train: 153 [  50/1171 (  4%)]  Loss:  2.693536 (2.6758)  Time: 0.587s, 1743.47/s  (2.197s,  466.12/s)  LR: 2.491e-05  Data: 0.024 (1.590)
Train: 153 [ 100/1171 (  9%)]  Loss:  3.122188 (2.8246)  Time: 0.586s, 1748.17/s  (2.191s,  467.37/s)  LR: 2.491e-05  Data: 0.019 (1.592)
Train: 153 [ 150/1171 ( 13%)]  Loss:  2.891266 (2.8413)  Time: 0.586s, 1748.83/s  (2.138s,  478.97/s)  LR: 2.491e-05  Data: 0.023 (1.541)
Train: 153 [ 200/1171 ( 17%)]  Loss:  2.672681 (2.8075)  Time: 0.583s, 1757.18/s  (2.282s,  448.76/s)  LR: 2.491e-05  Data: 0.018 (1.684)
Train: 153 [ 250/1171 ( 21%)]  Loss:  2.658034 (2.7826)  Time: 0.588s, 1741.22/s  (2.254s,  454.33/s)  LR: 2.491e-05  Data: 0.026 (1.654)
Train: 153 [ 300/1171 ( 26%)]  Loss:  2.762626 (2.7798)  Time: 0.584s, 1753.50/s  (2.255s,  454.19/s)  LR: 2.491e-05  Data: 0.020 (1.653)
Train: 153 [ 350/1171 ( 30%)]  Loss:  2.784266 (2.7803)  Time: 0.585s, 1749.09/s  (2.235s,  458.14/s)  LR: 2.491e-05  Data: 0.021 (1.634)
Train: 153 [ 400/1171 ( 34%)]  Loss:  2.786249 (2.7810)  Time: 0.585s, 1751.40/s  (2.225s,  460.15/s)  LR: 2.491e-05  Data: 0.021 (1.624)
Train: 153 [ 450/1171 ( 38%)]  Loss:  2.962073 (2.7991)  Time: 0.582s, 1758.33/s  (2.211s,  463.17/s)  LR: 2.491e-05  Data: 0.018 (1.610)
Train: 153 [ 500/1171 ( 43%)]  Loss:  2.579224 (2.7791)  Time: 2.148s,  476.79/s  (2.210s,  463.33/s)  LR: 2.491e-05  Data: 1.457 (1.610)
Train: 153 [ 550/1171 ( 47%)]  Loss:  2.924057 (2.7912)  Time: 0.582s, 1758.41/s  (2.198s,  465.81/s)  LR: 2.491e-05  Data: 0.017 (1.599)
Train: 153 [ 600/1171 ( 51%)]  Loss:  3.159111 (2.8195)  Time: 1.504s,  680.91/s  (2.239s,  457.30/s)  LR: 2.491e-05  Data: 0.940 (1.640)
Train: 153 [ 650/1171 ( 56%)]  Loss:  2.964998 (2.8299)  Time: 0.585s, 1751.39/s  (2.245s,  456.13/s)  LR: 2.491e-05  Data: 0.020 (1.645)
Train: 153 [ 700/1171 ( 60%)]  Loss:  2.942066 (2.8374)  Time: 4.704s,  217.67/s  (2.263s,  452.50/s)  LR: 2.491e-05  Data: 4.120 (1.663)
Train: 153 [ 750/1171 ( 64%)]  Loss:  3.333013 (2.8683)  Time: 0.585s, 1750.55/s  (2.258s,  453.40/s)  LR: 2.491e-05  Data: 0.022 (1.658)
Train: 153 [ 800/1171 ( 68%)]  Loss:  2.826564 (2.8659)  Time: 7.072s,  144.79/s  (2.256s,  453.84/s)  LR: 2.491e-05  Data: 6.488 (1.656)
Train: 153 [ 850/1171 ( 73%)]  Loss:  2.875038 (2.8664)  Time: 0.586s, 1746.22/s  (2.247s,  455.77/s)  LR: 2.491e-05  Data: 0.017 (1.648)
Train: 153 [ 900/1171 ( 77%)]  Loss:  2.983699 (2.8726)  Time: 6.717s,  152.44/s  (2.247s,  455.70/s)  LR: 2.491e-05  Data: 6.049 (1.650)
Train: 153 [ 950/1171 ( 81%)]  Loss:  3.108984 (2.8844)  Time: 0.584s, 1753.26/s  (2.237s,  457.81/s)  LR: 2.491e-05  Data: 0.019 (1.641)
Train: 153 [1000/1171 ( 85%)]  Loss:  3.111813 (2.8952)  Time: 5.954s,  172.00/s  (2.254s,  454.29/s)  LR: 2.491e-05  Data: 5.390 (1.659)
Train: 153 [1050/1171 ( 90%)]  Loss:  2.702104 (2.8864)  Time: 0.585s, 1750.09/s  (2.251s,  454.88/s)  LR: 2.491e-05  Data: 0.020 (1.656)
Train: 153 [1100/1171 ( 94%)]  Loss:  3.299465 (2.9044)  Time: 6.546s,  156.42/s  (2.251s,  454.96/s)  LR: 2.491e-05  Data: 5.936 (1.656)
Train: 153 [1150/1171 ( 98%)]  Loss:  3.002254 (2.9085)  Time: 0.587s, 1743.31/s  (2.250s,  455.04/s)  LR: 2.491e-05  Data: 0.020 (1.656)
Train: 153 [1170/1171 (100%)]  Loss:  2.561285 (2.8946)  Time: 0.563s, 1817.56/s  (2.250s,  455.11/s)  LR: 2.491e-05  Data: 0.000 (1.656)
Test: [   0/97]  Time: 12.463 (12.463)  Loss:  0.2917 (0.2917)  Acc@1: 96.9727 (96.9727)  Acc@5: 99.6094 (99.6094)
Test: [  50/97]  Time: 0.200 (2.934)  Loss:  0.4432 (0.3591)  Acc@1: 93.2617 (95.2129)  Acc@5: 98.3398 (98.9392)
Test: [  97/97]  Time: 0.119 (2.887)  Loss:  0.3299 (0.3697)  Acc@1: 94.3452 (94.7380)  Acc@5: 99.4048 (98.8270)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-151.pth.tar', 94.76999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-152.pth.tar', 94.73800002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-153.pth.tar', 94.73799999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-149.pth.tar', 94.7009999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-150.pth.tar', 94.64600003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-148.pth.tar', 94.64399997314453)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-147.pth.tar', 94.6409999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-146.pth.tar', 94.60800000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-144.pth.tar', 94.57800003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-145.pth.tar', 94.57100000732422)

Train: 154 [   0/1171 (  0%)]  Loss:  2.571613 (2.5716)  Time: 9.439s,  108.48/s  (9.439s,  108.48/s)  LR: 2.271e-05  Data: 8.747 (8.747)
Train: 154 [  50/1171 (  4%)]  Loss:  2.357521 (2.4646)  Time: 0.585s, 1751.88/s  (2.177s,  470.43/s)  LR: 2.271e-05  Data: 0.021 (1.591)
Train: 154 [ 100/1171 (  9%)]  Loss:  2.390761 (2.4400)  Time: 0.586s, 1748.78/s  (2.382s,  429.89/s)  LR: 2.271e-05  Data: 0.019 (1.789)
Train: 154 [ 150/1171 ( 13%)]  Loss:  2.783494 (2.5258)  Time: 0.586s, 1748.76/s  (2.298s,  445.66/s)  LR: 2.271e-05  Data: 0.018 (1.705)
Train: 154 [ 200/1171 ( 17%)]  Loss:  2.891358 (2.5989)  Time: 5.069s,  202.02/s  (2.294s,  446.31/s)  LR: 2.271e-05  Data: 4.411 (1.698)
Train: 154 [ 250/1171 ( 21%)]  Loss:  3.116204 (2.6852)  Time: 0.585s, 1749.99/s  (2.254s,  454.22/s)  LR: 2.271e-05  Data: 0.021 (1.655)
Train: 154 [ 300/1171 ( 26%)]  Loss:  2.507615 (2.6598)  Time: 4.673s,  219.11/s  (2.231s,  458.95/s)  LR: 2.271e-05  Data: 4.110 (1.632)
Train: 154 [ 350/1171 ( 30%)]  Loss:  3.039254 (2.7072)  Time: 0.586s, 1747.10/s  (2.208s,  463.70/s)  LR: 2.271e-05  Data: 0.021 (1.610)
Train: 154 [ 400/1171 ( 34%)]  Loss:  2.950395 (2.7342)  Time: 6.512s,  157.26/s  (2.199s,  465.77/s)  LR: 2.271e-05  Data: 5.767 (1.602)
Train: 154 [ 450/1171 ( 38%)]  Loss:  2.669822 (2.7278)  Time: 0.584s, 1754.12/s  (2.171s,  471.64/s)  LR: 2.271e-05  Data: 0.019 (1.576)
Train: 154 [ 500/1171 ( 43%)]  Loss:  2.786637 (2.7332)  Time: 6.991s,  146.48/s  (2.221s,  461.05/s)  LR: 2.271e-05  Data: 6.334 (1.626)
Train: 154 [ 550/1171 ( 47%)]  Loss:  2.452906 (2.7098)  Time: 0.587s, 1745.32/s  (2.236s,  458.01/s)  LR: 2.271e-05  Data: 0.017 (1.641)
Train: 154 [ 600/1171 ( 51%)]  Loss:  2.956136 (2.7287)  Time: 7.189s,  142.44/s  (2.248s,  455.55/s)  LR: 2.271e-05  Data: 6.626 (1.653)
Train: 154 [ 650/1171 ( 56%)]  Loss:  2.721855 (2.7283)  Time: 0.587s, 1744.15/s  (2.242s,  456.73/s)  LR: 2.271e-05  Data: 0.018 (1.648)
Train: 154 [ 700/1171 ( 60%)]  Loss:  2.931676 (2.7418)  Time: 6.949s,  147.36/s  (2.241s,  456.87/s)  LR: 2.271e-05  Data: 6.287 (1.647)
Train: 154 [ 750/1171 ( 64%)]  Loss:  2.772209 (2.7437)  Time: 0.586s, 1746.40/s  (2.224s,  460.39/s)  LR: 2.271e-05  Data: 0.019 (1.631)
Train: 154 [ 800/1171 ( 68%)]  Loss:  2.869729 (2.7511)  Time: 6.931s,  147.75/s  (2.216s,  462.03/s)  LR: 2.271e-05  Data: 6.346 (1.623)
Train: 154 [ 850/1171 ( 73%)]  Loss:  2.838187 (2.7560)  Time: 0.588s, 1742.70/s  (2.199s,  465.61/s)  LR: 2.271e-05  Data: 0.021 (1.607)
Train: 154 [ 900/1171 ( 77%)]  Loss:  2.879263 (2.7625)  Time: 7.112s,  143.98/s  (2.219s,  461.43/s)  LR: 2.271e-05  Data: 6.535 (1.628)
Train: 154 [ 950/1171 ( 81%)]  Loss:  2.321730 (2.7404)  Time: 0.586s, 1748.18/s  (2.206s,  464.27/s)  LR: 2.271e-05  Data: 0.020 (1.614)
Train: 154 [1000/1171 ( 85%)]  Loss:  3.074121 (2.7563)  Time: 6.892s,  148.58/s  (2.213s,  462.79/s)  LR: 2.271e-05  Data: 6.326 (1.622)
Train: 154 [1050/1171 ( 90%)]  Loss:  2.877901 (2.7618)  Time: 0.583s, 1755.31/s  (2.203s,  464.91/s)  LR: 2.271e-05  Data: 0.020 (1.612)
Train: 154 [1100/1171 ( 94%)]  Loss:  3.280984 (2.7844)  Time: 7.265s,  140.95/s  (2.204s,  464.66/s)  LR: 2.271e-05  Data: 6.577 (1.614)
Train: 154 [1150/1171 ( 98%)]  Loss:  2.480569 (2.7717)  Time: 0.584s, 1754.88/s  (2.194s,  466.64/s)  LR: 2.271e-05  Data: 0.020 (1.605)
Train: 154 [1170/1171 (100%)]  Loss:  2.625176 (2.7659)  Time: 0.565s, 1813.77/s  (2.191s,  467.40/s)  LR: 2.271e-05  Data: 0.000 (1.601)
Test: [   0/97]  Time: 12.159 (12.159)  Loss:  0.2965 (0.2965)  Acc@1: 96.4844 (96.4844)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (2.786)  Loss:  0.4382 (0.3586)  Acc@1: 93.1641 (95.2761)  Acc@5: 98.3398 (98.9430)
Test: [  97/97]  Time: 0.119 (2.887)  Loss:  0.3201 (0.3695)  Acc@1: 94.7917 (94.7780)  Acc@5: 99.2560 (98.8190)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-154.pth.tar', 94.77799998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-151.pth.tar', 94.76999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-152.pth.tar', 94.73800002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-153.pth.tar', 94.73799999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-149.pth.tar', 94.7009999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-150.pth.tar', 94.64600003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-148.pth.tar', 94.64399997314453)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-147.pth.tar', 94.6409999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-146.pth.tar', 94.60800000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-144.pth.tar', 94.57800003662109)

Train: 155 [   0/1171 (  0%)]  Loss:  2.686852 (2.6869)  Time: 13.101s,   78.16/s  (13.101s,   78.16/s)  LR: 2.069e-05  Data: 11.975 (11.975)
Train: 155 [  50/1171 (  4%)]  Loss:  2.183652 (2.4353)  Time: 0.584s, 1752.91/s  (2.315s,  442.31/s)  LR: 2.069e-05  Data: 0.019 (1.724)
Train: 155 [ 100/1171 (  9%)]  Loss:  2.881912 (2.5841)  Time: 4.593s,  222.93/s  (2.255s,  454.14/s)  LR: 2.069e-05  Data: 3.936 (1.669)
Train: 155 [ 150/1171 ( 13%)]  Loss:  2.614933 (2.5918)  Time: 0.583s, 1756.92/s  (2.210s,  463.32/s)  LR: 2.069e-05  Data: 0.018 (1.623)
Train: 155 [ 200/1171 ( 17%)]  Loss:  2.784423 (2.6304)  Time: 7.304s,  140.20/s  (2.241s,  457.04/s)  LR: 2.069e-05  Data: 6.719 (1.654)
Train: 155 [ 250/1171 ( 21%)]  Loss:  2.484674 (2.6061)  Time: 0.585s, 1749.22/s  (2.198s,  465.95/s)  LR: 2.069e-05  Data: 0.020 (1.610)
Train: 155 [ 300/1171 ( 26%)]  Loss:  2.950058 (2.6552)  Time: 6.366s,  160.85/s  (2.184s,  468.84/s)  LR: 2.069e-05  Data: 5.715 (1.595)
Train: 155 [ 350/1171 ( 30%)]  Loss:  2.944454 (2.6914)  Time: 0.583s, 1756.74/s  (2.153s,  475.60/s)  LR: 2.069e-05  Data: 0.018 (1.563)
Train: 155 [ 400/1171 ( 34%)]  Loss:  2.220704 (2.6391)  Time: 4.718s,  217.04/s  (2.141s,  478.28/s)  LR: 2.069e-05  Data: 4.029 (1.549)
Train: 155 [ 450/1171 ( 38%)]  Loss:  3.214505 (2.6966)  Time: 0.583s, 1756.78/s  (2.176s,  470.51/s)  LR: 2.069e-05  Data: 0.019 (1.582)
Train: 155 [ 500/1171 ( 43%)]  Loss:  3.418972 (2.7623)  Time: 7.463s,  137.22/s  (2.173s,  471.26/s)  LR: 2.069e-05  Data: 6.899 (1.579)
Train: 155 [ 550/1171 ( 47%)]  Loss:  2.376198 (2.7301)  Time: 0.586s, 1747.96/s  (2.185s,  468.73/s)  LR: 2.069e-05  Data: 0.020 (1.589)
Train: 155 [ 600/1171 ( 51%)]  Loss:  3.276496 (2.7721)  Time: 3.699s,  276.82/s  (2.188s,  467.95/s)  LR: 2.069e-05  Data: 3.024 (1.593)
Train: 155 [ 650/1171 ( 56%)]  Loss:  2.688432 (2.7662)  Time: 0.584s, 1754.10/s  (2.189s,  467.76/s)  LR: 2.069e-05  Data: 0.021 (1.594)
Train: 155 [ 700/1171 ( 60%)]  Loss:  3.292241 (2.8012)  Time: 5.767s,  177.56/s  (2.186s,  468.38/s)  LR: 2.069e-05  Data: 5.110 (1.591)
Train: 155 [ 750/1171 ( 64%)]  Loss:  2.849893 (2.8043)  Time: 0.584s, 1754.44/s  (2.175s,  470.71/s)  LR: 2.069e-05  Data: 0.019 (1.581)
Train: 155 [ 800/1171 ( 68%)]  Loss:  2.406180 (2.7809)  Time: 6.310s,  162.29/s  (2.174s,  470.99/s)  LR: 2.069e-05  Data: 5.744 (1.579)
Train: 155 [ 850/1171 ( 73%)]  Loss:  2.779897 (2.7808)  Time: 0.582s, 1759.78/s  (2.191s,  467.34/s)  LR: 2.069e-05  Data: 0.018 (1.596)
Train: 155 [ 900/1171 ( 77%)]  Loss:  2.812076 (2.7825)  Time: 8.042s,  127.32/s  (2.189s,  467.73/s)  LR: 2.069e-05  Data: 7.479 (1.594)
Train: 155 [ 950/1171 ( 81%)]  Loss:  3.105119 (2.7986)  Time: 1.222s,  837.63/s  (2.200s,  465.50/s)  LR: 2.069e-05  Data: 0.548 (1.604)
Train: 155 [1000/1171 ( 85%)]  Loss:  3.022538 (2.8092)  Time: 5.747s,  178.18/s  (2.201s,  465.26/s)  LR: 2.069e-05  Data: 5.039 (1.605)
Train: 155 [1050/1171 ( 90%)]  Loss:  2.631599 (2.8012)  Time: 2.019s,  507.13/s  (2.197s,  465.99/s)  LR: 2.069e-05  Data: 1.456 (1.602)
Train: 155 [1100/1171 ( 94%)]  Loss:  3.124717 (2.8152)  Time: 4.242s,  241.39/s  (2.195s,  466.45/s)  LR: 2.069e-05  Data: 3.664 (1.599)
Train: 155 [1150/1171 ( 98%)]  Loss:  3.205529 (2.8315)  Time: 1.781s,  575.09/s  (2.194s,  466.66/s)  LR: 2.069e-05  Data: 1.184 (1.598)
Train: 155 [1170/1171 (100%)]  Loss:  2.756876 (2.8285)  Time: 0.564s, 1814.81/s  (2.193s,  466.99/s)  LR: 2.069e-05  Data: 0.000 (1.597)
Test: [   0/97]  Time: 12.158 (12.158)  Loss:  0.2898 (0.2898)  Acc@1: 97.2656 (97.2656)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.197 (3.119)  Loss:  0.4575 (0.3638)  Acc@1: 92.6758 (95.2819)  Acc@5: 98.3398 (98.9354)
Test: [  97/97]  Time: 0.119 (3.034)  Loss:  0.3273 (0.3746)  Acc@1: 94.7917 (94.7810)  Acc@5: 99.4048 (98.8120)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-155.pth.tar', 94.78099998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-154.pth.tar', 94.77799998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-151.pth.tar', 94.76999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-152.pth.tar', 94.73800002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-153.pth.tar', 94.73799999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-149.pth.tar', 94.7009999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-150.pth.tar', 94.64600003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-148.pth.tar', 94.64399997314453)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-147.pth.tar', 94.6409999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-146.pth.tar', 94.60800000976562)

Train: 156 [   0/1171 (  0%)]  Loss:  2.447080 (2.4471)  Time: 10.988s,   93.19/s  (10.988s,   93.19/s)  LR: 1.884e-05  Data: 10.227 (10.227)
Train: 156 [  50/1171 (  4%)]  Loss:  3.136392 (2.7917)  Time: 2.042s,  501.45/s  (2.344s,  436.86/s)  LR: 1.884e-05  Data: 1.271 (1.750)
Train: 156 [ 100/1171 (  9%)]  Loss:  3.095665 (2.8930)  Time: 0.585s, 1749.81/s  (2.286s,  447.93/s)  LR: 1.884e-05  Data: 0.017 (1.686)
Train: 156 [ 150/1171 ( 13%)]  Loss:  2.622723 (2.8255)  Time: 0.584s, 1752.10/s  (2.201s,  465.17/s)  LR: 1.884e-05  Data: 0.019 (1.607)
Train: 156 [ 200/1171 ( 17%)]  Loss:  3.013781 (2.8631)  Time: 0.584s, 1754.09/s  (2.183s,  469.04/s)  LR: 1.884e-05  Data: 0.019 (1.587)
Train: 156 [ 250/1171 ( 21%)]  Loss:  3.096447 (2.9020)  Time: 2.120s,  482.95/s  (2.159s,  474.24/s)  LR: 1.884e-05  Data: 1.043 (1.558)
Train: 156 [ 300/1171 ( 26%)]  Loss:  3.070415 (2.9261)  Time: 0.585s, 1749.92/s  (2.136s,  479.50/s)  LR: 1.884e-05  Data: 0.020 (1.536)
Train: 156 [ 350/1171 ( 30%)]  Loss:  2.678436 (2.8951)  Time: 4.870s,  210.27/s  (2.185s,  468.59/s)  LR: 1.884e-05  Data: 4.267 (1.585)
Train: 156 [ 400/1171 ( 34%)]  Loss:  2.830482 (2.8879)  Time: 0.586s, 1748.15/s  (2.181s,  469.47/s)  LR: 1.884e-05  Data: 0.018 (1.580)
Train: 156 [ 450/1171 ( 38%)]  Loss:  2.712426 (2.8704)  Time: 6.301s,  162.52/s  (2.200s,  465.42/s)  LR: 1.884e-05  Data: 5.734 (1.600)
Train: 156 [ 500/1171 ( 43%)]  Loss:  3.063972 (2.8880)  Time: 0.587s, 1745.38/s  (2.205s,  464.47/s)  LR: 1.884e-05  Data: 0.018 (1.606)
Train: 156 [ 550/1171 ( 47%)]  Loss:  2.791206 (2.8799)  Time: 7.522s,  136.13/s  (2.214s,  462.53/s)  LR: 1.884e-05  Data: 6.847 (1.618)
Train: 156 [ 600/1171 ( 51%)]  Loss:  2.736201 (2.8689)  Time: 0.587s, 1744.27/s  (2.221s,  461.01/s)  LR: 1.884e-05  Data: 0.019 (1.624)
Train: 156 [ 650/1171 ( 56%)]  Loss:  2.804325 (2.8643)  Time: 6.619s,  154.72/s  (2.229s,  459.31/s)  LR: 1.884e-05  Data: 6.040 (1.633)
Train: 156 [ 700/1171 ( 60%)]  Loss:  2.940845 (2.8694)  Time: 0.585s, 1750.58/s  (2.222s,  460.80/s)  LR: 1.884e-05  Data: 0.021 (1.627)
Train: 156 [ 750/1171 ( 64%)]  Loss:  3.071664 (2.8820)  Time: 7.519s,  136.19/s  (2.257s,  453.77/s)  LR: 1.884e-05  Data: 6.837 (1.659)
Train: 156 [ 800/1171 ( 68%)]  Loss:  2.625665 (2.8669)  Time: 0.583s, 1757.18/s  (2.261s,  452.98/s)  LR: 1.884e-05  Data: 0.018 (1.662)
Train: 156 [ 850/1171 ( 73%)]  Loss:  2.987839 (2.8736)  Time: 8.379s,  122.21/s  (2.274s,  450.35/s)  LR: 1.884e-05  Data: 7.706 (1.676)
Train: 156 [ 900/1171 ( 77%)]  Loss:  3.219357 (2.8918)  Time: 0.586s, 1746.46/s  (2.275s,  450.20/s)  LR: 1.884e-05  Data: 0.022 (1.676)
Train: 156 [ 950/1171 ( 81%)]  Loss:  3.214071 (2.9079)  Time: 5.131s,  199.58/s  (2.280s,  449.14/s)  LR: 1.884e-05  Data: 4.562 (1.682)
Train: 156 [1000/1171 ( 85%)]  Loss:  2.719290 (2.8990)  Time: 0.586s, 1746.25/s  (2.274s,  450.29/s)  LR: 1.884e-05  Data: 0.021 (1.676)
Train: 156 [1050/1171 ( 90%)]  Loss:  2.650121 (2.8877)  Time: 1.111s,  921.57/s  (2.271s,  450.89/s)  LR: 1.884e-05  Data: 0.526 (1.673)
Train: 156 [1100/1171 ( 94%)]  Loss:  2.566981 (2.8737)  Time: 1.126s,  909.64/s  (2.264s,  452.26/s)  LR: 1.884e-05  Data: 0.564 (1.666)
Train: 156 [1150/1171 ( 98%)]  Loss:  2.897635 (2.8747)  Time: 1.267s,  808.51/s  (2.286s,  447.89/s)  LR: 1.884e-05  Data: 0.703 (1.687)
Train: 156 [1170/1171 (100%)]  Loss:  2.275307 (2.8507)  Time: 0.564s, 1815.28/s  (2.284s,  448.33/s)  LR: 1.884e-05  Data: 0.000 (1.685)
Test: [   0/97]  Time: 14.499 (14.499)  Loss:  0.2925 (0.2925)  Acc@1: 96.9727 (96.9727)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.196 (3.295)  Loss:  0.4431 (0.3576)  Acc@1: 93.4570 (95.3087)  Acc@5: 98.3398 (98.9411)
Test: [  97/97]  Time: 0.120 (3.157)  Loss:  0.3268 (0.3692)  Acc@1: 95.0893 (94.8030)  Acc@5: 99.5536 (98.8270)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-156.pth.tar', 94.80300000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-155.pth.tar', 94.78099998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-154.pth.tar', 94.77799998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-151.pth.tar', 94.76999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-152.pth.tar', 94.73800002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-153.pth.tar', 94.73799999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-149.pth.tar', 94.7009999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-150.pth.tar', 94.64600003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-148.pth.tar', 94.64399997314453)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-147.pth.tar', 94.6409999975586)

Train: 157 [   0/1171 (  0%)]  Loss:  2.345614 (2.3456)  Time: 10.030s,  102.09/s  (10.030s,  102.09/s)  LR: 1.716e-05  Data: 9.427 (9.427)
Train: 157 [  50/1171 (  4%)]  Loss:  3.002286 (2.6740)  Time: 1.167s,  877.65/s  (2.402s,  426.23/s)  LR: 1.716e-05  Data: 0.490 (1.819)
Train: 157 [ 100/1171 (  9%)]  Loss:  2.747679 (2.6985)  Time: 2.115s,  484.10/s  (2.352s,  435.38/s)  LR: 1.716e-05  Data: 1.548 (1.762)
Train: 157 [ 150/1171 ( 13%)]  Loss:  2.317858 (2.6034)  Time: 0.585s, 1750.20/s  (2.288s,  447.49/s)  LR: 1.716e-05  Data: 0.022 (1.699)
Train: 157 [ 200/1171 ( 17%)]  Loss:  2.617274 (2.6061)  Time: 0.589s, 1738.68/s  (2.409s,  425.11/s)  LR: 1.716e-05  Data: 0.025 (1.812)
Train: 157 [ 250/1171 ( 21%)]  Loss:  2.766374 (2.6328)  Time: 0.583s, 1755.45/s  (2.399s,  426.87/s)  LR: 1.716e-05  Data: 0.021 (1.804)
Train: 157 [ 300/1171 ( 26%)]  Loss:  3.056007 (2.6933)  Time: 0.588s, 1741.35/s  (2.433s,  420.86/s)  LR: 1.716e-05  Data: 0.022 (1.841)
Train: 157 [ 350/1171 ( 30%)]  Loss:  3.062041 (2.7394)  Time: 0.587s, 1745.33/s  (2.419s,  423.26/s)  LR: 1.716e-05  Data: 0.021 (1.826)
Train: 157 [ 400/1171 ( 34%)]  Loss:  3.228862 (2.7938)  Time: 0.587s, 1743.95/s  (2.424s,  422.47/s)  LR: 1.716e-05  Data: 0.021 (1.829)
Train: 157 [ 450/1171 ( 38%)]  Loss:  2.938819 (2.8083)  Time: 1.476s,  693.81/s  (2.402s,  426.37/s)  LR: 1.716e-05  Data: 0.622 (1.804)
Train: 157 [ 500/1171 ( 43%)]  Loss:  2.545634 (2.7844)  Time: 0.586s, 1747.72/s  (2.392s,  428.09/s)  LR: 1.716e-05  Data: 0.022 (1.794)
Train: 157 [ 550/1171 ( 47%)]  Loss:  2.916399 (2.7954)  Time: 10.581s,   96.77/s  (2.417s,  423.64/s)  LR: 1.716e-05  Data: 9.973 (1.818)
Train: 157 [ 600/1171 ( 51%)]  Loss:  2.030040 (2.7365)  Time: 0.586s, 1747.34/s  (2.428s,  421.72/s)  LR: 1.716e-05  Data: 0.020 (1.828)
Train: 157 [ 650/1171 ( 56%)]  Loss:  2.593827 (2.7263)  Time: 7.571s,  135.26/s  (2.446s,  418.66/s)  LR: 1.716e-05  Data: 7.007 (1.847)
Train: 157 [ 700/1171 ( 60%)]  Loss:  2.325061 (2.6996)  Time: 0.585s, 1749.12/s  (2.438s,  420.01/s)  LR: 1.716e-05  Data: 0.021 (1.839)
Train: 157 [ 750/1171 ( 64%)]  Loss:  2.384868 (2.6799)  Time: 7.501s,  136.52/s  (2.438s,  419.95/s)  LR: 1.716e-05  Data: 6.815 (1.839)
Train: 157 [ 800/1171 ( 68%)]  Loss:  3.027394 (2.7004)  Time: 3.552s,  288.27/s  (2.433s,  420.82/s)  LR: 1.716e-05  Data: 2.875 (1.833)
Train: 157 [ 850/1171 ( 73%)]  Loss:  2.739315 (2.7025)  Time: 7.608s,  134.59/s  (2.433s,  420.94/s)  LR: 1.716e-05  Data: 6.928 (1.833)
Train: 157 [ 900/1171 ( 77%)]  Loss:  3.339752 (2.7361)  Time: 1.234s,  829.97/s  (2.425s,  422.32/s)  LR: 1.716e-05  Data: 0.670 (1.826)
Train: 157 [ 950/1171 ( 81%)]  Loss:  2.969793 (2.7477)  Time: 5.865s,  174.61/s  (2.444s,  418.90/s)  LR: 1.716e-05  Data: 5.165 (1.845)
Train: 157 [1000/1171 ( 85%)]  Loss:  2.781606 (2.7494)  Time: 0.586s, 1748.70/s  (2.446s,  418.68/s)  LR: 1.716e-05  Data: 0.021 (1.846)
Train: 157 [1050/1171 ( 90%)]  Loss:  2.965478 (2.7592)  Time: 8.185s,  125.11/s  (2.451s,  417.83/s)  LR: 1.716e-05  Data: 7.622 (1.851)
Train: 157 [1100/1171 ( 94%)]  Loss:  2.753388 (2.7589)  Time: 2.847s,  359.69/s  (2.454s,  417.31/s)  LR: 1.716e-05  Data: 2.204 (1.854)
Train: 157 [1150/1171 ( 98%)]  Loss:  2.673665 (2.7554)  Time: 6.106s,  167.69/s  (2.453s,  417.39/s)  LR: 1.716e-05  Data: 5.522 (1.853)
Train: 157 [1170/1171 (100%)]  Loss:  3.237196 (2.7746)  Time: 0.565s, 1811.62/s  (2.447s,  418.48/s)  LR: 1.716e-05  Data: 0.000 (1.847)
Test: [   0/97]  Time: 13.362 (13.362)  Loss:  0.2950 (0.2950)  Acc@1: 96.8750 (96.8750)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (3.275)  Loss:  0.4454 (0.3534)  Acc@1: 92.7734 (95.3719)  Acc@5: 98.3398 (98.9430)
Test: [  97/97]  Time: 0.119 (3.369)  Loss:  0.3170 (0.3648)  Acc@1: 94.7917 (94.8470)  Acc@5: 99.4048 (98.8170)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-157.pth.tar', 94.84700003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-156.pth.tar', 94.80300000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-155.pth.tar', 94.78099998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-154.pth.tar', 94.77799998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-151.pth.tar', 94.76999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-152.pth.tar', 94.73800002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-153.pth.tar', 94.73799999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-149.pth.tar', 94.7009999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-150.pth.tar', 94.64600003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-148.pth.tar', 94.64399997314453)

Train: 158 [   0/1171 (  0%)]  Loss:  3.078362 (3.0784)  Time: 12.217s,   83.82/s  (12.217s,   83.82/s)  LR: 1.566e-05  Data: 11.226 (11.226)
Train: 158 [  50/1171 (  4%)]  Loss:  2.216342 (2.6474)  Time: 0.583s, 1755.87/s  (2.651s,  386.20/s)  LR: 1.566e-05  Data: 0.020 (2.041)
Train: 158 [ 100/1171 (  9%)]  Loss:  2.500513 (2.5984)  Time: 5.307s,  192.94/s  (2.549s,  401.73/s)  LR: 1.566e-05  Data: 4.741 (1.939)
Train: 158 [ 150/1171 ( 13%)]  Loss:  2.750944 (2.6365)  Time: 0.584s, 1751.93/s  (2.502s,  409.34/s)  LR: 1.566e-05  Data: 0.021 (1.878)
Train: 158 [ 200/1171 ( 17%)]  Loss:  2.685897 (2.6464)  Time: 7.297s,  140.33/s  (2.517s,  406.86/s)  LR: 1.566e-05  Data: 5.906 (1.888)
Train: 158 [ 250/1171 ( 21%)]  Loss:  2.548401 (2.6301)  Time: 0.591s, 1733.72/s  (2.491s,  411.11/s)  LR: 1.566e-05  Data: 0.021 (1.864)
Train: 158 [ 300/1171 ( 26%)]  Loss:  2.600523 (2.6259)  Time: 0.585s, 1749.57/s  (2.480s,  412.85/s)  LR: 1.566e-05  Data: 0.022 (1.858)
Train: 158 [ 350/1171 ( 30%)]  Loss:  2.588349 (2.6212)  Time: 0.583s, 1757.74/s  (2.525s,  405.47/s)  LR: 1.566e-05  Data: 0.019 (1.905)
Train: 158 [ 400/1171 ( 34%)]  Loss:  2.414181 (2.5982)  Time: 0.583s, 1755.79/s  (2.537s,  403.62/s)  LR: 1.566e-05  Data: 0.019 (1.916)
Train: 158 [ 450/1171 ( 38%)]  Loss:  2.802960 (2.6186)  Time: 0.587s, 1743.84/s  (2.553s,  401.12/s)  LR: 1.566e-05  Data: 0.024 (1.933)
Train: 158 [ 500/1171 ( 43%)]  Loss:  2.720444 (2.6279)  Time: 0.586s, 1746.83/s  (2.565s,  399.24/s)  LR: 1.566e-05  Data: 0.022 (1.945)
Train: 158 [ 550/1171 ( 47%)]  Loss:  2.917725 (2.6521)  Time: 0.585s, 1749.55/s  (2.575s,  397.71/s)  LR: 1.566e-05  Data: 0.021 (1.957)
Train: 158 [ 600/1171 ( 51%)]  Loss:  2.587306 (2.6471)  Time: 0.587s, 1743.32/s  (2.581s,  396.74/s)  LR: 1.566e-05  Data: 0.024 (1.962)
Train: 158 [ 650/1171 ( 56%)]  Loss:  2.692224 (2.6503)  Time: 1.553s,  659.37/s  (2.590s,  395.29/s)  LR: 1.566e-05  Data: 0.886 (1.972)
Train: 158 [ 700/1171 ( 60%)]  Loss:  3.019593 (2.6749)  Time: 0.588s, 1742.14/s  (2.629s,  389.46/s)  LR: 1.566e-05  Data: 0.021 (2.011)
Train: 158 [ 750/1171 ( 64%)]  Loss:  2.476104 (2.6625)  Time: 0.584s, 1752.39/s  (2.630s,  389.34/s)  LR: 1.566e-05  Data: 0.020 (2.012)
Train: 158 [ 800/1171 ( 68%)]  Loss:  2.526417 (2.6545)  Time: 0.583s, 1756.29/s  (2.626s,  389.90/s)  LR: 1.566e-05  Data: 0.021 (2.010)
Train: 158 [ 850/1171 ( 73%)]  Loss:  2.661206 (2.6549)  Time: 1.438s,  712.17/s  (2.628s,  389.64/s)  LR: 1.566e-05  Data: 0.718 (2.012)
Train: 158 [ 900/1171 ( 77%)]  Loss:  2.502887 (2.6469)  Time: 0.582s, 1760.83/s  (2.617s,  391.31/s)  LR: 1.566e-05  Data: 0.019 (2.002)
Train: 158 [ 950/1171 ( 81%)]  Loss:  2.512633 (2.6402)  Time: 3.606s,  283.98/s  (2.619s,  390.92/s)  LR: 1.566e-05  Data: 3.035 (2.004)
Train: 158 [1000/1171 ( 85%)]  Loss:  2.432727 (2.6303)  Time: 0.584s, 1752.13/s  (2.619s,  390.93/s)  LR: 1.566e-05  Data: 0.019 (2.004)
Train: 158 [1050/1171 ( 90%)]  Loss:  2.790676 (2.6376)  Time: 4.882s,  209.73/s  (2.636s,  388.49/s)  LR: 1.566e-05  Data: 4.318 (2.021)
Train: 158 [1100/1171 ( 94%)]  Loss:  3.092917 (2.6574)  Time: 0.586s, 1748.62/s  (2.631s,  389.20/s)  LR: 1.566e-05  Data: 0.020 (2.016)
Train: 158 [1150/1171 ( 98%)]  Loss:  2.715950 (2.6598)  Time: 4.362s,  234.75/s  (2.631s,  389.18/s)  LR: 1.566e-05  Data: 3.709 (2.016)
Train: 158 [1170/1171 (100%)]  Loss:  2.612185 (2.6579)  Time: 0.563s, 1817.53/s  (2.627s,  389.87/s)  LR: 1.566e-05  Data: 0.000 (2.012)
Test: [   0/97]  Time: 13.395 (13.395)  Loss:  0.3052 (0.3052)  Acc@1: 97.0703 (97.0703)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.196 (3.377)  Loss:  0.4651 (0.3678)  Acc@1: 91.7969 (95.2857)  Acc@5: 98.4375 (98.9679)
Test: [  97/97]  Time: 0.121 (3.321)  Loss:  0.3328 (0.3784)  Acc@1: 94.6429 (94.7950)  Acc@5: 99.4048 (98.8410)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-157.pth.tar', 94.84700003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-156.pth.tar', 94.80300000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-158.pth.tar', 94.79500002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-155.pth.tar', 94.78099998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-154.pth.tar', 94.77799998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-151.pth.tar', 94.76999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-152.pth.tar', 94.73800002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-153.pth.tar', 94.73799999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-149.pth.tar', 94.7009999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-150.pth.tar', 94.64600003417969)

Train: 159 [   0/1171 (  0%)]  Loss:  2.617694 (2.6177)  Time: 12.199s,   83.94/s  (12.199s,   83.94/s)  LR: 1.434e-05  Data: 11.347 (11.347)
Train: 159 [  50/1171 (  4%)]  Loss:  3.027388 (2.8225)  Time: 0.583s, 1755.74/s  (2.969s,  344.89/s)  LR: 1.434e-05  Data: 0.019 (2.350)
Train: 159 [ 100/1171 (  9%)]  Loss:  2.830035 (2.8250)  Time: 5.573s,  183.76/s  (2.875s,  356.23/s)  LR: 1.434e-05  Data: 4.781 (2.260)
Train: 159 [ 150/1171 ( 13%)]  Loss:  2.784006 (2.8148)  Time: 0.584s, 1752.30/s  (2.738s,  374.00/s)  LR: 1.434e-05  Data: 0.019 (2.130)
Train: 159 [ 200/1171 ( 17%)]  Loss:  2.941820 (2.8402)  Time: 0.585s, 1750.28/s  (2.766s,  370.21/s)  LR: 1.434e-05  Data: 0.019 (2.167)
Train: 159 [ 250/1171 ( 21%)]  Loss:  3.052186 (2.8755)  Time: 0.583s, 1756.47/s  (2.679s,  382.29/s)  LR: 1.434e-05  Data: 0.018 (2.080)
Train: 159 [ 300/1171 ( 26%)]  Loss:  2.757228 (2.8586)  Time: 1.041s,  983.23/s  (2.629s,  389.50/s)  LR: 1.434e-05  Data: 0.451 (2.032)
Train: 159 [ 350/1171 ( 30%)]  Loss:  3.022389 (2.8791)  Time: 0.583s, 1756.63/s  (2.583s,  396.48/s)  LR: 1.434e-05  Data: 0.018 (1.987)
Train: 159 [ 400/1171 ( 34%)]  Loss:  3.146739 (2.9088)  Time: 0.588s, 1741.67/s  (2.589s,  395.45/s)  LR: 1.434e-05  Data: 0.020 (1.993)
Train: 159 [ 450/1171 ( 38%)]  Loss:  3.007338 (2.9187)  Time: 0.587s, 1745.77/s  (2.569s,  398.67/s)  LR: 1.434e-05  Data: 0.024 (1.973)
Train: 159 [ 500/1171 ( 43%)]  Loss:  3.242432 (2.9481)  Time: 0.584s, 1753.40/s  (2.572s,  398.18/s)  LR: 1.434e-05  Data: 0.019 (1.978)
Train: 159 [ 550/1171 ( 47%)]  Loss:  2.680200 (2.9258)  Time: 0.582s, 1757.94/s  (2.551s,  401.40/s)  LR: 1.434e-05  Data: 0.019 (1.959)
Train: 159 [ 600/1171 ( 51%)]  Loss:  3.119880 (2.9407)  Time: 0.587s, 1744.39/s  (2.558s,  400.35/s)  LR: 1.434e-05  Data: 0.021 (1.965)
Train: 159 [ 650/1171 ( 56%)]  Loss:  2.964933 (2.9424)  Time: 0.583s, 1755.66/s  (2.541s,  403.06/s)  LR: 1.434e-05  Data: 0.021 (1.948)
Train: 159 [ 700/1171 ( 60%)]  Loss:  2.815009 (2.9340)  Time: 0.587s, 1743.84/s  (2.535s,  404.00/s)  LR: 1.434e-05  Data: 0.023 (1.942)
Train: 159 [ 750/1171 ( 64%)]  Loss:  2.941295 (2.9344)  Time: 0.582s, 1759.12/s  (2.515s,  407.09/s)  LR: 1.434e-05  Data: 0.019 (1.923)
Train: 159 [ 800/1171 ( 68%)]  Loss:  2.466593 (2.9069)  Time: 0.586s, 1746.06/s  (2.536s,  403.85/s)  LR: 1.434e-05  Data: 0.023 (1.944)
Train: 159 [ 850/1171 ( 73%)]  Loss:  2.437808 (2.8808)  Time: 0.587s, 1745.69/s  (2.527s,  405.27/s)  LR: 1.434e-05  Data: 0.022 (1.935)
Train: 159 [ 900/1171 ( 77%)]  Loss:  3.202775 (2.8978)  Time: 0.587s, 1744.95/s  (2.525s,  405.62/s)  LR: 1.434e-05  Data: 0.019 (1.932)
Train: 159 [ 950/1171 ( 81%)]  Loss:  2.788128 (2.8923)  Time: 0.582s, 1759.63/s  (2.508s,  408.33/s)  LR: 1.434e-05  Data: 0.019 (1.916)
Train: 159 [1000/1171 ( 85%)]  Loss:  3.008410 (2.8978)  Time: 0.583s, 1754.97/s  (2.501s,  409.48/s)  LR: 1.434e-05  Data: 0.019 (1.909)
Train: 159 [1050/1171 ( 90%)]  Loss:  3.088597 (2.9065)  Time: 4.083s,  250.82/s  (2.494s,  410.51/s)  LR: 1.434e-05  Data: 3.437 (1.902)
Train: 159 [1100/1171 ( 94%)]  Loss:  3.057530 (2.9131)  Time: 0.880s, 1163.07/s  (2.488s,  411.56/s)  LR: 1.434e-05  Data: 0.279 (1.895)
Train: 159 [1150/1171 ( 98%)]  Loss:  2.853451 (2.9106)  Time: 2.734s,  374.54/s  (2.494s,  410.57/s)  LR: 1.434e-05  Data: 2.170 (1.900)
Train: 159 [1170/1171 (100%)]  Loss:  3.146606 (2.9200)  Time: 0.564s, 1815.52/s  (2.489s,  411.47/s)  LR: 1.434e-05  Data: 0.000 (1.894)
Test: [   0/97]  Time: 12.016 (12.016)  Loss:  0.2778 (0.2778)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (3.111)  Loss:  0.4307 (0.3474)  Acc@1: 92.7734 (95.3623)  Acc@5: 98.5352 (98.9545)
Test: [  97/97]  Time: 0.119 (3.059)  Loss:  0.3148 (0.3585)  Acc@1: 95.3869 (94.8660)  Acc@5: 99.4048 (98.8330)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-159.pth.tar', 94.86600003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-157.pth.tar', 94.84700003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-156.pth.tar', 94.80300000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-158.pth.tar', 94.79500002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-155.pth.tar', 94.78099998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-154.pth.tar', 94.77799998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-151.pth.tar', 94.76999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-152.pth.tar', 94.73800002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-153.pth.tar', 94.73799999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-149.pth.tar', 94.7009999975586)

Train: 160 [   0/1171 (  0%)]  Loss:  3.065958 (3.0660)  Time: 11.271s,   90.86/s  (11.271s,   90.86/s)  LR: 1.319e-05  Data: 10.240 (10.240)
Train: 160 [  50/1171 (  4%)]  Loss:  3.031536 (3.0487)  Time: 0.586s, 1747.58/s  (2.335s,  438.52/s)  LR: 1.319e-05  Data: 0.021 (1.703)
Train: 160 [ 100/1171 (  9%)]  Loss:  2.748598 (2.9487)  Time: 2.489s,  411.44/s  (2.281s,  448.83/s)  LR: 1.319e-05  Data: 1.882 (1.657)
Train: 160 [ 150/1171 ( 13%)]  Loss:  2.607575 (2.8634)  Time: 2.174s,  471.05/s  (2.273s,  450.49/s)  LR: 1.319e-05  Data: 1.483 (1.645)
Train: 160 [ 200/1171 ( 17%)]  Loss:  2.586661 (2.8081)  Time: 0.584s, 1752.19/s  (2.242s,  456.71/s)  LR: 1.319e-05  Data: 0.021 (1.616)
Train: 160 [ 250/1171 ( 21%)]  Loss:  2.728945 (2.7949)  Time: 3.439s,  297.80/s  (2.340s,  437.60/s)  LR: 1.319e-05  Data: 2.847 (1.716)
Train: 160 [ 300/1171 ( 26%)]  Loss:  2.808847 (2.7969)  Time: 0.586s, 1747.67/s  (2.325s,  440.42/s)  LR: 1.319e-05  Data: 0.020 (1.705)
Train: 160 [ 350/1171 ( 30%)]  Loss:  2.740558 (2.7898)  Time: 0.652s, 1569.76/s  (2.362s,  433.57/s)  LR: 1.319e-05  Data: 0.076 (1.746)
Train: 160 [ 400/1171 ( 34%)]  Loss:  3.022327 (2.8157)  Time: 0.584s, 1752.51/s  (2.373s,  431.57/s)  LR: 1.319e-05  Data: 0.021 (1.761)
Train: 160 [ 450/1171 ( 38%)]  Loss:  3.236011 (2.8577)  Time: 7.614s,  134.49/s  (2.383s,  429.79/s)  LR: 1.319e-05  Data: 7.042 (1.772)
Train: 160 [ 500/1171 ( 43%)]  Loss:  2.765279 (2.8493)  Time: 0.590s, 1736.84/s  (2.385s,  429.27/s)  LR: 1.319e-05  Data: 0.023 (1.775)
Train: 160 [ 550/1171 ( 47%)]  Loss:  2.646158 (2.8324)  Time: 6.130s,  167.06/s  (2.386s,  429.26/s)  LR: 1.319e-05  Data: 5.552 (1.776)
Train: 160 [ 600/1171 ( 51%)]  Loss:  2.817769 (2.8312)  Time: 0.582s, 1758.72/s  (2.406s,  425.63/s)  LR: 1.319e-05  Data: 0.019 (1.796)
Train: 160 [ 650/1171 ( 56%)]  Loss:  2.845283 (2.8323)  Time: 6.728s,  152.19/s  (2.407s,  425.39/s)  LR: 1.319e-05  Data: 6.145 (1.796)
Train: 160 [ 700/1171 ( 60%)]  Loss:  2.882135 (2.8356)  Time: 0.592s, 1730.27/s  (2.387s,  428.91/s)  LR: 1.319e-05  Data: 0.019 (1.777)
Train: 160 [ 750/1171 ( 64%)]  Loss:  2.848886 (2.8364)  Time: 5.673s,  180.49/s  (2.385s,  429.40/s)  LR: 1.319e-05  Data: 4.984 (1.774)
Train: 160 [ 800/1171 ( 68%)]  Loss:  3.088368 (2.8512)  Time: 0.584s, 1752.61/s  (2.366s,  432.76/s)  LR: 1.319e-05  Data: 0.021 (1.756)
Train: 160 [ 850/1171 ( 73%)]  Loss:  3.277999 (2.8749)  Time: 6.345s,  161.39/s  (2.352s,  435.40/s)  LR: 1.319e-05  Data: 5.768 (1.743)
Train: 160 [ 900/1171 ( 77%)]  Loss:  2.883161 (2.8754)  Time: 0.583s, 1756.32/s  (2.340s,  437.53/s)  LR: 1.319e-05  Data: 0.020 (1.733)
Train: 160 [ 950/1171 ( 81%)]  Loss:  2.636068 (2.8634)  Time: 6.572s,  155.82/s  (2.329s,  439.69/s)  LR: 1.319e-05  Data: 5.879 (1.721)
Train: 160 [1000/1171 ( 85%)]  Loss:  3.143883 (2.8768)  Time: 0.583s, 1756.15/s  (2.329s,  439.70/s)  LR: 1.319e-05  Data: 0.019 (1.722)
Train: 160 [1050/1171 ( 90%)]  Loss:  2.694632 (2.8685)  Time: 6.283s,  162.97/s  (2.327s,  440.04/s)  LR: 1.319e-05  Data: 5.591 (1.721)
Train: 160 [1100/1171 ( 94%)]  Loss:  2.529047 (2.8537)  Time: 0.582s, 1758.18/s  (2.311s,  443.12/s)  LR: 1.319e-05  Data: 0.019 (1.706)
Train: 160 [1150/1171 ( 98%)]  Loss:  3.184415 (2.8675)  Time: 6.797s,  150.66/s  (2.312s,  442.99/s)  LR: 1.319e-05  Data: 6.214 (1.707)
Train: 160 [1170/1171 (100%)]  Loss:  2.908653 (2.8692)  Time: 0.562s, 1820.64/s  (2.303s,  444.60/s)  LR: 1.319e-05  Data: 0.000 (1.699)
Test: [   0/97]  Time: 11.739 (11.739)  Loss:  0.2742 (0.2742)  Acc@1: 96.9727 (96.9727)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.627 (2.842)  Loss:  0.4210 (0.3398)  Acc@1: 92.9688 (95.3546)  Acc@5: 98.3398 (98.9449)
Test: [  97/97]  Time: 0.119 (2.795)  Loss:  0.3020 (0.3505)  Acc@1: 94.9405 (94.8940)  Acc@5: 99.4048 (98.8350)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-160.pth.tar', 94.89399999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-159.pth.tar', 94.86600003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-157.pth.tar', 94.84700003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-156.pth.tar', 94.80300000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-158.pth.tar', 94.79500002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-155.pth.tar', 94.78099998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-154.pth.tar', 94.77799998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-151.pth.tar', 94.76999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-152.pth.tar', 94.73800002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-153.pth.tar', 94.73799999755859)

Train: 161 [   0/1171 (  0%)]  Loss:  2.800513 (2.8005)  Time: 10.781s,   94.98/s  (10.781s,   94.98/s)  LR: 1.221e-05  Data: 9.464 (9.464)
Train: 161 [  50/1171 (  4%)]  Loss:  2.953846 (2.8772)  Time: 0.587s, 1743.07/s  (2.189s,  467.87/s)  LR: 1.221e-05  Data: 0.024 (1.585)
Train: 161 [ 100/1171 (  9%)]  Loss:  3.028238 (2.9275)  Time: 0.585s, 1749.54/s  (2.116s,  483.88/s)  LR: 1.221e-05  Data: 0.021 (1.516)
Train: 161 [ 150/1171 ( 13%)]  Loss:  2.890982 (2.9184)  Time: 0.589s, 1739.73/s  (2.236s,  458.05/s)  LR: 1.221e-05  Data: 0.025 (1.636)
Train: 161 [ 200/1171 ( 17%)]  Loss:  2.380620 (2.8108)  Time: 0.583s, 1757.26/s  (2.188s,  468.00/s)  LR: 1.221e-05  Data: 0.019 (1.591)
Train: 161 [ 250/1171 ( 21%)]  Loss:  2.966690 (2.8368)  Time: 0.588s, 1742.42/s  (2.180s,  469.63/s)  LR: 1.221e-05  Data: 0.020 (1.581)
Train: 161 [ 300/1171 ( 26%)]  Loss:  2.506392 (2.7896)  Time: 0.587s, 1745.52/s  (2.184s,  468.91/s)  LR: 1.221e-05  Data: 0.023 (1.584)
Train: 161 [ 350/1171 ( 30%)]  Loss:  2.452414 (2.7475)  Time: 0.584s, 1754.53/s  (2.152s,  475.73/s)  LR: 1.221e-05  Data: 0.019 (1.556)
Train: 161 [ 400/1171 ( 34%)]  Loss:  3.084694 (2.7849)  Time: 0.585s, 1751.68/s  (2.164s,  473.23/s)  LR: 1.221e-05  Data: 0.021 (1.570)
Train: 161 [ 450/1171 ( 38%)]  Loss:  3.090242 (2.8155)  Time: 0.583s, 1756.91/s  (2.153s,  475.71/s)  LR: 1.221e-05  Data: 0.021 (1.556)
Train: 161 [ 500/1171 ( 43%)]  Loss:  2.492697 (2.7861)  Time: 0.584s, 1754.50/s  (2.147s,  476.99/s)  LR: 1.221e-05  Data: 0.021 (1.552)
Train: 161 [ 550/1171 ( 47%)]  Loss:  2.669632 (2.7764)  Time: 0.586s, 1747.55/s  (2.185s,  468.70/s)  LR: 1.221e-05  Data: 0.022 (1.591)
Train: 161 [ 600/1171 ( 51%)]  Loss:  3.059044 (2.7982)  Time: 0.587s, 1743.98/s  (2.200s,  465.36/s)  LR: 1.221e-05  Data: 0.023 (1.606)
Train: 161 [ 650/1171 ( 56%)]  Loss:  2.976310 (2.8109)  Time: 0.583s, 1756.69/s  (2.198s,  465.86/s)  LR: 1.221e-05  Data: 0.021 (1.604)
Train: 161 [ 700/1171 ( 60%)]  Loss:  3.220707 (2.8382)  Time: 0.585s, 1750.32/s  (2.206s,  464.11/s)  LR: 1.221e-05  Data: 0.019 (1.613)
Train: 161 [ 750/1171 ( 64%)]  Loss:  2.819574 (2.8370)  Time: 0.582s, 1758.71/s  (2.192s,  467.06/s)  LR: 1.221e-05  Data: 0.020 (1.600)
Train: 161 [ 800/1171 ( 68%)]  Loss:  2.564007 (2.8210)  Time: 0.585s, 1751.37/s  (2.190s,  467.66/s)  LR: 1.221e-05  Data: 0.017 (1.598)
Train: 161 [ 850/1171 ( 73%)]  Loss:  2.715881 (2.8151)  Time: 0.584s, 1753.35/s  (2.177s,  470.27/s)  LR: 1.221e-05  Data: 0.022 (1.587)
Train: 161 [ 900/1171 ( 77%)]  Loss:  3.140032 (2.8322)  Time: 0.588s, 1740.59/s  (2.173s,  471.23/s)  LR: 1.221e-05  Data: 0.021 (1.582)
Train: 161 [ 950/1171 ( 81%)]  Loss:  2.762042 (2.8287)  Time: 0.583s, 1756.85/s  (2.185s,  468.66/s)  LR: 1.221e-05  Data: 0.020 (1.594)
Train: 161 [1000/1171 ( 85%)]  Loss:  2.718440 (2.8235)  Time: 0.585s, 1749.49/s  (2.188s,  468.04/s)  LR: 1.221e-05  Data: 0.020 (1.597)
Train: 161 [1050/1171 ( 90%)]  Loss:  2.488209 (2.8082)  Time: 0.587s, 1743.47/s  (2.178s,  470.22/s)  LR: 1.221e-05  Data: 0.021 (1.587)
Train: 161 [1100/1171 ( 94%)]  Loss:  2.760362 (2.8062)  Time: 0.585s, 1751.88/s  (2.183s,  469.07/s)  LR: 1.221e-05  Data: 0.021 (1.593)
Train: 161 [1150/1171 ( 98%)]  Loss:  2.659194 (2.8000)  Time: 0.583s, 1757.79/s  (2.174s,  471.08/s)  LR: 1.221e-05  Data: 0.020 (1.583)
Train: 161 [1170/1171 (100%)]  Loss:  2.602643 (2.7921)  Time: 0.563s, 1818.08/s  (2.171s,  471.69/s)  LR: 1.221e-05  Data: 0.000 (1.580)
Test: [   0/97]  Time: 10.514 (10.514)  Loss:  0.2781 (0.2781)  Acc@1: 97.0703 (97.0703)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.200 (2.739)  Loss:  0.4306 (0.3479)  Acc@1: 93.4570 (95.4638)  Acc@5: 98.4375 (98.9526)
Test: [  97/97]  Time: 0.119 (2.691)  Loss:  0.3191 (0.3605)  Acc@1: 94.9405 (94.9020)  Acc@5: 99.2560 (98.8270)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-161.pth.tar', 94.9019999951172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-160.pth.tar', 94.89399999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-159.pth.tar', 94.86600003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-157.pth.tar', 94.84700003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-156.pth.tar', 94.80300000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-158.pth.tar', 94.79500002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-155.pth.tar', 94.78099998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-154.pth.tar', 94.77799998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-151.pth.tar', 94.76999998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-152.pth.tar', 94.73800002197265)

Train: 162 [   0/1171 (  0%)]  Loss:  2.660166 (2.6602)  Time: 10.176s,  100.63/s  (10.176s,  100.63/s)  LR: 1.142e-05  Data: 9.143 (9.143)
Train: 162 [  50/1171 (  4%)]  Loss:  2.997821 (2.8290)  Time: 0.589s, 1739.80/s  (2.044s,  500.94/s)  LR: 1.142e-05  Data: 0.019 (1.437)
Train: 162 [ 100/1171 (  9%)]  Loss:  2.959536 (2.8725)  Time: 0.585s, 1750.17/s  (2.283s,  448.45/s)  LR: 1.142e-05  Data: 0.021 (1.673)
Train: 162 [ 150/1171 ( 13%)]  Loss:  2.512833 (2.7826)  Time: 1.509s,  678.44/s  (2.205s,  464.34/s)  LR: 1.142e-05  Data: 0.852 (1.599)
Train: 162 [ 200/1171 ( 17%)]  Loss:  2.554332 (2.7369)  Time: 0.584s, 1752.69/s  (2.200s,  465.40/s)  LR: 1.142e-05  Data: 0.020 (1.594)
Train: 162 [ 250/1171 ( 21%)]  Loss:  2.634664 (2.7199)  Time: 0.617s, 1660.65/s  (2.154s,  475.38/s)  LR: 1.142e-05  Data: 0.048 (1.549)
Train: 162 [ 300/1171 ( 26%)]  Loss:  2.590942 (2.7015)  Time: 0.585s, 1751.67/s  (2.123s,  482.42/s)  LR: 1.142e-05  Data: 0.019 (1.521)
Train: 162 [ 350/1171 ( 30%)]  Loss:  3.118188 (2.7536)  Time: 0.586s, 1746.28/s  (2.102s,  487.15/s)  LR: 1.142e-05  Data: 0.019 (1.502)
Train: 162 [ 400/1171 ( 34%)]  Loss:  2.658175 (2.7430)  Time: 0.590s, 1735.86/s  (2.090s,  489.91/s)  LR: 1.142e-05  Data: 0.022 (1.489)
Train: 162 [ 450/1171 ( 38%)]  Loss:  2.969600 (2.7656)  Time: 0.587s, 1745.85/s  (2.065s,  495.90/s)  LR: 1.142e-05  Data: 0.021 (1.464)
Train: 162 [ 500/1171 ( 43%)]  Loss:  2.805848 (2.7693)  Time: 0.588s, 1742.65/s  (2.091s,  489.69/s)  LR: 1.142e-05  Data: 0.017 (1.491)
Train: 162 [ 550/1171 ( 47%)]  Loss:  2.904545 (2.7806)  Time: 0.588s, 1742.42/s  (2.101s,  487.43/s)  LR: 1.142e-05  Data: 0.019 (1.501)
Train: 162 [ 600/1171 ( 51%)]  Loss:  3.264413 (2.8178)  Time: 0.588s, 1740.96/s  (2.104s,  486.74/s)  LR: 1.142e-05  Data: 0.022 (1.506)
Train: 162 [ 650/1171 ( 56%)]  Loss:  3.099085 (2.8379)  Time: 0.585s, 1751.17/s  (2.110s,  485.41/s)  LR: 1.142e-05  Data: 0.020 (1.512)
Train: 162 [ 700/1171 ( 60%)]  Loss:  3.012391 (2.8495)  Time: 0.788s, 1298.69/s  (2.112s,  484.78/s)  LR: 1.142e-05  Data: 0.091 (1.515)
Train: 162 [ 750/1171 ( 64%)]  Loss:  3.056159 (2.8624)  Time: 0.588s, 1742.62/s  (2.099s,  487.86/s)  LR: 1.142e-05  Data: 0.021 (1.501)
Train: 162 [ 800/1171 ( 68%)]  Loss:  2.495922 (2.8409)  Time: 2.159s,  474.37/s  (2.100s,  487.51/s)  LR: 1.142e-05  Data: 1.569 (1.503)
Train: 162 [ 850/1171 ( 73%)]  Loss:  2.232408 (2.8071)  Time: 0.585s, 1750.54/s  (2.091s,  489.81/s)  LR: 1.142e-05  Data: 0.021 (1.493)
Train: 162 [ 900/1171 ( 77%)]  Loss:  2.495732 (2.7907)  Time: 0.584s, 1752.79/s  (2.086s,  490.83/s)  LR: 1.142e-05  Data: 0.022 (1.490)
Train: 162 [ 950/1171 ( 81%)]  Loss:  3.193132 (2.8108)  Time: 1.162s,  881.05/s  (2.100s,  487.53/s)  LR: 1.142e-05  Data: 0.488 (1.504)
Train: 162 [1000/1171 ( 85%)]  Loss:  2.702695 (2.8056)  Time: 0.585s, 1751.08/s  (2.097s,  488.33/s)  LR: 1.142e-05  Data: 0.021 (1.502)
Train: 162 [1050/1171 ( 90%)]  Loss:  2.343578 (2.7846)  Time: 0.587s, 1743.90/s  (2.090s,  489.91/s)  LR: 1.142e-05  Data: 0.024 (1.495)
Train: 162 [1100/1171 ( 94%)]  Loss:  2.918086 (2.7904)  Time: 0.591s, 1733.55/s  (2.099s,  487.94/s)  LR: 1.142e-05  Data: 0.021 (1.504)
Train: 162 [1150/1171 ( 98%)]  Loss:  2.739463 (2.7883)  Time: 0.591s, 1732.24/s  (2.095s,  488.77/s)  LR: 1.142e-05  Data: 0.019 (1.501)
Train: 162 [1170/1171 (100%)]  Loss:  3.202813 (2.8049)  Time: 0.564s, 1814.19/s  (2.095s,  488.85/s)  LR: 1.142e-05  Data: 0.000 (1.500)
Test: [   0/97]  Time: 11.890 (11.890)  Loss:  0.2840 (0.2840)  Acc@1: 96.9727 (96.9727)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (2.837)  Loss:  0.4400 (0.3516)  Acc@1: 92.6758 (95.4140)  Acc@5: 98.5352 (98.9526)
Test: [  97/97]  Time: 0.120 (2.746)  Loss:  0.3281 (0.3621)  Acc@1: 94.9405 (94.8960)  Acc@5: 99.4048 (98.8340)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-161.pth.tar', 94.9019999951172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-162.pth.tar', 94.8959999951172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-160.pth.tar', 94.89399999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-159.pth.tar', 94.86600003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-157.pth.tar', 94.84700003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-156.pth.tar', 94.80300000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-158.pth.tar', 94.79500002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-155.pth.tar', 94.78099998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-154.pth.tar', 94.77799998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-151.pth.tar', 94.76999998535156)

Train: 163 [   0/1171 (  0%)]  Loss:  3.197495 (3.1975)  Time: 9.860s,  103.86/s  (9.860s,  103.86/s)  LR: 1.080e-05  Data: 8.795 (8.795)
Train: 163 [  50/1171 (  4%)]  Loss:  2.099488 (2.6485)  Time: 0.586s, 1746.10/s  (2.287s,  447.73/s)  LR: 1.080e-05  Data: 0.021 (1.695)
Train: 163 [ 100/1171 (  9%)]  Loss:  2.645361 (2.6474)  Time: 0.586s, 1747.99/s  (2.291s,  447.02/s)  LR: 1.080e-05  Data: 0.022 (1.702)
Train: 163 [ 150/1171 ( 13%)]  Loss:  2.911059 (2.7134)  Time: 0.586s, 1746.82/s  (2.154s,  475.40/s)  LR: 1.080e-05  Data: 0.020 (1.566)
Train: 163 [ 200/1171 ( 17%)]  Loss:  3.007658 (2.7722)  Time: 0.591s, 1733.34/s  (2.196s,  466.39/s)  LR: 1.080e-05  Data: 0.019 (1.609)
Train: 163 [ 250/1171 ( 21%)]  Loss:  2.822900 (2.7807)  Time: 0.586s, 1746.28/s  (2.216s,  462.13/s)  LR: 1.080e-05  Data: 0.023 (1.625)
Train: 163 [ 300/1171 ( 26%)]  Loss:  2.782247 (2.7809)  Time: 1.470s,  696.54/s  (2.252s,  454.76/s)  LR: 1.080e-05  Data: 0.887 (1.656)
Train: 163 [ 350/1171 ( 30%)]  Loss:  3.006376 (2.8091)  Time: 0.586s, 1748.23/s  (2.262s,  452.61/s)  LR: 1.080e-05  Data: 0.022 (1.661)
Train: 163 [ 400/1171 ( 34%)]  Loss:  2.599279 (2.7858)  Time: 0.905s, 1132.03/s  (2.292s,  446.71/s)  LR: 1.080e-05  Data: 0.229 (1.690)
Train: 163 [ 450/1171 ( 38%)]  Loss:  2.884209 (2.7956)  Time: 0.587s, 1745.48/s  (2.364s,  433.16/s)  LR: 1.080e-05  Data: 0.023 (1.760)
Train: 163 [ 500/1171 ( 43%)]  Loss:  2.424896 (2.7619)  Time: 9.476s,  108.06/s  (2.422s,  422.75/s)  LR: 1.080e-05  Data: 8.786 (1.816)
Train: 163 [ 550/1171 ( 47%)]  Loss:  3.132064 (2.7928)  Time: 0.585s, 1749.02/s  (2.425s,  422.23/s)  LR: 1.080e-05  Data: 0.022 (1.818)
Train: 163 [ 600/1171 ( 51%)]  Loss:  2.918914 (2.8025)  Time: 7.153s,  143.16/s  (2.409s,  425.03/s)  LR: 1.080e-05  Data: 6.573 (1.803)
Train: 163 [ 650/1171 ( 56%)]  Loss:  3.161062 (2.8281)  Time: 0.588s, 1740.67/s  (2.386s,  429.08/s)  LR: 1.080e-05  Data: 0.024 (1.782)
Train: 163 [ 700/1171 ( 60%)]  Loss:  2.662665 (2.8170)  Time: 5.310s,  192.85/s  (2.364s,  433.24/s)  LR: 1.080e-05  Data: 4.714 (1.760)
Train: 163 [ 750/1171 ( 64%)]  Loss:  2.295326 (2.7844)  Time: 0.586s, 1748.11/s  (2.337s,  438.23/s)  LR: 1.080e-05  Data: 0.020 (1.733)
Train: 163 [ 800/1171 ( 68%)]  Loss:  3.208991 (2.8094)  Time: 3.518s,  291.06/s  (2.315s,  442.32/s)  LR: 1.080e-05  Data: 2.931 (1.712)
Train: 163 [ 850/1171 ( 73%)]  Loss:  2.788844 (2.8083)  Time: 2.477s,  413.46/s  (2.327s,  440.11/s)  LR: 1.080e-05  Data: 1.793 (1.721)
Train: 163 [ 900/1171 ( 77%)]  Loss:  3.001973 (2.8185)  Time: 6.347s,  161.33/s  (2.320s,  441.33/s)  LR: 1.080e-05  Data: 5.657 (1.714)
Train: 163 [ 950/1171 ( 81%)]  Loss:  3.156950 (2.8354)  Time: 0.585s, 1749.86/s  (2.303s,  444.55/s)  LR: 1.080e-05  Data: 0.020 (1.698)
Train: 163 [1000/1171 ( 85%)]  Loss:  3.068462 (2.8465)  Time: 4.919s,  208.16/s  (2.298s,  445.54/s)  LR: 1.080e-05  Data: 4.285 (1.694)
Train: 163 [1050/1171 ( 90%)]  Loss:  2.568480 (2.8339)  Time: 0.586s, 1747.64/s  (2.281s,  449.01/s)  LR: 1.080e-05  Data: 0.022 (1.676)
Train: 163 [1100/1171 ( 94%)]  Loss:  3.004045 (2.8412)  Time: 6.219s,  164.65/s  (2.272s,  450.73/s)  LR: 1.080e-05  Data: 5.656 (1.668)
Train: 163 [1150/1171 ( 98%)]  Loss:  2.944018 (2.8455)  Time: 0.588s, 1742.43/s  (2.259s,  453.37/s)  LR: 1.080e-05  Data: 0.023 (1.656)
Train: 163 [1170/1171 (100%)]  Loss:  3.060704 (2.8541)  Time: 0.564s, 1815.95/s  (2.253s,  454.51/s)  LR: 1.080e-05  Data: 0.000 (1.651)
Test: [   0/97]  Time: 11.248 (11.248)  Loss:  0.2792 (0.2792)  Acc@1: 96.9727 (96.9727)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.197 (2.694)  Loss:  0.4373 (0.3523)  Acc@1: 92.9688 (95.3565)  Acc@5: 98.4375 (98.9488)
Test: [  97/97]  Time: 0.119 (2.861)  Loss:  0.3155 (0.3627)  Acc@1: 95.2381 (94.8440)  Acc@5: 99.4048 (98.8350)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-161.pth.tar', 94.9019999951172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-162.pth.tar', 94.8959999951172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-160.pth.tar', 94.89399999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-159.pth.tar', 94.86600003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-157.pth.tar', 94.84700003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-163.pth.tar', 94.84400001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-156.pth.tar', 94.80300000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-158.pth.tar', 94.79500002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-155.pth.tar', 94.78099998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-154.pth.tar', 94.77799998291016)

Train: 164 [   0/1171 (  0%)]  Loss:  2.804983 (2.8050)  Time: 9.587s,  106.81/s  (9.587s,  106.81/s)  LR: 1.035e-05  Data: 8.420 (8.420)
Train: 164 [  50/1171 (  4%)]  Loss:  2.817131 (2.8111)  Time: 0.586s, 1746.49/s  (2.263s,  452.47/s)  LR: 1.035e-05  Data: 0.020 (1.651)
Train: 164 [ 100/1171 (  9%)]  Loss:  2.992775 (2.8716)  Time: 0.586s, 1748.65/s  (2.181s,  469.42/s)  LR: 1.035e-05  Data: 0.022 (1.577)
Train: 164 [ 150/1171 ( 13%)]  Loss:  2.639968 (2.8137)  Time: 0.586s, 1747.25/s  (2.124s,  482.11/s)  LR: 1.035e-05  Data: 0.021 (1.519)
Train: 164 [ 200/1171 ( 17%)]  Loss:  2.437959 (2.7386)  Time: 2.075s,  493.57/s  (2.149s,  476.39/s)  LR: 1.035e-05  Data: 1.508 (1.546)
Train: 164 [ 250/1171 ( 21%)]  Loss:  3.199685 (2.8154)  Time: 2.121s,  482.86/s  (2.099s,  487.85/s)  LR: 1.035e-05  Data: 1.424 (1.496)
Train: 164 [ 300/1171 ( 26%)]  Loss:  3.153732 (2.8637)  Time: 3.075s,  333.03/s  (2.082s,  491.95/s)  LR: 1.035e-05  Data: 2.512 (1.479)
Train: 164 [ 350/1171 ( 30%)]  Loss:  3.234354 (2.9101)  Time: 1.725s,  593.70/s  (2.062s,  496.66/s)  LR: 1.035e-05  Data: 1.149 (1.458)
Train: 164 [ 400/1171 ( 34%)]  Loss:  2.940736 (2.9135)  Time: 4.079s,  251.03/s  (2.110s,  485.34/s)  LR: 1.035e-05  Data: 3.500 (1.506)
Train: 164 [ 450/1171 ( 38%)]  Loss:  2.830490 (2.9052)  Time: 1.334s,  767.63/s  (2.097s,  488.30/s)  LR: 1.035e-05  Data: 0.659 (1.493)
Train: 164 [ 500/1171 ( 43%)]  Loss:  3.076743 (2.9208)  Time: 4.052s,  252.73/s  (2.105s,  486.49/s)  LR: 1.035e-05  Data: 3.138 (1.497)
Train: 164 [ 550/1171 ( 47%)]  Loss:  2.525311 (2.8878)  Time: 3.359s,  304.84/s  (2.119s,  483.31/s)  LR: 1.035e-05  Data: 2.765 (1.512)
Train: 164 [ 600/1171 ( 51%)]  Loss:  2.592352 (2.8651)  Time: 2.341s,  437.35/s  (2.129s,  480.97/s)  LR: 1.035e-05  Data: 1.361 (1.521)
Train: 164 [ 650/1171 ( 56%)]  Loss:  2.256128 (2.8216)  Time: 6.390s,  160.25/s  (2.136s,  479.35/s)  LR: 1.035e-05  Data: 5.827 (1.528)
Train: 164 [ 700/1171 ( 60%)]  Loss:  2.921429 (2.8283)  Time: 0.982s, 1042.56/s  (2.128s,  481.13/s)  LR: 1.035e-05  Data: 0.302 (1.521)
Train: 164 [ 750/1171 ( 64%)]  Loss:  3.087428 (2.8445)  Time: 6.164s,  166.14/s  (2.126s,  481.56/s)  LR: 1.035e-05  Data: 5.502 (1.520)
Train: 164 [ 800/1171 ( 68%)]  Loss:  3.300165 (2.8713)  Time: 0.583s, 1755.93/s  (2.139s,  478.78/s)  LR: 1.035e-05  Data: 0.020 (1.534)
Train: 164 [ 850/1171 ( 73%)]  Loss:  2.334863 (2.8415)  Time: 5.759s,  177.82/s  (2.140s,  478.50/s)  LR: 1.035e-05  Data: 5.085 (1.535)
Train: 164 [ 900/1171 ( 77%)]  Loss:  2.582994 (2.8279)  Time: 0.584s, 1754.43/s  (2.126s,  481.70/s)  LR: 1.035e-05  Data: 0.019 (1.523)
Train: 164 [ 950/1171 ( 81%)]  Loss:  3.259398 (2.8494)  Time: 6.483s,  157.95/s  (2.137s,  479.16/s)  LR: 1.035e-05  Data: 5.904 (1.534)
Train: 164 [1000/1171 ( 85%)]  Loss:  3.140042 (2.8633)  Time: 0.583s, 1755.05/s  (2.129s,  480.96/s)  LR: 1.035e-05  Data: 0.019 (1.527)
Train: 164 [1050/1171 ( 90%)]  Loss:  3.295622 (2.8829)  Time: 7.646s,  133.93/s  (2.131s,  480.61/s)  LR: 1.035e-05  Data: 7.083 (1.529)
Train: 164 [1100/1171 ( 94%)]  Loss:  2.488333 (2.8658)  Time: 0.584s, 1753.90/s  (2.122s,  482.51/s)  LR: 1.035e-05  Data: 0.021 (1.521)
Train: 164 [1150/1171 ( 98%)]  Loss:  3.068936 (2.8742)  Time: 6.343s,  161.44/s  (2.118s,  483.38/s)  LR: 1.035e-05  Data: 5.779 (1.518)
Train: 164 [1170/1171 (100%)]  Loss:  2.990797 (2.8789)  Time: 0.563s, 1818.42/s  (2.113s,  484.61/s)  LR: 1.035e-05  Data: 0.000 (1.513)
Test: [   0/97]  Time: 11.896 (11.896)  Loss:  0.2850 (0.2850)  Acc@1: 96.9727 (96.9727)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (3.001)  Loss:  0.4302 (0.3497)  Acc@1: 93.3594 (95.4255)  Acc@5: 98.3398 (98.9526)
Test: [  97/97]  Time: 0.120 (2.899)  Loss:  0.3168 (0.3603)  Acc@1: 95.0893 (94.9180)  Acc@5: 99.4048 (98.8470)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-164.pth.tar', 94.91800000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-161.pth.tar', 94.9019999951172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-162.pth.tar', 94.8959999951172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-160.pth.tar', 94.89399999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-159.pth.tar', 94.86600003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-157.pth.tar', 94.84700003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-163.pth.tar', 94.84400001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-156.pth.tar', 94.80300000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-158.pth.tar', 94.79500002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-155.pth.tar', 94.78099998291016)

Train: 165 [   0/1171 (  0%)]  Loss:  2.963761 (2.9638)  Time: 13.027s,   78.61/s  (13.027s,   78.61/s)  LR: 1.009e-05  Data: 11.107 (11.107)
Train: 165 [  50/1171 (  4%)]  Loss:  2.547478 (2.7556)  Time: 0.583s, 1756.70/s  (2.214s,  462.47/s)  LR: 1.009e-05  Data: 0.019 (1.601)
Train: 165 [ 100/1171 (  9%)]  Loss:  2.709446 (2.7402)  Time: 0.583s, 1757.04/s  (2.153s,  475.57/s)  LR: 1.009e-05  Data: 0.019 (1.553)
Train: 165 [ 150/1171 ( 13%)]  Loss:  3.272358 (2.8733)  Time: 0.585s, 1751.52/s  (2.115s,  484.21/s)  LR: 1.009e-05  Data: 0.021 (1.513)
Train: 165 [ 200/1171 ( 17%)]  Loss:  2.601640 (2.8189)  Time: 0.739s, 1386.03/s  (2.091s,  489.79/s)  LR: 1.009e-05  Data: 0.166 (1.488)
Train: 165 [ 250/1171 ( 21%)]  Loss:  2.846175 (2.8235)  Time: 0.584s, 1754.24/s  (2.077s,  492.94/s)  LR: 1.009e-05  Data: 0.019 (1.476)
Train: 165 [ 300/1171 ( 26%)]  Loss:  2.600158 (2.7916)  Time: 0.729s, 1405.15/s  (2.070s,  494.65/s)  LR: 1.009e-05  Data: 0.155 (1.466)
Train: 165 [ 350/1171 ( 30%)]  Loss:  2.338128 (2.7349)  Time: 0.585s, 1749.20/s  (2.108s,  485.77/s)  LR: 1.009e-05  Data: 0.019 (1.506)
Train: 165 [ 400/1171 ( 34%)]  Loss:  2.731289 (2.7345)  Time: 0.585s, 1751.25/s  (2.115s,  484.05/s)  LR: 1.009e-05  Data: 0.022 (1.512)
Train: 165 [ 450/1171 ( 38%)]  Loss:  2.917197 (2.7528)  Time: 0.584s, 1752.31/s  (2.112s,  484.77/s)  LR: 1.009e-05  Data: 0.020 (1.511)
Train: 165 [ 500/1171 ( 43%)]  Loss:  3.007093 (2.7759)  Time: 0.586s, 1748.08/s  (2.122s,  482.51/s)  LR: 1.009e-05  Data: 0.023 (1.524)
Train: 165 [ 550/1171 ( 47%)]  Loss:  2.600956 (2.7613)  Time: 0.586s, 1746.59/s  (2.132s,  480.27/s)  LR: 1.009e-05  Data: 0.019 (1.535)
Train: 165 [ 600/1171 ( 51%)]  Loss:  3.053106 (2.7838)  Time: 0.587s, 1745.31/s  (2.132s,  480.31/s)  LR: 1.009e-05  Data: 0.021 (1.536)
Train: 165 [ 650/1171 ( 56%)]  Loss:  2.936553 (2.7947)  Time: 0.585s, 1749.11/s  (2.139s,  478.80/s)  LR: 1.009e-05  Data: 0.020 (1.543)
Train: 165 [ 700/1171 ( 60%)]  Loss:  2.793340 (2.7946)  Time: 0.586s, 1746.86/s  (2.130s,  480.80/s)  LR: 1.009e-05  Data: 0.023 (1.535)
Train: 165 [ 750/1171 ( 64%)]  Loss:  2.810134 (2.7956)  Time: 0.586s, 1746.02/s  (2.128s,  481.22/s)  LR: 1.009e-05  Data: 0.019 (1.534)
Train: 165 [ 800/1171 ( 68%)]  Loss:  3.258455 (2.8228)  Time: 0.586s, 1748.22/s  (2.149s,  476.55/s)  LR: 1.009e-05  Data: 0.019 (1.555)
Train: 165 [ 850/1171 ( 73%)]  Loss:  2.939291 (2.8293)  Time: 0.588s, 1742.40/s  (2.144s,  477.51/s)  LR: 1.009e-05  Data: 0.024 (1.550)
Train: 165 [ 900/1171 ( 77%)]  Loss:  2.808559 (2.8282)  Time: 0.585s, 1751.72/s  (2.146s,  477.09/s)  LR: 1.009e-05  Data: 0.021 (1.552)
Train: 165 [ 950/1171 ( 81%)]  Loss:  2.599957 (2.8168)  Time: 0.586s, 1746.79/s  (2.145s,  477.33/s)  LR: 1.009e-05  Data: 0.018 (1.552)
Train: 165 [1000/1171 ( 85%)]  Loss:  2.906855 (2.8210)  Time: 1.382s,  741.11/s  (2.137s,  479.15/s)  LR: 1.009e-05  Data: 0.799 (1.543)
Train: 165 [1050/1171 ( 90%)]  Loss:  3.060293 (2.8319)  Time: 0.585s, 1750.17/s  (2.136s,  479.48/s)  LR: 1.009e-05  Data: 0.021 (1.541)
Train: 165 [1100/1171 ( 94%)]  Loss:  2.691684 (2.8258)  Time: 2.342s,  437.22/s  (2.130s,  480.80/s)  LR: 1.009e-05  Data: 1.778 (1.535)
Train: 165 [1150/1171 ( 98%)]  Loss:  2.566911 (2.8150)  Time: 0.586s, 1748.00/s  (2.124s,  482.14/s)  LR: 1.009e-05  Data: 0.022 (1.529)
Train: 165 [1170/1171 (100%)]  Loss:  3.123666 (2.8274)  Time: 0.566s, 1808.65/s  (2.120s,  482.93/s)  LR: 1.009e-05  Data: 0.000 (1.525)
Test: [   0/97]  Time: 11.167 (11.167)  Loss:  0.2799 (0.2799)  Acc@1: 97.0703 (97.0703)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.212 (3.077)  Loss:  0.4290 (0.3517)  Acc@1: 93.5547 (95.4216)  Acc@5: 98.5352 (98.9277)
Test: [  97/97]  Time: 0.119 (2.927)  Loss:  0.3159 (0.3613)  Acc@1: 94.9405 (94.9200)  Acc@5: 99.4048 (98.8250)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-165.pth.tar', 94.91999999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-164.pth.tar', 94.91800000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-161.pth.tar', 94.9019999951172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-162.pth.tar', 94.8959999951172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-160.pth.tar', 94.89399999511718)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-159.pth.tar', 94.86600003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-157.pth.tar', 94.84700003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-163.pth.tar', 94.84400001953125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-156.pth.tar', 94.80300000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-158.pth.tar', 94.79500002197265)

*** Best metric: 94.91999999511718 (epoch 165)

wandb: Waiting for W&B process to finish, PID 20090
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210603_123050-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1/logs/debug.log
wandb: Find internal logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210603_123050-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1/logs/debug-internal.log
wandb: Run summary:
wandb:    eval_top1 94.92
wandb:    eval_top5 98.825
wandb:   _timestamp 1622755165
wandb:   train_loss 2.82738
wandb:        _step 165
wandb:        epoch 165
wandb:     _runtime 526243
wandb:    eval_loss 0.36132
wandb: Run history:
wandb:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   train_loss ‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñá‚ñà‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÅ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÖ
wandb:    eval_loss ‚ñÑ‚ñÅ‚ñà‚ñÉ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ
wandb:    eval_top1 ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñà‚ñà
wandb:    eval_top5 ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñÜ
wandb:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced PreTraining_vit_deit_tiny_patch16_224_fake_1k: https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
--End--
Fri Jun 4 06:19:37 JST 2021

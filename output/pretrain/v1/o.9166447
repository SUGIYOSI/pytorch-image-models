--Start--
Fri May 28 15:06:43 JST 2021
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 0
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
wandb: Currently logged in as: gyama_x (use `wandb login --relogin` to force relogin)
wandb: WARNING Tried to auto resume run with id PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k but id PreTraining_vit_deit_tiny_patch16_224_1k is set.
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.10.27
wandb: Resuming run PreTraining_vit_deit_tiny_patch16_224_1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gyama_x/pytorch-image-models
wandb: üöÄ View run at https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_1k
wandb: Run data is saved locally in /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210528_150725-PreTraining_vit_deit_tiny_patch16_224_1k
wandb: Run `wandb offline` to turn off syncing.

Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar' (epoch 87)
Using native Torch DistributedDataParallel.
Scheduled epochs: 100
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Train: 88 [   0/1251 (  0%)]  Loss:  4.782439 (4.7824)  Time: 5.936s,  172.52/s  (5.936s,  172.52/s)  LR: 4.476e-05  Data: 4.715 (4.715)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 88 [  50/1251 (  4%)]  Loss:  4.789618 (4.7860)  Time: 0.582s, 1758.98/s  (1.910s,  536.23/s)  LR: 4.476e-05  Data: 0.019 (1.321)
Train: 88 [ 100/1251 (  8%)]  Loss:  4.853684 (4.8086)  Time: 0.586s, 1748.24/s  (1.914s,  534.93/s)  LR: 4.476e-05  Data: 0.023 (1.330)
Train: 88 [ 150/1251 ( 12%)]  Loss:  4.315748 (4.6854)  Time: 0.585s, 1751.29/s  (1.883s,  543.76/s)  LR: 4.476e-05  Data: 0.019 (1.294)
Train: 88 [ 200/1251 ( 16%)]  Loss:  4.396049 (4.6275)  Time: 3.151s,  324.98/s  (1.888s,  542.35/s)  LR: 4.476e-05  Data: 2.549 (1.293)
Train: 88 [ 250/1251 ( 20%)]  Loss:  4.253897 (4.5652)  Time: 0.583s, 1755.58/s  (1.933s,  529.62/s)  LR: 4.476e-05  Data: 0.021 (1.338)
Train: 88 [ 300/1251 ( 24%)]  Loss:  4.754532 (4.5923)  Time: 5.261s,  194.64/s  (1.955s,  523.77/s)  LR: 4.476e-05  Data: 4.696 (1.360)
Train: 88 [ 350/1251 ( 28%)]  Loss:  4.317820 (4.5580)  Time: 0.588s, 1742.51/s  (1.936s,  528.84/s)  LR: 4.476e-05  Data: 0.024 (1.342)
Train: 88 [ 400/1251 ( 32%)]  Loss:  4.918637 (4.5980)  Time: 4.904s,  208.80/s  (1.930s,  530.65/s)  LR: 4.476e-05  Data: 4.340 (1.335)
Train: 88 [ 450/1251 ( 36%)]  Loss:  4.531210 (4.5914)  Time: 0.584s, 1753.73/s  (1.927s,  531.52/s)  LR: 4.476e-05  Data: 0.020 (1.332)
Train: 88 [ 500/1251 ( 40%)]  Loss:  4.476924 (4.5810)  Time: 6.796s,  150.67/s  (1.937s,  528.54/s)  LR: 4.476e-05  Data: 6.142 (1.344)
Train: 88 [ 550/1251 ( 44%)]  Loss:  4.482874 (4.5728)  Time: 0.583s, 1757.76/s  (1.933s,  529.87/s)  LR: 4.476e-05  Data: 0.020 (1.340)
Train: 88 [ 600/1251 ( 48%)]  Loss:  5.178829 (4.6194)  Time: 5.701s,  179.62/s  (1.936s,  528.92/s)  LR: 4.476e-05  Data: 5.013 (1.343)
Train: 88 [ 650/1251 ( 52%)]  Loss:  4.457093 (4.6078)  Time: 0.582s, 1758.85/s  (1.927s,  531.45/s)  LR: 4.476e-05  Data: 0.020 (1.334)
Train: 88 [ 700/1251 ( 56%)]  Loss:  4.683015 (4.6128)  Time: 5.597s,  182.95/s  (1.949s,  525.44/s)  LR: 4.476e-05  Data: 4.838 (1.357)
Train: 88 [ 750/1251 ( 60%)]  Loss:  5.178433 (4.6482)  Time: 0.587s, 1743.03/s  (1.959s,  522.79/s)  LR: 4.476e-05  Data: 0.024 (1.367)
Train: 88 [ 800/1251 ( 64%)]  Loss:  4.481356 (4.6384)  Time: 8.096s,  126.48/s  (1.981s,  516.84/s)  LR: 4.476e-05  Data: 7.521 (1.391)
Train: 88 [ 850/1251 ( 68%)]  Loss:  4.643239 (4.6386)  Time: 0.584s, 1752.90/s  (1.993s,  513.87/s)  LR: 4.476e-05  Data: 0.021 (1.403)
Train: 88 [ 900/1251 ( 72%)]  Loss:  4.906906 (4.6528)  Time: 8.300s,  123.37/s  (2.010s,  509.46/s)  LR: 4.476e-05  Data: 6.797 (1.420)
Train: 88 [ 950/1251 ( 76%)]  Loss:  4.435311 (4.6419)  Time: 0.584s, 1753.88/s  (2.016s,  507.86/s)  LR: 4.476e-05  Data: 0.020 (1.426)
Train: 88 [1000/1251 ( 80%)]  Loss:  4.181311 (4.6199)  Time: 5.755s,  177.93/s  (2.025s,  505.72/s)  LR: 4.476e-05  Data: 5.079 (1.434)
Train: 88 [1050/1251 ( 84%)]  Loss:  4.586803 (4.6184)  Time: 0.585s, 1751.64/s  (2.026s,  505.46/s)  LR: 4.476e-05  Data: 0.019 (1.434)
Train: 88 [1100/1251 ( 88%)]  Loss:  4.567821 (4.6162)  Time: 8.608s,  118.96/s  (2.046s,  500.39/s)  LR: 4.476e-05  Data: 7.866 (1.455)
Train: 88 [1150/1251 ( 92%)]  Loss:  4.199035 (4.5989)  Time: 0.588s, 1741.77/s  (2.052s,  498.91/s)  LR: 4.476e-05  Data: 0.018 (1.461)
Train: 88 [1200/1251 ( 96%)]  Loss:  4.276423 (4.5860)  Time: 6.852s,  149.44/s  (2.068s,  495.27/s)  LR: 4.476e-05  Data: 6.267 (1.477)
Train: 88 [1250/1251 (100%)]  Loss:  4.601289 (4.5865)  Time: 0.564s, 1817.12/s  (2.073s,  493.86/s)  LR: 4.476e-05  Data: 0.000 (1.482)
Test: [   0/48]  Time: 14.366 (14.366)  Loss:  1.2067 (1.2067)  Acc@1: 75.0000 (75.0000)  Acc@5: 91.4062 (91.4062)
Test: [  48/48]  Time: 0.503 (3.293)  Loss:  1.1730 (2.0845)  Acc@1: 76.0613 (54.7800)  Acc@5: 89.9764 (78.9060)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-88.pth.tar', 54.780000073242185)

Train: 89 [   0/1251 (  0%)]  Loss:  4.566934 (4.5669)  Time: 5.339s,  191.81/s  (5.339s,  191.81/s)  LR: 3.926e-05  Data: 4.679 (4.679)
Train: 89 [  50/1251 (  4%)]  Loss:  4.545508 (4.5562)  Time: 0.586s, 1746.58/s  (2.104s,  486.75/s)  LR: 3.926e-05  Data: 0.023 (1.522)
Train: 89 [ 100/1251 (  8%)]  Loss:  4.824234 (4.6456)  Time: 0.583s, 1756.29/s  (2.116s,  483.91/s)  LR: 3.926e-05  Data: 0.020 (1.531)
Train: 89 [ 150/1251 ( 12%)]  Loss:  4.349272 (4.5715)  Time: 0.588s, 1740.66/s  (2.067s,  495.41/s)  LR: 3.926e-05  Data: 0.022 (1.480)
Train: 89 [ 200/1251 ( 16%)]  Loss:  4.685724 (4.5943)  Time: 0.587s, 1743.03/s  (2.157s,  474.80/s)  LR: 3.926e-05  Data: 0.020 (1.566)
Train: 89 [ 250/1251 ( 20%)]  Loss:  4.134794 (4.5177)  Time: 0.587s, 1744.77/s  (2.181s,  469.44/s)  LR: 3.926e-05  Data: 0.021 (1.591)
Train: 89 [ 300/1251 ( 24%)]  Loss:  5.220460 (4.6181)  Time: 0.585s, 1751.17/s  (2.191s,  467.37/s)  LR: 3.926e-05  Data: 0.019 (1.597)
Train: 89 [ 350/1251 ( 28%)]  Loss:  4.419277 (4.5933)  Time: 0.590s, 1735.41/s  (2.187s,  468.21/s)  LR: 3.926e-05  Data: 0.023 (1.592)
Train: 89 [ 400/1251 ( 32%)]  Loss:  4.636225 (4.5980)  Time: 0.585s, 1749.96/s  (2.175s,  470.83/s)  LR: 3.926e-05  Data: 0.021 (1.579)
Train: 89 [ 450/1251 ( 36%)]  Loss:  4.939851 (4.6322)  Time: 0.583s, 1756.51/s  (2.171s,  471.63/s)  LR: 3.926e-05  Data: 0.020 (1.577)
Train: 89 [ 500/1251 ( 40%)]  Loss:  4.230719 (4.5957)  Time: 0.586s, 1746.40/s  (2.160s,  474.11/s)  LR: 3.926e-05  Data: 0.018 (1.566)
Train: 89 [ 550/1251 ( 44%)]  Loss:  4.748585 (4.6085)  Time: 0.584s, 1752.16/s  (2.152s,  475.83/s)  LR: 3.926e-05  Data: 0.021 (1.559)
Train: 89 [ 600/1251 ( 48%)]  Loss:  4.867223 (4.6284)  Time: 0.583s, 1756.17/s  (2.180s,  469.64/s)  LR: 3.926e-05  Data: 0.018 (1.586)
Train: 89 [ 650/1251 ( 52%)]  Loss:  4.999140 (4.6549)  Time: 0.585s, 1749.67/s  (2.196s,  466.20/s)  LR: 3.926e-05  Data: 0.022 (1.603)
Train: 89 [ 700/1251 ( 56%)]  Loss:  4.414193 (4.6388)  Time: 0.584s, 1752.05/s  (2.209s,  463.49/s)  LR: 3.926e-05  Data: 0.019 (1.613)
Train: 89 [ 750/1251 ( 60%)]  Loss:  4.895763 (4.6549)  Time: 0.586s, 1748.79/s  (2.215s,  462.29/s)  LR: 3.926e-05  Data: 0.023 (1.619)
Train: 89 [ 800/1251 ( 64%)]  Loss:  5.147852 (4.6839)  Time: 0.582s, 1758.41/s  (2.209s,  463.48/s)  LR: 3.926e-05  Data: 0.020 (1.614)
Train: 89 [ 850/1251 ( 68%)]  Loss:  4.766194 (4.6884)  Time: 0.585s, 1749.91/s  (2.208s,  463.67/s)  LR: 3.926e-05  Data: 0.022 (1.614)
Train: 89 [ 900/1251 ( 72%)]  Loss:  4.749583 (4.6917)  Time: 0.584s, 1752.10/s  (2.201s,  465.22/s)  LR: 3.926e-05  Data: 0.020 (1.607)
Train: 89 [ 950/1251 ( 76%)]  Loss:  4.487358 (4.6814)  Time: 0.584s, 1752.07/s  (2.198s,  465.93/s)  LR: 3.926e-05  Data: 0.022 (1.604)
Train: 89 [1000/1251 ( 80%)]  Loss:  4.803824 (4.6873)  Time: 0.584s, 1754.56/s  (2.206s,  464.27/s)  LR: 3.926e-05  Data: 0.019 (1.613)
Train: 89 [1050/1251 ( 84%)]  Loss:  4.892656 (4.6966)  Time: 0.584s, 1754.49/s  (2.217s,  461.91/s)  LR: 3.926e-05  Data: 0.020 (1.624)
Train: 89 [1100/1251 ( 88%)]  Loss:  4.997952 (4.7097)  Time: 0.586s, 1747.64/s  (2.218s,  461.76/s)  LR: 3.926e-05  Data: 0.020 (1.625)
Train: 89 [1150/1251 ( 92%)]  Loss:  5.162498 (4.7286)  Time: 0.586s, 1747.56/s  (2.218s,  461.66/s)  LR: 3.926e-05  Data: 0.022 (1.626)
Train: 89 [1200/1251 ( 96%)]  Loss:  4.706537 (4.7277)  Time: 0.583s, 1756.74/s  (2.217s,  461.92/s)  LR: 3.926e-05  Data: 0.019 (1.625)
Train: 89 [1250/1251 (100%)]  Loss:  4.797698 (4.7304)  Time: 0.562s, 1821.26/s  (2.217s,  461.80/s)  LR: 3.926e-05  Data: 0.000 (1.626)
Test: [   0/48]  Time: 13.602 (13.602)  Loss:  1.1282 (1.1282)  Acc@1: 76.9531 (76.9531)  Acc@5: 92.4805 (92.4805)
Test: [  48/48]  Time: 0.149 (3.183)  Loss:  1.1748 (2.0688)  Acc@1: 75.2359 (55.1080)  Acc@5: 89.2689 (79.1480)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-89.pth.tar', 55.108000102539066)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-88.pth.tar', 54.780000073242185)

Train: 90 [   0/1251 (  0%)]  Loss:  4.819582 (4.8196)  Time: 10.127s,  101.11/s  (10.127s,  101.11/s)  LR: 3.423e-05  Data: 9.416 (9.416)
Train: 90 [  50/1251 (  4%)]  Loss:  4.157844 (4.4887)  Time: 0.586s, 1747.69/s  (2.386s,  429.20/s)  LR: 3.423e-05  Data: 0.020 (1.791)
Train: 90 [ 100/1251 (  8%)]  Loss:  4.384067 (4.4538)  Time: 2.528s,  405.03/s  (2.358s,  434.22/s)  LR: 3.423e-05  Data: 1.946 (1.736)
Train: 90 [ 150/1251 ( 12%)]  Loss:  5.064698 (4.6065)  Time: 0.585s, 1751.78/s  (2.295s,  446.28/s)  LR: 3.423e-05  Data: 0.019 (1.681)
Train: 90 [ 200/1251 ( 16%)]  Loss:  4.840749 (4.6534)  Time: 3.709s,  276.09/s  (2.291s,  446.98/s)  LR: 3.423e-05  Data: 3.003 (1.681)
Train: 90 [ 250/1251 ( 20%)]  Loss:  4.951673 (4.7031)  Time: 0.587s, 1743.81/s  (2.257s,  453.70/s)  LR: 3.423e-05  Data: 0.024 (1.650)
Train: 90 [ 300/1251 ( 24%)]  Loss:  4.777891 (4.7138)  Time: 2.595s,  394.67/s  (2.245s,  456.16/s)  LR: 3.423e-05  Data: 1.923 (1.634)
Train: 90 [ 350/1251 ( 28%)]  Loss:  4.716120 (4.7141)  Time: 0.585s, 1750.18/s  (2.222s,  460.84/s)  LR: 3.423e-05  Data: 0.020 (1.610)
Train: 90 [ 400/1251 ( 32%)]  Loss:  4.703566 (4.7129)  Time: 0.582s, 1759.71/s  (2.204s,  464.70/s)  LR: 3.423e-05  Data: 0.019 (1.593)
Train: 90 [ 450/1251 ( 36%)]  Loss:  4.744178 (4.7160)  Time: 0.583s, 1757.11/s  (2.198s,  465.96/s)  LR: 3.423e-05  Data: 0.020 (1.589)
Train: 90 [ 500/1251 ( 40%)]  Loss:  4.418046 (4.6889)  Time: 0.735s, 1393.96/s  (2.225s,  460.24/s)  LR: 3.423e-05  Data: 0.082 (1.617)
Train: 90 [ 550/1251 ( 44%)]  Loss:  4.149281 (4.6440)  Time: 0.583s, 1756.77/s  (2.244s,  456.26/s)  LR: 3.423e-05  Data: 0.020 (1.635)
Train: 90 [ 600/1251 ( 48%)]  Loss:  4.415367 (4.6264)  Time: 1.557s,  657.48/s  (2.252s,  454.75/s)  LR: 3.423e-05  Data: 0.888 (1.642)
Train: 90 [ 650/1251 ( 52%)]  Loss:  4.150666 (4.5924)  Time: 1.878s,  545.16/s  (2.257s,  453.71/s)  LR: 3.423e-05  Data: 1.193 (1.647)
Train: 90 [ 700/1251 ( 56%)]  Loss:  4.551573 (4.5897)  Time: 0.584s, 1754.18/s  (2.255s,  454.10/s)  LR: 3.423e-05  Data: 0.019 (1.647)
Train: 90 [ 750/1251 ( 60%)]  Loss:  4.975826 (4.6138)  Time: 0.853s, 1200.52/s  (2.250s,  455.12/s)  LR: 3.423e-05  Data: 0.082 (1.640)
Train: 90 [ 800/1251 ( 64%)]  Loss:  4.529158 (4.6088)  Time: 2.264s,  452.21/s  (2.251s,  454.92/s)  LR: 3.423e-05  Data: 1.682 (1.641)
Train: 90 [ 850/1251 ( 68%)]  Loss:  3.690125 (4.5578)  Time: 0.584s, 1753.17/s  (2.254s,  454.32/s)  LR: 3.423e-05  Data: 0.020 (1.645)
Train: 90 [ 900/1251 ( 72%)]  Loss:  4.730247 (4.5669)  Time: 0.587s, 1743.13/s  (2.263s,  452.48/s)  LR: 3.423e-05  Data: 0.018 (1.655)
Train: 90 [ 950/1251 ( 76%)]  Loss:  4.942488 (4.5857)  Time: 0.587s, 1745.26/s  (2.262s,  452.75/s)  LR: 3.423e-05  Data: 0.020 (1.655)
Train: 90 [1000/1251 ( 80%)]  Loss:  4.588541 (4.5858)  Time: 0.643s, 1593.19/s  (2.262s,  452.77/s)  LR: 3.423e-05  Data: 0.020 (1.655)
Train: 90 [1050/1251 ( 84%)]  Loss:  4.416363 (4.5781)  Time: 0.587s, 1745.83/s  (2.254s,  454.23/s)  LR: 3.423e-05  Data: 0.021 (1.648)
Train: 90 [1100/1251 ( 88%)]  Loss:  4.745934 (4.5854)  Time: 4.295s,  238.39/s  (2.252s,  454.67/s)  LR: 3.423e-05  Data: 3.637 (1.647)
Train: 90 [1150/1251 ( 92%)]  Loss:  4.437016 (4.5792)  Time: 0.587s, 1745.13/s  (2.241s,  456.84/s)  LR: 3.423e-05  Data: 0.019 (1.635)
Train: 90 [1200/1251 ( 96%)]  Loss:  4.532567 (4.5773)  Time: 1.857s,  551.33/s  (2.238s,  457.49/s)  LR: 3.423e-05  Data: 1.189 (1.633)
Train: 90 [1250/1251 (100%)]  Loss:  4.718150 (4.5828)  Time: 0.565s, 1812.04/s  (2.231s,  458.99/s)  LR: 3.423e-05  Data: 0.000 (1.625)
Test: [   0/48]  Time: 19.118 (19.118)  Loss:  1.1612 (1.1612)  Acc@1: 76.5625 (76.5625)  Acc@5: 91.7969 (91.7969)
Test: [  48/48]  Time: 0.149 (3.580)  Loss:  1.1866 (2.0647)  Acc@1: 75.5896 (55.2820)  Acc@5: 89.9764 (79.1780)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-90.pth.tar', 55.281999868164064)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-89.pth.tar', 55.108000102539066)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-88.pth.tar', 54.780000073242185)

Train: 91 [   0/1251 (  0%)]  Loss:  4.577558 (4.5776)  Time: 10.961s,   93.42/s  (10.961s,   93.42/s)  LR: 2.965e-05  Data: 10.226 (10.226)
Train: 91 [  50/1251 (  4%)]  Loss:  4.557768 (4.5677)  Time: 0.584s, 1754.80/s  (2.438s,  419.98/s)  LR: 2.965e-05  Data: 0.021 (1.845)
Train: 91 [ 100/1251 (  8%)]  Loss:  4.943399 (4.6929)  Time: 0.585s, 1749.42/s  (2.386s,  429.15/s)  LR: 2.965e-05  Data: 0.021 (1.801)
Train: 91 [ 150/1251 ( 12%)]  Loss:  4.711464 (4.6975)  Time: 0.586s, 1746.51/s  (2.292s,  446.78/s)  LR: 2.965e-05  Data: 0.022 (1.710)
Train: 91 [ 200/1251 ( 16%)]  Loss:  4.412050 (4.6404)  Time: 0.585s, 1751.49/s  (2.295s,  446.16/s)  LR: 2.965e-05  Data: 0.022 (1.713)
Train: 91 [ 250/1251 ( 20%)]  Loss:  4.861104 (4.6772)  Time: 0.586s, 1746.44/s  (2.244s,  456.37/s)  LR: 2.965e-05  Data: 0.024 (1.663)
Train: 91 [ 300/1251 ( 24%)]  Loss:  4.898461 (4.7088)  Time: 0.594s, 1724.62/s  (2.237s,  457.69/s)  LR: 2.965e-05  Data: 0.018 (1.656)
Train: 91 [ 350/1251 ( 28%)]  Loss:  4.856713 (4.7273)  Time: 0.586s, 1746.70/s  (2.252s,  454.75/s)  LR: 2.965e-05  Data: 0.024 (1.671)
Train: 91 [ 400/1251 ( 32%)]  Loss:  4.520509 (4.7043)  Time: 0.589s, 1739.91/s  (2.266s,  451.93/s)  LR: 2.965e-05  Data: 0.025 (1.684)
Train: 91 [ 450/1251 ( 36%)]  Loss:  3.934361 (4.6273)  Time: 0.589s, 1738.69/s  (2.256s,  453.86/s)  LR: 2.965e-05  Data: 0.024 (1.674)
Train: 91 [ 500/1251 ( 40%)]  Loss:  5.175538 (4.6772)  Time: 0.587s, 1743.34/s  (2.254s,  454.29/s)  LR: 2.965e-05  Data: 0.020 (1.672)
Train: 91 [ 550/1251 ( 44%)]  Loss:  4.899603 (4.6957)  Time: 0.589s, 1739.87/s  (2.244s,  456.41/s)  LR: 2.965e-05  Data: 0.025 (1.661)
Train: 91 [ 600/1251 ( 48%)]  Loss:  4.952061 (4.7154)  Time: 0.587s, 1745.11/s  (2.242s,  456.66/s)  LR: 2.965e-05  Data: 0.018 (1.659)
Train: 91 [ 650/1251 ( 52%)]  Loss:  4.960856 (4.7330)  Time: 2.261s,  453.00/s  (2.235s,  458.18/s)  LR: 2.965e-05  Data: 1.610 (1.651)
Train: 91 [ 700/1251 ( 56%)]  Loss:  5.142756 (4.7603)  Time: 0.589s, 1739.68/s  (2.229s,  459.45/s)  LR: 2.965e-05  Data: 0.018 (1.642)
Train: 91 [ 750/1251 ( 60%)]  Loss:  4.869513 (4.7671)  Time: 0.587s, 1743.03/s  (2.243s,  456.53/s)  LR: 2.965e-05  Data: 0.024 (1.653)
Train: 91 [ 800/1251 ( 64%)]  Loss:  4.664318 (4.7611)  Time: 0.587s, 1743.35/s  (2.256s,  453.85/s)  LR: 2.965e-05  Data: 0.019 (1.666)
Train: 91 [ 850/1251 ( 68%)]  Loss:  4.111451 (4.7250)  Time: 0.584s, 1752.17/s  (2.259s,  453.38/s)  LR: 2.965e-05  Data: 0.020 (1.667)
Train: 91 [ 900/1251 ( 72%)]  Loss:  4.162429 (4.6954)  Time: 0.587s, 1744.30/s  (2.265s,  452.12/s)  LR: 2.965e-05  Data: 0.018 (1.673)
Train: 91 [ 950/1251 ( 76%)]  Loss:  4.444253 (4.6828)  Time: 0.583s, 1757.55/s  (2.263s,  452.52/s)  LR: 2.965e-05  Data: 0.019 (1.670)
Train: 91 [1000/1251 ( 80%)]  Loss:  4.208735 (4.6602)  Time: 0.585s, 1750.49/s  (2.264s,  452.30/s)  LR: 2.965e-05  Data: 0.021 (1.671)
Train: 91 [1050/1251 ( 84%)]  Loss:  5.018013 (4.6765)  Time: 0.590s, 1736.39/s  (2.256s,  453.89/s)  LR: 2.965e-05  Data: 0.020 (1.663)
Train: 91 [1100/1251 ( 88%)]  Loss:  4.764292 (4.6803)  Time: 0.588s, 1741.80/s  (2.254s,  454.27/s)  LR: 2.965e-05  Data: 0.018 (1.660)
Train: 91 [1150/1251 ( 92%)]  Loss:  4.774469 (4.6842)  Time: 0.667s, 1535.42/s  (2.262s,  452.65/s)  LR: 2.965e-05  Data: 0.104 (1.668)
Train: 91 [1200/1251 ( 96%)]  Loss:  4.224747 (4.6659)  Time: 0.589s, 1737.64/s  (2.281s,  448.91/s)  LR: 2.965e-05  Data: 0.019 (1.687)
Train: 91 [1250/1251 (100%)]  Loss:  4.420484 (4.6564)  Time: 0.563s, 1820.34/s  (2.297s,  445.76/s)  LR: 2.965e-05  Data: 0.000 (1.702)
Test: [   0/48]  Time: 16.344 (16.344)  Loss:  1.1710 (1.1710)  Acc@1: 76.0742 (76.0742)  Acc@5: 91.9922 (91.9922)
Test: [  48/48]  Time: 0.149 (3.634)  Loss:  1.1476 (2.0682)  Acc@1: 76.2972 (55.2440)  Acc@5: 90.3302 (79.1920)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-90.pth.tar', 55.281999868164064)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-91.pth.tar', 55.24399991699219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-89.pth.tar', 55.108000102539066)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-88.pth.tar', 54.780000073242185)

Train: 92 [   0/1251 (  0%)]  Loss:  4.542266 (4.5423)  Time: 11.478s,   89.21/s  (11.478s,   89.21/s)  LR: 2.555e-05  Data: 10.826 (10.826)
Train: 92 [  50/1251 (  4%)]  Loss:  4.309990 (4.4261)  Time: 1.006s, 1017.46/s  (2.497s,  410.03/s)  LR: 2.555e-05  Data: 0.260 (1.880)
Train: 92 [ 100/1251 (  8%)]  Loss:  5.015933 (4.6227)  Time: 0.585s, 1751.32/s  (2.519s,  406.52/s)  LR: 2.555e-05  Data: 0.022 (1.909)
Train: 92 [ 150/1251 ( 12%)]  Loss:  3.924541 (4.4482)  Time: 0.587s, 1743.71/s  (2.417s,  423.75/s)  LR: 2.555e-05  Data: 0.022 (1.817)
Train: 92 [ 200/1251 ( 16%)]  Loss:  4.947581 (4.5481)  Time: 0.584s, 1752.53/s  (2.495s,  410.46/s)  LR: 2.555e-05  Data: 0.021 (1.901)
Train: 92 [ 250/1251 ( 20%)]  Loss:  5.026658 (4.6278)  Time: 0.583s, 1756.27/s  (2.447s,  418.53/s)  LR: 2.555e-05  Data: 0.020 (1.853)
Train: 92 [ 300/1251 ( 24%)]  Loss:  4.763279 (4.6472)  Time: 0.582s, 1758.73/s  (2.430s,  421.37/s)  LR: 2.555e-05  Data: 0.019 (1.837)
Train: 92 [ 350/1251 ( 28%)]  Loss:  4.280305 (4.6013)  Time: 0.586s, 1746.84/s  (2.397s,  427.13/s)  LR: 2.555e-05  Data: 0.023 (1.806)
Train: 92 [ 400/1251 ( 32%)]  Loss:  5.201905 (4.6681)  Time: 0.587s, 1745.01/s  (2.383s,  429.73/s)  LR: 2.555e-05  Data: 0.021 (1.794)
Train: 92 [ 450/1251 ( 36%)]  Loss:  4.303375 (4.6316)  Time: 0.584s, 1752.51/s  (2.343s,  437.05/s)  LR: 2.555e-05  Data: 0.020 (1.754)
Train: 92 [ 500/1251 ( 40%)]  Loss:  4.479800 (4.6178)  Time: 0.584s, 1753.97/s  (2.315s,  442.26/s)  LR: 2.555e-05  Data: 0.019 (1.727)
Train: 92 [ 550/1251 ( 44%)]  Loss:  4.414142 (4.6008)  Time: 0.585s, 1748.96/s  (2.291s,  446.91/s)  LR: 2.555e-05  Data: 0.023 (1.702)
Train: 92 [ 600/1251 ( 48%)]  Loss:  5.023926 (4.6334)  Time: 0.582s, 1759.86/s  (2.322s,  441.08/s)  LR: 2.555e-05  Data: 0.019 (1.732)
Train: 92 [ 650/1251 ( 52%)]  Loss:  4.323605 (4.6112)  Time: 0.585s, 1749.48/s  (2.322s,  440.95/s)  LR: 2.555e-05  Data: 0.021 (1.734)
Train: 92 [ 700/1251 ( 56%)]  Loss:  4.244219 (4.5868)  Time: 0.589s, 1739.12/s  (2.324s,  440.57/s)  LR: 2.555e-05  Data: 0.020 (1.733)
Train: 92 [ 750/1251 ( 60%)]  Loss:  4.532215 (4.5834)  Time: 0.588s, 1741.39/s  (2.315s,  442.34/s)  LR: 2.555e-05  Data: 0.020 (1.724)
Train: 92 [ 800/1251 ( 64%)]  Loss:  4.561930 (4.5821)  Time: 0.585s, 1751.85/s  (2.314s,  442.59/s)  LR: 2.555e-05  Data: 0.021 (1.723)
Train: 92 [ 850/1251 ( 68%)]  Loss:  4.512684 (4.5782)  Time: 0.586s, 1747.24/s  (2.302s,  444.84/s)  LR: 2.555e-05  Data: 0.022 (1.712)
Train: 92 [ 900/1251 ( 72%)]  Loss:  4.238721 (4.5604)  Time: 0.586s, 1748.85/s  (2.290s,  447.22/s)  LR: 2.555e-05  Data: 0.022 (1.700)
Train: 92 [ 950/1251 ( 76%)]  Loss:  4.755807 (4.5701)  Time: 0.589s, 1739.24/s  (2.271s,  450.87/s)  LR: 2.555e-05  Data: 0.022 (1.682)
Train: 92 [1000/1251 ( 80%)]  Loss:  4.773115 (4.5798)  Time: 0.587s, 1744.83/s  (2.281s,  448.84/s)  LR: 2.555e-05  Data: 0.023 (1.692)
Train: 92 [1050/1251 ( 84%)]  Loss:  4.226947 (4.5638)  Time: 0.585s, 1751.52/s  (2.279s,  449.24/s)  LR: 2.555e-05  Data: 0.020 (1.690)
Train: 92 [1100/1251 ( 88%)]  Loss:  4.560592 (4.5636)  Time: 0.582s, 1759.52/s  (2.281s,  448.91/s)  LR: 2.555e-05  Data: 0.019 (1.691)
Train: 92 [1150/1251 ( 92%)]  Loss:  5.122262 (4.5869)  Time: 0.584s, 1752.07/s  (2.275s,  450.02/s)  LR: 2.555e-05  Data: 0.020 (1.685)
Train: 92 [1200/1251 ( 96%)]  Loss:  5.151333 (4.6095)  Time: 3.047s,  336.07/s  (2.275s,  450.14/s)  LR: 2.555e-05  Data: 2.393 (1.684)
Train: 92 [1250/1251 (100%)]  Loss:  4.655824 (4.6113)  Time: 0.565s, 1812.12/s  (2.267s,  451.64/s)  LR: 2.555e-05  Data: 0.000 (1.676)
Test: [   0/48]  Time: 12.687 (12.687)  Loss:  1.1354 (1.1354)  Acc@1: 76.7578 (76.7578)  Acc@5: 91.6992 (91.6992)
Test: [  48/48]  Time: 0.149 (3.064)  Loss:  1.1584 (2.0479)  Acc@1: 75.7076 (55.5280)  Acc@5: 89.9764 (79.4580)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-92.pth.tar', 55.528000048828126)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-90.pth.tar', 55.281999868164064)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-91.pth.tar', 55.24399991699219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-89.pth.tar', 55.108000102539066)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-88.pth.tar', 54.780000073242185)

Train: 93 [   0/1251 (  0%)]  Loss:  4.700239 (4.7002)  Time: 10.450s,   97.99/s  (10.450s,   97.99/s)  LR: 2.192e-05  Data: 9.731 (9.731)
Train: 93 [  50/1251 (  4%)]  Loss:  5.017046 (4.8586)  Time: 0.584s, 1753.63/s  (2.359s,  434.11/s)  LR: 2.192e-05  Data: 0.019 (1.766)
Train: 93 [ 100/1251 (  8%)]  Loss:  4.429369 (4.7156)  Time: 0.589s, 1738.32/s  (2.335s,  438.57/s)  LR: 2.192e-05  Data: 0.022 (1.736)
Train: 93 [ 150/1251 ( 12%)]  Loss:  4.616006 (4.6907)  Time: 0.584s, 1754.44/s  (2.269s,  451.27/s)  LR: 2.192e-05  Data: 0.020 (1.672)
Train: 93 [ 200/1251 ( 16%)]  Loss:  4.770541 (4.7066)  Time: 0.584s, 1753.70/s  (2.256s,  453.92/s)  LR: 2.192e-05  Data: 0.021 (1.655)
Train: 93 [ 250/1251 ( 20%)]  Loss:  4.603685 (4.6895)  Time: 0.589s, 1739.32/s  (2.213s,  462.62/s)  LR: 2.192e-05  Data: 0.020 (1.612)
Train: 93 [ 300/1251 ( 24%)]  Loss:  4.340765 (4.6397)  Time: 0.588s, 1741.61/s  (2.202s,  464.99/s)  LR: 2.192e-05  Data: 0.023 (1.599)
Train: 93 [ 350/1251 ( 28%)]  Loss:  4.361173 (4.6049)  Time: 0.586s, 1746.41/s  (2.183s,  469.12/s)  LR: 2.192e-05  Data: 0.022 (1.579)
Train: 93 [ 400/1251 ( 32%)]  Loss:  4.889609 (4.6365)  Time: 0.585s, 1750.90/s  (2.175s,  470.80/s)  LR: 2.192e-05  Data: 0.020 (1.571)
Train: 93 [ 450/1251 ( 36%)]  Loss:  4.955340 (4.6684)  Time: 0.588s, 1741.80/s  (2.157s,  474.76/s)  LR: 2.192e-05  Data: 0.023 (1.553)
Train: 93 [ 500/1251 ( 40%)]  Loss:  4.456459 (4.6491)  Time: 0.583s, 1756.56/s  (2.183s,  469.11/s)  LR: 2.192e-05  Data: 0.019 (1.579)
Train: 93 [ 550/1251 ( 44%)]  Loss:  4.934570 (4.6729)  Time: 0.585s, 1751.74/s  (2.190s,  467.55/s)  LR: 2.192e-05  Data: 0.022 (1.587)
Train: 93 [ 600/1251 ( 48%)]  Loss:  4.921876 (4.6921)  Time: 0.584s, 1752.22/s  (2.211s,  463.15/s)  LR: 2.192e-05  Data: 0.019 (1.608)
Train: 93 [ 650/1251 ( 52%)]  Loss:  4.939930 (4.7098)  Time: 0.585s, 1749.30/s  (2.206s,  464.20/s)  LR: 2.192e-05  Data: 0.020 (1.605)
Train: 93 [ 700/1251 ( 56%)]  Loss:  5.013480 (4.7300)  Time: 0.583s, 1756.19/s  (2.207s,  464.03/s)  LR: 2.192e-05  Data: 0.020 (1.607)
Train: 93 [ 750/1251 ( 60%)]  Loss:  4.770644 (4.7325)  Time: 0.584s, 1752.15/s  (2.195s,  466.52/s)  LR: 2.192e-05  Data: 0.018 (1.596)
Train: 93 [ 800/1251 ( 64%)]  Loss:  4.837048 (4.7387)  Time: 0.587s, 1745.51/s  (2.193s,  466.87/s)  LR: 2.192e-05  Data: 0.019 (1.596)
Train: 93 [ 850/1251 ( 68%)]  Loss:  4.712364 (4.7372)  Time: 0.584s, 1752.22/s  (2.183s,  469.05/s)  LR: 2.192e-05  Data: 0.020 (1.586)
Train: 93 [ 900/1251 ( 72%)]  Loss:  4.656976 (4.7330)  Time: 0.587s, 1745.09/s  (2.198s,  465.97/s)  LR: 2.192e-05  Data: 0.021 (1.601)
Train: 93 [ 950/1251 ( 76%)]  Loss:  4.934472 (4.7431)  Time: 3.254s,  314.70/s  (2.202s,  465.08/s)  LR: 2.192e-05  Data: 2.607 (1.605)
Train: 93 [1000/1251 ( 80%)]  Loss:  3.867335 (4.7014)  Time: 0.584s, 1752.12/s  (2.200s,  465.38/s)  LR: 2.192e-05  Data: 0.018 (1.604)
Train: 93 [1050/1251 ( 84%)]  Loss:  4.343092 (4.6851)  Time: 0.585s, 1751.05/s  (2.200s,  465.35/s)  LR: 2.192e-05  Data: 0.022 (1.604)
Train: 93 [1100/1251 ( 88%)]  Loss:  5.121626 (4.7041)  Time: 0.584s, 1752.55/s  (2.205s,  464.37/s)  LR: 2.192e-05  Data: 0.022 (1.610)
Train: 93 [1150/1251 ( 92%)]  Loss:  4.477848 (4.6946)  Time: 1.167s,  877.38/s  (2.200s,  465.47/s)  LR: 2.192e-05  Data: 0.492 (1.604)
Train: 93 [1200/1251 ( 96%)]  Loss:  4.444702 (4.6846)  Time: 0.585s, 1750.86/s  (2.201s,  465.32/s)  LR: 2.192e-05  Data: 0.021 (1.604)
Train: 93 [1250/1251 (100%)]  Loss:  4.394012 (4.6735)  Time: 0.565s, 1813.67/s  (2.196s,  466.20/s)  LR: 2.192e-05  Data: 0.000 (1.600)
Test: [   0/48]  Time: 12.983 (12.983)  Loss:  1.1337 (1.1337)  Acc@1: 76.9531 (76.9531)  Acc@5: 92.3828 (92.3828)
Test: [  48/48]  Time: 0.149 (3.554)  Loss:  1.1410 (2.0498)  Acc@1: 76.7689 (55.5800)  Acc@5: 90.3302 (79.4340)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-93.pth.tar', 55.58000012207031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-92.pth.tar', 55.528000048828126)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-90.pth.tar', 55.281999868164064)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-91.pth.tar', 55.24399991699219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-89.pth.tar', 55.108000102539066)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-88.pth.tar', 54.780000073242185)

Train: 94 [   0/1251 (  0%)]  Loss:  4.420207 (4.4202)  Time: 12.070s,   84.84/s  (12.070s,   84.84/s)  LR: 1.877e-05  Data: 10.958 (10.958)
Train: 94 [  50/1251 (  4%)]  Loss:  4.591324 (4.5058)  Time: 0.586s, 1747.14/s  (2.504s,  408.95/s)  LR: 1.877e-05  Data: 0.019 (1.906)
Train: 94 [ 100/1251 (  8%)]  Loss:  4.687752 (4.5664)  Time: 0.585s, 1750.96/s  (2.457s,  416.76/s)  LR: 1.877e-05  Data: 0.019 (1.857)
Train: 94 [ 150/1251 ( 12%)]  Loss:  4.845546 (4.6362)  Time: 0.752s, 1361.55/s  (2.374s,  431.29/s)  LR: 1.877e-05  Data: 0.058 (1.781)
Train: 94 [ 200/1251 ( 16%)]  Loss:  4.959395 (4.7008)  Time: 0.585s, 1749.57/s  (2.347s,  436.29/s)  LR: 1.877e-05  Data: 0.020 (1.754)
Train: 94 [ 250/1251 ( 20%)]  Loss:  4.872471 (4.7294)  Time: 0.588s, 1742.61/s  (2.303s,  444.71/s)  LR: 1.877e-05  Data: 0.021 (1.712)
Train: 94 [ 300/1251 ( 24%)]  Loss:  5.006560 (4.7690)  Time: 0.586s, 1748.91/s  (2.253s,  454.57/s)  LR: 1.877e-05  Data: 0.021 (1.660)
Train: 94 [ 350/1251 ( 28%)]  Loss:  4.605979 (4.7487)  Time: 0.585s, 1749.48/s  (2.249s,  455.27/s)  LR: 1.877e-05  Data: 0.017 (1.656)
Train: 94 [ 400/1251 ( 32%)]  Loss:  4.634142 (4.7359)  Time: 0.586s, 1746.37/s  (2.249s,  455.33/s)  LR: 1.877e-05  Data: 0.019 (1.655)
Train: 94 [ 450/1251 ( 36%)]  Loss:  4.973268 (4.7597)  Time: 0.585s, 1751.11/s  (2.254s,  454.25/s)  LR: 1.877e-05  Data: 0.021 (1.659)
Train: 94 [ 500/1251 ( 40%)]  Loss:  4.555228 (4.7411)  Time: 3.932s,  260.46/s  (2.260s,  453.03/s)  LR: 1.877e-05  Data: 3.367 (1.665)
Train: 94 [ 550/1251 ( 44%)]  Loss:  4.179730 (4.6943)  Time: 0.589s, 1738.88/s  (2.257s,  453.65/s)  LR: 1.877e-05  Data: 0.021 (1.662)
Train: 94 [ 600/1251 ( 48%)]  Loss:  5.080098 (4.7240)  Time: 1.719s,  595.62/s  (2.253s,  454.57/s)  LR: 1.877e-05  Data: 1.064 (1.657)
Train: 94 [ 650/1251 ( 52%)]  Loss:  4.482090 (4.7067)  Time: 0.588s, 1740.49/s  (2.253s,  454.50/s)  LR: 1.877e-05  Data: 0.022 (1.656)
Train: 94 [ 700/1251 ( 56%)]  Loss:  4.921480 (4.7210)  Time: 0.586s, 1746.06/s  (2.242s,  456.64/s)  LR: 1.877e-05  Data: 0.023 (1.646)
Train: 94 [ 750/1251 ( 60%)]  Loss:  3.950675 (4.6729)  Time: 0.586s, 1746.20/s  (2.251s,  454.92/s)  LR: 1.877e-05  Data: 0.022 (1.656)
Train: 94 [ 800/1251 ( 64%)]  Loss:  4.460799 (4.6604)  Time: 0.588s, 1740.70/s  (2.252s,  454.78/s)  LR: 1.877e-05  Data: 0.024 (1.657)
Train: 94 [ 850/1251 ( 68%)]  Loss:  4.212939 (4.6355)  Time: 0.667s, 1535.45/s  (2.267s,  451.76/s)  LR: 1.877e-05  Data: 0.020 (1.673)
Train: 94 [ 900/1251 ( 72%)]  Loss:  5.059468 (4.6579)  Time: 0.585s, 1749.25/s  (2.269s,  451.26/s)  LR: 1.877e-05  Data: 0.020 (1.676)
Train: 94 [ 950/1251 ( 76%)]  Loss:  4.813867 (4.6657)  Time: 0.589s, 1738.39/s  (2.273s,  450.45/s)  LR: 1.877e-05  Data: 0.020 (1.681)
Train: 94 [1000/1251 ( 80%)]  Loss:  4.751505 (4.6697)  Time: 0.587s, 1744.94/s  (2.272s,  450.67/s)  LR: 1.877e-05  Data: 0.022 (1.680)
Train: 94 [1050/1251 ( 84%)]  Loss:  4.322739 (4.6540)  Time: 0.587s, 1743.84/s  (2.279s,  449.33/s)  LR: 1.877e-05  Data: 0.020 (1.686)
Train: 94 [1100/1251 ( 88%)]  Loss:  4.236284 (4.6358)  Time: 0.588s, 1742.45/s  (2.273s,  450.46/s)  LR: 1.877e-05  Data: 0.021 (1.681)
Train: 94 [1150/1251 ( 92%)]  Loss:  4.041950 (4.6111)  Time: 0.586s, 1746.72/s  (2.291s,  447.02/s)  LR: 1.877e-05  Data: 0.020 (1.699)
Train: 94 [1200/1251 ( 96%)]  Loss:  5.270619 (4.6374)  Time: 0.592s, 1730.99/s  (2.303s,  444.64/s)  LR: 1.877e-05  Data: 0.020 (1.711)
Train: 94 [1250/1251 (100%)]  Loss:  4.793193 (4.6434)  Time: 0.562s, 1820.61/s  (2.313s,  442.76/s)  LR: 1.877e-05  Data: 0.000 (1.721)
Test: [   0/48]  Time: 13.859 (13.859)  Loss:  1.1327 (1.1327)  Acc@1: 77.1484 (77.1484)  Acc@5: 92.1875 (92.1875)
Test: [  48/48]  Time: 0.150 (3.266)  Loss:  1.1559 (2.0501)  Acc@1: 76.1792 (55.5640)  Acc@5: 89.8585 (79.5180)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-93.pth.tar', 55.58000012207031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-94.pth.tar', 55.563999995117186)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-92.pth.tar', 55.528000048828126)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-90.pth.tar', 55.281999868164064)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-91.pth.tar', 55.24399991699219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-89.pth.tar', 55.108000102539066)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-88.pth.tar', 54.780000073242185)

Train: 95 [   0/1251 (  0%)]  Loss:  4.747289 (4.7473)  Time: 10.786s,   94.94/s  (10.786s,   94.94/s)  LR: 1.609e-05  Data: 9.702 (9.702)
Train: 95 [  50/1251 (  4%)]  Loss:  4.645311 (4.6963)  Time: 0.587s, 1745.85/s  (2.283s,  448.62/s)  LR: 1.609e-05  Data: 0.023 (1.687)
Train: 95 [ 100/1251 (  8%)]  Loss:  5.120667 (4.8378)  Time: 1.552s,  659.99/s  (2.260s,  453.02/s)  LR: 1.609e-05  Data: 0.730 (1.654)
Train: 95 [ 150/1251 ( 12%)]  Loss:  4.355691 (4.7172)  Time: 0.590s, 1734.49/s  (2.196s,  466.31/s)  LR: 1.609e-05  Data: 0.026 (1.590)
Train: 95 [ 200/1251 ( 16%)]  Loss:  4.918766 (4.7575)  Time: 0.587s, 1745.87/s  (2.259s,  453.35/s)  LR: 1.609e-05  Data: 0.020 (1.657)
Train: 95 [ 250/1251 ( 20%)]  Loss:  4.490299 (4.7130)  Time: 0.593s, 1726.65/s  (2.274s,  450.34/s)  LR: 1.609e-05  Data: 0.029 (1.675)
Train: 95 [ 300/1251 ( 24%)]  Loss:  4.613384 (4.6988)  Time: 4.137s,  247.52/s  (2.322s,  441.05/s)  LR: 1.609e-05  Data: 3.561 (1.723)
Train: 95 [ 350/1251 ( 28%)]  Loss:  4.699845 (4.6989)  Time: 1.890s,  541.87/s  (2.333s,  439.01/s)  LR: 1.609e-05  Data: 1.241 (1.730)
Train: 95 [ 400/1251 ( 32%)]  Loss:  4.759478 (4.7056)  Time: 4.560s,  224.57/s  (2.346s,  436.50/s)  LR: 1.609e-05  Data: 3.915 (1.744)
Train: 95 [ 450/1251 ( 36%)]  Loss:  5.058946 (4.7410)  Time: 4.312s,  237.45/s  (2.336s,  438.37/s)  LR: 1.609e-05  Data: 3.734 (1.735)
Train: 95 [ 500/1251 ( 40%)]  Loss:  4.191616 (4.6910)  Time: 4.725s,  216.72/s  (2.335s,  438.57/s)  LR: 1.609e-05  Data: 4.067 (1.734)
Train: 95 [ 550/1251 ( 44%)]  Loss:  4.146544 (4.6457)  Time: 1.867s,  548.47/s  (2.326s,  440.19/s)  LR: 1.609e-05  Data: 1.173 (1.724)
Train: 95 [ 600/1251 ( 48%)]  Loss:  4.295253 (4.6187)  Time: 7.789s,  131.46/s  (2.377s,  430.81/s)  LR: 1.609e-05  Data: 7.123 (1.775)
Train: 95 [ 650/1251 ( 52%)]  Loss:  4.546850 (4.6136)  Time: 3.125s,  327.64/s  (2.392s,  428.03/s)  LR: 1.609e-05  Data: 2.409 (1.790)
Train: 95 [ 700/1251 ( 56%)]  Loss:  5.013275 (4.6402)  Time: 2.543s,  402.73/s  (2.403s,  426.07/s)  LR: 1.609e-05  Data: 1.892 (1.801)
Train: 95 [ 750/1251 ( 60%)]  Loss:  4.961530 (4.6603)  Time: 8.142s,  125.77/s  (2.413s,  424.44/s)  LR: 1.609e-05  Data: 7.489 (1.811)
Train: 95 [ 800/1251 ( 64%)]  Loss:  4.432796 (4.6469)  Time: 0.585s, 1750.28/s  (2.403s,  426.05/s)  LR: 1.609e-05  Data: 0.021 (1.803)
Train: 95 [ 850/1251 ( 68%)]  Loss:  4.903200 (4.6612)  Time: 7.414s,  138.11/s  (2.404s,  426.00/s)  LR: 1.609e-05  Data: 6.837 (1.803)
Train: 95 [ 900/1251 ( 72%)]  Loss:  4.750690 (4.6659)  Time: 1.890s,  541.81/s  (2.390s,  428.46/s)  LR: 1.609e-05  Data: 1.239 (1.789)
Train: 95 [ 950/1251 ( 76%)]  Loss:  4.089586 (4.6371)  Time: 6.585s,  155.50/s  (2.399s,  426.92/s)  LR: 1.609e-05  Data: 5.917 (1.798)
Train: 95 [1000/1251 ( 80%)]  Loss:  4.375349 (4.6246)  Time: 3.955s,  258.93/s  (2.405s,  425.77/s)  LR: 1.609e-05  Data: 3.390 (1.804)
Train: 95 [1050/1251 ( 84%)]  Loss:  4.796515 (4.6324)  Time: 6.872s,  149.00/s  (2.411s,  424.78/s)  LR: 1.609e-05  Data: 6.198 (1.809)
Train: 95 [1100/1251 ( 88%)]  Loss:  4.542718 (4.6285)  Time: 0.586s, 1748.09/s  (2.406s,  425.60/s)  LR: 1.609e-05  Data: 0.021 (1.805)
Train: 95 [1150/1251 ( 92%)]  Loss:  4.611448 (4.6278)  Time: 8.025s,  127.61/s  (2.409s,  425.07/s)  LR: 1.609e-05  Data: 7.448 (1.809)
Train: 95 [1200/1251 ( 96%)]  Loss:  4.853887 (4.6368)  Time: 0.588s, 1742.54/s  (2.400s,  426.61/s)  LR: 1.609e-05  Data: 0.021 (1.801)
Train: 95 [1250/1251 (100%)]  Loss:  5.163490 (4.6571)  Time: 0.562s, 1820.68/s  (2.388s,  428.75/s)  LR: 1.609e-05  Data: 0.000 (1.789)
Test: [   0/48]  Time: 12.798 (12.798)  Loss:  1.1541 (1.1541)  Acc@1: 76.4648 (76.4648)  Acc@5: 91.9922 (91.9922)
Test: [  48/48]  Time: 0.149 (3.413)  Loss:  1.1557 (2.0451)  Acc@1: 76.0613 (55.5780)  Acc@5: 90.2123 (79.6000)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-93.pth.tar', 55.58000012207031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-95.pth.tar', 55.57800007324219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-94.pth.tar', 55.563999995117186)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-92.pth.tar', 55.528000048828126)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-90.pth.tar', 55.281999868164064)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-91.pth.tar', 55.24399991699219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-89.pth.tar', 55.108000102539066)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-88.pth.tar', 54.780000073242185)

Train: 96 [   0/1251 (  0%)]  Loss:  4.645505 (4.6455)  Time: 12.345s,   82.95/s  (12.345s,   82.95/s)  LR: 1.390e-05  Data: 11.128 (11.128)
Train: 96 [  50/1251 (  4%)]  Loss:  5.040709 (4.8431)  Time: 0.585s, 1748.96/s  (2.721s,  376.36/s)  LR: 1.390e-05  Data: 0.020 (2.122)
Train: 96 [ 100/1251 (  8%)]  Loss:  5.069791 (4.9187)  Time: 0.585s, 1751.65/s  (2.588s,  395.67/s)  LR: 1.390e-05  Data: 0.021 (1.989)
Train: 96 [ 150/1251 ( 12%)]  Loss:  4.620384 (4.8441)  Time: 0.590s, 1736.85/s  (2.511s,  407.78/s)  LR: 1.390e-05  Data: 0.023 (1.912)
Train: 96 [ 200/1251 ( 16%)]  Loss:  4.084990 (4.6923)  Time: 0.584s, 1753.54/s  (2.483s,  412.38/s)  LR: 1.390e-05  Data: 0.019 (1.887)
Train: 96 [ 250/1251 ( 20%)]  Loss:  4.977003 (4.7397)  Time: 1.669s,  613.45/s  (2.434s,  420.74/s)  LR: 1.390e-05  Data: 0.979 (1.835)
Train: 96 [ 300/1251 ( 24%)]  Loss:  5.047442 (4.7837)  Time: 0.585s, 1749.27/s  (2.412s,  424.61/s)  LR: 1.390e-05  Data: 0.020 (1.814)
Train: 96 [ 350/1251 ( 28%)]  Loss:  4.681292 (4.7709)  Time: 0.589s, 1739.74/s  (2.378s,  430.54/s)  LR: 1.390e-05  Data: 0.026 (1.782)
Train: 96 [ 400/1251 ( 32%)]  Loss:  4.403798 (4.7301)  Time: 0.587s, 1743.66/s  (2.403s,  426.14/s)  LR: 1.390e-05  Data: 0.019 (1.808)
Train: 96 [ 450/1251 ( 36%)]  Loss:  4.745840 (4.7317)  Time: 0.586s, 1746.19/s  (2.399s,  426.86/s)  LR: 1.390e-05  Data: 0.021 (1.806)
Train: 96 [ 500/1251 ( 40%)]  Loss:  5.308689 (4.7841)  Time: 0.583s, 1756.60/s  (2.413s,  424.32/s)  LR: 1.390e-05  Data: 0.019 (1.818)
Train: 96 [ 550/1251 ( 44%)]  Loss:  4.993321 (4.8016)  Time: 0.583s, 1756.66/s  (2.404s,  426.00/s)  LR: 1.390e-05  Data: 0.020 (1.810)
Train: 96 [ 600/1251 ( 48%)]  Loss:  4.247620 (4.7590)  Time: 0.585s, 1750.62/s  (2.416s,  423.86/s)  LR: 1.390e-05  Data: 0.020 (1.822)
Train: 96 [ 650/1251 ( 52%)]  Loss:  4.566026 (4.7452)  Time: 0.583s, 1755.44/s  (2.408s,  425.33/s)  LR: 1.390e-05  Data: 0.019 (1.814)
Train: 96 [ 700/1251 ( 56%)]  Loss:  4.375967 (4.7206)  Time: 0.584s, 1752.06/s  (2.412s,  424.53/s)  LR: 1.390e-05  Data: 0.022 (1.818)
Train: 96 [ 750/1251 ( 60%)]  Loss:  4.547227 (4.7097)  Time: 0.583s, 1756.21/s  (2.418s,  423.45/s)  LR: 1.390e-05  Data: 0.019 (1.825)
Train: 96 [ 800/1251 ( 64%)]  Loss:  5.009887 (4.7274)  Time: 0.588s, 1741.46/s  (2.439s,  419.90/s)  LR: 1.390e-05  Data: 0.023 (1.845)
Train: 96 [ 850/1251 ( 68%)]  Loss:  4.620732 (4.7215)  Time: 0.584s, 1752.33/s  (2.438s,  419.94/s)  LR: 1.390e-05  Data: 0.018 (1.843)
Train: 96 [ 900/1251 ( 72%)]  Loss:  4.770160 (4.7240)  Time: 0.584s, 1752.50/s  (2.447s,  418.43/s)  LR: 1.390e-05  Data: 0.021 (1.853)
Train: 96 [ 950/1251 ( 76%)]  Loss:  4.297254 (4.7027)  Time: 0.583s, 1755.36/s  (2.442s,  419.34/s)  LR: 1.390e-05  Data: 0.021 (1.848)
Train: 96 [1000/1251 ( 80%)]  Loss:  4.606915 (4.6981)  Time: 0.586s, 1746.07/s  (2.439s,  419.81/s)  LR: 1.390e-05  Data: 0.022 (1.845)
Train: 96 [1050/1251 ( 84%)]  Loss:  4.442429 (4.6865)  Time: 0.587s, 1743.67/s  (2.429s,  421.53/s)  LR: 1.390e-05  Data: 0.021 (1.836)
Train: 96 [1100/1251 ( 88%)]  Loss:  5.275671 (4.7121)  Time: 0.584s, 1753.11/s  (2.437s,  420.11/s)  LR: 1.390e-05  Data: 0.022 (1.843)
Train: 96 [1150/1251 ( 92%)]  Loss:  4.413334 (4.6997)  Time: 0.588s, 1741.79/s  (2.447s,  418.41/s)  LR: 1.390e-05  Data: 0.021 (1.853)
Train: 96 [1200/1251 ( 96%)]  Loss:  5.005891 (4.7119)  Time: 0.585s, 1749.13/s  (2.454s,  417.29/s)  LR: 1.390e-05  Data: 0.022 (1.860)
Train: 96 [1250/1251 (100%)]  Loss:  4.532407 (4.7050)  Time: 0.564s, 1816.70/s  (2.454s,  417.35/s)  LR: 1.390e-05  Data: 0.000 (1.860)
Test: [   0/48]  Time: 13.919 (13.919)  Loss:  1.1458 (1.1458)  Acc@1: 76.7578 (76.7578)  Acc@5: 91.8945 (91.8945)
Test: [  48/48]  Time: 0.153 (3.471)  Loss:  1.1512 (2.0397)  Acc@1: 75.8255 (55.7900)  Acc@5: 90.3302 (79.5800)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-96.pth.tar', 55.78999997070313)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-93.pth.tar', 55.58000012207031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-95.pth.tar', 55.57800007324219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-94.pth.tar', 55.563999995117186)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-92.pth.tar', 55.528000048828126)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-90.pth.tar', 55.281999868164064)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-91.pth.tar', 55.24399991699219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-89.pth.tar', 55.108000102539066)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-88.pth.tar', 54.780000073242185)

Train: 97 [   0/1251 (  0%)]  Loss:  4.757771 (4.7578)  Time: 10.894s,   94.00/s  (10.894s,   94.00/s)  LR: 1.220e-05  Data: 10.182 (10.182)
Train: 97 [  50/1251 (  4%)]  Loss:  4.427956 (4.5929)  Time: 0.584s, 1753.80/s  (2.436s,  420.36/s)  LR: 1.220e-05  Data: 0.019 (1.844)
Train: 97 [ 100/1251 (  8%)]  Loss:  4.084530 (4.4234)  Time: 3.462s,  295.78/s  (2.381s,  429.99/s)  LR: 1.220e-05  Data: 2.861 (1.791)
Train: 97 [ 150/1251 ( 12%)]  Loss:  4.740387 (4.5027)  Time: 0.584s, 1753.39/s  (2.377s,  430.75/s)  LR: 1.220e-05  Data: 0.020 (1.782)
Train: 97 [ 200/1251 ( 16%)]  Loss:  4.388454 (4.4798)  Time: 0.588s, 1740.10/s  (2.455s,  417.12/s)  LR: 1.220e-05  Data: 0.022 (1.864)
Train: 97 [ 250/1251 ( 20%)]  Loss:  4.771497 (4.5284)  Time: 0.583s, 1755.20/s  (2.449s,  418.14/s)  LR: 1.220e-05  Data: 0.020 (1.859)
Train: 97 [ 300/1251 ( 24%)]  Loss:  4.378354 (4.5070)  Time: 0.588s, 1742.48/s  (2.452s,  417.63/s)  LR: 1.220e-05  Data: 0.020 (1.859)
Train: 97 [ 350/1251 ( 28%)]  Loss:  5.092591 (4.5802)  Time: 0.584s, 1754.91/s  (2.420s,  423.17/s)  LR: 1.220e-05  Data: 0.020 (1.829)
Train: 97 [ 400/1251 ( 32%)]  Loss:  4.970592 (4.6236)  Time: 0.585s, 1749.16/s  (2.416s,  423.88/s)  LR: 1.220e-05  Data: 0.023 (1.825)
Train: 97 [ 450/1251 ( 36%)]  Loss:  4.506877 (4.6119)  Time: 0.584s, 1754.40/s  (2.385s,  429.27/s)  LR: 1.220e-05  Data: 0.020 (1.796)
Train: 97 [ 500/1251 ( 40%)]  Loss:  4.856903 (4.6342)  Time: 0.583s, 1755.28/s  (2.379s,  430.36/s)  LR: 1.220e-05  Data: 0.021 (1.790)
Train: 97 [ 550/1251 ( 44%)]  Loss:  4.090935 (4.5889)  Time: 0.585s, 1751.12/s  (2.400s,  426.58/s)  LR: 1.220e-05  Data: 0.021 (1.812)
Train: 97 [ 600/1251 ( 48%)]  Loss:  5.252755 (4.6400)  Time: 0.586s, 1747.55/s  (2.422s,  422.73/s)  LR: 1.220e-05  Data: 0.022 (1.834)
Train: 97 [ 650/1251 ( 52%)]  Loss:  4.777690 (4.6498)  Time: 0.586s, 1748.11/s  (2.418s,  423.48/s)  LR: 1.220e-05  Data: 0.018 (1.830)
Train: 97 [ 700/1251 ( 56%)]  Loss:  4.890953 (4.6659)  Time: 0.588s, 1741.60/s  (2.428s,  421.81/s)  LR: 1.220e-05  Data: 0.021 (1.840)
Train: 97 [ 750/1251 ( 60%)]  Loss:  4.322517 (4.6444)  Time: 0.584s, 1753.05/s  (2.423s,  422.66/s)  LR: 1.220e-05  Data: 0.021 (1.836)
Train: 97 [ 800/1251 ( 64%)]  Loss:  4.751780 (4.6507)  Time: 0.583s, 1757.92/s  (2.424s,  422.43/s)  LR: 1.220e-05  Data: 0.019 (1.837)
Train: 97 [ 850/1251 ( 68%)]  Loss:  4.698925 (4.6534)  Time: 0.585s, 1750.78/s  (2.414s,  424.18/s)  LR: 1.220e-05  Data: 0.019 (1.827)
Train: 97 [ 900/1251 ( 72%)]  Loss:  3.747959 (4.6058)  Time: 0.583s, 1756.58/s  (2.431s,  421.24/s)  LR: 1.220e-05  Data: 0.020 (1.844)
Train: 97 [ 950/1251 ( 76%)]  Loss:  4.594644 (4.6052)  Time: 0.587s, 1743.12/s  (2.440s,  419.68/s)  LR: 1.220e-05  Data: 0.021 (1.852)
Train: 97 [1000/1251 ( 80%)]  Loss:  4.434138 (4.5971)  Time: 0.584s, 1752.10/s  (2.442s,  419.31/s)  LR: 1.220e-05  Data: 0.019 (1.854)
Train: 97 [1050/1251 ( 84%)]  Loss:  4.591576 (4.5968)  Time: 0.585s, 1751.71/s  (2.441s,  419.43/s)  LR: 1.220e-05  Data: 0.022 (1.853)
Train: 97 [1100/1251 ( 88%)]  Loss:  4.767495 (4.6042)  Time: 0.587s, 1745.13/s  (2.440s,  419.64/s)  LR: 1.220e-05  Data: 0.019 (1.850)
Train: 97 [1150/1251 ( 92%)]  Loss:  4.666037 (4.6068)  Time: 0.585s, 1749.24/s  (2.428s,  421.76/s)  LR: 1.220e-05  Data: 0.022 (1.838)
Train: 97 [1200/1251 ( 96%)]  Loss:  4.462122 (4.6010)  Time: 0.584s, 1752.22/s  (2.422s,  422.86/s)  LR: 1.220e-05  Data: 0.021 (1.831)
Train: 97 [1250/1251 (100%)]  Loss:  4.830733 (4.6099)  Time: 0.562s, 1820.66/s  (2.417s,  423.64/s)  LR: 1.220e-05  Data: 0.000 (1.826)
Test: [   0/48]  Time: 16.444 (16.444)  Loss:  1.1513 (1.1513)  Acc@1: 76.5625 (76.5625)  Acc@5: 91.9922 (91.9922)
Test: [  48/48]  Time: 0.149 (3.582)  Loss:  1.1428 (2.0387)  Acc@1: 77.2406 (55.7460)  Acc@5: 90.0943 (79.6500)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-96.pth.tar', 55.78999997070313)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-97.pth.tar', 55.74600006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-93.pth.tar', 55.58000012207031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-95.pth.tar', 55.57800007324219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-94.pth.tar', 55.563999995117186)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-92.pth.tar', 55.528000048828126)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-90.pth.tar', 55.281999868164064)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-91.pth.tar', 55.24399991699219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-89.pth.tar', 55.108000102539066)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-88.pth.tar', 54.780000073242185)

Train: 98 [   0/1251 (  0%)]  Loss:  4.715878 (4.7159)  Time: 10.679s,   95.89/s  (10.679s,   95.89/s)  LR: 1.098e-05  Data: 10.115 (10.115)
Train: 98 [  50/1251 (  4%)]  Loss:  4.648324 (4.6821)  Time: 0.586s, 1746.51/s  (2.550s,  401.57/s)  LR: 1.098e-05  Data: 0.021 (1.958)
Train: 98 [ 100/1251 (  8%)]  Loss:  4.903746 (4.7560)  Time: 0.589s, 1739.17/s  (2.453s,  417.49/s)  LR: 1.098e-05  Data: 0.022 (1.859)
Train: 98 [ 150/1251 ( 12%)]  Loss:  4.168277 (4.6091)  Time: 0.588s, 1742.07/s  (2.396s,  427.35/s)  LR: 1.098e-05  Data: 0.019 (1.792)
Train: 98 [ 200/1251 ( 16%)]  Loss:  4.936697 (4.6746)  Time: 0.587s, 1744.08/s  (2.336s,  438.43/s)  LR: 1.098e-05  Data: 0.022 (1.738)
Train: 98 [ 250/1251 ( 20%)]  Loss:  4.644562 (4.6696)  Time: 0.587s, 1744.01/s  (2.315s,  442.28/s)  LR: 1.098e-05  Data: 0.021 (1.719)
Train: 98 [ 300/1251 ( 24%)]  Loss:  5.209621 (4.7467)  Time: 0.586s, 1746.13/s  (2.270s,  451.01/s)  LR: 1.098e-05  Data: 0.022 (1.676)
Train: 98 [ 350/1251 ( 28%)]  Loss:  4.150061 (4.6721)  Time: 0.585s, 1750.48/s  (2.323s,  440.81/s)  LR: 1.098e-05  Data: 0.019 (1.729)
Train: 98 [ 400/1251 ( 32%)]  Loss:  4.548352 (4.6584)  Time: 0.587s, 1744.09/s  (2.335s,  438.64/s)  LR: 1.098e-05  Data: 0.020 (1.741)
Train: 98 [ 450/1251 ( 36%)]  Loss:  3.868134 (4.5794)  Time: 0.587s, 1744.07/s  (2.347s,  436.31/s)  LR: 1.098e-05  Data: 0.022 (1.755)
Train: 98 [ 500/1251 ( 40%)]  Loss:  4.825638 (4.6018)  Time: 0.587s, 1744.94/s  (2.337s,  438.09/s)  LR: 1.098e-05  Data: 0.023 (1.743)
Train: 98 [ 550/1251 ( 44%)]  Loss:  4.500998 (4.5934)  Time: 0.587s, 1743.00/s  (2.332s,  439.16/s)  LR: 1.098e-05  Data: 0.023 (1.738)
Train: 98 [ 600/1251 ( 48%)]  Loss:  5.165253 (4.6373)  Time: 0.585s, 1750.15/s  (2.316s,  442.11/s)  LR: 1.098e-05  Data: 0.022 (1.724)
Train: 98 [ 650/1251 ( 52%)]  Loss:  4.855370 (4.6529)  Time: 0.587s, 1744.28/s  (2.308s,  443.61/s)  LR: 1.098e-05  Data: 0.021 (1.716)
Train: 98 [ 700/1251 ( 56%)]  Loss:  4.565238 (4.6471)  Time: 1.435s,  713.47/s  (2.291s,  447.01/s)  LR: 1.098e-05  Data: 0.812 (1.698)
Train: 98 [ 750/1251 ( 60%)]  Loss:  4.768422 (4.6547)  Time: 0.589s, 1738.33/s  (2.318s,  441.81/s)  LR: 1.098e-05  Data: 0.020 (1.724)
Train: 98 [ 800/1251 ( 64%)]  Loss:  4.681949 (4.6563)  Time: 1.791s,  571.65/s  (2.325s,  440.44/s)  LR: 1.098e-05  Data: 1.100 (1.729)
Train: 98 [ 850/1251 ( 68%)]  Loss:  4.981808 (4.6744)  Time: 0.583s, 1755.07/s  (2.334s,  438.73/s)  LR: 1.098e-05  Data: 0.021 (1.739)
Train: 98 [ 900/1251 ( 72%)]  Loss:  4.763212 (4.6790)  Time: 0.584s, 1754.09/s  (2.329s,  439.74/s)  LR: 1.098e-05  Data: 0.019 (1.734)
Train: 98 [ 950/1251 ( 76%)]  Loss:  4.253587 (4.6578)  Time: 1.479s,  692.36/s  (2.329s,  439.75/s)  LR: 1.098e-05  Data: 0.914 (1.733)
Train: 98 [1000/1251 ( 80%)]  Loss:  4.679464 (4.6588)  Time: 0.584s, 1753.38/s  (2.319s,  441.66/s)  LR: 1.098e-05  Data: 0.019 (1.724)
Train: 98 [1050/1251 ( 84%)]  Loss:  4.458078 (4.6497)  Time: 0.583s, 1755.62/s  (2.310s,  443.33/s)  LR: 1.098e-05  Data: 0.018 (1.715)
Train: 98 [1100/1251 ( 88%)]  Loss:  4.314048 (4.6351)  Time: 1.994s,  513.43/s  (2.307s,  443.89/s)  LR: 1.098e-05  Data: 1.243 (1.712)
Train: 98 [1150/1251 ( 92%)]  Loss:  4.426599 (4.6264)  Time: 0.585s, 1749.02/s  (2.317s,  442.04/s)  LR: 1.098e-05  Data: 0.021 (1.721)
Train: 98 [1200/1251 ( 96%)]  Loss:  4.784236 (4.6327)  Time: 0.591s, 1733.17/s  (2.320s,  441.43/s)  LR: 1.098e-05  Data: 0.028 (1.723)
Train: 98 [1250/1251 (100%)]  Loss:  4.876943 (4.6421)  Time: 0.563s, 1818.76/s  (2.318s,  441.77/s)  LR: 1.098e-05  Data: 0.000 (1.721)
Test: [   0/48]  Time: 6.576 (6.576)  Loss:  1.1410 (1.1410)  Acc@1: 76.8555 (76.8555)  Acc@5: 92.1875 (92.1875)
Test: [  48/48]  Time: 0.149 (2.398)  Loss:  1.1628 (2.0375)  Acc@1: 76.1792 (55.8620)  Acc@5: 89.9764 (79.7080)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-98.pth.tar', 55.86199999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-96.pth.tar', 55.78999997070313)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-97.pth.tar', 55.74600006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-93.pth.tar', 55.58000012207031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-95.pth.tar', 55.57800007324219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-94.pth.tar', 55.563999995117186)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-92.pth.tar', 55.528000048828126)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-90.pth.tar', 55.281999868164064)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-91.pth.tar', 55.24399991699219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-89.pth.tar', 55.108000102539066)

Train: 99 [   0/1251 (  0%)]  Loss:  4.875224 (4.8752)  Time: 9.542s,  107.31/s  (9.542s,  107.31/s)  LR: 1.024e-05  Data: 8.597 (8.597)
Train: 99 [  50/1251 (  4%)]  Loss:  4.481406 (4.6783)  Time: 0.591s, 1734.10/s  (2.151s,  475.99/s)  LR: 1.024e-05  Data: 0.027 (1.531)
Train: 99 [ 100/1251 (  8%)]  Loss:  4.712739 (4.6898)  Time: 1.234s,  829.55/s  (2.095s,  488.86/s)  LR: 1.024e-05  Data: 0.584 (1.491)
Train: 99 [ 150/1251 ( 12%)]  Loss:  5.053086 (4.7806)  Time: 0.587s, 1744.35/s  (2.030s,  504.49/s)  LR: 1.024e-05  Data: 0.021 (1.429)
Train: 99 [ 200/1251 ( 16%)]  Loss:  4.329616 (4.6904)  Time: 0.955s, 1072.63/s  (2.069s,  494.84/s)  LR: 1.024e-05  Data: 0.020 (1.466)
Train: 99 [ 250/1251 ( 20%)]  Loss:  4.316799 (4.6281)  Time: 0.588s, 1742.84/s  (2.107s,  485.99/s)  LR: 1.024e-05  Data: 0.024 (1.508)
Train: 99 [ 300/1251 ( 24%)]  Loss:  3.915159 (4.5263)  Time: 0.586s, 1747.44/s  (2.140s,  478.46/s)  LR: 1.024e-05  Data: 0.021 (1.541)
Train: 99 [ 350/1251 ( 28%)]  Loss:  4.817447 (4.5627)  Time: 0.591s, 1733.35/s  (2.139s,  478.67/s)  LR: 1.024e-05  Data: 0.026 (1.542)
Train: 99 [ 400/1251 ( 32%)]  Loss:  4.839747 (4.5935)  Time: 0.585s, 1750.01/s  (2.141s,  478.23/s)  LR: 1.024e-05  Data: 0.019 (1.545)
Train: 99 [ 450/1251 ( 36%)]  Loss:  4.771496 (4.6113)  Time: 0.590s, 1736.90/s  (2.130s,  480.73/s)  LR: 1.024e-05  Data: 0.027 (1.534)
Train: 99 [ 500/1251 ( 40%)]  Loss:  4.808706 (4.6292)  Time: 0.587s, 1744.23/s  (2.118s,  483.39/s)  LR: 1.024e-05  Data: 0.021 (1.523)
Train: 99 [ 550/1251 ( 44%)]  Loss:  4.664113 (4.6321)  Time: 0.590s, 1736.10/s  (2.102s,  487.05/s)  LR: 1.024e-05  Data: 0.026 (1.507)
Train: 99 [ 600/1251 ( 48%)]  Loss:  4.471429 (4.6198)  Time: 0.585s, 1751.88/s  (2.091s,  489.68/s)  LR: 1.024e-05  Data: 0.021 (1.496)
Train: 99 [ 650/1251 ( 52%)]  Loss:  4.866826 (4.6374)  Time: 0.588s, 1741.31/s  (2.109s,  485.63/s)  LR: 1.024e-05  Data: 0.024 (1.512)
Train: 99 [ 700/1251 ( 56%)]  Loss:  4.530720 (4.6303)  Time: 0.822s, 1245.57/s  (2.127s,  481.34/s)  LR: 1.024e-05  Data: 0.256 (1.530)
Train: 99 [ 750/1251 ( 60%)]  Loss:  4.482102 (4.6210)  Time: 1.072s,  955.06/s  (2.142s,  477.96/s)  LR: 1.024e-05  Data: 0.418 (1.544)
Train: 99 [ 800/1251 ( 64%)]  Loss:  4.481803 (4.6128)  Time: 0.585s, 1748.97/s  (2.152s,  475.92/s)  LR: 1.024e-05  Data: 0.021 (1.552)
Train: 99 [ 850/1251 ( 68%)]  Loss:  4.178018 (4.5887)  Time: 0.595s, 1721.74/s  (2.156s,  474.98/s)  LR: 1.024e-05  Data: 0.028 (1.556)
Train: 99 [ 900/1251 ( 72%)]  Loss:  4.025788 (4.5591)  Time: 2.538s,  403.40/s  (2.157s,  474.69/s)  LR: 1.024e-05  Data: 1.971 (1.557)
Train: 99 [ 950/1251 ( 76%)]  Loss:  4.627449 (4.5625)  Time: 0.587s, 1745.83/s  (2.154s,  475.49/s)  LR: 1.024e-05  Data: 0.021 (1.553)
Train: 99 [1000/1251 ( 80%)]  Loss:  4.211668 (4.5458)  Time: 4.187s,  244.56/s  (2.153s,  475.65/s)  LR: 1.024e-05  Data: 3.606 (1.552)
Train: 99 [1050/1251 ( 84%)]  Loss:  4.852709 (4.5597)  Time: 0.587s, 1744.78/s  (2.157s,  474.66/s)  LR: 1.024e-05  Data: 0.023 (1.556)
Train: 99 [1100/1251 ( 88%)]  Loss:  4.823139 (4.5712)  Time: 3.254s,  314.68/s  (2.170s,  471.92/s)  LR: 1.024e-05  Data: 2.637 (1.568)
Train: 99 [1150/1251 ( 92%)]  Loss:  4.815146 (4.5813)  Time: 0.587s, 1743.46/s  (2.172s,  471.39/s)  LR: 1.024e-05  Data: 0.019 (1.571)
Train: 99 [1200/1251 ( 96%)]  Loss:  4.777012 (4.5892)  Time: 4.151s,  246.66/s  (2.178s,  470.23/s)  LR: 1.024e-05  Data: 3.491 (1.576)
Train: 99 [1250/1251 (100%)]  Loss:  4.214380 (4.5748)  Time: 0.565s, 1811.86/s  (2.176s,  470.52/s)  LR: 1.024e-05  Data: 0.000 (1.575)
Test: [   0/48]  Time: 10.931 (10.931)  Loss:  1.1419 (1.1419)  Acc@1: 76.7578 (76.7578)  Acc@5: 92.1875 (92.1875)
Test: [  48/48]  Time: 0.149 (2.889)  Loss:  1.1507 (2.0401)  Acc@1: 76.4151 (55.7980)  Acc@5: 89.6226 (79.6920)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-98.pth.tar', 55.86199999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-99.pth.tar', 55.79799996826172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-96.pth.tar', 55.78999997070313)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-97.pth.tar', 55.74600006835937)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-93.pth.tar', 55.58000012207031)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-95.pth.tar', 55.57800007324219)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-94.pth.tar', 55.563999995117186)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-92.pth.tar', 55.528000048828126)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-90.pth.tar', 55.281999868164064)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-91.pth.tar', 55.24399991699219)

*** Best metric: 55.86199999511719 (epoch 98)

wandb: Waiting for W&B process to finish, PID 31631
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210528_150725-PreTraining_vit_deit_tiny_patch16_224_1k/logs/debug.log
wandb: Find internal logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210528_150725-PreTraining_vit_deit_tiny_patch16_224_1k/logs/debug-internal.log
wandb: Run summary:
wandb:    eval_loss 2.04007
wandb:    eval_top1 55.798
wandb:    eval_top5 79.692
wandb:   _timestamp 1622218277
wandb:   train_loss 4.57476
wandb:        _step 99
wandb:        epoch 99
wandb:     _runtime 329256
wandb: Run history:
wandb:        epoch ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb:   train_loss ‚ñÇ‚ñà‚ñÅ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÉ‚ñÑ‚ñÅ
wandb:    eval_loss ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    eval_top1 ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñá‚ñà‚ñà
wandb:    eval_top5 ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:     _runtime ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb:   _timestamp ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb:        _step ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced PreTraining_vit_deit_tiny_patch16_224_1k: https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_1k
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
--End--
Sat May 29 01:11:29 JST 2021

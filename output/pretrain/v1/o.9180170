--Start--
Tue Jun 1 22:39:57 JST 2021
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 0
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
wandb: Currently logged in as: gyama_x (use `wandb login --relogin` to force relogin)
wandb: WARNING Tried to auto resume run with id PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1 but id PreTraining_vit_deit_tiny_patch16_224_1k is set.
wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
wandb: Tracking run with wandb version 0.10.27
wandb: Resuming run PreTraining_vit_deit_tiny_patch16_224_1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gyama_x/pytorch-image-models
wandb: üöÄ View run at https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_1k
wandb: Run data is saved locally in /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210601_224040-PreTraining_vit_deit_tiny_patch16_224_1k
wandb: Run `wandb offline` to turn off syncing.

Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar' (epoch 143)
Using native Torch DistributedDataParallel.
Scheduled epochs: 166
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Train: 144 [   0/1251 (  0%)]  Loss:  4.690602 (4.6906)  Time: 14.899s,   68.73/s  (14.899s,   68.73/s)  LR: 5.229e-05  Data: 13.740 (13.740)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 144 [  50/1251 (  4%)]  Loss:  4.822827 (4.7567)  Time: 0.584s, 1752.98/s  (2.483s,  412.45/s)  LR: 5.229e-05  Data: 0.020 (1.887)
Train: 144 [ 100/1251 (  8%)]  Loss:  4.647158 (4.7202)  Time: 0.584s, 1752.24/s  (2.513s,  407.44/s)  LR: 5.229e-05  Data: 0.021 (1.921)
Train: 144 [ 150/1251 ( 12%)]  Loss:  4.276861 (4.6094)  Time: 0.841s, 1217.57/s  (2.402s,  426.29/s)  LR: 5.229e-05  Data: 0.261 (1.807)
Train: 144 [ 200/1251 ( 16%)]  Loss:  4.450461 (4.5776)  Time: 0.583s, 1756.96/s  (2.394s,  427.67/s)  LR: 5.229e-05  Data: 0.020 (1.797)
Train: 144 [ 250/1251 ( 20%)]  Loss:  4.101699 (4.4983)  Time: 4.377s,  233.95/s  (2.457s,  416.79/s)  LR: 5.229e-05  Data: 3.698 (1.856)
Train: 144 [ 300/1251 ( 24%)]  Loss:  4.728553 (4.5312)  Time: 0.592s, 1730.78/s  (2.436s,  420.41/s)  LR: 5.229e-05  Data: 0.018 (1.834)
Train: 144 [ 350/1251 ( 28%)]  Loss:  4.219921 (4.4923)  Time: 2.174s,  470.97/s  (2.417s,  423.63/s)  LR: 5.229e-05  Data: 1.575 (1.815)
Train: 144 [ 400/1251 ( 32%)]  Loss:  4.723628 (4.5180)  Time: 0.581s, 1761.44/s  (2.390s,  428.51/s)  LR: 5.229e-05  Data: 0.019 (1.788)
Train: 144 [ 450/1251 ( 36%)]  Loss:  4.236647 (4.4898)  Time: 0.587s, 1744.89/s  (2.389s,  428.65/s)  LR: 5.229e-05  Data: 0.020 (1.790)
Train: 144 [ 500/1251 ( 40%)]  Loss:  4.417862 (4.4833)  Time: 0.583s, 1757.88/s  (2.399s,  426.79/s)  LR: 5.229e-05  Data: 0.019 (1.802)
Train: 144 [ 550/1251 ( 44%)]  Loss:  4.548090 (4.4887)  Time: 0.584s, 1752.95/s  (2.374s,  431.42/s)  LR: 5.229e-05  Data: 0.017 (1.778)
Train: 144 [ 600/1251 ( 48%)]  Loss:  5.195280 (4.5430)  Time: 0.583s, 1755.26/s  (2.357s,  434.42/s)  LR: 5.229e-05  Data: 0.020 (1.761)
Train: 144 [ 650/1251 ( 52%)]  Loss:  4.416590 (4.5340)  Time: 0.583s, 1755.62/s  (2.335s,  438.45/s)  LR: 5.229e-05  Data: 0.019 (1.740)
Train: 144 [ 700/1251 ( 56%)]  Loss:  4.645158 (4.5414)  Time: 0.588s, 1741.93/s  (2.324s,  440.68/s)  LR: 5.229e-05  Data: 0.025 (1.729)
Train: 144 [ 750/1251 ( 60%)]  Loss:  4.975517 (4.5686)  Time: 0.584s, 1753.41/s  (2.308s,  443.67/s)  LR: 5.229e-05  Data: 0.019 (1.713)
Train: 144 [ 800/1251 ( 64%)]  Loss:  4.501794 (4.5646)  Time: 0.584s, 1754.25/s  (2.300s,  445.30/s)  LR: 5.229e-05  Data: 0.021 (1.706)
Train: 144 [ 850/1251 ( 68%)]  Loss:  4.557328 (4.5642)  Time: 0.586s, 1747.03/s  (2.279s,  449.36/s)  LR: 5.229e-05  Data: 0.023 (1.686)
Train: 144 [ 900/1251 ( 72%)]  Loss:  4.737297 (4.5733)  Time: 0.585s, 1749.26/s  (2.284s,  448.35/s)  LR: 5.229e-05  Data: 0.022 (1.692)
Train: 144 [ 950/1251 ( 76%)]  Loss:  4.288052 (4.5591)  Time: 0.585s, 1749.56/s  (2.277s,  449.74/s)  LR: 5.229e-05  Data: 0.021 (1.686)
Train: 144 [1000/1251 ( 80%)]  Loss:  4.031796 (4.5340)  Time: 0.587s, 1744.40/s  (2.277s,  449.81/s)  LR: 5.229e-05  Data: 0.020 (1.685)
Train: 144 [1050/1251 ( 84%)]  Loss:  4.623732 (4.5380)  Time: 0.585s, 1750.18/s  (2.270s,  451.11/s)  LR: 5.229e-05  Data: 0.018 (1.679)
Train: 144 [1100/1251 ( 88%)]  Loss:  4.547302 (4.5384)  Time: 0.585s, 1750.48/s  (2.268s,  451.47/s)  LR: 5.229e-05  Data: 0.018 (1.677)
Train: 144 [1150/1251 ( 92%)]  Loss:  4.179070 (4.5235)  Time: 0.585s, 1751.29/s  (2.260s,  453.01/s)  LR: 5.229e-05  Data: 0.020 (1.669)
Train: 144 [1200/1251 ( 96%)]  Loss:  4.243634 (4.5123)  Time: 0.586s, 1746.79/s  (2.255s,  454.08/s)  LR: 5.229e-05  Data: 0.023 (1.663)
Train: 144 [1250/1251 (100%)]  Loss:  4.535351 (4.5132)  Time: 0.563s, 1819.41/s  (2.246s,  455.91/s)  LR: 5.229e-05  Data: 0.000 (1.654)
Test: [   0/48]  Time: 13.838 (13.838)  Loss:  1.1162 (1.1162)  Acc@1: 76.2695 (76.2695)  Acc@5: 92.1875 (92.1875)
Test: [  48/48]  Time: 0.518 (3.437)  Loss:  1.0950 (1.9663)  Acc@1: 77.3585 (56.9520)  Acc@5: 91.3915 (80.7800)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-144.pth.tar', 56.95199999023438)

Train: 145 [   0/1251 (  0%)]  Loss:  4.459843 (4.4598)  Time: 10.672s,   95.95/s  (10.672s,   95.95/s)  LR: 4.858e-05  Data: 9.753 (9.753)
Train: 145 [  50/1251 (  4%)]  Loss:  4.285009 (4.3724)  Time: 0.588s, 1740.24/s  (2.486s,  411.92/s)  LR: 4.858e-05  Data: 0.019 (1.896)
Train: 145 [ 100/1251 (  8%)]  Loss:  4.811609 (4.5188)  Time: 0.586s, 1748.39/s  (2.402s,  426.23/s)  LR: 4.858e-05  Data: 0.021 (1.814)
Train: 145 [ 150/1251 ( 12%)]  Loss:  4.281488 (4.4595)  Time: 0.585s, 1750.46/s  (2.323s,  440.77/s)  LR: 4.858e-05  Data: 0.018 (1.736)
Train: 145 [ 200/1251 ( 16%)]  Loss:  4.521384 (4.4719)  Time: 0.586s, 1747.71/s  (2.330s,  439.55/s)  LR: 4.858e-05  Data: 0.020 (1.739)
Train: 145 [ 250/1251 ( 20%)]  Loss:  3.829108 (4.3647)  Time: 0.586s, 1748.51/s  (2.284s,  448.37/s)  LR: 4.858e-05  Data: 0.022 (1.696)
Train: 145 [ 300/1251 ( 24%)]  Loss:  5.246102 (4.4906)  Time: 0.588s, 1741.86/s  (2.270s,  451.19/s)  LR: 4.858e-05  Data: 0.019 (1.680)
Train: 145 [ 350/1251 ( 28%)]  Loss:  4.233515 (4.4585)  Time: 2.117s,  483.81/s  (2.238s,  457.52/s)  LR: 4.858e-05  Data: 1.554 (1.648)
Train: 145 [ 400/1251 ( 32%)]  Loss:  4.673565 (4.4824)  Time: 0.583s, 1757.51/s  (2.268s,  451.53/s)  LR: 4.858e-05  Data: 0.019 (1.674)
Train: 145 [ 450/1251 ( 36%)]  Loss:  4.834086 (4.5176)  Time: 1.047s,  978.10/s  (2.281s,  448.87/s)  LR: 4.858e-05  Data: 0.429 (1.684)
Train: 145 [ 500/1251 ( 40%)]  Loss:  4.275743 (4.4956)  Time: 1.846s,  554.66/s  (2.283s,  448.59/s)  LR: 4.858e-05  Data: 1.257 (1.684)
Train: 145 [ 550/1251 ( 44%)]  Loss:  4.717838 (4.5141)  Time: 0.590s, 1736.15/s  (2.286s,  448.02/s)  LR: 4.858e-05  Data: 0.019 (1.687)
Train: 145 [ 600/1251 ( 48%)]  Loss:  4.791574 (4.5355)  Time: 0.585s, 1751.67/s  (2.288s,  447.46/s)  LR: 4.858e-05  Data: 0.018 (1.689)
Train: 145 [ 650/1251 ( 52%)]  Loss:  4.935190 (4.5640)  Time: 3.250s,  315.04/s  (2.299s,  445.35/s)  LR: 4.858e-05  Data: 2.688 (1.700)
Train: 145 [ 700/1251 ( 56%)]  Loss:  4.378230 (4.5516)  Time: 0.587s, 1743.35/s  (2.298s,  445.61/s)  LR: 4.858e-05  Data: 0.020 (1.698)
Train: 145 [ 750/1251 ( 60%)]  Loss:  4.864783 (4.5712)  Time: 2.898s,  353.33/s  (2.313s,  442.78/s)  LR: 4.858e-05  Data: 2.193 (1.712)
Train: 145 [ 800/1251 ( 64%)]  Loss:  5.070901 (4.6006)  Time: 0.590s, 1734.63/s  (2.310s,  443.28/s)  LR: 4.858e-05  Data: 0.024 (1.710)
Train: 145 [ 850/1251 ( 68%)]  Loss:  4.602036 (4.6007)  Time: 0.585s, 1751.74/s  (2.316s,  442.13/s)  LR: 4.858e-05  Data: 0.021 (1.715)
Train: 145 [ 900/1251 ( 72%)]  Loss:  4.639535 (4.6027)  Time: 0.586s, 1748.62/s  (2.318s,  441.83/s)  LR: 4.858e-05  Data: 0.022 (1.716)
Train: 145 [ 950/1251 ( 76%)]  Loss:  4.277034 (4.5864)  Time: 1.896s,  539.98/s  (2.319s,  441.65/s)  LR: 4.858e-05  Data: 1.334 (1.716)
Train: 145 [1000/1251 ( 80%)]  Loss:  4.663446 (4.5901)  Time: 0.586s, 1748.23/s  (2.318s,  441.84/s)  LR: 4.858e-05  Data: 0.022 (1.716)
Train: 145 [1050/1251 ( 84%)]  Loss:  4.795832 (4.5994)  Time: 0.811s, 1263.05/s  (2.309s,  443.44/s)  LR: 4.858e-05  Data: 0.130 (1.708)
Train: 145 [1100/1251 ( 88%)]  Loss:  4.994758 (4.6166)  Time: 0.585s, 1750.31/s  (2.309s,  443.39/s)  LR: 4.858e-05  Data: 0.020 (1.709)
Train: 145 [1150/1251 ( 92%)]  Loss:  5.008527 (4.6330)  Time: 0.587s, 1744.75/s  (2.312s,  442.93/s)  LR: 4.858e-05  Data: 0.021 (1.712)
Train: 145 [1200/1251 ( 96%)]  Loss:  4.536597 (4.6291)  Time: 0.587s, 1743.73/s  (2.307s,  443.80/s)  LR: 4.858e-05  Data: 0.018 (1.708)
Train: 145 [1250/1251 (100%)]  Loss:  4.769353 (4.6345)  Time: 0.563s, 1819.98/s  (2.300s,  445.16/s)  LR: 4.858e-05  Data: 0.000 (1.702)
Test: [   0/48]  Time: 13.500 (13.500)  Loss:  1.0920 (1.0920)  Acc@1: 77.4414 (77.4414)  Acc@5: 92.1875 (92.1875)
Test: [  48/48]  Time: 0.149 (3.204)  Loss:  1.1182 (1.9516)  Acc@1: 75.9434 (57.0420)  Acc@5: 91.1557 (80.8140)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-145.pth.tar', 57.042000021972655)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-144.pth.tar', 56.95199999023438)

Train: 146 [   0/1251 (  0%)]  Loss:  4.690775 (4.6908)  Time: 11.753s,   87.13/s  (11.753s,   87.13/s)  LR: 4.504e-05  Data: 10.338 (10.338)
Train: 146 [  50/1251 (  4%)]  Loss:  4.294343 (4.4926)  Time: 0.585s, 1750.35/s  (2.266s,  451.80/s)  LR: 4.504e-05  Data: 0.022 (1.666)
Train: 146 [ 100/1251 (  8%)]  Loss:  4.360450 (4.4485)  Time: 4.126s,  248.20/s  (2.258s,  453.54/s)  LR: 4.504e-05  Data: 3.506 (1.650)
Train: 146 [ 150/1251 ( 12%)]  Loss:  4.915571 (4.5653)  Time: 0.585s, 1749.60/s  (2.182s,  469.27/s)  LR: 4.504e-05  Data: 0.023 (1.574)
Train: 146 [ 200/1251 ( 16%)]  Loss:  4.877958 (4.6278)  Time: 6.018s,  170.16/s  (2.166s,  472.80/s)  LR: 4.504e-05  Data: 5.356 (1.560)
Train: 146 [ 250/1251 ( 20%)]  Loss:  4.940470 (4.6799)  Time: 0.588s, 1741.79/s  (2.204s,  464.62/s)  LR: 4.504e-05  Data: 0.025 (1.600)
Train: 146 [ 300/1251 ( 24%)]  Loss:  4.732737 (4.6875)  Time: 7.583s,  135.04/s  (2.238s,  457.62/s)  LR: 4.504e-05  Data: 6.881 (1.632)
Train: 146 [ 350/1251 ( 28%)]  Loss:  4.568593 (4.6726)  Time: 0.586s, 1748.29/s  (2.232s,  458.72/s)  LR: 4.504e-05  Data: 0.019 (1.629)
Train: 146 [ 400/1251 ( 32%)]  Loss:  4.641914 (4.6692)  Time: 7.181s,  142.60/s  (2.238s,  457.63/s)  LR: 4.504e-05  Data: 6.515 (1.638)
Train: 146 [ 450/1251 ( 36%)]  Loss:  4.661861 (4.6685)  Time: 0.589s, 1739.26/s  (2.231s,  458.90/s)  LR: 4.504e-05  Data: 0.020 (1.633)
Train: 146 [ 500/1251 ( 40%)]  Loss:  4.283528 (4.6335)  Time: 7.018s,  145.92/s  (2.242s,  456.78/s)  LR: 4.504e-05  Data: 6.349 (1.644)
Train: 146 [ 550/1251 ( 44%)]  Loss:  4.179682 (4.5957)  Time: 0.587s, 1743.22/s  (2.237s,  457.75/s)  LR: 4.504e-05  Data: 0.019 (1.638)
Train: 146 [ 600/1251 ( 48%)]  Loss:  4.201649 (4.5653)  Time: 6.253s,  163.75/s  (2.241s,  457.00/s)  LR: 4.504e-05  Data: 5.590 (1.643)
Train: 146 [ 650/1251 ( 52%)]  Loss:  3.951103 (4.5215)  Time: 0.583s, 1755.13/s  (2.279s,  449.37/s)  LR: 4.504e-05  Data: 0.021 (1.682)
Train: 146 [ 700/1251 ( 56%)]  Loss:  4.424321 (4.5150)  Time: 8.433s,  121.43/s  (2.306s,  444.13/s)  LR: 4.504e-05  Data: 7.849 (1.710)
Train: 146 [ 750/1251 ( 60%)]  Loss:  4.960851 (4.5429)  Time: 0.589s, 1740.01/s  (2.298s,  445.53/s)  LR: 4.504e-05  Data: 0.024 (1.703)
Train: 146 [ 800/1251 ( 64%)]  Loss:  4.574391 (4.5447)  Time: 7.031s,  145.64/s  (2.301s,  445.12/s)  LR: 4.504e-05  Data: 6.468 (1.706)
Train: 146 [ 850/1251 ( 68%)]  Loss:  3.761193 (4.5012)  Time: 0.584s, 1753.08/s  (2.298s,  445.69/s)  LR: 4.504e-05  Data: 0.020 (1.703)
Train: 146 [ 900/1251 ( 72%)]  Loss:  4.701511 (4.5117)  Time: 4.880s,  209.82/s  (2.299s,  445.35/s)  LR: 4.504e-05  Data: 4.257 (1.705)
Train: 146 [ 950/1251 ( 76%)]  Loss:  4.890335 (4.5307)  Time: 0.586s, 1746.17/s  (2.292s,  446.82/s)  LR: 4.504e-05  Data: 0.023 (1.698)
Train: 146 [1000/1251 ( 80%)]  Loss:  4.588685 (4.5334)  Time: 5.167s,  198.20/s  (2.304s,  444.37/s)  LR: 4.504e-05  Data: 4.518 (1.711)
Train: 146 [1050/1251 ( 84%)]  Loss:  4.417271 (4.5281)  Time: 0.584s, 1753.43/s  (2.311s,  443.14/s)  LR: 4.504e-05  Data: 0.020 (1.717)
Train: 146 [1100/1251 ( 88%)]  Loss:  4.683263 (4.5349)  Time: 7.780s,  131.63/s  (2.326s,  440.23/s)  LR: 4.504e-05  Data: 7.078 (1.732)
Train: 146 [1150/1251 ( 92%)]  Loss:  4.169204 (4.5197)  Time: 0.588s, 1742.18/s  (2.325s,  440.41/s)  LR: 4.504e-05  Data: 0.021 (1.731)
Train: 146 [1200/1251 ( 96%)]  Loss:  4.490135 (4.5185)  Time: 7.996s,  128.06/s  (2.331s,  439.29/s)  LR: 4.504e-05  Data: 7.375 (1.738)
Train: 146 [1250/1251 (100%)]  Loss:  4.552880 (4.5198)  Time: 0.563s, 1817.91/s  (2.331s,  439.24/s)  LR: 4.504e-05  Data: 0.000 (1.738)
Test: [   0/48]  Time: 14.450 (14.450)  Loss:  1.1184 (1.1184)  Acc@1: 76.3672 (76.3672)  Acc@5: 91.5039 (91.5039)
Test: [  48/48]  Time: 0.148 (3.258)  Loss:  1.1192 (1.9573)  Acc@1: 75.9434 (56.8880)  Acc@5: 90.8019 (80.7160)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-145.pth.tar', 57.042000021972655)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-144.pth.tar', 56.95199999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-146.pth.tar', 56.88800002197266)

Train: 147 [   0/1251 (  0%)]  Loss:  4.520943 (4.5209)  Time: 10.935s,   93.64/s  (10.935s,   93.64/s)  LR: 4.166e-05  Data: 9.755 (9.755)
Train: 147 [  50/1251 (  4%)]  Loss:  4.374543 (4.4477)  Time: 0.590s, 1735.47/s  (2.606s,  392.89/s)  LR: 4.166e-05  Data: 0.023 (2.005)
Train: 147 [ 100/1251 (  8%)]  Loss:  4.668842 (4.5214)  Time: 0.717s, 1428.55/s  (2.459s,  416.41/s)  LR: 4.166e-05  Data: 0.117 (1.857)
Train: 147 [ 150/1251 ( 12%)]  Loss:  4.661263 (4.5564)  Time: 0.588s, 1741.82/s  (2.405s,  425.81/s)  LR: 4.166e-05  Data: 0.022 (1.808)
Train: 147 [ 200/1251 ( 16%)]  Loss:  4.497404 (4.5446)  Time: 1.782s,  574.53/s  (2.377s,  430.88/s)  LR: 4.166e-05  Data: 1.113 (1.778)
Train: 147 [ 250/1251 ( 20%)]  Loss:  4.611396 (4.5557)  Time: 0.587s, 1743.94/s  (2.348s,  436.20/s)  LR: 4.166e-05  Data: 0.020 (1.746)
Train: 147 [ 300/1251 ( 24%)]  Loss:  4.795776 (4.5900)  Time: 1.613s,  634.76/s  (2.354s,  435.09/s)  LR: 4.166e-05  Data: 1.047 (1.750)
Train: 147 [ 350/1251 ( 28%)]  Loss:  4.813461 (4.6180)  Time: 0.587s, 1745.82/s  (2.353s,  435.27/s)  LR: 4.166e-05  Data: 0.021 (1.750)
Train: 147 [ 400/1251 ( 32%)]  Loss:  4.363977 (4.5897)  Time: 0.589s, 1738.17/s  (2.323s,  440.72/s)  LR: 4.166e-05  Data: 0.023 (1.724)
Train: 147 [ 450/1251 ( 36%)]  Loss:  3.989239 (4.5297)  Time: 1.026s,  997.69/s  (2.326s,  440.26/s)  LR: 4.166e-05  Data: 0.452 (1.729)
Train: 147 [ 500/1251 ( 40%)]  Loss:  5.054996 (4.5774)  Time: 0.587s, 1744.49/s  (2.331s,  439.32/s)  LR: 4.166e-05  Data: 0.022 (1.734)
Train: 147 [ 550/1251 ( 44%)]  Loss:  4.768155 (4.5933)  Time: 0.588s, 1742.89/s  (2.330s,  439.55/s)  LR: 4.166e-05  Data: 0.024 (1.733)
Train: 147 [ 600/1251 ( 48%)]  Loss:  4.841327 (4.6124)  Time: 0.587s, 1744.92/s  (2.345s,  436.65/s)  LR: 4.166e-05  Data: 0.022 (1.750)
Train: 147 [ 650/1251 ( 52%)]  Loss:  4.870811 (4.6309)  Time: 0.588s, 1741.60/s  (2.348s,  436.17/s)  LR: 4.166e-05  Data: 0.024 (1.754)
Train: 147 [ 700/1251 ( 56%)]  Loss:  4.952314 (4.6523)  Time: 0.585s, 1751.04/s  (2.357s,  434.48/s)  LR: 4.166e-05  Data: 0.022 (1.764)
Train: 147 [ 750/1251 ( 60%)]  Loss:  4.842552 (4.6642)  Time: 0.586s, 1746.57/s  (2.355s,  434.81/s)  LR: 4.166e-05  Data: 0.024 (1.763)
Train: 147 [ 800/1251 ( 64%)]  Loss:  4.692590 (4.6659)  Time: 0.586s, 1748.41/s  (2.353s,  435.28/s)  LR: 4.166e-05  Data: 0.019 (1.761)
Train: 147 [ 850/1251 ( 68%)]  Loss:  4.151536 (4.6373)  Time: 0.583s, 1755.92/s  (2.360s,  433.81/s)  LR: 4.166e-05  Data: 0.020 (1.769)
Train: 147 [ 900/1251 ( 72%)]  Loss:  4.252559 (4.6170)  Time: 0.594s, 1724.29/s  (2.369s,  432.33/s)  LR: 4.166e-05  Data: 0.017 (1.777)
Train: 147 [ 950/1251 ( 76%)]  Loss:  4.398064 (4.6061)  Time: 0.584s, 1753.17/s  (2.364s,  433.15/s)  LR: 4.166e-05  Data: 0.021 (1.773)
Train: 147 [1000/1251 ( 80%)]  Loss:  4.139190 (4.5839)  Time: 0.587s, 1743.38/s  (2.367s,  432.68/s)  LR: 4.166e-05  Data: 0.020 (1.776)
Train: 147 [1050/1251 ( 84%)]  Loss:  4.899526 (4.5982)  Time: 0.953s, 1075.02/s  (2.362s,  433.54/s)  LR: 4.166e-05  Data: 0.368 (1.772)
Train: 147 [1100/1251 ( 88%)]  Loss:  4.717493 (4.6034)  Time: 0.589s, 1739.24/s  (2.362s,  433.61/s)  LR: 4.166e-05  Data: 0.020 (1.771)
Train: 147 [1150/1251 ( 92%)]  Loss:  4.575769 (4.6022)  Time: 0.589s, 1738.36/s  (2.353s,  435.24/s)  LR: 4.166e-05  Data: 0.021 (1.762)
Train: 147 [1200/1251 ( 96%)]  Loss:  4.390003 (4.5937)  Time: 0.588s, 1742.04/s  (2.362s,  433.56/s)  LR: 4.166e-05  Data: 0.025 (1.772)
Train: 147 [1250/1251 (100%)]  Loss:  4.363606 (4.5849)  Time: 0.563s, 1819.87/s  (2.357s,  434.54/s)  LR: 4.166e-05  Data: 0.000 (1.766)
Test: [   0/48]  Time: 15.752 (15.752)  Loss:  1.0839 (1.0839)  Acc@1: 77.2461 (77.2461)  Acc@5: 92.1875 (92.1875)
Test: [  48/48]  Time: 0.149 (3.425)  Loss:  1.0849 (1.9480)  Acc@1: 77.3585 (57.1560)  Acc@5: 91.2736 (80.8360)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-147.pth.tar', 57.15599999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-145.pth.tar', 57.042000021972655)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-144.pth.tar', 56.95199999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-146.pth.tar', 56.88800002197266)

Train: 148 [   0/1251 (  0%)]  Loss:  4.546706 (4.5467)  Time: 12.006s,   85.29/s  (12.006s,   85.29/s)  LR: 3.844e-05  Data: 10.836 (10.836)
Train: 148 [  50/1251 (  4%)]  Loss:  4.258740 (4.4027)  Time: 0.588s, 1740.46/s  (2.444s,  419.04/s)  LR: 3.844e-05  Data: 0.020 (1.848)
Train: 148 [ 100/1251 (  8%)]  Loss:  4.882102 (4.5625)  Time: 0.589s, 1738.12/s  (2.360s,  433.81/s)  LR: 3.844e-05  Data: 0.022 (1.756)
Train: 148 [ 150/1251 ( 12%)]  Loss:  3.844350 (4.3830)  Time: 1.823s,  561.71/s  (2.273s,  450.60/s)  LR: 3.844e-05  Data: 1.224 (1.672)
Train: 148 [ 200/1251 ( 16%)]  Loss:  4.974665 (4.5013)  Time: 0.584s, 1752.16/s  (2.251s,  454.82/s)  LR: 3.844e-05  Data: 0.019 (1.655)
Train: 148 [ 250/1251 ( 20%)]  Loss:  4.792124 (4.5498)  Time: 3.379s,  303.01/s  (2.225s,  460.32/s)  LR: 3.844e-05  Data: 2.801 (1.627)
Train: 148 [ 300/1251 ( 24%)]  Loss:  4.661400 (4.5657)  Time: 0.586s, 1748.44/s  (2.259s,  453.26/s)  LR: 3.844e-05  Data: 0.020 (1.661)
Train: 148 [ 350/1251 ( 28%)]  Loss:  4.161975 (4.5153)  Time: 0.586s, 1747.40/s  (2.265s,  452.11/s)  LR: 3.844e-05  Data: 0.019 (1.668)
Train: 148 [ 400/1251 ( 32%)]  Loss:  5.098773 (4.5801)  Time: 0.585s, 1750.74/s  (2.278s,  449.56/s)  LR: 3.844e-05  Data: 0.022 (1.681)
Train: 148 [ 450/1251 ( 36%)]  Loss:  4.214081 (4.5435)  Time: 0.586s, 1746.86/s  (2.276s,  449.99/s)  LR: 3.844e-05  Data: 0.020 (1.679)
Train: 148 [ 500/1251 ( 40%)]  Loss:  4.203139 (4.5126)  Time: 1.347s,  760.25/s  (2.272s,  450.61/s)  LR: 3.844e-05  Data: 0.682 (1.675)
Train: 148 [ 550/1251 ( 44%)]  Loss:  4.337379 (4.4980)  Time: 0.586s, 1747.40/s  (2.264s,  452.36/s)  LR: 3.844e-05  Data: 0.020 (1.665)
Train: 148 [ 600/1251 ( 48%)]  Loss:  4.897767 (4.5287)  Time: 0.585s, 1751.08/s  (2.267s,  451.76/s)  LR: 3.844e-05  Data: 0.022 (1.668)
Train: 148 [ 650/1251 ( 52%)]  Loss:  4.319305 (4.5138)  Time: 0.586s, 1748.22/s  (2.264s,  452.27/s)  LR: 3.844e-05  Data: 0.018 (1.667)
Train: 148 [ 700/1251 ( 56%)]  Loss:  4.148129 (4.4894)  Time: 0.589s, 1738.15/s  (2.305s,  444.34/s)  LR: 3.844e-05  Data: 0.026 (1.707)
Train: 148 [ 750/1251 ( 60%)]  Loss:  4.548691 (4.4931)  Time: 0.588s, 1742.15/s  (2.316s,  442.20/s)  LR: 3.844e-05  Data: 0.018 (1.717)
Train: 148 [ 800/1251 ( 64%)]  Loss:  4.439749 (4.4899)  Time: 0.590s, 1736.86/s  (2.319s,  441.63/s)  LR: 3.844e-05  Data: 0.026 (1.718)
Train: 148 [ 850/1251 ( 68%)]  Loss:  4.455288 (4.4880)  Time: 0.586s, 1748.44/s  (2.320s,  441.42/s)  LR: 3.844e-05  Data: 0.018 (1.720)
Train: 148 [ 900/1251 ( 72%)]  Loss:  4.116677 (4.4685)  Time: 0.587s, 1743.96/s  (2.323s,  440.90/s)  LR: 3.844e-05  Data: 0.024 (1.723)
Train: 148 [ 950/1251 ( 76%)]  Loss:  4.675602 (4.4788)  Time: 0.589s, 1739.80/s  (2.321s,  441.22/s)  LR: 3.844e-05  Data: 0.020 (1.722)
Train: 148 [1000/1251 ( 80%)]  Loss:  4.718200 (4.4902)  Time: 0.587s, 1744.05/s  (2.321s,  441.22/s)  LR: 3.844e-05  Data: 0.024 (1.722)
Train: 148 [1050/1251 ( 84%)]  Loss:  4.280625 (4.4807)  Time: 0.586s, 1747.50/s  (2.332s,  439.13/s)  LR: 3.844e-05  Data: 0.021 (1.733)
Train: 148 [1100/1251 ( 88%)]  Loss:  4.356088 (4.4753)  Time: 0.590s, 1736.39/s  (2.333s,  438.98/s)  LR: 3.844e-05  Data: 0.026 (1.734)
Train: 148 [1150/1251 ( 92%)]  Loss:  5.065678 (4.4999)  Time: 0.585s, 1751.09/s  (2.334s,  438.81/s)  LR: 3.844e-05  Data: 0.022 (1.735)
Train: 148 [1200/1251 ( 96%)]  Loss:  5.061328 (4.5223)  Time: 0.587s, 1744.05/s  (2.334s,  438.66/s)  LR: 3.844e-05  Data: 0.020 (1.736)
Train: 148 [1250/1251 (100%)]  Loss:  4.461907 (4.5200)  Time: 0.564s, 1817.15/s  (2.336s,  438.35/s)  LR: 3.844e-05  Data: 0.000 (1.738)
Test: [   0/48]  Time: 14.479 (14.479)  Loss:  1.0393 (1.0393)  Acc@1: 77.8320 (77.8320)  Acc@5: 92.4805 (92.4805)
Test: [  48/48]  Time: 0.148 (3.315)  Loss:  1.0559 (1.9380)  Acc@1: 77.3585 (57.2160)  Acc@5: 91.8632 (80.9660)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-148.pth.tar', 57.21599999023437)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-147.pth.tar', 57.15599999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-145.pth.tar', 57.042000021972655)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-144.pth.tar', 56.95199999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-146.pth.tar', 56.88800002197266)

Train: 149 [   0/1251 (  0%)]  Loss:  4.778304 (4.7783)  Time: 9.396s,  108.99/s  (9.396s,  108.99/s)  LR: 3.540e-05  Data: 8.634 (8.634)
Train: 149 [  50/1251 (  4%)]  Loss:  4.827219 (4.8028)  Time: 0.588s, 1742.77/s  (2.332s,  439.08/s)  LR: 3.540e-05  Data: 0.020 (1.733)
Train: 149 [ 100/1251 (  8%)]  Loss:  4.298823 (4.6348)  Time: 0.585s, 1749.45/s  (2.505s,  408.73/s)  LR: 3.540e-05  Data: 0.021 (1.906)
Train: 149 [ 150/1251 ( 12%)]  Loss:  4.391167 (4.5739)  Time: 0.585s, 1750.96/s  (2.411s,  424.73/s)  LR: 3.540e-05  Data: 0.020 (1.814)
Train: 149 [ 200/1251 ( 16%)]  Loss:  4.689254 (4.5970)  Time: 1.985s,  515.79/s  (2.405s,  425.79/s)  LR: 3.540e-05  Data: 1.308 (1.806)
Train: 149 [ 250/1251 ( 20%)]  Loss:  4.634079 (4.6031)  Time: 0.586s, 1748.07/s  (2.381s,  430.05/s)  LR: 3.540e-05  Data: 0.020 (1.781)
Train: 149 [ 300/1251 ( 24%)]  Loss:  4.341173 (4.5657)  Time: 2.508s,  408.34/s  (2.376s,  430.94/s)  LR: 3.540e-05  Data: 1.866 (1.772)
Train: 149 [ 350/1251 ( 28%)]  Loss:  4.336744 (4.5371)  Time: 0.587s, 1743.87/s  (2.345s,  436.60/s)  LR: 3.540e-05  Data: 0.020 (1.742)
Train: 149 [ 400/1251 ( 32%)]  Loss:  4.947624 (4.5827)  Time: 0.969s, 1056.35/s  (2.339s,  437.82/s)  LR: 3.540e-05  Data: 0.304 (1.734)
Train: 149 [ 450/1251 ( 36%)]  Loss:  4.798838 (4.6043)  Time: 1.341s,  763.69/s  (2.327s,  440.00/s)  LR: 3.540e-05  Data: 0.763 (1.722)
Train: 149 [ 500/1251 ( 40%)]  Loss:  4.254103 (4.5725)  Time: 2.432s,  420.97/s  (2.366s,  432.77/s)  LR: 3.540e-05  Data: 1.341 (1.757)
Train: 149 [ 550/1251 ( 44%)]  Loss:  4.887786 (4.5988)  Time: 3.707s,  276.23/s  (2.381s,  430.12/s)  LR: 3.540e-05  Data: 2.797 (1.769)
Train: 149 [ 600/1251 ( 48%)]  Loss:  4.797011 (4.6140)  Time: 0.927s, 1104.06/s  (2.404s,  425.91/s)  LR: 3.540e-05  Data: 0.112 (1.792)
Train: 149 [ 650/1251 ( 52%)]  Loss:  4.886401 (4.6335)  Time: 2.455s,  417.02/s  (2.410s,  424.81/s)  LR: 3.540e-05  Data: 1.886 (1.800)
Train: 149 [ 700/1251 ( 56%)]  Loss:  4.945735 (4.6543)  Time: 0.586s, 1748.28/s  (2.416s,  423.83/s)  LR: 3.540e-05  Data: 0.021 (1.807)
Train: 149 [ 750/1251 ( 60%)]  Loss:  4.795429 (4.6631)  Time: 2.015s,  508.24/s  (2.404s,  425.99/s)  LR: 3.540e-05  Data: 1.436 (1.796)
Train: 149 [ 800/1251 ( 64%)]  Loss:  4.751744 (4.6683)  Time: 0.585s, 1749.55/s  (2.383s,  429.66/s)  LR: 3.540e-05  Data: 0.021 (1.776)
Train: 149 [ 850/1251 ( 68%)]  Loss:  4.507858 (4.6594)  Time: 2.818s,  363.33/s  (2.387s,  429.05/s)  LR: 3.540e-05  Data: 2.123 (1.781)
Train: 149 [ 900/1251 ( 72%)]  Loss:  4.481730 (4.6501)  Time: 0.586s, 1747.70/s  (2.387s,  429.00/s)  LR: 3.540e-05  Data: 0.020 (1.781)
Train: 149 [ 950/1251 ( 76%)]  Loss:  4.740758 (4.6546)  Time: 0.585s, 1749.20/s  (2.386s,  429.23/s)  LR: 3.540e-05  Data: 0.020 (1.779)
Train: 149 [1000/1251 ( 80%)]  Loss:  3.908399 (4.6191)  Time: 0.584s, 1753.80/s  (2.389s,  428.70/s)  LR: 3.540e-05  Data: 0.019 (1.782)
Train: 149 [1050/1251 ( 84%)]  Loss:  4.298071 (4.6045)  Time: 0.587s, 1745.04/s  (2.386s,  429.20/s)  LR: 3.540e-05  Data: 0.020 (1.780)
Train: 149 [1100/1251 ( 88%)]  Loss:  5.123611 (4.6270)  Time: 0.587s, 1743.71/s  (2.384s,  429.49/s)  LR: 3.540e-05  Data: 0.019 (1.779)
Train: 149 [1150/1251 ( 92%)]  Loss:  4.500848 (4.6218)  Time: 0.588s, 1741.58/s  (2.380s,  430.32/s)  LR: 3.540e-05  Data: 0.022 (1.774)
Train: 149 [1200/1251 ( 96%)]  Loss:  4.342025 (4.6106)  Time: 0.591s, 1732.30/s  (2.373s,  431.46/s)  LR: 3.540e-05  Data: 0.018 (1.767)
Train: 149 [1250/1251 (100%)]  Loss:  4.316516 (4.5993)  Time: 0.564s, 1816.13/s  (2.382s,  429.81/s)  LR: 3.540e-05  Data: 0.000 (1.776)
Test: [   0/48]  Time: 14.846 (14.846)  Loss:  1.0729 (1.0729)  Acc@1: 78.7109 (78.7109)  Acc@5: 92.6758 (92.6758)
Test: [  48/48]  Time: 0.148 (3.397)  Loss:  1.0850 (1.9425)  Acc@1: 77.4764 (57.5640)  Acc@5: 91.2736 (81.1020)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-149.pth.tar', 57.563999912109374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-148.pth.tar', 57.21599999023437)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-147.pth.tar', 57.15599999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-145.pth.tar', 57.042000021972655)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-144.pth.tar', 56.95199999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-146.pth.tar', 56.88800002197266)

Train: 150 [   0/1251 (  0%)]  Loss:  4.232398 (4.2324)  Time: 11.241s,   91.10/s  (11.241s,   91.10/s)  LR: 3.252e-05  Data: 10.388 (10.388)
Train: 150 [  50/1251 (  4%)]  Loss:  4.492315 (4.3624)  Time: 0.584s, 1754.37/s  (2.509s,  408.08/s)  LR: 3.252e-05  Data: 0.021 (1.917)
Train: 150 [ 100/1251 (  8%)]  Loss:  4.519986 (4.4149)  Time: 0.586s, 1746.62/s  (2.428s,  421.77/s)  LR: 3.252e-05  Data: 0.020 (1.830)
Train: 150 [ 150/1251 ( 12%)]  Loss:  4.719442 (4.4910)  Time: 0.587s, 1743.28/s  (2.350s,  435.72/s)  LR: 3.252e-05  Data: 0.023 (1.753)
Train: 150 [ 200/1251 ( 16%)]  Loss:  4.710258 (4.5349)  Time: 0.585s, 1750.11/s  (2.345s,  436.65/s)  LR: 3.252e-05  Data: 0.020 (1.752)
Train: 150 [ 250/1251 ( 20%)]  Loss:  4.831771 (4.5844)  Time: 0.585s, 1750.97/s  (2.294s,  446.39/s)  LR: 3.252e-05  Data: 0.022 (1.700)
Train: 150 [ 300/1251 ( 24%)]  Loss:  4.910264 (4.6309)  Time: 0.582s, 1758.37/s  (2.346s,  436.54/s)  LR: 3.252e-05  Data: 0.020 (1.752)
Train: 150 [ 350/1251 ( 28%)]  Loss:  4.568637 (4.6231)  Time: 0.584s, 1753.08/s  (2.316s,  442.08/s)  LR: 3.252e-05  Data: 0.021 (1.720)
Train: 150 [ 400/1251 ( 32%)]  Loss:  4.513646 (4.6110)  Time: 0.583s, 1757.64/s  (2.337s,  438.13/s)  LR: 3.252e-05  Data: 0.019 (1.742)
Train: 150 [ 450/1251 ( 36%)]  Loss:  4.993867 (4.6493)  Time: 0.589s, 1739.39/s  (2.333s,  438.83/s)  LR: 3.252e-05  Data: 0.019 (1.740)
Train: 150 [ 500/1251 ( 40%)]  Loss:  4.561494 (4.6413)  Time: 0.590s, 1736.40/s  (2.330s,  439.40/s)  LR: 3.252e-05  Data: 0.023 (1.737)
Train: 150 [ 550/1251 ( 44%)]  Loss:  4.173158 (4.6023)  Time: 0.584s, 1752.31/s  (2.320s,  441.36/s)  LR: 3.252e-05  Data: 0.022 (1.728)
Train: 150 [ 600/1251 ( 48%)]  Loss:  5.010881 (4.6337)  Time: 0.584s, 1754.04/s  (2.325s,  440.40/s)  LR: 3.252e-05  Data: 0.021 (1.732)
Train: 150 [ 650/1251 ( 52%)]  Loss:  4.424618 (4.6188)  Time: 0.585s, 1749.80/s  (2.318s,  441.73/s)  LR: 3.252e-05  Data: 0.023 (1.726)
Train: 150 [ 700/1251 ( 56%)]  Loss:  4.778749 (4.6294)  Time: 1.083s,  945.17/s  (2.352s,  435.30/s)  LR: 3.252e-05  Data: 0.023 (1.761)
Train: 150 [ 750/1251 ( 60%)]  Loss:  3.975001 (4.5885)  Time: 0.594s, 1723.62/s  (2.358s,  434.21/s)  LR: 3.252e-05  Data: 0.023 (1.767)
Train: 150 [ 800/1251 ( 64%)]  Loss:  4.243145 (4.5682)  Time: 0.586s, 1748.14/s  (2.366s,  432.74/s)  LR: 3.252e-05  Data: 0.023 (1.775)
Train: 150 [ 850/1251 ( 68%)]  Loss:  4.071131 (4.5406)  Time: 0.585s, 1750.15/s  (2.361s,  433.74/s)  LR: 3.252e-05  Data: 0.020 (1.769)
Train: 150 [ 900/1251 ( 72%)]  Loss:  5.077981 (4.5689)  Time: 0.590s, 1735.96/s  (2.364s,  433.23/s)  LR: 3.252e-05  Data: 0.026 (1.772)
Train: 150 [ 950/1251 ( 76%)]  Loss:  4.638775 (4.5724)  Time: 0.584s, 1752.38/s  (2.359s,  434.12/s)  LR: 3.252e-05  Data: 0.021 (1.768)
Train: 150 [1000/1251 ( 80%)]  Loss:  4.685912 (4.5778)  Time: 0.585s, 1751.40/s  (2.359s,  433.99/s)  LR: 3.252e-05  Data: 0.022 (1.769)
Train: 150 [1050/1251 ( 84%)]  Loss:  4.196982 (4.5605)  Time: 0.587s, 1743.72/s  (2.367s,  432.67/s)  LR: 3.252e-05  Data: 0.019 (1.776)
Train: 150 [1100/1251 ( 88%)]  Loss:  4.241202 (4.5466)  Time: 0.588s, 1741.40/s  (2.364s,  433.18/s)  LR: 3.252e-05  Data: 0.024 (1.774)
Train: 150 [1150/1251 ( 92%)]  Loss:  3.829900 (4.5167)  Time: 0.588s, 1742.52/s  (2.364s,  433.12/s)  LR: 3.252e-05  Data: 0.020 (1.775)
Train: 150 [1200/1251 ( 96%)]  Loss:  5.082272 (4.5394)  Time: 0.587s, 1743.97/s  (2.366s,  432.71/s)  LR: 3.252e-05  Data: 0.024 (1.776)
Train: 150 [1250/1251 (100%)]  Loss:  4.770815 (4.5483)  Time: 0.563s, 1818.69/s  (2.359s,  433.99/s)  LR: 3.252e-05  Data: 0.000 (1.769)
Test: [   0/48]  Time: 15.512 (15.512)  Loss:  1.0871 (1.0871)  Acc@1: 77.8320 (77.8320)  Acc@5: 92.6758 (92.6758)
Test: [  48/48]  Time: 0.149 (3.320)  Loss:  1.1082 (1.9449)  Acc@1: 76.6509 (57.3120)  Acc@5: 90.8019 (81.0020)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-149.pth.tar', 57.563999912109374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-150.pth.tar', 57.31199994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-148.pth.tar', 57.21599999023437)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-147.pth.tar', 57.15599999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-145.pth.tar', 57.042000021972655)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-144.pth.tar', 56.95199999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-146.pth.tar', 56.88800002197266)

Train: 151 [   0/1251 (  0%)]  Loss:  4.756227 (4.7562)  Time: 10.643s,   96.22/s  (10.643s,   96.22/s)  LR: 2.981e-05  Data: 9.812 (9.812)
Train: 151 [  50/1251 (  4%)]  Loss:  4.616610 (4.6864)  Time: 0.592s, 1729.37/s  (2.340s,  437.70/s)  LR: 2.981e-05  Data: 0.024 (1.737)
Train: 151 [ 100/1251 (  8%)]  Loss:  5.020888 (4.7979)  Time: 0.586s, 1746.50/s  (2.358s,  434.31/s)  LR: 2.981e-05  Data: 0.021 (1.765)
Train: 151 [ 150/1251 ( 12%)]  Loss:  4.172412 (4.6415)  Time: 0.585s, 1749.54/s  (2.357s,  434.38/s)  LR: 2.981e-05  Data: 0.023 (1.765)
Train: 151 [ 200/1251 ( 16%)]  Loss:  4.681897 (4.6496)  Time: 0.588s, 1740.08/s  (2.372s,  431.70/s)  LR: 2.981e-05  Data: 0.022 (1.779)
Train: 151 [ 250/1251 ( 20%)]  Loss:  4.204168 (4.5754)  Time: 0.584s, 1752.32/s  (2.343s,  437.05/s)  LR: 2.981e-05  Data: 0.019 (1.751)
Train: 151 [ 300/1251 ( 24%)]  Loss:  4.655441 (4.5868)  Time: 1.003s, 1020.70/s  (2.335s,  438.53/s)  LR: 2.981e-05  Data: 0.440 (1.744)
Train: 151 [ 350/1251 ( 28%)]  Loss:  4.656605 (4.5955)  Time: 0.587s, 1745.15/s  (2.332s,  439.13/s)  LR: 2.981e-05  Data: 0.023 (1.738)
Train: 151 [ 400/1251 ( 32%)]  Loss:  4.819416 (4.6204)  Time: 1.078s,  950.17/s  (2.315s,  442.33/s)  LR: 2.981e-05  Data: 0.375 (1.720)
Train: 151 [ 450/1251 ( 36%)]  Loss:  4.991659 (4.6575)  Time: 0.587s, 1743.33/s  (2.312s,  442.87/s)  LR: 2.981e-05  Data: 0.024 (1.717)
Train: 151 [ 500/1251 ( 40%)]  Loss:  4.111493 (4.6079)  Time: 0.587s, 1745.87/s  (2.333s,  438.83/s)  LR: 2.981e-05  Data: 0.020 (1.736)
Train: 151 [ 550/1251 ( 44%)]  Loss:  4.117852 (4.5671)  Time: 0.586s, 1747.19/s  (2.325s,  440.37/s)  LR: 2.981e-05  Data: 0.023 (1.729)
Train: 151 [ 600/1251 ( 48%)]  Loss:  4.194062 (4.5384)  Time: 0.584s, 1753.33/s  (2.354s,  435.02/s)  LR: 2.981e-05  Data: 0.021 (1.757)
Train: 151 [ 650/1251 ( 52%)]  Loss:  4.502069 (4.5358)  Time: 0.585s, 1751.78/s  (2.368s,  432.40/s)  LR: 2.981e-05  Data: 0.021 (1.770)
Train: 151 [ 700/1251 ( 56%)]  Loss:  4.911683 (4.5608)  Time: 0.588s, 1740.74/s  (2.374s,  431.31/s)  LR: 2.981e-05  Data: 0.021 (1.776)
Train: 151 [ 750/1251 ( 60%)]  Loss:  4.859921 (4.5795)  Time: 0.587s, 1744.61/s  (2.367s,  432.63/s)  LR: 2.981e-05  Data: 0.023 (1.769)
Train: 151 [ 800/1251 ( 64%)]  Loss:  4.436810 (4.5711)  Time: 0.586s, 1747.64/s  (2.362s,  433.49/s)  LR: 2.981e-05  Data: 0.023 (1.763)
Train: 151 [ 850/1251 ( 68%)]  Loss:  4.740779 (4.5806)  Time: 0.584s, 1753.42/s  (2.357s,  434.44/s)  LR: 2.981e-05  Data: 0.021 (1.756)
Train: 151 [ 900/1251 ( 72%)]  Loss:  4.625756 (4.5829)  Time: 0.585s, 1750.84/s  (2.371s,  431.79/s)  LR: 2.981e-05  Data: 0.022 (1.771)
Train: 151 [ 950/1251 ( 76%)]  Loss:  3.943810 (4.5510)  Time: 0.585s, 1750.76/s  (2.374s,  431.28/s)  LR: 2.981e-05  Data: 0.019 (1.775)
Train: 151 [1000/1251 ( 80%)]  Loss:  4.382622 (4.5430)  Time: 0.585s, 1751.01/s  (2.376s,  431.07/s)  LR: 2.981e-05  Data: 0.021 (1.776)
Train: 151 [1050/1251 ( 84%)]  Loss:  4.748796 (4.5523)  Time: 2.549s,  401.67/s  (2.369s,  432.26/s)  LR: 2.981e-05  Data: 1.893 (1.769)
Train: 151 [1100/1251 ( 88%)]  Loss:  4.512819 (4.5506)  Time: 0.584s, 1753.31/s  (2.365s,  433.07/s)  LR: 2.981e-05  Data: 0.022 (1.764)
Train: 151 [1150/1251 ( 92%)]  Loss:  4.666672 (4.5554)  Time: 1.329s,  770.77/s  (2.361s,  433.74/s)  LR: 2.981e-05  Data: 0.766 (1.760)
Train: 151 [1200/1251 ( 96%)]  Loss:  4.817058 (4.5659)  Time: 0.585s, 1749.37/s  (2.355s,  434.81/s)  LR: 2.981e-05  Data: 0.023 (1.755)
Train: 151 [1250/1251 (100%)]  Loss:  5.015621 (4.5832)  Time: 0.566s, 1809.84/s  (2.364s,  433.25/s)  LR: 2.981e-05  Data: 0.000 (1.763)
Test: [   0/48]  Time: 17.171 (17.171)  Loss:  1.1160 (1.1160)  Acc@1: 77.4414 (77.4414)  Acc@5: 92.1875 (92.1875)
Test: [  48/48]  Time: 0.149 (3.468)  Loss:  1.1232 (1.9652)  Acc@1: 77.4764 (57.4260)  Acc@5: 91.0377 (81.0100)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-149.pth.tar', 57.563999912109374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-151.pth.tar', 57.426000041503904)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-150.pth.tar', 57.31199994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-148.pth.tar', 57.21599999023437)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-147.pth.tar', 57.15599999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-145.pth.tar', 57.042000021972655)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-144.pth.tar', 56.95199999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-146.pth.tar', 56.88800002197266)

Train: 152 [   0/1251 (  0%)]  Loss:  4.676939 (4.6769)  Time: 11.690s,   87.60/s  (11.690s,   87.60/s)  LR: 2.727e-05  Data: 10.589 (10.589)
Train: 152 [  50/1251 (  4%)]  Loss:  4.772864 (4.7249)  Time: 0.588s, 1740.96/s  (2.456s,  416.94/s)  LR: 2.727e-05  Data: 0.021 (1.834)
Train: 152 [ 100/1251 (  8%)]  Loss:  4.864493 (4.7714)  Time: 0.585s, 1750.66/s  (2.458s,  416.63/s)  LR: 2.727e-05  Data: 0.022 (1.845)
Train: 152 [ 150/1251 ( 12%)]  Loss:  4.486221 (4.7001)  Time: 0.586s, 1746.54/s  (2.378s,  430.54/s)  LR: 2.727e-05  Data: 0.020 (1.769)
Train: 152 [ 200/1251 ( 16%)]  Loss:  3.904164 (4.5409)  Time: 1.157s,  885.10/s  (2.345s,  436.62/s)  LR: 2.727e-05  Data: 0.595 (1.733)
Train: 152 [ 250/1251 ( 20%)]  Loss:  4.938277 (4.6072)  Time: 0.591s, 1731.83/s  (2.318s,  441.69/s)  LR: 2.727e-05  Data: 0.020 (1.703)
Train: 152 [ 300/1251 ( 24%)]  Loss:  4.989100 (4.6617)  Time: 0.740s, 1383.98/s  (2.292s,  446.79/s)  LR: 2.727e-05  Data: 0.177 (1.674)
Train: 152 [ 350/1251 ( 28%)]  Loss:  4.573373 (4.6507)  Time: 0.986s, 1038.90/s  (2.334s,  438.74/s)  LR: 2.727e-05  Data: 0.338 (1.718)
Train: 152 [ 400/1251 ( 32%)]  Loss:  4.208893 (4.6016)  Time: 0.587s, 1743.77/s  (2.340s,  437.63/s)  LR: 2.727e-05  Data: 0.024 (1.725)
Train: 152 [ 450/1251 ( 36%)]  Loss:  4.484325 (4.5899)  Time: 1.816s,  563.78/s  (2.338s,  437.96/s)  LR: 2.727e-05  Data: 1.170 (1.724)
Train: 152 [ 500/1251 ( 40%)]  Loss:  5.155817 (4.6413)  Time: 0.587s, 1745.44/s  (2.334s,  438.65/s)  LR: 2.727e-05  Data: 0.021 (1.720)
Train: 152 [ 550/1251 ( 44%)]  Loss:  4.947817 (4.6669)  Time: 0.584s, 1752.00/s  (2.332s,  439.10/s)  LR: 2.727e-05  Data: 0.019 (1.720)
Train: 152 [ 600/1251 ( 48%)]  Loss:  4.138445 (4.6262)  Time: 0.586s, 1746.95/s  (2.332s,  439.09/s)  LR: 2.727e-05  Data: 0.022 (1.720)
Train: 152 [ 650/1251 ( 52%)]  Loss:  4.515870 (4.6183)  Time: 0.589s, 1738.90/s  (2.339s,  437.87/s)  LR: 2.727e-05  Data: 0.020 (1.727)
Train: 152 [ 700/1251 ( 56%)]  Loss:  4.348844 (4.6004)  Time: 0.586s, 1746.72/s  (2.365s,  432.90/s)  LR: 2.727e-05  Data: 0.021 (1.755)
Train: 152 [ 750/1251 ( 60%)]  Loss:  4.353113 (4.5849)  Time: 0.586s, 1746.40/s  (2.359s,  434.11/s)  LR: 2.727e-05  Data: 0.023 (1.749)
Train: 152 [ 800/1251 ( 64%)]  Loss:  5.124815 (4.6167)  Time: 0.586s, 1747.58/s  (2.364s,  433.26/s)  LR: 2.727e-05  Data: 0.021 (1.754)
Train: 152 [ 850/1251 ( 68%)]  Loss:  4.588151 (4.6151)  Time: 0.586s, 1746.90/s  (2.369s,  432.19/s)  LR: 2.727e-05  Data: 0.022 (1.761)
Train: 152 [ 900/1251 ( 72%)]  Loss:  4.659584 (4.6174)  Time: 0.587s, 1744.47/s  (2.369s,  432.28/s)  LR: 2.727e-05  Data: 0.023 (1.760)
Train: 152 [ 950/1251 ( 76%)]  Loss:  4.052303 (4.5892)  Time: 0.586s, 1747.47/s  (2.361s,  433.70/s)  LR: 2.727e-05  Data: 0.023 (1.753)
Train: 152 [1000/1251 ( 80%)]  Loss:  4.675890 (4.5933)  Time: 0.593s, 1727.80/s  (2.360s,  433.91/s)  LR: 2.727e-05  Data: 0.019 (1.752)
Train: 152 [1050/1251 ( 84%)]  Loss:  4.399808 (4.5845)  Time: 0.589s, 1739.17/s  (2.352s,  435.42/s)  LR: 2.727e-05  Data: 0.019 (1.745)
Train: 152 [1100/1251 ( 88%)]  Loss:  5.245547 (4.6132)  Time: 0.593s, 1726.23/s  (2.367s,  432.63/s)  LR: 2.727e-05  Data: 0.021 (1.761)
Train: 152 [1150/1251 ( 92%)]  Loss:  4.457995 (4.6068)  Time: 0.586s, 1747.54/s  (2.367s,  432.70/s)  LR: 2.727e-05  Data: 0.020 (1.762)
Train: 152 [1200/1251 ( 96%)]  Loss:  5.015436 (4.6231)  Time: 0.590s, 1736.08/s  (2.368s,  432.47/s)  LR: 2.727e-05  Data: 0.023 (1.763)
Train: 152 [1250/1251 (100%)]  Loss:  4.674665 (4.6251)  Time: 0.563s, 1817.38/s  (2.364s,  433.15/s)  LR: 2.727e-05  Data: 0.000 (1.759)
Test: [   0/48]  Time: 14.483 (14.483)  Loss:  1.0648 (1.0648)  Acc@1: 78.8086 (78.8086)  Acc@5: 92.9688 (92.9688)
Test: [  48/48]  Time: 0.149 (3.299)  Loss:  1.0875 (1.9244)  Acc@1: 77.2406 (57.7040)  Acc@5: 91.1557 (81.3200)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-152.pth.tar', 57.70400006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-149.pth.tar', 57.563999912109374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-151.pth.tar', 57.426000041503904)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-150.pth.tar', 57.31199994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-148.pth.tar', 57.21599999023437)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-147.pth.tar', 57.15599999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-145.pth.tar', 57.042000021972655)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-144.pth.tar', 56.95199999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-146.pth.tar', 56.88800002197266)

Train: 153 [   0/1251 (  0%)]  Loss:  4.558066 (4.5581)  Time: 10.425s,   98.23/s  (10.425s,   98.23/s)  LR: 2.491e-05  Data: 9.584 (9.584)
Train: 153 [  50/1251 (  4%)]  Loss:  4.383238 (4.4707)  Time: 0.584s, 1752.21/s  (2.343s,  437.00/s)  LR: 2.491e-05  Data: 0.021 (1.744)
Train: 153 [ 100/1251 (  8%)]  Loss:  4.083134 (4.3415)  Time: 3.755s,  272.70/s  (2.296s,  446.01/s)  LR: 2.491e-05  Data: 3.170 (1.696)
Train: 153 [ 150/1251 ( 12%)]  Loss:  4.514391 (4.3847)  Time: 0.582s, 1759.94/s  (2.345s,  436.65/s)  LR: 2.491e-05  Data: 0.020 (1.747)
Train: 153 [ 200/1251 ( 16%)]  Loss:  4.182006 (4.3442)  Time: 1.348s,  759.42/s  (2.335s,  438.54/s)  LR: 2.491e-05  Data: 0.742 (1.739)
Train: 153 [ 250/1251 ( 20%)]  Loss:  4.800704 (4.4203)  Time: 0.588s, 1741.59/s  (2.323s,  440.81/s)  LR: 2.491e-05  Data: 0.026 (1.726)
Train: 153 [ 300/1251 ( 24%)]  Loss:  4.412439 (4.4191)  Time: 1.702s,  601.64/s  (2.337s,  438.11/s)  LR: 2.491e-05  Data: 1.137 (1.741)
Train: 153 [ 350/1251 ( 28%)]  Loss:  4.912675 (4.4808)  Time: 0.588s, 1742.97/s  (2.322s,  441.07/s)  LR: 2.491e-05  Data: 0.019 (1.724)
Train: 153 [ 400/1251 ( 32%)]  Loss:  4.831552 (4.5198)  Time: 5.023s,  203.87/s  (2.320s,  441.46/s)  LR: 2.491e-05  Data: 4.325 (1.720)
Train: 153 [ 450/1251 ( 36%)]  Loss:  4.321709 (4.5000)  Time: 0.585s, 1751.02/s  (2.300s,  445.29/s)  LR: 2.491e-05  Data: 0.020 (1.701)
Train: 153 [ 500/1251 ( 40%)]  Loss:  4.838500 (4.5308)  Time: 4.257s,  240.54/s  (2.299s,  445.41/s)  LR: 2.491e-05  Data: 3.659 (1.701)
Train: 153 [ 550/1251 ( 44%)]  Loss:  4.121540 (4.4967)  Time: 0.585s, 1749.79/s  (2.330s,  439.49/s)  LR: 2.491e-05  Data: 0.022 (1.728)
Train: 153 [ 600/1251 ( 48%)]  Loss:  5.023515 (4.5372)  Time: 8.003s,  127.96/s  (2.352s,  435.36/s)  LR: 2.491e-05  Data: 7.344 (1.750)
Train: 153 [ 650/1251 ( 52%)]  Loss:  4.734728 (4.5513)  Time: 0.585s, 1750.09/s  (2.358s,  434.27/s)  LR: 2.491e-05  Data: 0.021 (1.756)
Train: 153 [ 700/1251 ( 56%)]  Loss:  4.922894 (4.5761)  Time: 6.334s,  161.68/s  (2.373s,  431.61/s)  LR: 2.491e-05  Data: 5.644 (1.770)
Train: 153 [ 750/1251 ( 60%)]  Loss:  4.364033 (4.5628)  Time: 0.586s, 1746.45/s  (2.371s,  431.95/s)  LR: 2.491e-05  Data: 0.020 (1.768)
Train: 153 [ 800/1251 ( 64%)]  Loss:  4.723112 (4.5722)  Time: 6.113s,  167.50/s  (2.373s,  431.47/s)  LR: 2.491e-05  Data: 5.499 (1.770)
Train: 153 [ 850/1251 ( 68%)]  Loss:  4.630615 (4.5755)  Time: 0.586s, 1746.88/s  (2.365s,  433.01/s)  LR: 2.491e-05  Data: 0.020 (1.762)
Train: 153 [ 900/1251 ( 72%)]  Loss:  3.927386 (4.5414)  Time: 7.501s,  136.52/s  (2.386s,  429.25/s)  LR: 2.491e-05  Data: 6.888 (1.783)
Train: 153 [ 950/1251 ( 76%)]  Loss:  4.361416 (4.5324)  Time: 0.588s, 1742.49/s  (2.376s,  431.01/s)  LR: 2.491e-05  Data: 0.020 (1.774)
Train: 153 [1000/1251 ( 80%)]  Loss:  4.272058 (4.5200)  Time: 7.478s,  136.93/s  (2.382s,  429.83/s)  LR: 2.491e-05  Data: 6.743 (1.781)
Train: 153 [1050/1251 ( 84%)]  Loss:  4.514257 (4.5197)  Time: 0.588s, 1742.52/s  (2.378s,  430.57/s)  LR: 2.491e-05  Data: 0.021 (1.777)
Train: 153 [1100/1251 ( 88%)]  Loss:  4.609962 (4.5236)  Time: 7.198s,  142.27/s  (2.377s,  430.86/s)  LR: 2.491e-05  Data: 6.559 (1.776)
Train: 153 [1150/1251 ( 92%)]  Loss:  4.678966 (4.5301)  Time: 0.586s, 1746.04/s  (2.371s,  431.83/s)  LR: 2.491e-05  Data: 0.024 (1.771)
Train: 153 [1200/1251 ( 96%)]  Loss:  4.446501 (4.5268)  Time: 7.190s,  142.41/s  (2.369s,  432.16/s)  LR: 2.491e-05  Data: 6.591 (1.769)
Train: 153 [1250/1251 (100%)]  Loss:  4.569328 (4.5284)  Time: 0.563s, 1819.38/s  (2.361s,  433.79/s)  LR: 2.491e-05  Data: 0.000 (1.761)
Test: [   0/48]  Time: 13.921 (13.921)  Loss:  1.0668 (1.0668)  Acc@1: 77.9297 (77.9297)  Acc@5: 92.3828 (92.3828)
Test: [  48/48]  Time: 0.149 (3.569)  Loss:  1.1012 (1.9245)  Acc@1: 77.1226 (57.7160)  Acc@5: 91.5094 (81.3300)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-153.pth.tar', 57.71600001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-152.pth.tar', 57.70400006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-149.pth.tar', 57.563999912109374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-151.pth.tar', 57.426000041503904)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-150.pth.tar', 57.31199994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-148.pth.tar', 57.21599999023437)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-147.pth.tar', 57.15599999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-145.pth.tar', 57.042000021972655)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-144.pth.tar', 56.95199999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-146.pth.tar', 56.88800002197266)

Train: 154 [   0/1251 (  0%)]  Loss:  4.499029 (4.4990)  Time: 11.957s,   85.64/s  (11.957s,   85.64/s)  LR: 2.271e-05  Data: 11.378 (11.378)
Train: 154 [  50/1251 (  4%)]  Loss:  4.702488 (4.6008)  Time: 0.587s, 1743.59/s  (2.476s,  413.50/s)  LR: 2.271e-05  Data: 0.021 (1.891)
Train: 154 [ 100/1251 (  8%)]  Loss:  4.753134 (4.6516)  Time: 0.585s, 1750.48/s  (2.419s,  423.32/s)  LR: 2.271e-05  Data: 0.019 (1.836)
Train: 154 [ 150/1251 ( 12%)]  Loss:  3.958306 (4.4782)  Time: 0.618s, 1657.81/s  (2.322s,  441.00/s)  LR: 2.271e-05  Data: 0.055 (1.734)
Train: 154 [ 200/1251 ( 16%)]  Loss:  4.724518 (4.5275)  Time: 0.587s, 1745.00/s  (2.331s,  439.25/s)  LR: 2.271e-05  Data: 0.021 (1.740)
Train: 154 [ 250/1251 ( 20%)]  Loss:  4.603136 (4.5401)  Time: 0.589s, 1739.79/s  (2.288s,  447.48/s)  LR: 2.271e-05  Data: 0.020 (1.698)
Train: 154 [ 300/1251 ( 24%)]  Loss:  4.920718 (4.5945)  Time: 0.587s, 1744.57/s  (2.281s,  448.84/s)  LR: 2.271e-05  Data: 0.020 (1.690)
Train: 154 [ 350/1251 ( 28%)]  Loss:  3.986268 (4.5184)  Time: 0.588s, 1742.30/s  (2.309s,  443.53/s)  LR: 2.271e-05  Data: 0.020 (1.717)
Train: 154 [ 400/1251 ( 32%)]  Loss:  4.567358 (4.5239)  Time: 0.587s, 1744.19/s  (2.330s,  439.43/s)  LR: 2.271e-05  Data: 0.024 (1.739)
Train: 154 [ 450/1251 ( 36%)]  Loss:  3.942508 (4.4657)  Time: 0.587s, 1745.89/s  (2.325s,  440.52/s)  LR: 2.271e-05  Data: 0.021 (1.733)
Train: 154 [ 500/1251 ( 40%)]  Loss:  4.908710 (4.5060)  Time: 2.820s,  363.14/s  (2.330s,  439.55/s)  LR: 2.271e-05  Data: 2.167 (1.736)
Train: 154 [ 550/1251 ( 44%)]  Loss:  4.379708 (4.4955)  Time: 0.588s, 1742.49/s  (2.325s,  440.45/s)  LR: 2.271e-05  Data: 0.019 (1.732)
Train: 154 [ 600/1251 ( 48%)]  Loss:  5.035262 (4.5370)  Time: 6.177s,  165.77/s  (2.335s,  438.59/s)  LR: 2.271e-05  Data: 5.501 (1.740)
Train: 154 [ 650/1251 ( 52%)]  Loss:  4.707175 (4.5492)  Time: 0.589s, 1738.89/s  (2.333s,  438.95/s)  LR: 2.271e-05  Data: 0.024 (1.736)
Train: 154 [ 700/1251 ( 56%)]  Loss:  4.178723 (4.5245)  Time: 3.787s,  270.43/s  (2.337s,  438.10/s)  LR: 2.271e-05  Data: 3.195 (1.740)
Train: 154 [ 750/1251 ( 60%)]  Loss:  4.691710 (4.5349)  Time: 0.588s, 1741.51/s  (2.349s,  435.97/s)  LR: 2.271e-05  Data: 0.020 (1.751)
Train: 154 [ 800/1251 ( 64%)]  Loss:  4.670497 (4.5429)  Time: 2.852s,  359.10/s  (2.356s,  434.69/s)  LR: 2.271e-05  Data: 2.288 (1.758)
Train: 154 [ 850/1251 ( 68%)]  Loss:  4.946040 (4.5653)  Time: 0.586s, 1746.99/s  (2.353s,  435.12/s)  LR: 2.271e-05  Data: 0.021 (1.756)
Train: 154 [ 900/1251 ( 72%)]  Loss:  4.628696 (4.5686)  Time: 2.966s,  345.29/s  (2.354s,  435.03/s)  LR: 2.271e-05  Data: 2.315 (1.756)
Train: 154 [ 950/1251 ( 76%)]  Loss:  4.117239 (4.5461)  Time: 0.586s, 1747.44/s  (2.341s,  437.48/s)  LR: 2.271e-05  Data: 0.019 (1.743)
Train: 154 [1000/1251 ( 80%)]  Loss:  4.575680 (4.5475)  Time: 0.591s, 1732.83/s  (2.332s,  439.15/s)  LR: 2.271e-05  Data: 0.019 (1.735)
Train: 154 [1050/1251 ( 84%)]  Loss:  4.443336 (4.5427)  Time: 0.587s, 1744.51/s  (2.317s,  441.96/s)  LR: 2.271e-05  Data: 0.018 (1.720)
Train: 154 [1100/1251 ( 88%)]  Loss:  4.286720 (4.5316)  Time: 0.587s, 1743.93/s  (2.309s,  443.41/s)  LR: 2.271e-05  Data: 0.020 (1.713)
Train: 154 [1150/1251 ( 92%)]  Loss:  4.340970 (4.5237)  Time: 0.584s, 1753.39/s  (2.323s,  440.74/s)  LR: 2.271e-05  Data: 0.022 (1.726)
Train: 154 [1200/1251 ( 96%)]  Loss:  4.784386 (4.5341)  Time: 0.589s, 1739.56/s  (2.341s,  437.41/s)  LR: 2.271e-05  Data: 0.019 (1.744)
Train: 154 [1250/1251 (100%)]  Loss:  4.834812 (4.5457)  Time: 0.563s, 1817.73/s  (2.341s,  437.43/s)  LR: 2.271e-05  Data: 0.000 (1.740)
Test: [   0/48]  Time: 22.358 (22.358)  Loss:  1.0664 (1.0664)  Acc@1: 78.3203 (78.3203)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.148 (5.231)  Loss:  1.1121 (1.9346)  Acc@1: 77.2406 (57.7320)  Acc@5: 90.4481 (81.1740)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-154.pth.tar', 57.732000068359376)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-153.pth.tar', 57.71600001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-152.pth.tar', 57.70400006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-149.pth.tar', 57.563999912109374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-151.pth.tar', 57.426000041503904)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-150.pth.tar', 57.31199994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-148.pth.tar', 57.21599999023437)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-147.pth.tar', 57.15599999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-145.pth.tar', 57.042000021972655)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-144.pth.tar', 56.95199999023438)

Train: 155 [   0/1251 (  0%)]  Loss:  4.629013 (4.6290)  Time: 10.854s,   94.34/s  (10.854s,   94.34/s)  LR: 2.069e-05  Data: 10.087 (10.087)
Train: 155 [  50/1251 (  4%)]  Loss:  4.365145 (4.4971)  Time: 0.585s, 1749.68/s  (2.333s,  439.00/s)  LR: 2.069e-05  Data: 0.019 (1.739)
Train: 155 [ 100/1251 (  8%)]  Loss:  4.640937 (4.5450)  Time: 0.586s, 1748.00/s  (2.317s,  442.04/s)  LR: 2.069e-05  Data: 0.019 (1.660)
Train: 155 [ 150/1251 ( 12%)]  Loss:  5.031789 (4.6667)  Time: 0.589s, 1739.52/s  (2.510s,  407.95/s)  LR: 2.069e-05  Data: 0.020 (1.814)
Train: 155 [ 200/1251 ( 16%)]  Loss:  4.274686 (4.5883)  Time: 0.597s, 1715.95/s  (2.504s,  408.87/s)  LR: 2.069e-05  Data: 0.032 (1.821)
Train: 155 [ 250/1251 ( 20%)]  Loss:  4.240861 (4.5304)  Time: 0.586s, 1748.32/s  (2.438s,  419.97/s)  LR: 2.069e-05  Data: 0.018 (1.774)
Train: 155 [ 300/1251 ( 24%)]  Loss:  3.895838 (4.4398)  Time: 0.587s, 1744.66/s  (2.446s,  418.57/s)  LR: 2.069e-05  Data: 0.020 (1.768)
Train: 155 [ 350/1251 ( 28%)]  Loss:  4.864151 (4.4928)  Time: 0.586s, 1746.44/s  (2.418s,  423.42/s)  LR: 2.069e-05  Data: 0.021 (1.747)
Train: 155 [ 400/1251 ( 32%)]  Loss:  4.726398 (4.5188)  Time: 0.584s, 1753.54/s  (2.410s,  424.86/s)  LR: 2.069e-05  Data: 0.018 (1.741)
Train: 155 [ 450/1251 ( 36%)]  Loss:  4.647941 (4.5317)  Time: 0.586s, 1746.37/s  (2.376s,  431.05/s)  LR: 2.069e-05  Data: 0.021 (1.715)
Train: 155 [ 500/1251 ( 40%)]  Loss:  4.576231 (4.5357)  Time: 0.585s, 1750.64/s  (2.385s,  429.29/s)  LR: 2.069e-05  Data: 0.020 (1.728)
Train: 155 [ 550/1251 ( 44%)]  Loss:  4.582539 (4.5396)  Time: 0.584s, 1752.64/s  (2.397s,  427.26/s)  LR: 2.069e-05  Data: 0.020 (1.733)
Train: 155 [ 600/1251 ( 48%)]  Loss:  4.544103 (4.5400)  Time: 0.588s, 1740.67/s  (2.417s,  423.62/s)  LR: 2.069e-05  Data: 0.021 (1.755)
Train: 155 [ 650/1251 ( 52%)]  Loss:  4.951358 (4.5694)  Time: 0.585s, 1749.44/s  (2.433s,  420.96/s)  LR: 2.069e-05  Data: 0.019 (1.776)
Train: 155 [ 700/1251 ( 56%)]  Loss:  4.290112 (4.5507)  Time: 0.585s, 1749.75/s  (2.442s,  419.38/s)  LR: 2.069e-05  Data: 0.022 (1.790)
Train: 155 [ 750/1251 ( 60%)]  Loss:  4.370643 (4.5395)  Time: 0.587s, 1743.24/s  (2.445s,  418.73/s)  LR: 2.069e-05  Data: 0.020 (1.799)
Train: 155 [ 800/1251 ( 64%)]  Loss:  4.310593 (4.5260)  Time: 0.586s, 1748.14/s  (2.447s,  418.53/s)  LR: 2.069e-05  Data: 0.022 (1.803)
Train: 155 [ 850/1251 ( 68%)]  Loss:  3.937719 (4.4933)  Time: 0.586s, 1748.70/s  (2.446s,  418.64/s)  LR: 2.069e-05  Data: 0.021 (1.806)
Train: 155 [ 900/1251 ( 72%)]  Loss:  3.934551 (4.4639)  Time: 0.585s, 1749.06/s  (2.486s,  411.98/s)  LR: 2.069e-05  Data: 0.022 (1.849)
Train: 155 [ 950/1251 ( 76%)]  Loss:  4.544549 (4.4680)  Time: 0.584s, 1754.65/s  (2.488s,  411.59/s)  LR: 2.069e-05  Data: 0.020 (1.854)
Train: 155 [1000/1251 ( 80%)]  Loss:  4.084561 (4.4497)  Time: 0.593s, 1727.54/s  (2.492s,  410.87/s)  LR: 2.069e-05  Data: 0.022 (1.859)
Train: 155 [1050/1251 ( 84%)]  Loss:  4.652375 (4.4589)  Time: 2.115s,  484.20/s  (2.507s,  408.54/s)  LR: 2.069e-05  Data: 1.515 (1.873)
Train: 155 [1100/1251 ( 88%)]  Loss:  4.638361 (4.4667)  Time: 2.317s,  441.98/s  (2.505s,  408.78/s)  LR: 2.069e-05  Data: 1.710 (1.870)
Train: 155 [1150/1251 ( 92%)]  Loss:  4.647449 (4.4742)  Time: 3.676s,  278.54/s  (2.509s,  408.18/s)  LR: 2.069e-05  Data: 3.110 (1.875)
Train: 155 [1200/1251 ( 96%)]  Loss:  4.545254 (4.4771)  Time: 7.949s,  128.82/s  (2.533s,  404.23/s)  LR: 2.069e-05  Data: 7.160 (1.901)
Train: 155 [1250/1251 (100%)]  Loss:  4.138619 (4.4641)  Time: 0.563s, 1818.35/s  (2.530s,  404.72/s)  LR: 2.069e-05  Data: 0.000 (1.897)
Test: [   0/48]  Time: 24.604 (24.604)  Loss:  1.0536 (1.0536)  Acc@1: 78.5156 (78.5156)  Acc@5: 92.7734 (92.7734)
Test: [  48/48]  Time: 0.149 (5.025)  Loss:  1.0861 (1.9301)  Acc@1: 78.0660 (57.8020)  Acc@5: 91.5094 (81.3460)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-155.pth.tar', 57.8020000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-154.pth.tar', 57.732000068359376)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-153.pth.tar', 57.71600001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-152.pth.tar', 57.70400006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-149.pth.tar', 57.563999912109374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-151.pth.tar', 57.426000041503904)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-150.pth.tar', 57.31199994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-148.pth.tar', 57.21599999023437)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-147.pth.tar', 57.15599999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-145.pth.tar', 57.042000021972655)

Train: 156 [   0/1251 (  0%)]  Loss:  4.646641 (4.6466)  Time: 14.661s,   69.84/s  (14.661s,   69.84/s)  LR: 1.884e-05  Data: 13.950 (13.950)
Train: 156 [  50/1251 (  4%)]  Loss:  4.223177 (4.4349)  Time: 0.587s, 1745.77/s  (2.561s,  399.85/s)  LR: 1.884e-05  Data: 0.024 (1.936)
Train: 156 [ 100/1251 (  8%)]  Loss:  4.716311 (4.5287)  Time: 0.589s, 1739.43/s  (2.471s,  414.34/s)  LR: 1.884e-05  Data: 0.021 (1.858)
Train: 156 [ 150/1251 ( 12%)]  Loss:  4.301195 (4.4718)  Time: 0.587s, 1745.27/s  (2.451s,  417.86/s)  LR: 1.884e-05  Data: 0.022 (1.848)
Train: 156 [ 200/1251 ( 16%)]  Loss:  4.902081 (4.5579)  Time: 0.587s, 1744.31/s  (2.560s,  400.02/s)  LR: 1.884e-05  Data: 0.020 (1.945)
Train: 156 [ 250/1251 ( 20%)]  Loss:  4.619506 (4.5682)  Time: 0.585s, 1751.41/s  (2.496s,  410.18/s)  LR: 1.884e-05  Data: 0.020 (1.888)
Train: 156 [ 300/1251 ( 24%)]  Loss:  4.972199 (4.6259)  Time: 0.586s, 1747.96/s  (2.463s,  415.79/s)  LR: 1.884e-05  Data: 0.020 (1.859)
Train: 156 [ 350/1251 ( 28%)]  Loss:  4.684310 (4.6332)  Time: 0.585s, 1751.60/s  (2.462s,  416.00/s)  LR: 1.884e-05  Data: 0.019 (1.860)
Train: 156 [ 400/1251 ( 32%)]  Loss:  4.772075 (4.6486)  Time: 0.585s, 1749.60/s  (2.449s,  418.06/s)  LR: 1.884e-05  Data: 0.021 (1.844)
Train: 156 [ 450/1251 ( 36%)]  Loss:  4.708935 (4.6546)  Time: 0.584s, 1753.26/s  (2.439s,  419.92/s)  LR: 1.884e-05  Data: 0.019 (1.817)
Train: 156 [ 500/1251 ( 40%)]  Loss:  4.376140 (4.6293)  Time: 0.586s, 1747.34/s  (2.418s,  423.45/s)  LR: 1.884e-05  Data: 0.020 (1.793)
Train: 156 [ 550/1251 ( 44%)]  Loss:  4.217356 (4.5950)  Time: 0.586s, 1747.92/s  (2.399s,  426.91/s)  LR: 1.884e-05  Data: 0.022 (1.748)
Train: 156 [ 600/1251 ( 48%)]  Loss:  4.580455 (4.5939)  Time: 0.589s, 1737.80/s  (2.433s,  420.88/s)  LR: 1.884e-05  Data: 0.021 (1.782)
Train: 156 [ 650/1251 ( 52%)]  Loss:  4.424542 (4.5818)  Time: 0.587s, 1745.69/s  (2.441s,  419.44/s)  LR: 1.884e-05  Data: 0.019 (1.788)
Train: 156 [ 700/1251 ( 56%)]  Loss:  4.401417 (4.5698)  Time: 0.585s, 1749.34/s  (2.471s,  414.36/s)  LR: 1.884e-05  Data: 0.019 (1.822)
Train: 156 [ 750/1251 ( 60%)]  Loss:  4.655927 (4.5751)  Time: 0.587s, 1744.92/s  (2.457s,  416.72/s)  LR: 1.884e-05  Data: 0.022 (1.812)
Train: 156 [ 800/1251 ( 64%)]  Loss:  4.273955 (4.5574)  Time: 0.584s, 1754.55/s  (2.444s,  418.95/s)  LR: 1.884e-05  Data: 0.021 (1.801)
Train: 156 [ 850/1251 ( 68%)]  Loss:  4.558903 (4.5575)  Time: 2.119s,  483.29/s  (2.426s,  422.01/s)  LR: 1.884e-05  Data: 1.469 (1.786)
Train: 156 [ 900/1251 ( 72%)]  Loss:  4.035025 (4.5300)  Time: 2.921s,  350.55/s  (2.413s,  424.45/s)  LR: 1.884e-05  Data: 2.273 (1.772)
Train: 156 [ 950/1251 ( 76%)]  Loss:  4.369560 (4.5220)  Time: 0.703s, 1456.75/s  (2.420s,  423.12/s)  LR: 1.884e-05  Data: 0.134 (1.780)
Train: 156 [1000/1251 ( 80%)]  Loss:  4.474813 (4.5197)  Time: 5.260s,  194.67/s  (2.412s,  424.59/s)  LR: 1.884e-05  Data: 4.508 (1.772)
Train: 156 [1050/1251 ( 84%)]  Loss:  4.471678 (4.5176)  Time: 2.556s,  400.55/s  (2.412s,  424.48/s)  LR: 1.884e-05  Data: 1.994 (1.774)
Train: 156 [1100/1251 ( 88%)]  Loss:  4.970812 (4.5373)  Time: 5.245s,  195.22/s  (2.412s,  424.54/s)  LR: 1.884e-05  Data: 4.682 (1.775)
Train: 156 [1150/1251 ( 92%)]  Loss:  4.785982 (4.5476)  Time: 0.950s, 1078.06/s  (2.404s,  425.95/s)  LR: 1.884e-05  Data: 0.379 (1.767)
Train: 156 [1200/1251 ( 96%)]  Loss:  4.214749 (4.5343)  Time: 1.872s,  546.95/s  (2.401s,  426.45/s)  LR: 1.884e-05  Data: 1.297 (1.766)
Train: 156 [1250/1251 (100%)]  Loss:  4.919812 (4.5491)  Time: 0.566s, 1808.85/s  (2.394s,  427.73/s)  LR: 1.884e-05  Data: 0.000 (1.758)
Test: [   0/48]  Time: 14.455 (14.455)  Loss:  1.0452 (1.0452)  Acc@1: 78.6133 (78.6133)  Acc@5: 93.0664 (93.0664)
Test: [  48/48]  Time: 0.150 (3.370)  Loss:  1.0847 (1.9142)  Acc@1: 78.0660 (57.9000)  Acc@5: 91.5094 (81.4820)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-156.pth.tar', 57.900000168457034)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-155.pth.tar', 57.8020000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-154.pth.tar', 57.732000068359376)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-153.pth.tar', 57.71600001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-152.pth.tar', 57.70400006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-149.pth.tar', 57.563999912109374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-151.pth.tar', 57.426000041503904)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-150.pth.tar', 57.31199994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-148.pth.tar', 57.21599999023437)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-147.pth.tar', 57.15599999023438)

Train: 157 [   0/1251 (  0%)]  Loss:  4.924117 (4.9241)  Time: 15.612s,   65.59/s  (15.612s,   65.59/s)  LR: 1.716e-05  Data: 14.896 (14.896)
Train: 157 [  50/1251 (  4%)]  Loss:  4.784253 (4.8542)  Time: 0.588s, 1742.37/s  (2.548s,  401.81/s)  LR: 1.716e-05  Data: 0.022 (1.943)
Train: 157 [ 100/1251 (  8%)]  Loss:  4.276241 (4.6615)  Time: 0.589s, 1739.29/s  (2.529s,  404.92/s)  LR: 1.716e-05  Data: 0.020 (1.925)
Train: 157 [ 150/1251 ( 12%)]  Loss:  4.390679 (4.5938)  Time: 0.587s, 1745.21/s  (2.466s,  415.22/s)  LR: 1.716e-05  Data: 0.022 (1.871)
Train: 157 [ 200/1251 ( 16%)]  Loss:  4.108537 (4.4968)  Time: 0.584s, 1754.08/s  (2.465s,  415.43/s)  LR: 1.716e-05  Data: 0.022 (1.867)
Train: 157 [ 250/1251 ( 20%)]  Loss:  4.031797 (4.4193)  Time: 0.590s, 1736.18/s  (2.417s,  423.72/s)  LR: 1.716e-05  Data: 0.023 (1.816)
Train: 157 [ 300/1251 ( 24%)]  Loss:  3.908541 (4.3463)  Time: 0.585s, 1750.13/s  (2.399s,  426.83/s)  LR: 1.716e-05  Data: 0.023 (1.797)
Train: 157 [ 350/1251 ( 28%)]  Loss:  4.660839 (4.3856)  Time: 0.587s, 1743.04/s  (2.377s,  430.77/s)  LR: 1.716e-05  Data: 0.020 (1.773)
Train: 157 [ 400/1251 ( 32%)]  Loss:  4.875373 (4.4400)  Time: 0.586s, 1746.93/s  (2.413s,  424.39/s)  LR: 1.716e-05  Data: 0.022 (1.810)
Train: 157 [ 450/1251 ( 36%)]  Loss:  4.950034 (4.4910)  Time: 0.585s, 1750.41/s  (2.402s,  426.29/s)  LR: 1.716e-05  Data: 0.019 (1.799)
Train: 157 [ 500/1251 ( 40%)]  Loss:  4.455521 (4.4878)  Time: 0.589s, 1738.59/s  (2.403s,  426.07/s)  LR: 1.716e-05  Data: 0.018 (1.801)
Train: 157 [ 550/1251 ( 44%)]  Loss:  4.481107 (4.4873)  Time: 0.586s, 1746.04/s  (2.394s,  427.76/s)  LR: 1.716e-05  Data: 0.020 (1.791)
Train: 157 [ 600/1251 ( 48%)]  Loss:  4.310997 (4.4737)  Time: 0.587s, 1744.43/s  (2.403s,  426.12/s)  LR: 1.716e-05  Data: 0.019 (1.801)
Train: 157 [ 650/1251 ( 52%)]  Loss:  4.667315 (4.4875)  Time: 0.590s, 1736.64/s  (2.412s,  424.60/s)  LR: 1.716e-05  Data: 0.020 (1.810)
Train: 157 [ 700/1251 ( 56%)]  Loss:  4.180866 (4.4671)  Time: 0.605s, 1691.59/s  (2.409s,  425.16/s)  LR: 1.716e-05  Data: 0.039 (1.807)
Train: 157 [ 750/1251 ( 60%)]  Loss:  4.807072 (4.4883)  Time: 0.588s, 1742.15/s  (2.423s,  422.68/s)  LR: 1.716e-05  Data: 0.020 (1.820)
Train: 157 [ 800/1251 ( 64%)]  Loss:  4.543261 (4.4916)  Time: 0.585s, 1750.07/s  (2.414s,  424.20/s)  LR: 1.716e-05  Data: 0.022 (1.813)
Train: 157 [ 850/1251 ( 68%)]  Loss:  4.740492 (4.5054)  Time: 0.592s, 1729.71/s  (2.423s,  422.54/s)  LR: 1.716e-05  Data: 0.022 (1.823)
Train: 157 [ 900/1251 ( 72%)]  Loss:  4.569280 (4.5088)  Time: 0.585s, 1751.72/s  (2.420s,  423.15/s)  LR: 1.716e-05  Data: 0.021 (1.819)
Train: 157 [ 950/1251 ( 76%)]  Loss:  5.010311 (4.5338)  Time: 0.588s, 1741.69/s  (2.417s,  423.61/s)  LR: 1.716e-05  Data: 0.021 (1.818)
Train: 157 [1000/1251 ( 80%)]  Loss:  4.674517 (4.5405)  Time: 0.584s, 1752.98/s  (2.407s,  425.38/s)  LR: 1.716e-05  Data: 0.022 (1.807)
Train: 157 [1050/1251 ( 84%)]  Loss:  4.839296 (4.5541)  Time: 0.587s, 1744.26/s  (2.400s,  426.73/s)  LR: 1.716e-05  Data: 0.021 (1.800)
Train: 157 [1100/1251 ( 88%)]  Loss:  4.995020 (4.5733)  Time: 0.585s, 1750.44/s  (2.390s,  428.38/s)  LR: 1.716e-05  Data: 0.022 (1.791)
Train: 157 [1150/1251 ( 92%)]  Loss:  4.655815 (4.5767)  Time: 0.593s, 1728.02/s  (2.396s,  427.43/s)  LR: 1.716e-05  Data: 0.024 (1.797)
Train: 157 [1200/1251 ( 96%)]  Loss:  4.890525 (4.5893)  Time: 0.587s, 1745.20/s  (2.392s,  428.14/s)  LR: 1.716e-05  Data: 0.021 (1.793)
Train: 157 [1250/1251 (100%)]  Loss:  4.753550 (4.5956)  Time: 0.563s, 1818.24/s  (2.390s,  428.53/s)  LR: 1.716e-05  Data: 0.000 (1.790)
Test: [   0/48]  Time: 13.271 (13.271)  Loss:  1.0573 (1.0573)  Acc@1: 77.9297 (77.9297)  Acc@5: 92.4805 (92.4805)
Test: [  48/48]  Time: 0.148 (3.323)  Loss:  1.0865 (1.9036)  Acc@1: 78.0660 (58.0820)  Acc@5: 91.5094 (81.5200)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-157.pth.tar', 58.0820000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-156.pth.tar', 57.900000168457034)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-155.pth.tar', 57.8020000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-154.pth.tar', 57.732000068359376)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-153.pth.tar', 57.71600001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-152.pth.tar', 57.70400006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-149.pth.tar', 57.563999912109374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-151.pth.tar', 57.426000041503904)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-150.pth.tar', 57.31199994140625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-148.pth.tar', 57.21599999023437)

Train: 158 [   0/1251 (  0%)]  Loss:  4.807495 (4.8075)  Time: 12.691s,   80.69/s  (12.691s,   80.69/s)  LR: 1.566e-05  Data: 11.185 (11.185)
Train: 158 [  50/1251 (  4%)]  Loss:  4.259768 (4.5336)  Time: 0.589s, 1739.87/s  (2.415s,  424.05/s)  LR: 1.566e-05  Data: 0.022 (1.818)
Train: 158 [ 100/1251 (  8%)]  Loss:  4.693082 (4.5868)  Time: 0.587s, 1743.74/s  (2.366s,  432.82/s)  LR: 1.566e-05  Data: 0.020 (1.764)
Train: 158 [ 150/1251 ( 12%)]  Loss:  4.607112 (4.5919)  Time: 0.584s, 1753.05/s  (2.309s,  443.41/s)  LR: 1.566e-05  Data: 0.020 (1.717)
Train: 158 [ 200/1251 ( 16%)]  Loss:  4.493530 (4.5722)  Time: 0.586s, 1747.96/s  (2.401s,  426.44/s)  LR: 1.566e-05  Data: 0.019 (1.809)
Train: 158 [ 250/1251 ( 20%)]  Loss:  4.697097 (4.5930)  Time: 0.588s, 1741.72/s  (2.366s,  432.82/s)  LR: 1.566e-05  Data: 0.019 (1.774)
Train: 158 [ 300/1251 ( 24%)]  Loss:  4.884834 (4.6347)  Time: 0.584s, 1754.35/s  (2.394s,  427.72/s)  LR: 1.566e-05  Data: 0.020 (1.803)
Train: 158 [ 350/1251 ( 28%)]  Loss:  4.526833 (4.6212)  Time: 0.586s, 1748.75/s  (2.385s,  429.26/s)  LR: 1.566e-05  Data: 0.021 (1.795)
Train: 158 [ 400/1251 ( 32%)]  Loss:  4.917690 (4.6542)  Time: 0.587s, 1744.04/s  (2.392s,  428.01/s)  LR: 1.566e-05  Data: 0.018 (1.802)
Train: 158 [ 450/1251 ( 36%)]  Loss:  5.022086 (4.6910)  Time: 0.584s, 1754.09/s  (2.369s,  432.16/s)  LR: 1.566e-05  Data: 0.021 (1.780)
Train: 158 [ 500/1251 ( 40%)]  Loss:  4.820127 (4.7027)  Time: 0.586s, 1748.41/s  (2.373s,  431.52/s)  LR: 1.566e-05  Data: 0.022 (1.783)
Train: 158 [ 550/1251 ( 44%)]  Loss:  5.010957 (4.7284)  Time: 0.588s, 1740.35/s  (2.380s,  430.31/s)  LR: 1.566e-05  Data: 0.021 (1.790)
Train: 158 [ 600/1251 ( 48%)]  Loss:  4.373474 (4.7011)  Time: 0.585s, 1749.41/s  (2.420s,  423.09/s)  LR: 1.566e-05  Data: 0.022 (1.829)
Train: 158 [ 650/1251 ( 52%)]  Loss:  4.687235 (4.7001)  Time: 0.585s, 1751.66/s  (2.434s,  420.72/s)  LR: 1.566e-05  Data: 0.021 (1.843)
Train: 158 [ 700/1251 ( 56%)]  Loss:  4.800596 (4.7068)  Time: 0.586s, 1746.34/s  (2.449s,  418.14/s)  LR: 1.566e-05  Data: 0.020 (1.859)
Train: 158 [ 750/1251 ( 60%)]  Loss:  4.393672 (4.6872)  Time: 0.583s, 1757.05/s  (2.442s,  419.41/s)  LR: 1.566e-05  Data: 0.020 (1.851)
Train: 158 [ 800/1251 ( 64%)]  Loss:  4.622062 (4.6834)  Time: 0.585s, 1750.87/s  (2.438s,  420.05/s)  LR: 1.566e-05  Data: 0.021 (1.847)
Train: 158 [ 850/1251 ( 68%)]  Loss:  4.080423 (4.6499)  Time: 0.586s, 1747.10/s  (2.425s,  422.30/s)  LR: 1.566e-05  Data: 0.019 (1.835)
Train: 158 [ 900/1251 ( 72%)]  Loss:  4.217256 (4.6271)  Time: 0.584s, 1752.19/s  (2.420s,  423.13/s)  LR: 1.566e-05  Data: 0.019 (1.830)
Train: 158 [ 950/1251 ( 76%)]  Loss:  4.977933 (4.6447)  Time: 0.588s, 1740.46/s  (2.428s,  421.69/s)  LR: 1.566e-05  Data: 0.021 (1.839)
Train: 158 [1000/1251 ( 80%)]  Loss:  4.517231 (4.6386)  Time: 0.586s, 1747.98/s  (2.435s,  420.56/s)  LR: 1.566e-05  Data: 0.020 (1.846)
Train: 158 [1050/1251 ( 84%)]  Loss:  4.486225 (4.6317)  Time: 0.587s, 1743.29/s  (2.430s,  421.37/s)  LR: 1.566e-05  Data: 0.020 (1.842)
Train: 158 [1100/1251 ( 88%)]  Loss:  4.841501 (4.6408)  Time: 0.585s, 1749.18/s  (2.430s,  421.36/s)  LR: 1.566e-05  Data: 0.020 (1.841)
Train: 158 [1150/1251 ( 92%)]  Loss:  5.011653 (4.6562)  Time: 2.395s,  427.47/s  (2.424s,  422.43/s)  LR: 1.566e-05  Data: 1.745 (1.835)
Train: 158 [1200/1251 ( 96%)]  Loss:  4.478116 (4.6491)  Time: 0.586s, 1747.65/s  (2.416s,  423.83/s)  LR: 1.566e-05  Data: 0.021 (1.826)
Train: 158 [1250/1251 (100%)]  Loss:  4.751857 (4.6531)  Time: 0.566s, 1810.60/s  (2.405s,  425.70/s)  LR: 1.566e-05  Data: 0.000 (1.815)
Test: [   0/48]  Time: 13.673 (13.673)  Loss:  1.0300 (1.0300)  Acc@1: 78.5156 (78.5156)  Acc@5: 93.0664 (93.0664)
Test: [  48/48]  Time: 0.149 (3.423)  Loss:  1.0793 (1.8949)  Acc@1: 77.3585 (58.1840)  Acc@5: 91.2736 (81.5860)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-158.pth.tar', 58.183999990234376)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-157.pth.tar', 58.0820000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-156.pth.tar', 57.900000168457034)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-155.pth.tar', 57.8020000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-154.pth.tar', 57.732000068359376)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-153.pth.tar', 57.71600001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-152.pth.tar', 57.70400006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-149.pth.tar', 57.563999912109374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-151.pth.tar', 57.426000041503904)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-150.pth.tar', 57.31199994140625)

Train: 159 [   0/1251 (  0%)]  Loss:  4.582738 (4.5827)  Time: 12.283s,   83.36/s  (12.283s,   83.36/s)  LR: 1.434e-05  Data: 11.654 (11.654)
Train: 159 [  50/1251 (  4%)]  Loss:  4.793533 (4.6881)  Time: 0.585s, 1750.05/s  (2.521s,  406.15/s)  LR: 1.434e-05  Data: 0.020 (1.917)
Train: 159 [ 100/1251 (  8%)]  Loss:  4.555177 (4.6438)  Time: 3.804s,  269.16/s  (2.733s,  374.62/s)  LR: 1.434e-05  Data: 3.030 (2.127)
Train: 159 [ 150/1251 ( 12%)]  Loss:  5.000750 (4.7330)  Time: 0.584s, 1753.10/s  (2.561s,  399.77/s)  LR: 1.434e-05  Data: 0.021 (1.950)
Train: 159 [ 200/1251 ( 16%)]  Loss:  5.036937 (4.7938)  Time: 4.283s,  239.07/s  (2.518s,  406.64/s)  LR: 1.434e-05  Data: 3.632 (1.911)
Train: 159 [ 250/1251 ( 20%)]  Loss:  4.406365 (4.7293)  Time: 0.584s, 1754.37/s  (2.461s,  416.05/s)  LR: 1.434e-05  Data: 0.020 (1.856)
Train: 159 [ 300/1251 ( 24%)]  Loss:  4.651649 (4.7182)  Time: 1.166s,  878.03/s  (2.433s,  420.90/s)  LR: 1.434e-05  Data: 0.596 (1.829)
Train: 159 [ 350/1251 ( 28%)]  Loss:  4.170799 (4.6497)  Time: 0.583s, 1756.54/s  (2.398s,  427.10/s)  LR: 1.434e-05  Data: 0.021 (1.796)
Train: 159 [ 400/1251 ( 32%)]  Loss:  4.861953 (4.6733)  Time: 0.589s, 1738.98/s  (2.428s,  421.72/s)  LR: 1.434e-05  Data: 0.026 (1.826)
Train: 159 [ 450/1251 ( 36%)]  Loss:  4.069026 (4.6129)  Time: 0.584s, 1752.31/s  (2.428s,  421.78/s)  LR: 1.434e-05  Data: 0.021 (1.825)
Train: 159 [ 500/1251 ( 40%)]  Loss:  4.106738 (4.5669)  Time: 0.585s, 1751.40/s  (2.426s,  422.03/s)  LR: 1.434e-05  Data: 0.020 (1.823)
Train: 159 [ 550/1251 ( 44%)]  Loss:  4.232419 (4.5390)  Time: 7.269s,  140.88/s  (2.421s,  422.89/s)  LR: 1.434e-05  Data: 6.586 (1.817)
Train: 159 [ 600/1251 ( 48%)]  Loss:  4.874564 (4.5648)  Time: 0.586s, 1748.91/s  (2.420s,  423.07/s)  LR: 1.434e-05  Data: 0.018 (1.816)
Train: 159 [ 650/1251 ( 52%)]  Loss:  4.713514 (4.5754)  Time: 6.766s,  151.35/s  (2.429s,  421.59/s)  LR: 1.434e-05  Data: 6.082 (1.825)
Train: 159 [ 700/1251 ( 56%)]  Loss:  4.457389 (4.5676)  Time: 0.591s, 1731.24/s  (2.425s,  422.20/s)  LR: 1.434e-05  Data: 0.019 (1.821)
Train: 159 [ 750/1251 ( 60%)]  Loss:  3.890360 (4.5252)  Time: 7.832s,  130.74/s  (2.447s,  418.46/s)  LR: 1.434e-05  Data: 6.952 (1.843)
Train: 159 [ 800/1251 ( 64%)]  Loss:  4.445735 (4.5206)  Time: 0.588s, 1742.41/s  (2.441s,  419.46/s)  LR: 1.434e-05  Data: 0.019 (1.838)
Train: 159 [ 850/1251 ( 68%)]  Loss:  4.918114 (4.5427)  Time: 3.429s,  298.61/s  (2.439s,  419.88/s)  LR: 1.434e-05  Data: 2.748 (1.835)
Train: 159 [ 900/1251 ( 72%)]  Loss:  4.453110 (4.5379)  Time: 0.587s, 1744.31/s  (2.437s,  420.11/s)  LR: 1.434e-05  Data: 0.020 (1.834)
Train: 159 [ 950/1251 ( 76%)]  Loss:  4.775972 (4.5498)  Time: 2.312s,  442.94/s  (2.431s,  421.24/s)  LR: 1.434e-05  Data: 1.745 (1.828)
Train: 159 [1000/1251 ( 80%)]  Loss:  4.702811 (4.5571)  Time: 0.588s, 1740.34/s  (2.445s,  418.88/s)  LR: 1.434e-05  Data: 0.021 (1.841)
Train: 159 [1050/1251 ( 84%)]  Loss:  4.287625 (4.5449)  Time: 3.337s,  306.89/s  (2.435s,  420.56/s)  LR: 1.434e-05  Data: 2.774 (1.832)
Train: 159 [1100/1251 ( 88%)]  Loss:  4.394969 (4.5384)  Time: 0.585s, 1751.53/s  (2.440s,  419.66/s)  LR: 1.434e-05  Data: 0.021 (1.837)
Train: 159 [1150/1251 ( 92%)]  Loss:  4.662652 (4.5435)  Time: 3.727s,  274.77/s  (2.432s,  421.09/s)  LR: 1.434e-05  Data: 3.132 (1.829)
Train: 159 [1200/1251 ( 96%)]  Loss:  4.409177 (4.5382)  Time: 0.587s, 1744.91/s  (2.431s,  421.17/s)  LR: 1.434e-05  Data: 0.018 (1.828)
Train: 159 [1250/1251 (100%)]  Loss:  4.351014 (4.5310)  Time: 0.562s, 1820.84/s  (2.426s,  422.09/s)  LR: 1.434e-05  Data: 0.000 (1.823)
Test: [   0/48]  Time: 14.567 (14.567)  Loss:  1.0508 (1.0508)  Acc@1: 78.2227 (78.2227)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.149 (3.327)  Loss:  1.0746 (1.9077)  Acc@1: 77.8302 (58.0920)  Acc@5: 91.9811 (81.4980)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-158.pth.tar', 58.183999990234376)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-159.pth.tar', 58.09200006591797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-157.pth.tar', 58.0820000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-156.pth.tar', 57.900000168457034)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-155.pth.tar', 57.8020000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-154.pth.tar', 57.732000068359376)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-153.pth.tar', 57.71600001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-152.pth.tar', 57.70400006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-149.pth.tar', 57.563999912109374)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-151.pth.tar', 57.426000041503904)

Train: 160 [   0/1251 (  0%)]  Loss:  3.788354 (3.7884)  Time: 10.131s,  101.08/s  (10.131s,  101.08/s)  LR: 1.319e-05  Data: 9.416 (9.416)
Train: 160 [  50/1251 (  4%)]  Loss:  4.085657 (3.9370)  Time: 0.594s, 1723.97/s  (2.370s,  432.10/s)  LR: 1.319e-05  Data: 0.021 (1.786)
Train: 160 [ 100/1251 (  8%)]  Loss:  4.577388 (4.1505)  Time: 0.587s, 1743.47/s  (2.352s,  435.29/s)  LR: 1.319e-05  Data: 0.022 (1.774)
Train: 160 [ 150/1251 ( 12%)]  Loss:  4.734623 (4.2965)  Time: 0.587s, 1743.00/s  (2.286s,  448.03/s)  LR: 1.319e-05  Data: 0.023 (1.703)
Train: 160 [ 200/1251 ( 16%)]  Loss:  4.258511 (4.2889)  Time: 0.586s, 1746.99/s  (2.388s,  428.78/s)  LR: 1.319e-05  Data: 0.021 (1.801)
Train: 160 [ 250/1251 ( 20%)]  Loss:  4.382518 (4.3045)  Time: 0.587s, 1744.02/s  (2.399s,  426.84/s)  LR: 1.319e-05  Data: 0.020 (1.812)
Train: 160 [ 300/1251 ( 24%)]  Loss:  4.704693 (4.3617)  Time: 0.590s, 1735.18/s  (2.402s,  426.25/s)  LR: 1.319e-05  Data: 0.024 (1.815)
Train: 160 [ 350/1251 ( 28%)]  Loss:  4.884619 (4.4270)  Time: 0.584s, 1754.76/s  (2.384s,  429.52/s)  LR: 1.319e-05  Data: 0.021 (1.795)
Train: 160 [ 400/1251 ( 32%)]  Loss:  4.013993 (4.3812)  Time: 0.587s, 1745.57/s  (2.388s,  428.86/s)  LR: 1.319e-05  Data: 0.023 (1.800)
Train: 160 [ 450/1251 ( 36%)]  Loss:  4.453710 (4.3884)  Time: 0.592s, 1731.12/s  (2.368s,  432.40/s)  LR: 1.319e-05  Data: 0.019 (1.781)
Train: 160 [ 500/1251 ( 40%)]  Loss:  4.268952 (4.3775)  Time: 0.586s, 1748.72/s  (2.360s,  433.89/s)  LR: 1.319e-05  Data: 0.020 (1.773)
Train: 160 [ 550/1251 ( 44%)]  Loss:  4.750061 (4.4086)  Time: 0.587s, 1745.87/s  (2.399s,  426.92/s)  LR: 1.319e-05  Data: 0.020 (1.812)
Train: 160 [ 600/1251 ( 48%)]  Loss:  4.594156 (4.4229)  Time: 0.588s, 1741.27/s  (2.422s,  422.84/s)  LR: 1.319e-05  Data: 0.020 (1.835)
Train: 160 [ 650/1251 ( 52%)]  Loss:  4.457815 (4.4254)  Time: 0.584s, 1752.22/s  (2.427s,  421.98/s)  LR: 1.319e-05  Data: 0.019 (1.840)
Train: 160 [ 700/1251 ( 56%)]  Loss:  4.688468 (4.4429)  Time: 0.590s, 1734.39/s  (2.432s,  421.03/s)  LR: 1.319e-05  Data: 0.021 (1.845)
Train: 160 [ 750/1251 ( 60%)]  Loss:  4.382113 (4.4391)  Time: 0.583s, 1755.98/s  (2.417s,  423.61/s)  LR: 1.319e-05  Data: 0.021 (1.831)
Train: 160 [ 800/1251 ( 64%)]  Loss:  4.356163 (4.4342)  Time: 0.587s, 1744.83/s  (2.412s,  424.58/s)  LR: 1.319e-05  Data: 0.023 (1.825)
Train: 160 [ 850/1251 ( 68%)]  Loss:  4.557087 (4.4410)  Time: 0.589s, 1738.27/s  (2.400s,  426.61/s)  LR: 1.319e-05  Data: 0.022 (1.814)
Train: 160 [ 900/1251 ( 72%)]  Loss:  4.411088 (4.4395)  Time: 0.583s, 1754.97/s  (2.409s,  425.13/s)  LR: 1.319e-05  Data: 0.019 (1.822)
Train: 160 [ 950/1251 ( 76%)]  Loss:  4.469741 (4.4410)  Time: 0.584s, 1754.17/s  (2.405s,  425.73/s)  LR: 1.319e-05  Data: 0.021 (1.817)
Train: 160 [1000/1251 ( 80%)]  Loss:  4.194669 (4.4293)  Time: 2.406s,  425.63/s  (2.398s,  426.97/s)  LR: 1.319e-05  Data: 1.844 (1.810)
Train: 160 [1050/1251 ( 84%)]  Loss:  4.683693 (4.4408)  Time: 0.585s, 1750.56/s  (2.395s,  427.48/s)  LR: 1.319e-05  Data: 0.020 (1.807)
Train: 160 [1100/1251 ( 88%)]  Loss:  4.718504 (4.4529)  Time: 5.471s,  187.17/s  (2.394s,  427.67/s)  LR: 1.319e-05  Data: 4.864 (1.805)
Train: 160 [1150/1251 ( 92%)]  Loss:  4.284883 (4.4459)  Time: 0.586s, 1748.06/s  (2.381s,  429.99/s)  LR: 1.319e-05  Data: 0.022 (1.792)
Train: 160 [1200/1251 ( 96%)]  Loss:  4.455335 (4.4463)  Time: 4.719s,  217.00/s  (2.374s,  431.29/s)  LR: 1.319e-05  Data: 4.070 (1.784)
Train: 160 [1250/1251 (100%)]  Loss:  4.549525 (4.4502)  Time: 0.566s, 1809.38/s  (2.363s,  433.39/s)  LR: 1.319e-05  Data: 0.000 (1.772)
Test: [   0/48]  Time: 13.821 (13.821)  Loss:  1.0242 (1.0242)  Acc@1: 77.9297 (77.9297)  Acc@5: 92.5781 (92.5781)
Test: [  48/48]  Time: 0.149 (3.498)  Loss:  1.0659 (1.8936)  Acc@1: 78.0660 (58.1880)  Acc@5: 91.7453 (81.6500)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-160.pth.tar', 58.1880000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-158.pth.tar', 58.183999990234376)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-159.pth.tar', 58.09200006591797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-157.pth.tar', 58.0820000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-156.pth.tar', 57.900000168457034)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-155.pth.tar', 57.8020000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-154.pth.tar', 57.732000068359376)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-153.pth.tar', 57.71600001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-152.pth.tar', 57.70400006835938)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-149.pth.tar', 57.563999912109374)

Train: 161 [   0/1251 (  0%)]  Loss:  4.617858 (4.6179)  Time: 11.514s,   88.93/s  (11.514s,   88.93/s)  LR: 1.221e-05  Data: 10.350 (10.350)
Train: 161 [  50/1251 (  4%)]  Loss:  4.452963 (4.5354)  Time: 0.590s, 1736.42/s  (2.310s,  443.34/s)  LR: 1.221e-05  Data: 0.020 (1.699)
Train: 161 [ 100/1251 (  8%)]  Loss:  4.920631 (4.6638)  Time: 1.065s,  961.92/s  (2.343s,  437.09/s)  LR: 1.221e-05  Data: 0.238 (1.741)
Train: 161 [ 150/1251 ( 12%)]  Loss:  4.278963 (4.5676)  Time: 0.589s, 1739.69/s  (2.443s,  419.11/s)  LR: 1.221e-05  Data: 0.019 (1.838)
Train: 161 [ 200/1251 ( 16%)]  Loss:  4.902415 (4.6346)  Time: 4.034s,  253.86/s  (2.408s,  425.30/s)  LR: 1.221e-05  Data: 3.375 (1.801)
Train: 161 [ 250/1251 ( 20%)]  Loss:  4.170145 (4.5572)  Time: 0.590s, 1736.78/s  (2.365s,  432.93/s)  LR: 1.221e-05  Data: 0.018 (1.762)
Train: 161 [ 300/1251 ( 24%)]  Loss:  4.565330 (4.5583)  Time: 1.869s,  547.83/s  (2.342s,  437.20/s)  LR: 1.221e-05  Data: 1.220 (1.740)
Train: 161 [ 350/1251 ( 28%)]  Loss:  4.523004 (4.5539)  Time: 0.584s, 1754.69/s  (2.317s,  441.94/s)  LR: 1.221e-05  Data: 0.019 (1.714)
Train: 161 [ 400/1251 ( 32%)]  Loss:  4.933466 (4.5961)  Time: 2.493s,  410.75/s  (2.360s,  433.91/s)  LR: 1.221e-05  Data: 1.829 (1.756)
Train: 161 [ 450/1251 ( 36%)]  Loss:  4.109310 (4.5474)  Time: 0.584s, 1754.45/s  (2.355s,  434.84/s)  LR: 1.221e-05  Data: 0.020 (1.752)
Train: 161 [ 500/1251 ( 40%)]  Loss:  4.358449 (4.5302)  Time: 0.883s, 1160.13/s  (2.352s,  435.33/s)  LR: 1.221e-05  Data: 0.289 (1.750)
Train: 161 [ 550/1251 ( 44%)]  Loss:  4.803855 (4.5530)  Time: 0.588s, 1740.54/s  (2.359s,  434.11/s)  LR: 1.221e-05  Data: 0.020 (1.758)
Train: 161 [ 600/1251 ( 48%)]  Loss:  4.743598 (4.5677)  Time: 0.803s, 1275.26/s  (2.365s,  432.99/s)  LR: 1.221e-05  Data: 0.192 (1.763)
Train: 161 [ 650/1251 ( 52%)]  Loss:  4.191879 (4.5408)  Time: 0.586s, 1747.12/s  (2.370s,  432.11/s)  LR: 1.221e-05  Data: 0.019 (1.769)
Train: 161 [ 700/1251 ( 56%)]  Loss:  3.931910 (4.5003)  Time: 0.589s, 1739.49/s  (2.369s,  432.28/s)  LR: 1.221e-05  Data: 0.025 (1.768)
Train: 161 [ 750/1251 ( 60%)]  Loss:  4.344522 (4.4905)  Time: 0.586s, 1748.56/s  (2.385s,  429.30/s)  LR: 1.221e-05  Data: 0.020 (1.784)
Train: 161 [ 800/1251 ( 64%)]  Loss:  4.880379 (4.5135)  Time: 0.590s, 1734.38/s  (2.381s,  430.12/s)  LR: 1.221e-05  Data: 0.025 (1.780)
Train: 161 [ 850/1251 ( 68%)]  Loss:  4.470973 (4.5111)  Time: 0.586s, 1746.08/s  (2.382s,  429.86/s)  LR: 1.221e-05  Data: 0.020 (1.782)
Train: 161 [ 900/1251 ( 72%)]  Loss:  4.907910 (4.5320)  Time: 0.589s, 1739.66/s  (2.379s,  430.41/s)  LR: 1.221e-05  Data: 0.019 (1.779)
Train: 161 [ 950/1251 ( 76%)]  Loss:  4.521581 (4.5315)  Time: 0.584s, 1752.10/s  (2.369s,  432.31/s)  LR: 1.221e-05  Data: 0.020 (1.770)
Train: 161 [1000/1251 ( 80%)]  Loss:  5.027809 (4.5551)  Time: 0.589s, 1739.81/s  (2.365s,  432.90/s)  LR: 1.221e-05  Data: 0.021 (1.767)
Train: 161 [1050/1251 ( 84%)]  Loss:  4.085084 (4.5337)  Time: 0.584s, 1752.05/s  (2.373s,  431.55/s)  LR: 1.221e-05  Data: 0.020 (1.774)
Train: 161 [1100/1251 ( 88%)]  Loss:  4.039384 (4.5122)  Time: 0.586s, 1746.54/s  (2.375s,  431.22/s)  LR: 1.221e-05  Data: 0.020 (1.777)
Train: 161 [1150/1251 ( 92%)]  Loss:  4.582861 (4.5152)  Time: 0.587s, 1742.98/s  (2.384s,  429.61/s)  LR: 1.221e-05  Data: 0.023 (1.786)
Train: 161 [1200/1251 ( 96%)]  Loss:  4.218637 (4.5033)  Time: 0.583s, 1755.15/s  (2.380s,  430.26/s)  LR: 1.221e-05  Data: 0.018 (1.783)
Train: 161 [1250/1251 (100%)]  Loss:  5.092152 (4.5260)  Time: 0.566s, 1809.22/s  (2.376s,  431.01/s)  LR: 1.221e-05  Data: 0.000 (1.779)
Test: [   0/48]  Time: 13.923 (13.923)  Loss:  1.0281 (1.0281)  Acc@1: 78.6133 (78.6133)  Acc@5: 92.5781 (92.5781)
Test: [  48/48]  Time: 0.149 (3.381)  Loss:  1.0638 (1.8982)  Acc@1: 78.0660 (58.1960)  Acc@5: 91.7453 (81.5980)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-161.pth.tar', 58.1960000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-160.pth.tar', 58.1880000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-158.pth.tar', 58.183999990234376)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-159.pth.tar', 58.09200006591797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-157.pth.tar', 58.0820000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-156.pth.tar', 57.900000168457034)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-155.pth.tar', 57.8020000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-154.pth.tar', 57.732000068359376)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-153.pth.tar', 57.71600001708985)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-152.pth.tar', 57.70400006835938)

Train: 162 [   0/1251 (  0%)]  Loss:  5.029075 (5.0291)  Time: 11.144s,   91.89/s  (11.144s,   91.89/s)  LR: 1.142e-05  Data: 10.306 (10.306)
Train: 162 [  50/1251 (  4%)]  Loss:  4.670797 (4.8499)  Time: 0.586s, 1748.31/s  (2.409s,  425.10/s)  LR: 1.142e-05  Data: 0.023 (1.814)
Train: 162 [ 100/1251 (  8%)]  Loss:  4.949521 (4.8831)  Time: 0.696s, 1470.67/s  (2.342s,  437.23/s)  LR: 1.142e-05  Data: 0.132 (1.750)
Train: 162 [ 150/1251 ( 12%)]  Loss:  4.748363 (4.8494)  Time: 0.587s, 1744.01/s  (2.270s,  451.15/s)  LR: 1.142e-05  Data: 0.021 (1.681)
Train: 162 [ 200/1251 ( 16%)]  Loss:  4.924729 (4.8645)  Time: 1.366s,  749.51/s  (2.372s,  431.68/s)  LR: 1.142e-05  Data: 0.779 (1.782)
Train: 162 [ 250/1251 ( 20%)]  Loss:  4.251010 (4.7622)  Time: 0.585s, 1749.32/s  (2.304s,  444.40/s)  LR: 1.142e-05  Data: 0.021 (1.711)
Train: 162 [ 300/1251 ( 24%)]  Loss:  4.821218 (4.7707)  Time: 0.586s, 1748.57/s  (2.331s,  439.24/s)  LR: 1.142e-05  Data: 0.022 (1.737)
Train: 162 [ 350/1251 ( 28%)]  Loss:  4.868010 (4.7828)  Time: 0.592s, 1731.02/s  (2.310s,  443.26/s)  LR: 1.142e-05  Data: 0.028 (1.716)
Train: 162 [ 400/1251 ( 32%)]  Loss:  4.451002 (4.7460)  Time: 0.584s, 1754.12/s  (2.319s,  441.58/s)  LR: 1.142e-05  Data: 0.020 (1.726)
Train: 162 [ 450/1251 ( 36%)]  Loss:  4.797162 (4.7511)  Time: 0.585s, 1751.42/s  (2.297s,  445.73/s)  LR: 1.142e-05  Data: 0.020 (1.707)
Train: 162 [ 500/1251 ( 40%)]  Loss:  4.674339 (4.7441)  Time: 0.587s, 1744.92/s  (2.290s,  447.09/s)  LR: 1.142e-05  Data: 0.024 (1.701)
Train: 162 [ 550/1251 ( 44%)]  Loss:  4.314036 (4.7083)  Time: 0.584s, 1751.94/s  (2.283s,  448.60/s)  LR: 1.142e-05  Data: 0.020 (1.694)
Train: 162 [ 600/1251 ( 48%)]  Loss:  4.632546 (4.7024)  Time: 0.589s, 1739.87/s  (2.356s,  434.55/s)  LR: 1.142e-05  Data: 0.023 (1.768)
Train: 162 [ 650/1251 ( 52%)]  Loss:  3.857983 (4.6421)  Time: 0.584s, 1754.56/s  (2.363s,  433.37/s)  LR: 1.142e-05  Data: 0.018 (1.775)
Train: 162 [ 700/1251 ( 56%)]  Loss:  4.916917 (4.6604)  Time: 0.590s, 1736.25/s  (2.372s,  431.75/s)  LR: 1.142e-05  Data: 0.022 (1.785)
Train: 162 [ 750/1251 ( 60%)]  Loss:  4.272612 (4.6362)  Time: 0.589s, 1737.85/s  (2.361s,  433.67/s)  LR: 1.142e-05  Data: 0.019 (1.772)
Train: 162 [ 800/1251 ( 64%)]  Loss:  4.687587 (4.6392)  Time: 0.587s, 1742.98/s  (2.358s,  434.26/s)  LR: 1.142e-05  Data: 0.025 (1.769)
Train: 162 [ 850/1251 ( 68%)]  Loss:  4.978082 (4.6581)  Time: 0.584s, 1753.58/s  (2.344s,  436.90/s)  LR: 1.142e-05  Data: 0.019 (1.755)
Train: 162 [ 900/1251 ( 72%)]  Loss:  4.860974 (4.6687)  Time: 0.591s, 1731.23/s  (2.332s,  439.20/s)  LR: 1.142e-05  Data: 0.022 (1.743)
Train: 162 [ 950/1251 ( 76%)]  Loss:  4.493635 (4.6600)  Time: 0.587s, 1743.95/s  (2.332s,  439.15/s)  LR: 1.142e-05  Data: 0.021 (1.744)
Train: 162 [1000/1251 ( 80%)]  Loss:  4.062594 (4.6315)  Time: 0.587s, 1745.68/s  (2.333s,  438.96/s)  LR: 1.142e-05  Data: 0.024 (1.743)
Train: 162 [1050/1251 ( 84%)]  Loss:  4.417506 (4.6218)  Time: 0.588s, 1742.38/s  (2.322s,  440.91/s)  LR: 1.142e-05  Data: 0.020 (1.733)
Train: 162 [1100/1251 ( 88%)]  Loss:  4.943612 (4.6358)  Time: 0.587s, 1743.82/s  (2.321s,  441.11/s)  LR: 1.142e-05  Data: 0.021 (1.732)
Train: 162 [1150/1251 ( 92%)]  Loss:  4.907877 (4.6471)  Time: 0.589s, 1739.65/s  (2.309s,  443.51/s)  LR: 1.142e-05  Data: 0.021 (1.720)
Train: 162 [1200/1251 ( 96%)]  Loss:  4.670373 (4.6481)  Time: 0.586s, 1748.12/s  (2.307s,  443.82/s)  LR: 1.142e-05  Data: 0.024 (1.719)
Train: 162 [1250/1251 (100%)]  Loss:  4.901582 (4.6578)  Time: 0.565s, 1812.21/s  (2.297s,  445.86/s)  LR: 1.142e-05  Data: 0.000 (1.708)
Test: [   0/48]  Time: 13.711 (13.711)  Loss:  1.0435 (1.0435)  Acc@1: 78.0273 (78.0273)  Acc@5: 92.7734 (92.7734)
Test: [  48/48]  Time: 0.149 (3.097)  Loss:  1.0620 (1.8911)  Acc@1: 78.1840 (58.4000)  Acc@5: 91.6274 (81.7080)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-162.pth.tar', 58.3999999609375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-161.pth.tar', 58.1960000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-160.pth.tar', 58.1880000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-158.pth.tar', 58.183999990234376)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-159.pth.tar', 58.09200006591797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-157.pth.tar', 58.0820000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-156.pth.tar', 57.900000168457034)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-155.pth.tar', 57.8020000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-154.pth.tar', 57.732000068359376)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-153.pth.tar', 57.71600001708985)

Train: 163 [   0/1251 (  0%)]  Loss:  4.878375 (4.8784)  Time: 9.758s,  104.93/s  (9.758s,  104.93/s)  LR: 1.080e-05  Data: 9.061 (9.061)
Train: 163 [  50/1251 (  4%)]  Loss:  4.802000 (4.8402)  Time: 0.587s, 1743.25/s  (2.566s,  399.13/s)  LR: 1.080e-05  Data: 0.024 (1.979)
Train: 163 [ 100/1251 (  8%)]  Loss:  4.615407 (4.7653)  Time: 0.585s, 1750.82/s  (2.355s,  434.84/s)  LR: 1.080e-05  Data: 0.021 (1.772)
Train: 163 [ 150/1251 ( 12%)]  Loss:  4.531525 (4.7068)  Time: 0.587s, 1745.52/s  (2.303s,  444.66/s)  LR: 1.080e-05  Data: 0.023 (1.716)
Train: 163 [ 200/1251 ( 16%)]  Loss:  4.148126 (4.5951)  Time: 0.587s, 1744.76/s  (2.277s,  449.65/s)  LR: 1.080e-05  Data: 0.024 (1.691)
Train: 163 [ 250/1251 ( 20%)]  Loss:  4.452243 (4.5713)  Time: 0.584s, 1753.52/s  (2.328s,  439.91/s)  LR: 1.080e-05  Data: 0.020 (1.739)
Train: 163 [ 300/1251 ( 24%)]  Loss:  3.788925 (4.4595)  Time: 0.588s, 1742.97/s  (2.293s,  446.58/s)  LR: 1.080e-05  Data: 0.024 (1.706)
Train: 163 [ 350/1251 ( 28%)]  Loss:  4.753988 (4.4963)  Time: 0.586s, 1746.67/s  (2.261s,  452.87/s)  LR: 1.080e-05  Data: 0.022 (1.674)
Train: 163 [ 400/1251 ( 32%)]  Loss:  4.990987 (4.5513)  Time: 0.590s, 1734.94/s  (2.249s,  455.38/s)  LR: 1.080e-05  Data: 0.021 (1.660)
Train: 163 [ 450/1251 ( 36%)]  Loss:  4.207452 (4.5169)  Time: 0.591s, 1733.24/s  (2.263s,  452.49/s)  LR: 1.080e-05  Data: 0.022 (1.676)
Train: 163 [ 500/1251 ( 40%)]  Loss:  4.100800 (4.4791)  Time: 0.587s, 1743.63/s  (2.265s,  452.02/s)  LR: 1.080e-05  Data: 0.019 (1.674)
Train: 163 [ 550/1251 ( 44%)]  Loss:  4.756207 (4.5022)  Time: 2.022s,  506.54/s  (2.260s,  453.16/s)  LR: 1.080e-05  Data: 1.436 (1.667)
Train: 163 [ 600/1251 ( 48%)]  Loss:  4.965206 (4.5378)  Time: 0.590s, 1736.83/s  (2.257s,  453.61/s)  LR: 1.080e-05  Data: 0.019 (1.663)
Train: 163 [ 650/1251 ( 52%)]  Loss:  4.704160 (4.5497)  Time: 0.586s, 1746.84/s  (2.258s,  453.53/s)  LR: 1.080e-05  Data: 0.022 (1.662)
Train: 163 [ 700/1251 ( 56%)]  Loss:  5.081609 (4.5851)  Time: 0.588s, 1740.48/s  (2.258s,  453.41/s)  LR: 1.080e-05  Data: 0.019 (1.661)
Train: 163 [ 750/1251 ( 60%)]  Loss:  4.193379 (4.5606)  Time: 0.588s, 1740.63/s  (2.252s,  454.80/s)  LR: 1.080e-05  Data: 0.025 (1.653)
Train: 163 [ 800/1251 ( 64%)]  Loss:  4.896562 (4.5804)  Time: 0.587s, 1743.32/s  (2.248s,  455.56/s)  LR: 1.080e-05  Data: 0.020 (1.650)
Train: 163 [ 850/1251 ( 68%)]  Loss:  4.658519 (4.5847)  Time: 0.991s, 1033.62/s  (2.266s,  451.97/s)  LR: 1.080e-05  Data: 0.023 (1.666)
Train: 163 [ 900/1251 ( 72%)]  Loss:  4.185977 (4.5638)  Time: 0.716s, 1429.47/s  (2.258s,  453.44/s)  LR: 1.080e-05  Data: 0.153 (1.659)
Train: 163 [ 950/1251 ( 76%)]  Loss:  4.648107 (4.5680)  Time: 0.587s, 1744.83/s  (2.264s,  452.29/s)  LR: 1.080e-05  Data: 0.024 (1.663)
Train: 163 [1000/1251 ( 80%)]  Loss:  4.957972 (4.5865)  Time: 0.834s, 1227.40/s  (2.260s,  453.05/s)  LR: 1.080e-05  Data: 0.110 (1.657)
Train: 163 [1050/1251 ( 84%)]  Loss:  3.896446 (4.5552)  Time: 1.064s,  962.84/s  (2.259s,  453.25/s)  LR: 1.080e-05  Data: 0.416 (1.655)
Train: 163 [1100/1251 ( 88%)]  Loss:  4.600331 (4.5571)  Time: 0.586s, 1747.16/s  (2.254s,  454.22/s)  LR: 1.080e-05  Data: 0.019 (1.649)
Train: 163 [1150/1251 ( 92%)]  Loss:  3.758601 (4.5239)  Time: 1.166s,  878.20/s  (2.254s,  454.35/s)  LR: 1.080e-05  Data: 0.503 (1.648)
Train: 163 [1200/1251 ( 96%)]  Loss:  4.417015 (4.5196)  Time: 0.586s, 1746.00/s  (2.265s,  452.06/s)  LR: 1.080e-05  Data: 0.023 (1.659)
Train: 163 [1250/1251 (100%)]  Loss:  4.774804 (4.5294)  Time: 0.564s, 1816.41/s  (2.278s,  449.58/s)  LR: 1.080e-05  Data: 0.000 (1.672)
Test: [   0/48]  Time: 13.657 (13.657)  Loss:  1.0336 (1.0336)  Acc@1: 77.5391 (77.5391)  Acc@5: 93.1641 (93.1641)
Test: [  48/48]  Time: 0.149 (3.324)  Loss:  1.0577 (1.8956)  Acc@1: 77.9481 (58.3400)  Acc@5: 91.9811 (81.6660)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-162.pth.tar', 58.3999999609375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-163.pth.tar', 58.33999998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-161.pth.tar', 58.1960000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-160.pth.tar', 58.1880000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-158.pth.tar', 58.183999990234376)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-159.pth.tar', 58.09200006591797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-157.pth.tar', 58.0820000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-156.pth.tar', 57.900000168457034)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-155.pth.tar', 57.8020000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-154.pth.tar', 57.732000068359376)

Train: 164 [   0/1251 (  0%)]  Loss:  4.368197 (4.3682)  Time: 11.187s,   91.54/s  (11.187s,   91.54/s)  LR: 1.035e-05  Data: 10.275 (10.275)
Train: 164 [  50/1251 (  4%)]  Loss:  4.703788 (4.5360)  Time: 0.584s, 1754.00/s  (2.417s,  423.71/s)  LR: 1.035e-05  Data: 0.019 (1.804)
Train: 164 [ 100/1251 (  8%)]  Loss:  4.362338 (4.4781)  Time: 1.608s,  636.71/s  (2.344s,  436.87/s)  LR: 1.035e-05  Data: 1.017 (1.736)
Train: 164 [ 150/1251 ( 12%)]  Loss:  4.900185 (4.5836)  Time: 0.585s, 1749.11/s  (2.274s,  450.35/s)  LR: 1.035e-05  Data: 0.023 (1.675)
Train: 164 [ 200/1251 ( 16%)]  Loss:  4.966747 (4.6603)  Time: 0.807s, 1269.67/s  (2.255s,  454.05/s)  LR: 1.035e-05  Data: 0.244 (1.660)
Train: 164 [ 250/1251 ( 20%)]  Loss:  4.444753 (4.6243)  Time: 0.584s, 1752.44/s  (2.228s,  459.60/s)  LR: 1.035e-05  Data: 0.022 (1.635)
Train: 164 [ 300/1251 ( 24%)]  Loss:  4.316278 (4.5803)  Time: 0.584s, 1754.63/s  (2.272s,  450.80/s)  LR: 1.035e-05  Data: 0.020 (1.679)
Train: 164 [ 350/1251 ( 28%)]  Loss:  4.852531 (4.6144)  Time: 0.586s, 1748.44/s  (2.255s,  454.16/s)  LR: 1.035e-05  Data: 0.022 (1.665)
Train: 164 [ 400/1251 ( 32%)]  Loss:  4.475131 (4.5989)  Time: 0.585s, 1749.54/s  (2.257s,  453.78/s)  LR: 1.035e-05  Data: 0.022 (1.667)
Train: 164 [ 450/1251 ( 36%)]  Loss:  4.024364 (4.5414)  Time: 0.587s, 1745.77/s  (2.254s,  454.34/s)  LR: 1.035e-05  Data: 0.022 (1.665)
Train: 164 [ 500/1251 ( 40%)]  Loss:  4.403770 (4.5289)  Time: 0.590s, 1736.40/s  (2.259s,  453.29/s)  LR: 1.035e-05  Data: 0.024 (1.670)
Train: 164 [ 550/1251 ( 44%)]  Loss:  4.607516 (4.5355)  Time: 1.462s,  700.24/s  (2.250s,  455.02/s)  LR: 1.035e-05  Data: 0.881 (1.660)
Train: 164 [ 600/1251 ( 48%)]  Loss:  4.678864 (4.5465)  Time: 0.587s, 1745.55/s  (2.253s,  454.51/s)  LR: 1.035e-05  Data: 0.023 (1.660)
Train: 164 [ 650/1251 ( 52%)]  Loss:  4.321421 (4.5304)  Time: 0.588s, 1740.85/s  (2.257s,  453.75/s)  LR: 1.035e-05  Data: 0.021 (1.662)
Train: 164 [ 700/1251 ( 56%)]  Loss:  4.620285 (4.5364)  Time: 0.587s, 1745.38/s  (2.286s,  447.92/s)  LR: 1.035e-05  Data: 0.020 (1.691)
Train: 164 [ 750/1251 ( 60%)]  Loss:  4.585136 (4.5395)  Time: 0.584s, 1754.47/s  (2.282s,  448.63/s)  LR: 1.035e-05  Data: 0.020 (1.687)
Train: 164 [ 800/1251 ( 64%)]  Loss:  4.992187 (4.5661)  Time: 0.587s, 1744.47/s  (2.309s,  443.54/s)  LR: 1.035e-05  Data: 0.019 (1.713)
Train: 164 [ 850/1251 ( 68%)]  Loss:  4.503363 (4.5626)  Time: 1.769s,  578.73/s  (2.302s,  444.86/s)  LR: 1.035e-05  Data: 1.067 (1.706)
Train: 164 [ 900/1251 ( 72%)]  Loss:  4.002249 (4.5331)  Time: 0.587s, 1743.61/s  (2.296s,  445.93/s)  LR: 1.035e-05  Data: 0.022 (1.699)
Train: 164 [ 950/1251 ( 76%)]  Loss:  4.259273 (4.5194)  Time: 0.584s, 1753.15/s  (2.293s,  446.67/s)  LR: 1.035e-05  Data: 0.019 (1.695)
Train: 164 [1000/1251 ( 80%)]  Loss:  4.719786 (4.5290)  Time: 0.587s, 1744.35/s  (2.286s,  447.86/s)  LR: 1.035e-05  Data: 0.024 (1.689)
Train: 164 [1050/1251 ( 84%)]  Loss:  4.163305 (4.5123)  Time: 0.582s, 1759.07/s  (2.285s,  448.08/s)  LR: 1.035e-05  Data: 0.020 (1.689)
Train: 164 [1100/1251 ( 88%)]  Loss:  4.465919 (4.5103)  Time: 0.583s, 1757.07/s  (2.293s,  446.53/s)  LR: 1.035e-05  Data: 0.020 (1.698)
Train: 164 [1150/1251 ( 92%)]  Loss:  4.369955 (4.5045)  Time: 0.731s, 1399.87/s  (2.285s,  448.09/s)  LR: 1.035e-05  Data: 0.017 (1.690)
Train: 164 [1200/1251 ( 96%)]  Loss:  4.688836 (4.5118)  Time: 0.583s, 1756.80/s  (2.286s,  447.97/s)  LR: 1.035e-05  Data: 0.020 (1.691)
Train: 164 [1250/1251 (100%)]  Loss:  4.948494 (4.5286)  Time: 0.563s, 1818.37/s  (2.283s,  448.46/s)  LR: 1.035e-05  Data: 0.000 (1.688)
Test: [   0/48]  Time: 14.024 (14.024)  Loss:  1.0512 (1.0512)  Acc@1: 77.7344 (77.7344)  Acc@5: 92.7734 (92.7734)
Test: [  48/48]  Time: 0.149 (3.309)  Loss:  1.0692 (1.8943)  Acc@1: 77.3585 (58.3300)  Acc@5: 91.6274 (81.7060)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-162.pth.tar', 58.3999999609375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-163.pth.tar', 58.33999998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-164.pth.tar', 58.32999999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-161.pth.tar', 58.1960000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-160.pth.tar', 58.1880000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-158.pth.tar', 58.183999990234376)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-159.pth.tar', 58.09200006591797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-157.pth.tar', 58.0820000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-156.pth.tar', 57.900000168457034)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-155.pth.tar', 57.8020000390625)

Train: 165 [   0/1251 (  0%)]  Loss:  4.958815 (4.9588)  Time: 11.501s,   89.04/s  (11.501s,   89.04/s)  LR: 1.009e-05  Data: 10.385 (10.385)
Train: 165 [  50/1251 (  4%)]  Loss:  4.736924 (4.8479)  Time: 0.588s, 1742.42/s  (2.341s,  437.47/s)  LR: 1.009e-05  Data: 0.021 (1.734)
Train: 165 [ 100/1251 (  8%)]  Loss:  4.653147 (4.7830)  Time: 1.447s,  707.91/s  (2.260s,  453.16/s)  LR: 1.009e-05  Data: 0.778 (1.659)
Train: 165 [ 150/1251 ( 12%)]  Loss:  4.581729 (4.7327)  Time: 3.493s,  293.17/s  (2.293s,  446.62/s)  LR: 1.009e-05  Data: 2.901 (1.684)
Train: 165 [ 200/1251 ( 16%)]  Loss:  4.178144 (4.6218)  Time: 2.392s,  428.04/s  (2.309s,  443.39/s)  LR: 1.009e-05  Data: 1.710 (1.700)
Train: 165 [ 250/1251 ( 20%)]  Loss:  4.587376 (4.6160)  Time: 1.383s,  740.34/s  (2.285s,  448.17/s)  LR: 1.009e-05  Data: 0.713 (1.675)
Train: 165 [ 300/1251 ( 24%)]  Loss:  4.541327 (4.6054)  Time: 0.594s, 1723.49/s  (2.295s,  446.18/s)  LR: 1.009e-05  Data: 0.023 (1.684)
Train: 165 [ 350/1251 ( 28%)]  Loss:  4.437702 (4.5844)  Time: 1.071s,  955.81/s  (2.264s,  452.26/s)  LR: 1.009e-05  Data: 0.444 (1.653)
Train: 165 [ 400/1251 ( 32%)]  Loss:  4.637887 (4.5903)  Time: 0.585s, 1749.21/s  (2.309s,  443.45/s)  LR: 1.009e-05  Data: 0.021 (1.698)
Train: 165 [ 450/1251 ( 36%)]  Loss:  4.882986 (4.6196)  Time: 0.587s, 1744.81/s  (2.296s,  445.91/s)  LR: 1.009e-05  Data: 0.019 (1.687)
Train: 165 [ 500/1251 ( 40%)]  Loss:  4.305314 (4.5910)  Time: 0.587s, 1745.48/s  (2.292s,  446.77/s)  LR: 1.009e-05  Data: 0.020 (1.684)
Train: 165 [ 550/1251 ( 44%)]  Loss:  4.759455 (4.6051)  Time: 0.589s, 1739.91/s  (2.319s,  441.60/s)  LR: 1.009e-05  Data: 0.024 (1.711)
Train: 165 [ 600/1251 ( 48%)]  Loss:  4.974010 (4.6334)  Time: 1.385s,  739.15/s  (2.314s,  442.53/s)  LR: 1.009e-05  Data: 0.021 (1.706)
Train: 165 [ 650/1251 ( 52%)]  Loss:  4.123833 (4.5970)  Time: 0.586s, 1747.93/s  (2.307s,  443.92/s)  LR: 1.009e-05  Data: 0.019 (1.700)
Train: 165 [ 700/1251 ( 56%)]  Loss:  4.355439 (4.5809)  Time: 0.586s, 1748.45/s  (2.297s,  445.75/s)  LR: 1.009e-05  Data: 0.020 (1.692)
Train: 165 [ 750/1251 ( 60%)]  Loss:  4.505848 (4.5762)  Time: 0.587s, 1744.33/s  (2.301s,  445.06/s)  LR: 1.009e-05  Data: 0.021 (1.696)
Train: 165 [ 800/1251 ( 64%)]  Loss:  4.826929 (4.5910)  Time: 0.808s, 1267.68/s  (2.286s,  447.92/s)  LR: 1.009e-05  Data: 0.245 (1.682)
Train: 165 [ 850/1251 ( 68%)]  Loss:  3.918569 (4.5536)  Time: 0.585s, 1750.48/s  (2.278s,  449.51/s)  LR: 1.009e-05  Data: 0.020 (1.674)
Train: 165 [ 900/1251 ( 72%)]  Loss:  3.721313 (4.5098)  Time: 0.586s, 1748.77/s  (2.269s,  451.32/s)  LR: 1.009e-05  Data: 0.019 (1.666)
Train: 165 [ 950/1251 ( 76%)]  Loss:  4.203603 (4.4945)  Time: 0.585s, 1749.25/s  (2.288s,  447.61/s)  LR: 1.009e-05  Data: 0.021 (1.685)
Train: 165 [1000/1251 ( 80%)]  Loss:  4.677064 (4.5032)  Time: 0.583s, 1756.56/s  (2.284s,  448.34/s)  LR: 1.009e-05  Data: 0.019 (1.683)
Train: 165 [1050/1251 ( 84%)]  Loss:  4.742733 (4.5141)  Time: 0.586s, 1748.03/s  (2.276s,  449.95/s)  LR: 1.009e-05  Data: 0.020 (1.675)
Train: 165 [1100/1251 ( 88%)]  Loss:  4.581667 (4.5170)  Time: 0.587s, 1743.22/s  (2.261s,  452.87/s)  LR: 1.009e-05  Data: 0.019 (1.661)
Train: 165 [1150/1251 ( 92%)]  Loss:  4.709364 (4.5250)  Time: 0.586s, 1746.63/s  (2.270s,  451.13/s)  LR: 1.009e-05  Data: 0.019 (1.670)
Train: 165 [1200/1251 ( 96%)]  Loss:  4.779987 (4.5352)  Time: 0.587s, 1744.40/s  (2.267s,  451.62/s)  LR: 1.009e-05  Data: 0.020 (1.668)
Train: 165 [1250/1251 (100%)]  Loss:  4.668984 (4.5404)  Time: 0.563s, 1818.09/s  (2.262s,  452.72/s)  LR: 1.009e-05  Data: 0.000 (1.663)
Test: [   0/48]  Time: 13.412 (13.412)  Loss:  1.0319 (1.0319)  Acc@1: 78.1250 (78.1250)  Acc@5: 92.8711 (92.8711)
Test: [  48/48]  Time: 0.149 (3.056)  Loss:  1.0744 (1.8916)  Acc@1: 77.9481 (58.4440)  Acc@5: 91.6274 (81.7640)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-165.pth.tar', 58.44399998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-162.pth.tar', 58.3999999609375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-163.pth.tar', 58.33999998779297)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-164.pth.tar', 58.32999999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-161.pth.tar', 58.1960000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-160.pth.tar', 58.1880000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-158.pth.tar', 58.183999990234376)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-159.pth.tar', 58.09200006591797)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-157.pth.tar', 58.0820000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-156.pth.tar', 57.900000168457034)

*** Best metric: 58.44399998779297 (epoch 165)

wandb: Waiting for W&B process to finish, PID 24684
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: | 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210601_224040-PreTraining_vit_deit_tiny_patch16_224_1k/logs/debug.log
wandb: Find internal logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210601_224040-PreTraining_vit_deit_tiny_patch16_224_1k/logs/debug-internal.log
wandb: Run summary:
wandb:    eval_top5 81.764
wandb:   _timestamp 1622623462
wandb:   train_loss 4.54039
wandb:        _step 165
wandb:        epoch 165
wandb:     _runtime 526122
wandb:    eval_loss 1.89161
wandb:    eval_top1 58.444
wandb: Run history:
wandb:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   train_loss ‚ñÉ‚ñá‚ñÉ‚ñÜ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÑ‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñÑ‚ñÅ‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÑ
wandb:    eval_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    eval_top1 ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà
wandb:    eval_top5 ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà
wandb:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced PreTraining_vit_deit_tiny_patch16_224_1k: https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_1k
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
--End--
Wed Jun 2 17:44:38 JST 2021

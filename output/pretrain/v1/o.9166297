--Start--
Fri May 28 14:50:42 JST 2021
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 3
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
wandb: Currently logged in as: gyama_x (use `wandb login --relogin` to force relogin)
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.10.27
wandb: Resuming run PreTraining_vit_deit_tiny_patch16_224_fake_1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gyama_x/pytorch-image-models
wandb: üöÄ View run at https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run data is saved locally in /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210528_145118-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run `wandb offline` to turn off syncing.

Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar' (epoch 65)
Using native Torch DistributedDataParallel.
Scheduled epochs: 88
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Train: 66 [   0/1171 (  0%)]  Loss:  3.064849 (3.0648)  Time: 10.775s,   95.04/s  (10.775s,   95.04/s)  LR: 1.550e-04  Data: 9.322 (9.322)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 66 [  50/1171 (  4%)]  Loss:  3.242073 (3.1535)  Time: 0.865s, 1183.47/s  (2.404s,  425.91/s)  LR: 1.550e-04  Data: 0.296 (1.797)
Train: 66 [ 100/1171 (  9%)]  Loss:  3.281178 (3.1960)  Time: 0.589s, 1739.39/s  (2.199s,  465.72/s)  LR: 1.550e-04  Data: 0.024 (1.599)
Train: 66 [ 150/1171 ( 13%)]  Loss:  2.642628 (3.0577)  Time: 0.587s, 1744.14/s  (2.097s,  488.26/s)  LR: 1.550e-04  Data: 0.023 (1.501)
Train: 66 [ 200/1171 ( 17%)]  Loss:  2.811300 (3.0084)  Time: 0.588s, 1741.48/s  (2.089s,  490.23/s)  LR: 1.550e-04  Data: 0.024 (1.494)
Train: 66 [ 250/1171 ( 21%)]  Loss:  2.532136 (2.9290)  Time: 0.585s, 1750.20/s  (2.124s,  482.05/s)  LR: 1.550e-04  Data: 0.019 (1.526)
Train: 66 [ 300/1171 ( 26%)]  Loss:  3.248190 (2.9746)  Time: 0.587s, 1743.82/s  (2.118s,  483.50/s)  LR: 1.550e-04  Data: 0.025 (1.521)
Train: 66 [ 350/1171 ( 30%)]  Loss:  2.808098 (2.9538)  Time: 0.583s, 1756.78/s  (2.114s,  484.29/s)  LR: 1.550e-04  Data: 0.020 (1.516)
Train: 66 [ 400/1171 ( 34%)]  Loss:  3.231614 (2.9847)  Time: 0.588s, 1741.81/s  (2.110s,  485.32/s)  LR: 1.550e-04  Data: 0.025 (1.513)
Train: 66 [ 450/1171 ( 38%)]  Loss:  2.762393 (2.9624)  Time: 0.587s, 1744.35/s  (2.099s,  487.79/s)  LR: 1.550e-04  Data: 0.024 (1.501)
Train: 66 [ 500/1171 ( 43%)]  Loss:  2.672261 (2.9361)  Time: 0.586s, 1747.54/s  (2.095s,  488.73/s)  LR: 1.550e-04  Data: 0.020 (1.497)
Train: 66 [ 550/1171 ( 47%)]  Loss:  2.995822 (2.9410)  Time: 0.589s, 1739.47/s  (2.097s,  488.35/s)  LR: 1.550e-04  Data: 0.022 (1.498)
Train: 66 [ 600/1171 ( 51%)]  Loss:  3.675296 (2.9975)  Time: 0.582s, 1758.90/s  (2.093s,  489.33/s)  LR: 1.550e-04  Data: 0.020 (1.494)
Train: 66 [ 650/1171 ( 56%)]  Loss:  2.898756 (2.9905)  Time: 0.585s, 1749.32/s  (2.105s,  486.48/s)  LR: 1.550e-04  Data: 0.023 (1.506)
Train: 66 [ 700/1171 ( 60%)]  Loss:  3.200961 (3.0045)  Time: 0.584s, 1752.95/s  (2.129s,  481.00/s)  LR: 1.550e-04  Data: 0.018 (1.531)
Train: 66 [ 750/1171 ( 64%)]  Loss:  3.350898 (3.0262)  Time: 0.585s, 1749.76/s  (2.153s,  475.71/s)  LR: 1.550e-04  Data: 0.023 (1.555)
Train: 66 [ 800/1171 ( 68%)]  Loss:  2.898065 (3.0186)  Time: 0.584s, 1754.89/s  (2.168s,  472.37/s)  LR: 1.550e-04  Data: 0.021 (1.571)
Train: 66 [ 850/1171 ( 73%)]  Loss:  3.071406 (3.0216)  Time: 0.587s, 1745.06/s  (2.190s,  467.49/s)  LR: 1.550e-04  Data: 0.024 (1.593)
Train: 66 [ 900/1171 ( 77%)]  Loss:  3.302917 (3.0364)  Time: 0.588s, 1742.60/s  (2.204s,  464.69/s)  LR: 1.550e-04  Data: 0.022 (1.606)
Train: 66 [ 950/1171 ( 81%)]  Loss:  2.831623 (3.0261)  Time: 0.663s, 1545.21/s  (2.217s,  461.90/s)  LR: 1.550e-04  Data: 0.022 (1.619)
Train: 66 [1000/1171 ( 85%)]  Loss:  2.544673 (3.0032)  Time: 0.587s, 1743.88/s  (2.223s,  460.71/s)  LR: 1.550e-04  Data: 0.023 (1.625)
Train: 66 [1050/1171 ( 90%)]  Loss:  3.029076 (3.0044)  Time: 0.585s, 1751.67/s  (2.253s,  454.58/s)  LR: 1.550e-04  Data: 0.020 (1.654)
Train: 66 [1100/1171 ( 94%)]  Loss:  3.077454 (3.0076)  Time: 0.584s, 1752.47/s  (2.266s,  451.89/s)  LR: 1.550e-04  Data: 0.021 (1.669)
Train: 66 [1150/1171 ( 98%)]  Loss:  2.688738 (2.9943)  Time: 0.587s, 1745.58/s  (2.281s,  448.88/s)  LR: 1.550e-04  Data: 0.021 (1.684)
Train: 66 [1170/1171 (100%)]  Loss:  2.999382 (2.9945)  Time: 0.565s, 1812.27/s  (2.286s,  448.03/s)  LR: 1.550e-04  Data: 0.000 (1.688)
Test: [   0/97]  Time: 15.432 (15.432)  Loss:  0.3173 (0.3173)  Acc@1: 95.2148 (95.2148)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 2.972 (3.295)  Loss:  0.5187 (0.3941)  Acc@1: 90.1367 (93.7366)  Acc@5: 98.0469 (98.7669)
Test: [  97/97]  Time: 0.421 (3.172)  Loss:  0.3569 (0.4115)  Acc@1: 93.7500 (93.2470)  Acc@5: 98.6607 (98.5010)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-66.pth.tar', 93.24700005126954)

Train: 67 [   0/1171 (  0%)]  Loss:  3.080444 (3.0804)  Time: 5.912s,  173.22/s  (5.912s,  173.22/s)  LR: 1.427e-04  Data: 4.589 (4.589)
Train: 67 [  50/1171 (  4%)]  Loss:  3.408343 (3.2444)  Time: 0.585s, 1749.68/s  (2.146s,  477.16/s)  LR: 1.427e-04  Data: 0.018 (1.531)
Train: 67 [ 100/1171 (  9%)]  Loss:  3.240379 (3.2431)  Time: 5.949s,  172.14/s  (2.417s,  423.62/s)  LR: 1.427e-04  Data: 5.372 (1.814)
Train: 67 [ 150/1171 ( 13%)]  Loss:  2.925165 (3.1636)  Time: 1.231s,  832.15/s  (2.386s,  429.16/s)  LR: 1.427e-04  Data: 0.188 (1.775)
Train: 67 [ 200/1171 ( 17%)]  Loss:  2.870848 (3.1050)  Time: 4.899s,  209.01/s  (2.420s,  423.17/s)  LR: 1.427e-04  Data: 4.326 (1.812)
Train: 67 [ 250/1171 ( 21%)]  Loss:  2.743075 (3.0447)  Time: 0.584s, 1754.69/s  (2.394s,  427.73/s)  LR: 1.427e-04  Data: 0.019 (1.785)
Train: 67 [ 300/1171 ( 26%)]  Loss:  2.916265 (3.0264)  Time: 8.023s,  127.63/s  (2.414s,  424.20/s)  LR: 1.427e-04  Data: 7.461 (1.804)
Train: 67 [ 350/1171 ( 30%)]  Loss:  2.886971 (3.0089)  Time: 0.585s, 1749.60/s  (2.383s,  429.65/s)  LR: 1.427e-04  Data: 0.019 (1.776)
Train: 67 [ 400/1171 ( 34%)]  Loss:  3.379023 (3.0501)  Time: 6.410s,  159.74/s  (2.377s,  430.88/s)  LR: 1.427e-04  Data: 5.849 (1.773)
Train: 67 [ 450/1171 ( 38%)]  Loss:  2.818606 (3.0269)  Time: 0.587s, 1743.58/s  (2.377s,  430.77/s)  LR: 1.427e-04  Data: 0.024 (1.774)
Train: 67 [ 500/1171 ( 43%)]  Loss:  3.116083 (3.0350)  Time: 8.694s,  117.78/s  (2.421s,  422.97/s)  LR: 1.427e-04  Data: 8.023 (1.819)
Train: 67 [ 550/1171 ( 47%)]  Loss:  2.903269 (3.0240)  Time: 0.588s, 1742.33/s  (2.437s,  420.17/s)  LR: 1.427e-04  Data: 0.020 (1.836)
Train: 67 [ 600/1171 ( 51%)]  Loss:  3.276407 (3.0435)  Time: 8.696s,  117.75/s  (2.455s,  417.10/s)  LR: 1.427e-04  Data: 8.131 (1.856)
Train: 67 [ 650/1171 ( 56%)]  Loss:  3.273793 (3.0599)  Time: 0.588s, 1740.61/s  (2.451s,  417.83/s)  LR: 1.427e-04  Data: 0.025 (1.853)
Train: 67 [ 700/1171 ( 60%)]  Loss:  2.903966 (3.0495)  Time: 7.071s,  144.81/s  (2.447s,  418.53/s)  LR: 1.427e-04  Data: 6.495 (1.850)
Train: 67 [ 750/1171 ( 64%)]  Loss:  2.895641 (3.0399)  Time: 0.584s, 1753.49/s  (2.430s,  421.34/s)  LR: 1.427e-04  Data: 0.020 (1.835)
Train: 67 [ 800/1171 ( 68%)]  Loss:  2.810167 (3.0264)  Time: 7.835s,  130.69/s  (2.426s,  422.04/s)  LR: 1.427e-04  Data: 7.272 (1.831)
Train: 67 [ 850/1171 ( 73%)]  Loss:  3.047096 (3.0275)  Time: 0.584s, 1753.45/s  (2.444s,  418.91/s)  LR: 1.427e-04  Data: 0.020 (1.849)
Train: 67 [ 900/1171 ( 77%)]  Loss:  3.243846 (3.0389)  Time: 9.788s,  104.62/s  (2.454s,  417.27/s)  LR: 1.427e-04  Data: 9.226 (1.860)
Train: 67 [ 950/1171 ( 81%)]  Loss:  3.149781 (3.0445)  Time: 0.583s, 1756.16/s  (2.453s,  417.51/s)  LR: 1.427e-04  Data: 0.020 (1.859)
Train: 67 [1000/1171 ( 85%)]  Loss:  3.583730 (3.0701)  Time: 7.397s,  138.43/s  (2.454s,  417.28/s)  LR: 1.427e-04  Data: 6.707 (1.860)
Train: 67 [1050/1171 ( 90%)]  Loss:  3.347302 (3.0827)  Time: 0.585s, 1751.78/s  (2.443s,  419.21/s)  LR: 1.427e-04  Data: 0.020 (1.849)
Train: 67 [1100/1171 ( 94%)]  Loss:  3.082978 (3.0827)  Time: 7.945s,  128.89/s  (2.440s,  419.74/s)  LR: 1.427e-04  Data: 7.327 (1.846)
Train: 67 [1150/1171 ( 98%)]  Loss:  2.931745 (3.0765)  Time: 0.590s, 1736.39/s  (2.431s,  421.20/s)  LR: 1.427e-04  Data: 0.019 (1.838)
Train: 67 [1170/1171 (100%)]  Loss:  3.017283 (3.0741)  Time: 0.564s, 1815.53/s  (2.428s,  421.80/s)  LR: 1.427e-04  Data: 0.000 (1.835)
Test: [   0/97]  Time: 16.958 (16.958)  Loss:  0.3167 (0.3167)  Acc@1: 96.2891 (96.2891)  Acc@5: 99.6094 (99.6094)
Test: [  50/97]  Time: 0.196 (3.464)  Loss:  0.5220 (0.4046)  Acc@1: 89.7461 (93.7385)  Acc@5: 98.3398 (98.7764)
Test: [  97/97]  Time: 0.119 (3.351)  Loss:  0.3489 (0.4177)  Acc@1: 94.4940 (93.3000)  Acc@5: 98.8095 (98.4810)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-67.pth.tar', 93.30000000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-66.pth.tar', 93.24700005126954)

Train: 68 [   0/1171 (  0%)]  Loss:  3.017784 (3.0178)  Time: 11.858s,   86.36/s  (11.858s,   86.36/s)  LR: 1.309e-04  Data: 10.778 (10.778)
Train: 68 [  50/1171 (  4%)]  Loss:  2.891684 (2.9547)  Time: 0.588s, 1741.92/s  (2.468s,  414.88/s)  LR: 1.309e-04  Data: 0.021 (1.874)
Train: 68 [ 100/1171 (  9%)]  Loss:  3.588736 (3.1661)  Time: 5.011s,  204.37/s  (2.414s,  424.26/s)  LR: 1.309e-04  Data: 4.079 (1.808)
Train: 68 [ 150/1171 ( 13%)]  Loss:  2.455404 (2.9884)  Time: 0.587s, 1743.71/s  (2.330s,  439.54/s)  LR: 1.309e-04  Data: 0.022 (1.732)
Train: 68 [ 200/1171 ( 17%)]  Loss:  2.787915 (2.9483)  Time: 3.475s,  294.69/s  (2.319s,  441.50/s)  LR: 1.309e-04  Data: 2.782 (1.720)
Train: 68 [ 250/1171 ( 21%)]  Loss:  2.799110 (2.9234)  Time: 0.588s, 1740.49/s  (2.273s,  450.48/s)  LR: 1.309e-04  Data: 0.023 (1.670)
Train: 68 [ 300/1171 ( 26%)]  Loss:  2.597265 (2.8768)  Time: 1.192s,  858.78/s  (2.348s,  436.15/s)  LR: 1.309e-04  Data: 0.609 (1.741)
Train: 68 [ 350/1171 ( 30%)]  Loss:  2.888951 (2.8784)  Time: 3.322s,  308.23/s  (2.373s,  431.48/s)  LR: 1.309e-04  Data: 2.741 (1.763)
Train: 68 [ 400/1171 ( 34%)]  Loss:  3.085161 (2.9013)  Time: 0.585s, 1751.46/s  (2.375s,  431.08/s)  LR: 1.309e-04  Data: 0.021 (1.766)
Train: 68 [ 450/1171 ( 38%)]  Loss:  2.887078 (2.8999)  Time: 4.829s,  212.07/s  (2.394s,  427.67/s)  LR: 1.309e-04  Data: 4.164 (1.782)
Train: 68 [ 500/1171 ( 43%)]  Loss:  3.187133 (2.9260)  Time: 0.585s, 1750.53/s  (2.397s,  427.29/s)  LR: 1.309e-04  Data: 0.020 (1.783)
Train: 68 [ 550/1171 ( 47%)]  Loss:  2.971060 (2.9298)  Time: 5.634s,  181.76/s  (2.410s,  424.87/s)  LR: 1.309e-04  Data: 4.942 (1.798)
Train: 68 [ 600/1171 ( 51%)]  Loss:  2.834208 (2.9224)  Time: 0.584s, 1752.48/s  (2.409s,  425.08/s)  LR: 1.309e-04  Data: 0.020 (1.797)
Train: 68 [ 650/1171 ( 56%)]  Loss:  3.204615 (2.9426)  Time: 5.378s,  190.41/s  (2.448s,  418.32/s)  LR: 1.309e-04  Data: 4.673 (1.836)
Train: 68 [ 700/1171 ( 60%)]  Loss:  3.207956 (2.9603)  Time: 0.589s, 1739.06/s  (2.460s,  416.28/s)  LR: 1.309e-04  Data: 0.024 (1.849)
Train: 68 [ 750/1171 ( 64%)]  Loss:  3.053286 (2.9661)  Time: 5.524s,  185.38/s  (2.468s,  414.86/s)  LR: 1.309e-04  Data: 4.957 (1.858)
Train: 68 [ 800/1171 ( 68%)]  Loss:  3.014884 (2.9690)  Time: 0.582s, 1759.26/s  (2.465s,  415.37/s)  LR: 1.309e-04  Data: 0.019 (1.855)
Train: 68 [ 850/1171 ( 73%)]  Loss:  2.463624 (2.9409)  Time: 5.459s,  187.59/s  (2.467s,  415.01/s)  LR: 1.309e-04  Data: 4.451 (1.857)
Train: 68 [ 900/1171 ( 77%)]  Loss:  2.861465 (2.9367)  Time: 0.586s, 1747.43/s  (2.455s,  417.17/s)  LR: 1.309e-04  Data: 0.021 (1.846)
Train: 68 [ 950/1171 ( 81%)]  Loss:  3.477952 (2.9638)  Time: 4.800s,  213.35/s  (2.455s,  417.17/s)  LR: 1.309e-04  Data: 4.218 (1.847)
Train: 68 [1000/1171 ( 85%)]  Loss:  2.903964 (2.9609)  Time: 0.590s, 1736.71/s  (2.464s,  415.63/s)  LR: 1.309e-04  Data: 0.025 (1.856)
Train: 68 [1050/1171 ( 90%)]  Loss:  3.210916 (2.9723)  Time: 5.050s,  202.76/s  (2.475s,  413.77/s)  LR: 1.309e-04  Data: 4.393 (1.868)
Train: 68 [1100/1171 ( 94%)]  Loss:  3.462450 (2.9936)  Time: 0.586s, 1747.56/s  (2.472s,  414.19/s)  LR: 1.309e-04  Data: 0.023 (1.866)
Train: 68 [1150/1171 ( 98%)]  Loss:  3.095659 (2.9978)  Time: 3.915s,  261.54/s  (2.469s,  414.77/s)  LR: 1.309e-04  Data: 2.866 (1.863)
Train: 68 [1170/1171 (100%)]  Loss:  3.358648 (3.0123)  Time: 0.564s, 1815.56/s  (2.463s,  415.76/s)  LR: 1.309e-04  Data: 0.000 (1.857)
Test: [   0/97]  Time: 12.984 (12.984)  Loss:  0.3558 (0.3558)  Acc@1: 95.8984 (95.8984)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.923 (3.117)  Loss:  0.5675 (0.4525)  Acc@1: 89.7461 (93.8572)  Acc@5: 98.2422 (98.7726)
Test: [  97/97]  Time: 0.119 (2.972)  Loss:  0.4190 (0.4591)  Acc@1: 93.0060 (93.3790)  Acc@5: 98.5119 (98.5320)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-68.pth.tar', 93.37899999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-67.pth.tar', 93.30000000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-66.pth.tar', 93.24700005126954)

Train: 69 [   0/1171 (  0%)]  Loss:  3.329234 (3.3292)  Time: 10.627s,   96.36/s  (10.627s,   96.36/s)  LR: 1.196e-04  Data: 9.502 (9.502)
Train: 69 [  50/1171 (  4%)]  Loss:  3.186095 (3.2577)  Time: 0.584s, 1754.68/s  (2.461s,  416.04/s)  LR: 1.196e-04  Data: 0.022 (1.863)
Train: 69 [ 100/1171 (  9%)]  Loss:  2.954297 (3.1565)  Time: 1.787s,  573.09/s  (2.589s,  395.55/s)  LR: 1.196e-04  Data: 1.115 (1.985)
Train: 69 [ 150/1171 ( 13%)]  Loss:  3.093163 (3.1407)  Time: 0.583s, 1757.46/s  (2.555s,  400.76/s)  LR: 1.196e-04  Data: 0.020 (1.954)
Train: 69 [ 200/1171 ( 17%)]  Loss:  2.566084 (3.0258)  Time: 2.101s,  487.38/s  (2.560s,  399.93/s)  LR: 1.196e-04  Data: 1.435 (1.956)
Train: 69 [ 250/1171 ( 21%)]  Loss:  2.958703 (3.0146)  Time: 0.586s, 1747.47/s  (2.524s,  405.63/s)  LR: 1.196e-04  Data: 0.022 (1.921)
Train: 69 [ 300/1171 ( 26%)]  Loss:  3.270070 (3.0511)  Time: 2.675s,  382.80/s  (2.514s,  407.29/s)  LR: 1.196e-04  Data: 2.011 (1.911)
Train: 69 [ 350/1171 ( 30%)]  Loss:  3.374164 (3.0915)  Time: 0.585s, 1751.61/s  (2.472s,  414.23/s)  LR: 1.196e-04  Data: 0.021 (1.872)
Train: 69 [ 400/1171 ( 34%)]  Loss:  2.979986 (3.0791)  Time: 5.018s,  204.07/s  (2.464s,  415.60/s)  LR: 1.196e-04  Data: 4.438 (1.866)
Train: 69 [ 450/1171 ( 38%)]  Loss:  3.228313 (3.0940)  Time: 0.588s, 1742.62/s  (2.485s,  412.13/s)  LR: 1.196e-04  Data: 0.023 (1.883)
Train: 69 [ 500/1171 ( 43%)]  Loss:  2.910391 (3.0773)  Time: 8.125s,  126.03/s  (2.511s,  407.73/s)  LR: 1.196e-04  Data: 7.532 (1.910)
Train: 69 [ 550/1171 ( 47%)]  Loss:  2.722972 (3.0478)  Time: 0.587s, 1744.37/s  (2.514s,  407.38/s)  LR: 1.196e-04  Data: 0.022 (1.911)
Train: 69 [ 600/1171 ( 51%)]  Loss:  2.618583 (3.0148)  Time: 3.208s,  319.25/s  (2.518s,  406.61/s)  LR: 1.196e-04  Data: 2.644 (1.916)
Train: 69 [ 650/1171 ( 56%)]  Loss:  3.277458 (3.0335)  Time: 3.797s,  269.70/s  (2.512s,  407.68/s)  LR: 1.196e-04  Data: 3.235 (1.910)
Train: 69 [ 700/1171 ( 60%)]  Loss:  2.914916 (3.0256)  Time: 5.709s,  179.37/s  (2.499s,  409.81/s)  LR: 1.196e-04  Data: 5.038 (1.897)
Train: 69 [ 750/1171 ( 64%)]  Loss:  3.118909 (3.0315)  Time: 0.589s, 1737.23/s  (2.483s,  412.37/s)  LR: 1.196e-04  Data: 0.025 (1.882)
Train: 69 [ 800/1171 ( 68%)]  Loss:  3.391309 (3.0526)  Time: 8.061s,  127.03/s  (2.502s,  409.27/s)  LR: 1.196e-04  Data: 7.404 (1.901)
Train: 69 [ 850/1171 ( 73%)]  Loss:  3.305386 (3.0667)  Time: 1.052s,  973.62/s  (2.507s,  408.39/s)  LR: 1.196e-04  Data: 0.436 (1.907)
Train: 69 [ 900/1171 ( 77%)]  Loss:  3.103088 (3.0686)  Time: 8.694s,  117.78/s  (2.519s,  406.55/s)  LR: 1.196e-04  Data: 7.908 (1.918)
Train: 69 [ 950/1171 ( 81%)]  Loss:  2.817328 (3.0560)  Time: 0.590s, 1734.56/s  (2.513s,  407.50/s)  LR: 1.196e-04  Data: 0.019 (1.913)
Train: 69 [1000/1171 ( 85%)]  Loss:  2.899899 (3.0486)  Time: 8.482s,  120.72/s  (2.515s,  407.09/s)  LR: 1.196e-04  Data: 7.909 (1.916)
Train: 69 [1050/1171 ( 90%)]  Loss:  3.169417 (3.0541)  Time: 0.586s, 1748.12/s  (2.510s,  407.95/s)  LR: 1.196e-04  Data: 0.019 (1.911)
Train: 69 [1100/1171 ( 94%)]  Loss:  2.999821 (3.0517)  Time: 6.782s,  151.00/s  (2.506s,  408.57/s)  LR: 1.196e-04  Data: 6.119 (1.908)
Train: 69 [1150/1171 ( 98%)]  Loss:  3.276142 (3.0611)  Time: 0.588s, 1741.93/s  (2.515s,  407.19/s)  LR: 1.196e-04  Data: 0.018 (1.917)
Train: 69 [1170/1171 (100%)]  Loss:  3.204945 (3.0668)  Time: 0.565s, 1812.03/s  (2.519s,  406.48/s)  LR: 1.196e-04  Data: 0.000 (1.921)
Test: [   0/97]  Time: 18.216 (18.216)  Loss:  0.3167 (0.3167)  Acc@1: 96.5820 (96.5820)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (3.882)  Loss:  0.4857 (0.3938)  Acc@1: 91.0156 (94.1234)  Acc@5: 98.2422 (98.7764)
Test: [  97/97]  Time: 0.120 (3.703)  Loss:  0.3658 (0.4072)  Acc@1: 93.6012 (93.5420)  Acc@5: 98.8095 (98.5490)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-69.pth.tar', 93.5420000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-68.pth.tar', 93.37899999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-67.pth.tar', 93.30000000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-66.pth.tar', 93.24700005126954)

Train: 70 [   0/1171 (  0%)]  Loss:  2.955820 (2.9558)  Time: 13.494s,   75.89/s  (13.494s,   75.89/s)  LR: 1.087e-04  Data: 12.379 (12.379)
Train: 70 [  50/1171 (  4%)]  Loss:  2.889304 (2.9226)  Time: 0.583s, 1757.34/s  (2.744s,  373.20/s)  LR: 1.087e-04  Data: 0.021 (2.132)
Train: 70 [ 100/1171 (  9%)]  Loss:  2.808906 (2.8847)  Time: 0.587s, 1744.53/s  (2.712s,  377.53/s)  LR: 1.087e-04  Data: 0.023 (2.100)
Train: 70 [ 150/1171 ( 13%)]  Loss:  2.886338 (2.8851)  Time: 5.864s,  174.61/s  (2.657s,  385.42/s)  LR: 1.087e-04  Data: 4.788 (2.040)
Train: 70 [ 200/1171 ( 17%)]  Loss:  3.508663 (3.0098)  Time: 0.590s, 1736.95/s  (2.723s,  376.10/s)  LR: 1.087e-04  Data: 0.023 (2.112)
Train: 70 [ 250/1171 ( 21%)]  Loss:  2.775284 (2.9707)  Time: 0.582s, 1758.23/s  (2.688s,  380.92/s)  LR: 1.087e-04  Data: 0.020 (2.084)
Train: 70 [ 300/1171 ( 26%)]  Loss:  3.308489 (3.0190)  Time: 0.588s, 1740.17/s  (2.679s,  382.26/s)  LR: 1.087e-04  Data: 0.024 (2.079)
Train: 70 [ 350/1171 ( 30%)]  Loss:  2.730637 (2.9829)  Time: 0.586s, 1747.46/s  (2.641s,  387.71/s)  LR: 1.087e-04  Data: 0.021 (2.042)
Train: 70 [ 400/1171 ( 34%)]  Loss:  3.287516 (3.0168)  Time: 0.588s, 1742.45/s  (2.606s,  393.00/s)  LR: 1.087e-04  Data: 0.024 (2.008)
Train: 70 [ 450/1171 ( 38%)]  Loss:  2.890277 (3.0041)  Time: 0.586s, 1746.71/s  (2.558s,  400.30/s)  LR: 1.087e-04  Data: 0.018 (1.961)
Train: 70 [ 500/1171 ( 43%)]  Loss:  3.083684 (3.0114)  Time: 0.585s, 1751.39/s  (2.545s,  402.31/s)  LR: 1.087e-04  Data: 0.022 (1.950)
Train: 70 [ 550/1171 ( 47%)]  Loss:  3.086283 (3.0176)  Time: 0.584s, 1753.31/s  (2.572s,  398.13/s)  LR: 1.087e-04  Data: 0.021 (1.977)
Train: 70 [ 600/1171 ( 51%)]  Loss:  3.332396 (3.0418)  Time: 0.586s, 1747.34/s  (2.583s,  396.49/s)  LR: 1.087e-04  Data: 0.019 (1.988)
Train: 70 [ 650/1171 ( 56%)]  Loss:  3.480218 (3.0731)  Time: 0.586s, 1748.86/s  (2.572s,  398.19/s)  LR: 1.087e-04  Data: 0.020 (1.978)
Train: 70 [ 700/1171 ( 60%)]  Loss:  3.118363 (3.0761)  Time: 0.584s, 1752.02/s  (2.571s,  398.26/s)  LR: 1.087e-04  Data: 0.020 (1.977)
Train: 70 [ 750/1171 ( 64%)]  Loss:  2.718236 (3.0538)  Time: 5.040s,  203.19/s  (2.558s,  400.26/s)  LR: 1.087e-04  Data: 4.378 (1.965)
Train: 70 [ 800/1171 ( 68%)]  Loss:  3.151590 (3.0595)  Time: 0.585s, 1749.09/s  (2.543s,  402.69/s)  LR: 1.087e-04  Data: 0.018 (1.949)
Train: 70 [ 850/1171 ( 73%)]  Loss:  2.984751 (3.0554)  Time: 0.862s, 1187.27/s  (2.515s,  407.12/s)  LR: 1.087e-04  Data: 0.188 (1.921)
Train: 70 [ 900/1171 ( 77%)]  Loss:  3.088748 (3.0571)  Time: 0.586s, 1746.61/s  (2.523s,  405.85/s)  LR: 1.087e-04  Data: 0.021 (1.929)
Train: 70 [ 950/1171 ( 81%)]  Loss:  3.345642 (3.0716)  Time: 0.587s, 1743.94/s  (2.520s,  406.32/s)  LR: 1.087e-04  Data: 0.019 (1.926)
Train: 70 [1000/1171 ( 85%)]  Loss:  2.709338 (3.0543)  Time: 0.585s, 1750.86/s  (2.525s,  405.51/s)  LR: 1.087e-04  Data: 0.022 (1.930)
Train: 70 [1050/1171 ( 90%)]  Loss:  2.388266 (3.0240)  Time: 0.584s, 1752.11/s  (2.515s,  407.08/s)  LR: 1.087e-04  Data: 0.019 (1.921)
Train: 70 [1100/1171 ( 94%)]  Loss:  3.176165 (3.0306)  Time: 0.588s, 1741.82/s  (2.514s,  407.36/s)  LR: 1.087e-04  Data: 0.020 (1.919)
Train: 70 [1150/1171 ( 98%)]  Loss:  3.248935 (3.0397)  Time: 0.588s, 1741.46/s  (2.505s,  408.71/s)  LR: 1.087e-04  Data: 0.024 (1.911)
Train: 70 [1170/1171 (100%)]  Loss:  2.852255 (3.0322)  Time: 0.564s, 1815.43/s  (2.499s,  409.76/s)  LR: 1.087e-04  Data: 0.000 (1.905)
Test: [   0/97]  Time: 11.221 (11.221)  Loss:  0.3219 (0.3219)  Acc@1: 96.5820 (96.5820)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.758 (2.954)  Loss:  0.5289 (0.4079)  Acc@1: 90.1367 (94.1234)  Acc@5: 98.2422 (98.8051)
Test: [  97/97]  Time: 0.119 (3.107)  Loss:  0.3558 (0.4231)  Acc@1: 94.3452 (93.5790)  Acc@5: 98.8095 (98.5570)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-70.pth.tar', 93.5789999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-69.pth.tar', 93.5420000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-68.pth.tar', 93.37899999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-67.pth.tar', 93.30000000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-66.pth.tar', 93.24700005126954)

Train: 71 [   0/1171 (  0%)]  Loss:  2.862217 (2.8622)  Time: 11.805s,   86.74/s  (11.805s,   86.74/s)  LR: 9.840e-05  Data: 10.304 (10.304)
Train: 71 [  50/1171 (  4%)]  Loss:  2.631255 (2.7467)  Time: 0.585s, 1751.53/s  (2.424s,  422.38/s)  LR: 9.840e-05  Data: 0.018 (1.819)
Train: 71 [ 100/1171 (  9%)]  Loss:  3.174942 (2.8895)  Time: 1.271s,  805.67/s  (2.385s,  429.30/s)  LR: 9.840e-05  Data: 0.588 (1.782)
Train: 71 [ 150/1171 ( 13%)]  Loss:  3.282444 (2.9877)  Time: 0.586s, 1746.82/s  (2.316s,  442.12/s)  LR: 9.840e-05  Data: 0.021 (1.712)
Train: 71 [ 200/1171 ( 17%)]  Loss:  2.564885 (2.9031)  Time: 2.011s,  509.21/s  (2.321s,  441.10/s)  LR: 9.840e-05  Data: 1.449 (1.718)
Train: 71 [ 250/1171 ( 21%)]  Loss:  2.930588 (2.9077)  Time: 0.584s, 1753.21/s  (2.292s,  446.85/s)  LR: 9.840e-05  Data: 0.021 (1.691)
Train: 71 [ 300/1171 ( 26%)]  Loss:  3.404963 (2.9788)  Time: 7.132s,  143.57/s  (2.296s,  445.95/s)  LR: 9.840e-05  Data: 6.569 (1.697)
Train: 71 [ 350/1171 ( 30%)]  Loss:  3.591403 (3.0553)  Time: 0.587s, 1745.94/s  (2.313s,  442.64/s)  LR: 9.840e-05  Data: 0.019 (1.716)
Train: 71 [ 400/1171 ( 34%)]  Loss:  2.955311 (3.0442)  Time: 1.522s,  672.84/s  (2.339s,  437.71/s)  LR: 9.840e-05  Data: 0.885 (1.740)
Train: 71 [ 450/1171 ( 38%)]  Loss:  3.376490 (3.0774)  Time: 0.583s, 1755.47/s  (2.366s,  432.79/s)  LR: 9.840e-05  Data: 0.021 (1.764)
Train: 71 [ 500/1171 ( 43%)]  Loss:  2.828122 (3.0548)  Time: 7.222s,  141.79/s  (2.386s,  429.21/s)  LR: 9.840e-05  Data: 6.559 (1.785)
Train: 71 [ 550/1171 ( 47%)]  Loss:  2.902394 (3.0421)  Time: 0.589s, 1739.36/s  (2.382s,  429.91/s)  LR: 9.840e-05  Data: 0.025 (1.782)
Train: 71 [ 600/1171 ( 51%)]  Loss:  3.269437 (3.0596)  Time: 5.592s,  183.13/s  (2.389s,  428.62/s)  LR: 9.840e-05  Data: 4.945 (1.787)
Train: 71 [ 650/1171 ( 56%)]  Loss:  3.033567 (3.0577)  Time: 0.591s, 1734.07/s  (2.378s,  430.63/s)  LR: 9.840e-05  Data: 0.019 (1.776)
Train: 71 [ 700/1171 ( 60%)]  Loss:  2.762343 (3.0380)  Time: 11.094s,   92.30/s  (2.378s,  430.64/s)  LR: 9.840e-05  Data: 10.391 (1.776)
Train: 71 [ 750/1171 ( 64%)]  Loss:  2.854985 (3.0266)  Time: 0.590s, 1736.15/s  (2.395s,  427.53/s)  LR: 9.840e-05  Data: 0.025 (1.794)
Train: 71 [ 800/1171 ( 68%)]  Loss:  3.243217 (3.0393)  Time: 4.799s,  213.36/s  (2.401s,  426.52/s)  LR: 9.840e-05  Data: 4.131 (1.799)
Train: 71 [ 850/1171 ( 73%)]  Loss:  3.295094 (3.0535)  Time: 0.586s, 1748.79/s  (2.398s,  426.94/s)  LR: 9.840e-05  Data: 0.021 (1.797)
Train: 71 [ 900/1171 ( 77%)]  Loss:  2.704623 (3.0352)  Time: 5.531s,  185.14/s  (2.409s,  425.09/s)  LR: 9.840e-05  Data: 4.796 (1.806)
Train: 71 [ 950/1171 ( 81%)]  Loss:  3.343429 (3.0506)  Time: 0.589s, 1738.49/s  (2.407s,  425.42/s)  LR: 9.840e-05  Data: 0.022 (1.804)
Train: 71 [1000/1171 ( 85%)]  Loss:  3.311884 (3.0630)  Time: 4.948s,  206.96/s  (2.406s,  425.66/s)  LR: 9.840e-05  Data: 4.374 (1.802)
Train: 71 [1050/1171 ( 90%)]  Loss:  3.450297 (3.0806)  Time: 0.591s, 1732.14/s  (2.399s,  426.88/s)  LR: 9.840e-05  Data: 0.024 (1.796)
Train: 71 [1100/1171 ( 94%)]  Loss:  3.288117 (3.0897)  Time: 4.021s,  254.68/s  (2.419s,  423.32/s)  LR: 9.840e-05  Data: 3.227 (1.815)
Train: 71 [1150/1171 ( 98%)]  Loss:  3.259371 (3.0967)  Time: 0.591s, 1731.70/s  (2.427s,  421.98/s)  LR: 9.840e-05  Data: 0.024 (1.822)
Train: 71 [1170/1171 (100%)]  Loss:  3.508528 (3.1132)  Time: 0.565s, 1813.85/s  (2.433s,  420.92/s)  LR: 9.840e-05  Data: 0.000 (1.828)
Test: [   0/97]  Time: 15.165 (15.165)  Loss:  0.3422 (0.3422)  Acc@1: 95.9961 (95.9961)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.201 (3.468)  Loss:  0.5301 (0.4053)  Acc@1: 89.6484 (94.1923)  Acc@5: 98.3398 (98.7975)
Test: [  97/97]  Time: 0.120 (3.328)  Loss:  0.3591 (0.4199)  Acc@1: 94.1964 (93.6600)  Acc@5: 98.9583 (98.5660)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-71.pth.tar', 93.6600000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-70.pth.tar', 93.5789999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-69.pth.tar', 93.5420000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-68.pth.tar', 93.37899999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-67.pth.tar', 93.30000000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-66.pth.tar', 93.24700005126954)

Train: 72 [   0/1171 (  0%)]  Loss:  3.531784 (3.5318)  Time: 11.906s,   86.00/s  (11.906s,   86.00/s)  LR: 8.858e-05  Data: 10.966 (10.966)
Train: 72 [  50/1171 (  4%)]  Loss:  2.910659 (3.2212)  Time: 0.583s, 1755.99/s  (2.390s,  428.43/s)  LR: 8.858e-05  Data: 0.019 (1.809)
Train: 72 [ 100/1171 (  9%)]  Loss:  3.267427 (3.2366)  Time: 0.586s, 1746.15/s  (2.317s,  442.00/s)  LR: 8.858e-05  Data: 0.023 (1.726)
Train: 72 [ 150/1171 ( 13%)]  Loss:  3.370768 (3.2702)  Time: 0.588s, 1740.69/s  (2.371s,  431.92/s)  LR: 8.858e-05  Data: 0.021 (1.777)
Train: 72 [ 200/1171 ( 17%)]  Loss:  3.128751 (3.2419)  Time: 2.559s,  400.13/s  (2.437s,  420.13/s)  LR: 8.858e-05  Data: 1.997 (1.836)
Train: 72 [ 250/1171 ( 21%)]  Loss:  3.186468 (3.2326)  Time: 0.586s, 1747.19/s  (2.448s,  418.29/s)  LR: 8.858e-05  Data: 0.019 (1.845)
Train: 72 [ 300/1171 ( 26%)]  Loss:  2.473034 (3.1241)  Time: 6.131s,  167.01/s  (2.459s,  416.48/s)  LR: 8.858e-05  Data: 5.547 (1.857)
Train: 72 [ 350/1171 ( 30%)]  Loss:  2.575702 (3.0556)  Time: 0.589s, 1738.20/s  (2.440s,  419.72/s)  LR: 8.858e-05  Data: 0.020 (1.837)
Train: 72 [ 400/1171 ( 34%)]  Loss:  2.963432 (3.0453)  Time: 5.086s,  201.33/s  (2.425s,  422.19/s)  LR: 8.858e-05  Data: 4.496 (1.822)
Train: 72 [ 450/1171 ( 38%)]  Loss:  3.189785 (3.0598)  Time: 0.586s, 1746.71/s  (2.406s,  425.53/s)  LR: 8.858e-05  Data: 0.021 (1.800)
Train: 72 [ 500/1171 ( 43%)]  Loss:  3.115266 (3.0648)  Time: 8.030s,  127.52/s  (2.440s,  419.63/s)  LR: 8.858e-05  Data: 7.342 (1.833)
Train: 72 [ 550/1171 ( 47%)]  Loss:  3.138397 (3.0710)  Time: 4.936s,  207.44/s  (2.471s,  414.34/s)  LR: 8.858e-05  Data: 4.374 (1.866)
Train: 72 [ 600/1171 ( 51%)]  Loss:  3.346678 (3.0922)  Time: 5.776s,  177.28/s  (2.502s,  409.27/s)  LR: 8.858e-05  Data: 5.190 (1.896)
Train: 72 [ 650/1171 ( 56%)]  Loss:  2.963533 (3.0830)  Time: 1.678s,  610.08/s  (2.504s,  408.93/s)  LR: 8.858e-05  Data: 1.018 (1.897)
Train: 72 [ 700/1171 ( 60%)]  Loss:  2.867720 (3.0686)  Time: 1.355s,  755.78/s  (2.509s,  408.16/s)  LR: 8.858e-05  Data: 0.742 (1.902)
Train: 72 [ 750/1171 ( 64%)]  Loss:  3.106675 (3.0710)  Time: 2.508s,  408.33/s  (2.508s,  408.31/s)  LR: 8.858e-05  Data: 1.869 (1.902)
Train: 72 [ 800/1171 ( 68%)]  Loss:  2.975339 (3.0654)  Time: 2.852s,  359.09/s  (2.510s,  407.91/s)  LR: 8.858e-05  Data: 2.201 (1.903)
Train: 72 [ 850/1171 ( 73%)]  Loss:  2.969734 (3.0601)  Time: 0.588s, 1740.32/s  (2.522s,  405.95/s)  LR: 8.858e-05  Data: 0.025 (1.914)
Train: 72 [ 900/1171 ( 77%)]  Loss:  3.175961 (3.0662)  Time: 7.135s,  143.52/s  (2.552s,  401.22/s)  LR: 8.858e-05  Data: 6.342 (1.943)
Train: 72 [ 950/1171 ( 81%)]  Loss:  3.252267 (3.0755)  Time: 0.586s, 1746.74/s  (2.557s,  400.47/s)  LR: 8.858e-05  Data: 0.022 (1.948)
Train: 72 [1000/1171 ( 85%)]  Loss:  2.577724 (3.0518)  Time: 8.975s,  114.10/s  (2.563s,  399.47/s)  LR: 8.858e-05  Data: 8.261 (1.954)
Train: 72 [1050/1171 ( 90%)]  Loss:  3.115064 (3.0546)  Time: 0.586s, 1747.83/s  (2.558s,  400.28/s)  LR: 8.858e-05  Data: 0.022 (1.950)
Train: 72 [1100/1171 ( 94%)]  Loss:  2.991693 (3.0519)  Time: 7.595s,  134.82/s  (2.555s,  400.85/s)  LR: 8.858e-05  Data: 7.009 (1.948)
Train: 72 [1150/1171 ( 98%)]  Loss:  2.902605 (3.0457)  Time: 0.584s, 1753.58/s  (2.544s,  402.54/s)  LR: 8.858e-05  Data: 0.021 (1.938)
Train: 72 [1170/1171 (100%)]  Loss:  3.415374 (3.0605)  Time: 0.563s, 1817.66/s  (2.540s,  403.23/s)  LR: 8.858e-05  Data: 0.000 (1.934)
Test: [   0/97]  Time: 17.861 (17.861)  Loss:  0.3322 (0.3322)  Acc@1: 96.1914 (96.1914)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (3.584)  Loss:  0.5388 (0.4169)  Acc@1: 90.5273 (94.1674)  Acc@5: 97.8516 (98.7994)
Test: [  97/97]  Time: 0.120 (3.509)  Loss:  0.3931 (0.4297)  Acc@1: 93.3036 (93.6420)  Acc@5: 99.1071 (98.5610)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-71.pth.tar', 93.6600000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-72.pth.tar', 93.64200001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-70.pth.tar', 93.5789999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-69.pth.tar', 93.5420000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-68.pth.tar', 93.37899999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-67.pth.tar', 93.30000000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-66.pth.tar', 93.24700005126954)

Train: 73 [   0/1171 (  0%)]  Loss:  3.491779 (3.4918)  Time: 12.431s,   82.38/s  (12.431s,   82.38/s)  LR: 7.929e-05  Data: 11.659 (11.659)
Train: 73 [  50/1171 (  4%)]  Loss:  3.003679 (3.2477)  Time: 0.586s, 1746.02/s  (2.779s,  368.51/s)  LR: 7.929e-05  Data: 0.024 (2.193)
Train: 73 [ 100/1171 (  9%)]  Loss:  2.793049 (3.0962)  Time: 0.589s, 1738.53/s  (2.672s,  383.23/s)  LR: 7.929e-05  Data: 0.022 (2.081)
Train: 73 [ 150/1171 ( 13%)]  Loss:  2.632030 (2.9801)  Time: 0.588s, 1741.86/s  (2.582s,  396.67/s)  LR: 7.929e-05  Data: 0.019 (1.994)
Train: 73 [ 200/1171 ( 17%)]  Loss:  3.441006 (3.0723)  Time: 0.584s, 1753.08/s  (2.559s,  400.23/s)  LR: 7.929e-05  Data: 0.020 (1.974)
Train: 73 [ 250/1171 ( 21%)]  Loss:  3.018150 (3.0633)  Time: 0.587s, 1745.84/s  (2.595s,  394.60/s)  LR: 7.929e-05  Data: 0.020 (2.011)
Train: 73 [ 300/1171 ( 26%)]  Loss:  3.409203 (3.1127)  Time: 0.585s, 1750.09/s  (2.649s,  386.56/s)  LR: 7.929e-05  Data: 0.021 (2.062)
Train: 73 [ 350/1171 ( 30%)]  Loss:  3.500022 (3.1611)  Time: 0.583s, 1756.11/s  (2.647s,  386.91/s)  LR: 7.929e-05  Data: 0.021 (2.061)
Train: 73 [ 400/1171 ( 34%)]  Loss:  2.690433 (3.1088)  Time: 0.586s, 1747.18/s  (2.662s,  384.72/s)  LR: 7.929e-05  Data: 0.018 (2.074)
Train: 73 [ 450/1171 ( 38%)]  Loss:  3.206228 (3.1186)  Time: 1.193s,  858.42/s  (2.647s,  386.91/s)  LR: 7.929e-05  Data: 0.616 (2.059)
Train: 73 [ 500/1171 ( 43%)]  Loss:  3.274675 (3.1328)  Time: 0.585s, 1749.32/s  (2.658s,  385.20/s)  LR: 7.929e-05  Data: 0.022 (2.072)
Train: 73 [ 550/1171 ( 47%)]  Loss:  3.020339 (3.1234)  Time: 0.583s, 1757.23/s  (2.644s,  387.29/s)  LR: 7.929e-05  Data: 0.019 (2.056)
Train: 73 [ 600/1171 ( 51%)]  Loss:  3.177214 (3.1275)  Time: 0.586s, 1746.90/s  (2.693s,  380.19/s)  LR: 7.929e-05  Data: 0.019 (2.105)
Train: 73 [ 650/1171 ( 56%)]  Loss:  3.317622 (3.1411)  Time: 0.588s, 1741.17/s  (2.698s,  379.47/s)  LR: 7.929e-05  Data: 0.019 (2.109)
Train: 73 [ 700/1171 ( 60%)]  Loss:  2.871688 (3.1231)  Time: 3.523s,  290.66/s  (2.707s,  378.27/s)  LR: 7.929e-05  Data: 2.961 (2.116)
Train: 73 [ 750/1171 ( 64%)]  Loss:  3.044496 (3.1182)  Time: 0.588s, 1740.51/s  (2.696s,  379.79/s)  LR: 7.929e-05  Data: 0.018 (2.106)
Train: 73 [ 800/1171 ( 68%)]  Loss:  3.081953 (3.1161)  Time: 1.325s,  772.87/s  (2.689s,  380.84/s)  LR: 7.929e-05  Data: 0.742 (2.098)
Train: 73 [ 850/1171 ( 73%)]  Loss:  3.158270 (3.1184)  Time: 0.584s, 1753.01/s  (2.668s,  383.85/s)  LR: 7.929e-05  Data: 0.020 (2.077)
Train: 73 [ 900/1171 ( 77%)]  Loss:  2.842086 (3.1039)  Time: 0.589s, 1737.79/s  (2.672s,  383.24/s)  LR: 7.929e-05  Data: 0.019 (2.081)
Train: 73 [ 950/1171 ( 81%)]  Loss:  3.225785 (3.1100)  Time: 0.585s, 1749.40/s  (2.678s,  382.33/s)  LR: 7.929e-05  Data: 0.021 (2.087)
Train: 73 [1000/1171 ( 85%)]  Loss:  3.196171 (3.1141)  Time: 0.590s, 1737.06/s  (2.689s,  380.84/s)  LR: 7.929e-05  Data: 0.020 (2.096)
Train: 73 [1050/1171 ( 90%)]  Loss:  2.997461 (3.1088)  Time: 0.586s, 1747.64/s  (2.683s,  381.66/s)  LR: 7.929e-05  Data: 0.022 (2.091)
Train: 73 [1100/1171 ( 94%)]  Loss:  3.213852 (3.1134)  Time: 1.043s,  981.49/s  (2.683s,  381.61/s)  LR: 7.929e-05  Data: 0.364 (2.092)
Train: 73 [1150/1171 ( 98%)]  Loss:  2.740993 (3.0978)  Time: 0.583s, 1757.48/s  (2.675s,  382.74/s)  LR: 7.929e-05  Data: 0.020 (2.083)
Train: 73 [1170/1171 (100%)]  Loss:  3.212124 (3.1024)  Time: 0.564s, 1816.55/s  (2.674s,  383.01/s)  LR: 7.929e-05  Data: 0.000 (2.081)
Test: [   0/97]  Time: 14.656 (14.656)  Loss:  0.3323 (0.3323)  Acc@1: 96.1914 (96.1914)  Acc@5: 99.6094 (99.6094)
Test: [  50/97]  Time: 0.204 (3.358)  Loss:  0.4902 (0.4081)  Acc@1: 91.1133 (94.2900)  Acc@5: 98.4375 (98.8071)
Test: [  97/97]  Time: 0.120 (3.332)  Loss:  0.3605 (0.4198)  Acc@1: 95.3869 (93.7620)  Acc@5: 98.6607 (98.5800)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-73.pth.tar', 93.76200003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-71.pth.tar', 93.6600000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-72.pth.tar', 93.64200001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-70.pth.tar', 93.5789999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-69.pth.tar', 93.5420000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-68.pth.tar', 93.37899999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-67.pth.tar', 93.30000000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-66.pth.tar', 93.24700005126954)

Train: 74 [   0/1171 (  0%)]  Loss:  2.758531 (2.7585)  Time: 12.957s,   79.03/s  (12.957s,   79.03/s)  LR: 7.055e-05  Data: 12.095 (12.095)
Train: 74 [  50/1171 (  4%)]  Loss:  2.840899 (2.7997)  Time: 0.586s, 1746.20/s  (2.839s,  360.74/s)  LR: 7.055e-05  Data: 0.020 (2.243)
Train: 74 [ 100/1171 (  9%)]  Loss:  2.826459 (2.8086)  Time: 1.470s,  696.52/s  (2.726s,  375.63/s)  LR: 7.055e-05  Data: 0.879 (2.122)
Train: 74 [ 150/1171 ( 13%)]  Loss:  3.000320 (2.8566)  Time: 0.583s, 1756.25/s  (2.643s,  387.51/s)  LR: 7.055e-05  Data: 0.021 (2.040)
Train: 74 [ 200/1171 ( 17%)]  Loss:  3.034795 (2.8922)  Time: 0.587s, 1744.21/s  (2.608s,  392.67/s)  LR: 7.055e-05  Data: 0.021 (2.003)
Train: 74 [ 250/1171 ( 21%)]  Loss:  3.245780 (2.9511)  Time: 0.585s, 1749.67/s  (2.581s,  396.79/s)  LR: 7.055e-05  Data: 0.022 (1.971)
Train: 74 [ 300/1171 ( 26%)]  Loss:  2.812628 (2.9313)  Time: 0.588s, 1741.29/s  (2.624s,  390.18/s)  LR: 7.055e-05  Data: 0.022 (2.012)
Train: 74 [ 350/1171 ( 30%)]  Loss:  2.902476 (2.9277)  Time: 0.585s, 1749.80/s  (2.669s,  383.69/s)  LR: 7.055e-05  Data: 0.020 (2.057)
Train: 74 [ 400/1171 ( 34%)]  Loss:  2.500091 (2.8802)  Time: 5.829s,  175.68/s  (2.705s,  378.51/s)  LR: 7.055e-05  Data: 5.266 (2.094)
Train: 74 [ 450/1171 ( 38%)]  Loss:  2.927221 (2.8849)  Time: 0.586s, 1746.43/s  (2.702s,  378.95/s)  LR: 7.055e-05  Data: 0.019 (2.090)
Train: 74 [ 500/1171 ( 43%)]  Loss:  3.258905 (2.9189)  Time: 3.595s,  284.81/s  (2.723s,  376.08/s)  LR: 7.055e-05  Data: 2.939 (2.111)
Train: 74 [ 550/1171 ( 47%)]  Loss:  3.234543 (2.9452)  Time: 0.587s, 1744.91/s  (2.722s,  376.16/s)  LR: 7.055e-05  Data: 0.019 (2.112)
Train: 74 [ 600/1171 ( 51%)]  Loss:  2.994733 (2.9490)  Time: 0.592s, 1730.64/s  (2.744s,  373.18/s)  LR: 7.055e-05  Data: 0.026 (2.136)
Train: 74 [ 650/1171 ( 56%)]  Loss:  3.304760 (2.9744)  Time: 0.589s, 1738.26/s  (2.765s,  370.35/s)  LR: 7.055e-05  Data: 0.020 (2.155)
Train: 74 [ 700/1171 ( 60%)]  Loss:  3.043662 (2.9791)  Time: 0.588s, 1742.23/s  (2.767s,  370.14/s)  LR: 7.055e-05  Data: 0.023 (2.158)
Train: 74 [ 750/1171 ( 64%)]  Loss:  2.971379 (2.9786)  Time: 0.589s, 1738.32/s  (2.773s,  369.29/s)  LR: 7.055e-05  Data: 0.018 (2.166)
Train: 74 [ 800/1171 ( 68%)]  Loss:  3.027268 (2.9814)  Time: 0.587s, 1743.94/s  (2.764s,  370.47/s)  LR: 7.055e-05  Data: 0.021 (2.156)
Train: 74 [ 850/1171 ( 73%)]  Loss:  3.358448 (3.0024)  Time: 0.586s, 1746.76/s  (2.758s,  371.28/s)  LR: 7.055e-05  Data: 0.019 (2.150)
Train: 74 [ 900/1171 ( 77%)]  Loss:  2.926541 (2.9984)  Time: 0.588s, 1741.78/s  (2.745s,  373.04/s)  LR: 7.055e-05  Data: 0.020 (2.138)
Train: 74 [ 950/1171 ( 81%)]  Loss:  3.190681 (3.0080)  Time: 0.588s, 1741.41/s  (2.759s,  371.14/s)  LR: 7.055e-05  Data: 0.018 (2.153)
Train: 74 [1000/1171 ( 85%)]  Loss:  3.497620 (3.0313)  Time: 0.589s, 1738.40/s  (2.758s,  371.22/s)  LR: 7.055e-05  Data: 0.019 (2.153)
Train: 74 [1050/1171 ( 90%)]  Loss:  2.632939 (3.0132)  Time: 0.587s, 1743.09/s  (2.763s,  370.57/s)  LR: 7.055e-05  Data: 0.020 (2.158)
Train: 74 [1100/1171 ( 94%)]  Loss:  3.074207 (3.0159)  Time: 0.586s, 1746.99/s  (2.756s,  371.51/s)  LR: 7.055e-05  Data: 0.020 (2.151)
Train: 74 [1150/1171 ( 98%)]  Loss:  3.193472 (3.0233)  Time: 0.588s, 1742.72/s  (2.755s,  371.73/s)  LR: 7.055e-05  Data: 0.019 (2.151)
Train: 74 [1170/1171 (100%)]  Loss:  2.769330 (3.0131)  Time: 0.564s, 1814.55/s  (2.751s,  372.24/s)  LR: 7.055e-05  Data: 0.000 (2.147)
Test: [   0/97]  Time: 14.236 (14.236)  Loss:  0.3086 (0.3086)  Acc@1: 96.4844 (96.4844)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.200 (3.372)  Loss:  0.4818 (0.3771)  Acc@1: 91.0156 (94.3838)  Acc@5: 98.2422 (98.8071)
Test: [  97/97]  Time: 0.119 (3.464)  Loss:  0.3437 (0.3917)  Acc@1: 93.4524 (93.8330)  Acc@5: 98.9583 (98.6010)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-74.pth.tar', 93.83300002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-73.pth.tar', 93.76200003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-71.pth.tar', 93.6600000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-72.pth.tar', 93.64200001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-70.pth.tar', 93.5789999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-69.pth.tar', 93.5420000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-68.pth.tar', 93.37899999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-67.pth.tar', 93.30000000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-66.pth.tar', 93.24700005126954)

Train: 75 [   0/1171 (  0%)]  Loss:  2.948721 (2.9487)  Time: 13.955s,   73.38/s  (13.955s,   73.38/s)  LR: 6.236e-05  Data: 13.378 (13.378)
Train: 75 [  50/1171 (  4%)]  Loss:  2.813926 (2.8813)  Time: 0.585s, 1750.89/s  (2.905s,  352.50/s)  LR: 6.236e-05  Data: 0.019 (2.295)
Train: 75 [ 100/1171 (  9%)]  Loss:  3.094147 (2.9523)  Time: 0.588s, 1741.12/s  (2.855s,  358.72/s)  LR: 6.236e-05  Data: 0.022 (2.248)
Train: 75 [ 150/1171 ( 13%)]  Loss:  2.976829 (2.9584)  Time: 0.583s, 1757.04/s  (2.744s,  373.24/s)  LR: 6.236e-05  Data: 0.019 (2.142)
Train: 75 [ 200/1171 ( 17%)]  Loss:  2.842477 (2.9352)  Time: 0.584s, 1752.39/s  (2.715s,  377.13/s)  LR: 6.236e-05  Data: 0.021 (2.114)
Train: 75 [ 250/1171 ( 21%)]  Loss:  2.871797 (2.9246)  Time: 0.588s, 1741.72/s  (2.652s,  386.13/s)  LR: 6.236e-05  Data: 0.022 (2.052)
Train: 75 [ 300/1171 ( 26%)]  Loss:  2.796889 (2.9064)  Time: 7.474s,  137.01/s  (2.703s,  378.78/s)  LR: 6.236e-05  Data: 6.724 (2.102)
Train: 75 [ 350/1171 ( 30%)]  Loss:  2.898964 (2.9055)  Time: 0.590s, 1736.97/s  (2.735s,  374.42/s)  LR: 6.236e-05  Data: 0.022 (2.136)
Train: 75 [ 400/1171 ( 34%)]  Loss:  2.962725 (2.9118)  Time: 9.441s,  108.46/s  (2.755s,  371.67/s)  LR: 6.236e-05  Data: 8.763 (2.154)
Train: 75 [ 450/1171 ( 38%)]  Loss:  3.086468 (2.9293)  Time: 0.584s, 1753.46/s  (2.747s,  372.75/s)  LR: 6.236e-05  Data: 0.019 (2.149)
Train: 75 [ 500/1171 ( 43%)]  Loss:  2.670332 (2.9058)  Time: 7.943s,  128.91/s  (2.759s,  371.18/s)  LR: 6.236e-05  Data: 7.301 (2.159)
Train: 75 [ 550/1171 ( 47%)]  Loss:  3.066686 (2.9192)  Time: 0.583s, 1755.12/s  (2.738s,  373.99/s)  LR: 6.236e-05  Data: 0.020 (2.141)
Train: 75 [ 600/1171 ( 51%)]  Loss:  3.499413 (2.9638)  Time: 8.233s,  124.38/s  (2.727s,  375.48/s)  LR: 6.236e-05  Data: 7.671 (2.130)
Train: 75 [ 650/1171 ( 56%)]  Loss:  3.148558 (2.9770)  Time: 0.586s, 1747.31/s  (2.743s,  373.28/s)  LR: 6.236e-05  Data: 0.022 (2.146)
Train: 75 [ 700/1171 ( 60%)]  Loss:  3.145165 (2.9882)  Time: 3.782s,  270.73/s  (2.750s,  372.43/s)  LR: 6.236e-05  Data: 3.170 (2.152)
Train: 75 [ 750/1171 ( 64%)]  Loss:  3.534643 (3.0224)  Time: 0.586s, 1747.94/s  (2.738s,  374.01/s)  LR: 6.236e-05  Data: 0.019 (2.137)
Train: 75 [ 800/1171 ( 68%)]  Loss:  2.901855 (3.0153)  Time: 0.584s, 1752.42/s  (2.729s,  375.27/s)  LR: 6.236e-05  Data: 0.021 (2.126)
Train: 75 [ 850/1171 ( 73%)]  Loss:  2.952571 (3.0118)  Time: 2.136s,  479.40/s  (2.711s,  377.73/s)  LR: 6.236e-05  Data: 1.496 (2.109)
Train: 75 [ 900/1171 ( 77%)]  Loss:  3.064728 (3.0146)  Time: 0.585s, 1749.09/s  (2.692s,  380.37/s)  LR: 6.236e-05  Data: 0.023 (2.090)
Train: 75 [ 950/1171 ( 81%)]  Loss:  3.189078 (3.0233)  Time: 0.583s, 1757.85/s  (2.686s,  381.21/s)  LR: 6.236e-05  Data: 0.020 (2.084)
Train: 75 [1000/1171 ( 85%)]  Loss:  3.272143 (3.0351)  Time: 0.584s, 1752.07/s  (2.690s,  380.66/s)  LR: 6.236e-05  Data: 0.021 (2.088)
Train: 75 [1050/1171 ( 90%)]  Loss:  2.916337 (3.0297)  Time: 0.584s, 1754.76/s  (2.697s,  379.67/s)  LR: 6.236e-05  Data: 0.020 (2.096)
Train: 75 [1100/1171 ( 94%)]  Loss:  3.451600 (3.0481)  Time: 2.757s,  371.41/s  (2.688s,  380.95/s)  LR: 6.236e-05  Data: 2.187 (2.087)
Train: 75 [1150/1171 ( 98%)]  Loss:  3.175073 (3.0534)  Time: 0.587s, 1745.71/s  (2.682s,  381.76/s)  LR: 6.236e-05  Data: 0.021 (2.081)
Train: 75 [1170/1171 (100%)]  Loss:  2.681361 (3.0385)  Time: 0.564s, 1814.78/s  (2.678s,  382.31/s)  LR: 6.236e-05  Data: 0.000 (2.077)
Test: [   0/97]  Time: 14.207 (14.207)  Loss:  0.3172 (0.3172)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.6094 (99.6094)
Test: [  50/97]  Time: 1.549 (3.196)  Loss:  0.5172 (0.4068)  Acc@1: 91.2109 (94.5561)  Acc@5: 98.3398 (98.8530)
Test: [  97/97]  Time: 0.119 (3.070)  Loss:  0.3818 (0.4218)  Acc@1: 93.6012 (93.9130)  Acc@5: 98.6607 (98.5980)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-75.pth.tar', 93.9130000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-74.pth.tar', 93.83300002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-73.pth.tar', 93.76200003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-71.pth.tar', 93.6600000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-72.pth.tar', 93.64200001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-70.pth.tar', 93.5789999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-69.pth.tar', 93.5420000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-68.pth.tar', 93.37899999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-67.pth.tar', 93.30000000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-66.pth.tar', 93.24700005126954)

Train: 76 [   0/1171 (  0%)]  Loss:  2.726869 (2.7269)  Time: 18.890s,   54.21/s  (18.890s,   54.21/s)  LR: 5.473e-05  Data: 17.719 (17.719)
Train: 76 [  50/1171 (  4%)]  Loss:  2.439278 (2.5831)  Time: 0.582s, 1758.70/s  (3.034s,  337.47/s)  LR: 5.473e-05  Data: 0.020 (2.438)
Train: 76 [ 100/1171 (  9%)]  Loss:  2.493047 (2.5531)  Time: 0.584s, 1753.12/s  (2.910s,  351.89/s)  LR: 5.473e-05  Data: 0.022 (2.313)
Train: 76 [ 150/1171 ( 13%)]  Loss:  2.835572 (2.6237)  Time: 0.586s, 1746.81/s  (2.776s,  368.86/s)  LR: 5.473e-05  Data: 0.021 (2.184)
Train: 76 [ 200/1171 ( 17%)]  Loss:  3.065491 (2.7121)  Time: 0.587s, 1744.45/s  (2.745s,  373.10/s)  LR: 5.473e-05  Data: 0.022 (2.151)
Train: 76 [ 250/1171 ( 21%)]  Loss:  3.197268 (2.7929)  Time: 0.585s, 1751.55/s  (2.664s,  384.35/s)  LR: 5.473e-05  Data: 0.021 (2.073)
Train: 76 [ 300/1171 ( 26%)]  Loss:  2.795813 (2.7933)  Time: 0.586s, 1746.24/s  (2.621s,  390.71/s)  LR: 5.473e-05  Data: 0.020 (2.030)
Train: 76 [ 350/1171 ( 30%)]  Loss:  3.155219 (2.8386)  Time: 1.665s,  614.92/s  (2.557s,  400.50/s)  LR: 5.473e-05  Data: 1.005 (1.966)
Train: 76 [ 400/1171 ( 34%)]  Loss:  3.138237 (2.8719)  Time: 0.586s, 1748.35/s  (2.609s,  392.51/s)  LR: 5.473e-05  Data: 0.020 (2.015)
Train: 76 [ 450/1171 ( 38%)]  Loss:  2.834447 (2.8681)  Time: 2.142s,  478.10/s  (2.618s,  391.21/s)  LR: 5.473e-05  Data: 1.554 (2.023)
Train: 76 [ 500/1171 ( 43%)]  Loss:  2.867981 (2.8681)  Time: 0.588s, 1742.96/s  (2.621s,  390.63/s)  LR: 5.473e-05  Data: 0.022 (2.025)
Train: 76 [ 550/1171 ( 47%)]  Loss:  2.651174 (2.8500)  Time: 3.522s,  290.73/s  (2.598s,  394.12/s)  LR: 5.473e-05  Data: 2.847 (2.002)
Train: 76 [ 600/1171 ( 51%)]  Loss:  2.972313 (2.8594)  Time: 0.587s, 1743.36/s  (2.574s,  397.82/s)  LR: 5.473e-05  Data: 0.024 (1.977)
Train: 76 [ 650/1171 ( 56%)]  Loss:  2.931625 (2.8646)  Time: 2.767s,  370.05/s  (2.548s,  401.82/s)  LR: 5.473e-05  Data: 2.163 (1.951)
Train: 76 [ 700/1171 ( 60%)]  Loss:  3.028540 (2.8755)  Time: 0.588s, 1740.25/s  (2.522s,  405.99/s)  LR: 5.473e-05  Data: 0.025 (1.923)
Train: 76 [ 750/1171 ( 64%)]  Loss:  2.947452 (2.8800)  Time: 0.597s, 1716.15/s  (2.523s,  405.87/s)  LR: 5.473e-05  Data: 0.029 (1.924)
Train: 76 [ 800/1171 ( 68%)]  Loss:  2.986930 (2.8863)  Time: 0.589s, 1737.89/s  (2.532s,  404.44/s)  LR: 5.473e-05  Data: 0.023 (1.933)
Train: 76 [ 850/1171 ( 73%)]  Loss:  3.021824 (2.8938)  Time: 0.589s, 1739.48/s  (2.525s,  405.53/s)  LR: 5.473e-05  Data: 0.020 (1.926)
Train: 76 [ 900/1171 ( 77%)]  Loss:  3.035817 (2.9013)  Time: 0.585s, 1749.79/s  (2.522s,  406.02/s)  LR: 5.473e-05  Data: 0.020 (1.922)
Train: 76 [ 950/1171 ( 81%)]  Loss:  2.485878 (2.8805)  Time: 1.611s,  635.48/s  (2.508s,  408.32/s)  LR: 5.473e-05  Data: 1.043 (1.908)
Train: 76 [1000/1171 ( 85%)]  Loss:  3.243013 (2.8978)  Time: 0.585s, 1749.11/s  (2.495s,  410.45/s)  LR: 5.473e-05  Data: 0.019 (1.894)
Train: 76 [1050/1171 ( 90%)]  Loss:  2.957505 (2.9005)  Time: 1.132s,  904.21/s  (2.479s,  413.09/s)  LR: 5.473e-05  Data: 0.567 (1.878)
Train: 76 [1100/1171 ( 94%)]  Loss:  3.406830 (2.9225)  Time: 0.584s, 1754.40/s  (2.473s,  414.06/s)  LR: 5.473e-05  Data: 0.019 (1.871)
Train: 76 [1150/1171 ( 98%)]  Loss:  2.507514 (2.9052)  Time: 0.587s, 1743.30/s  (2.475s,  413.75/s)  LR: 5.473e-05  Data: 0.023 (1.873)
Train: 76 [1170/1171 (100%)]  Loss:  2.825773 (2.9021)  Time: 0.565s, 1813.52/s  (2.477s,  413.44/s)  LR: 5.473e-05  Data: 0.000 (1.875)
Test: [   0/97]  Time: 13.759 (13.759)  Loss:  0.3244 (0.3244)  Acc@1: 96.2891 (96.2891)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (3.222)  Loss:  0.5103 (0.4003)  Acc@1: 91.2109 (94.4336)  Acc@5: 98.1445 (98.8549)
Test: [  97/97]  Time: 0.119 (3.155)  Loss:  0.3718 (0.4149)  Acc@1: 94.1964 (93.9310)  Acc@5: 99.1071 (98.6290)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-76.pth.tar', 93.93099998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-75.pth.tar', 93.9130000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-74.pth.tar', 93.83300002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-73.pth.tar', 93.76200003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-71.pth.tar', 93.6600000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-72.pth.tar', 93.64200001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-70.pth.tar', 93.5789999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-69.pth.tar', 93.5420000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-68.pth.tar', 93.37899999023438)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-67.pth.tar', 93.30000000976563)

Train: 77 [   0/1171 (  0%)]  Loss:  2.855920 (2.8559)  Time: 10.946s,   93.55/s  (10.946s,   93.55/s)  LR: 4.768e-05  Data: 10.159 (10.159)
Train: 77 [  50/1171 (  4%)]  Loss:  2.155469 (2.5057)  Time: 0.585s, 1749.12/s  (2.378s,  430.69/s)  LR: 4.768e-05  Data: 0.021 (1.786)
Train: 77 [ 100/1171 (  9%)]  Loss:  3.010672 (2.6740)  Time: 0.588s, 1740.14/s  (2.329s,  439.63/s)  LR: 4.768e-05  Data: 0.018 (1.744)
Train: 77 [ 150/1171 ( 13%)]  Loss:  2.730648 (2.6882)  Time: 0.584s, 1752.95/s  (2.246s,  455.98/s)  LR: 4.768e-05  Data: 0.022 (1.656)
Train: 77 [ 200/1171 ( 17%)]  Loss:  2.960939 (2.7427)  Time: 0.588s, 1742.14/s  (2.319s,  441.57/s)  LR: 4.768e-05  Data: 0.020 (1.731)
Train: 77 [ 250/1171 ( 21%)]  Loss:  2.429321 (2.6905)  Time: 1.809s,  565.92/s  (2.354s,  434.96/s)  LR: 4.768e-05  Data: 1.220 (1.757)
Train: 77 [ 300/1171 ( 26%)]  Loss:  3.122406 (2.7522)  Time: 0.588s, 1740.29/s  (2.392s,  428.03/s)  LR: 4.768e-05  Data: 0.021 (1.793)
Train: 77 [ 350/1171 ( 30%)]  Loss:  3.150100 (2.8019)  Time: 2.470s,  414.61/s  (2.399s,  426.76/s)  LR: 4.768e-05  Data: 1.753 (1.793)
Train: 77 [ 400/1171 ( 34%)]  Loss:  2.297610 (2.7459)  Time: 2.065s,  495.86/s  (2.400s,  426.68/s)  LR: 4.768e-05  Data: 1.416 (1.792)
Train: 77 [ 450/1171 ( 38%)]  Loss:  3.362250 (2.8075)  Time: 0.607s, 1688.37/s  (2.388s,  428.90/s)  LR: 4.768e-05  Data: 0.023 (1.775)
Train: 77 [ 500/1171 ( 43%)]  Loss:  3.613707 (2.8808)  Time: 0.585s, 1750.65/s  (2.392s,  428.16/s)  LR: 4.768e-05  Data: 0.021 (1.780)
Train: 77 [ 550/1171 ( 47%)]  Loss:  2.527035 (2.8513)  Time: 1.246s,  822.16/s  (2.419s,  423.28/s)  LR: 4.768e-05  Data: 0.665 (1.810)
Train: 77 [ 600/1171 ( 51%)]  Loss:  3.320347 (2.8874)  Time: 0.585s, 1750.28/s  (2.440s,  419.62/s)  LR: 4.768e-05  Data: 0.021 (1.832)
Train: 77 [ 650/1171 ( 56%)]  Loss:  2.742451 (2.8771)  Time: 0.589s, 1739.04/s  (2.454s,  417.26/s)  LR: 4.768e-05  Data: 0.022 (1.846)
Train: 77 [ 700/1171 ( 60%)]  Loss:  3.429966 (2.9139)  Time: 0.588s, 1741.86/s  (2.457s,  416.82/s)  LR: 4.768e-05  Data: 0.020 (1.847)
Train: 77 [ 750/1171 ( 64%)]  Loss:  3.023569 (2.9208)  Time: 0.589s, 1737.89/s  (2.455s,  417.18/s)  LR: 4.768e-05  Data: 0.023 (1.846)
Train: 77 [ 800/1171 ( 68%)]  Loss:  2.476366 (2.8946)  Time: 0.588s, 1740.16/s  (2.447s,  418.40/s)  LR: 4.768e-05  Data: 0.021 (1.840)
Train: 77 [ 850/1171 ( 73%)]  Loss:  2.942184 (2.8973)  Time: 0.588s, 1742.90/s  (2.442s,  419.26/s)  LR: 4.768e-05  Data: 0.019 (1.834)
Train: 77 [ 900/1171 ( 77%)]  Loss:  2.947572 (2.8999)  Time: 0.585s, 1750.70/s  (2.428s,  421.67/s)  LR: 4.768e-05  Data: 0.020 (1.820)
Train: 77 [ 950/1171 ( 81%)]  Loss:  3.199588 (2.9149)  Time: 0.869s, 1178.44/s  (2.454s,  417.28/s)  LR: 4.768e-05  Data: 0.200 (1.846)
Train: 77 [1000/1171 ( 85%)]  Loss:  3.089968 (2.9232)  Time: 0.586s, 1746.90/s  (2.455s,  417.10/s)  LR: 4.768e-05  Data: 0.021 (1.847)
Train: 77 [1050/1171 ( 90%)]  Loss:  2.881371 (2.9213)  Time: 0.594s, 1724.69/s  (2.462s,  415.92/s)  LR: 4.768e-05  Data: 0.027 (1.855)
Train: 77 [1100/1171 ( 94%)]  Loss:  3.362800 (2.9405)  Time: 0.585s, 1750.92/s  (2.455s,  417.08/s)  LR: 4.768e-05  Data: 0.022 (1.848)
Train: 77 [1150/1171 ( 98%)]  Loss:  3.363486 (2.9582)  Time: 0.592s, 1728.59/s  (2.457s,  416.85/s)  LR: 4.768e-05  Data: 0.018 (1.850)
Train: 77 [1170/1171 (100%)]  Loss:  2.932462 (2.9571)  Time: 0.564s, 1815.73/s  (2.456s,  416.93/s)  LR: 4.768e-05  Data: 0.000 (1.850)
Test: [   0/97]  Time: 13.059 (13.059)  Loss:  0.3160 (0.3160)  Acc@1: 97.0703 (97.0703)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (3.038)  Loss:  0.4969 (0.4043)  Acc@1: 91.4062 (94.5485)  Acc@5: 98.0469 (98.8358)
Test: [  97/97]  Time: 0.120 (3.145)  Loss:  0.3685 (0.4162)  Acc@1: 94.1964 (94.0010)  Acc@5: 99.1071 (98.6150)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-77.pth.tar', 94.0010000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-76.pth.tar', 93.93099998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-75.pth.tar', 93.9130000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-74.pth.tar', 93.83300002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-73.pth.tar', 93.76200003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-71.pth.tar', 93.6600000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-72.pth.tar', 93.64200001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-70.pth.tar', 93.5789999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-69.pth.tar', 93.5420000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-68.pth.tar', 93.37899999023438)

Train: 78 [   0/1171 (  0%)]  Loss:  2.552914 (2.5529)  Time: 12.730s,   80.44/s  (12.730s,   80.44/s)  LR: 4.121e-05  Data: 11.397 (11.397)
Train: 78 [  50/1171 (  4%)]  Loss:  3.212022 (2.8825)  Time: 0.585s, 1749.35/s  (2.630s,  389.41/s)  LR: 4.121e-05  Data: 0.020 (2.030)
Train: 78 [ 100/1171 (  9%)]  Loss:  3.222167 (2.9957)  Time: 0.585s, 1749.22/s  (2.558s,  400.35/s)  LR: 4.121e-05  Data: 0.018 (1.962)
Train: 78 [ 150/1171 ( 13%)]  Loss:  2.646188 (2.9083)  Time: 0.585s, 1749.76/s  (2.479s,  413.04/s)  LR: 4.121e-05  Data: 0.022 (1.885)
Train: 78 [ 200/1171 ( 17%)]  Loss:  3.138189 (2.9543)  Time: 0.584s, 1753.71/s  (2.470s,  414.64/s)  LR: 4.121e-05  Data: 0.021 (1.880)
Train: 78 [ 250/1171 ( 21%)]  Loss:  3.181996 (2.9922)  Time: 0.587s, 1744.29/s  (2.443s,  419.18/s)  LR: 4.121e-05  Data: 0.021 (1.849)
Train: 78 [ 300/1171 ( 26%)]  Loss:  3.265898 (3.0313)  Time: 0.587s, 1745.92/s  (2.418s,  423.57/s)  LR: 4.121e-05  Data: 0.020 (1.826)
Train: 78 [ 350/1171 ( 30%)]  Loss:  2.779030 (2.9998)  Time: 0.587s, 1745.30/s  (2.432s,  421.01/s)  LR: 4.121e-05  Data: 0.020 (1.843)
Train: 78 [ 400/1171 ( 34%)]  Loss:  2.946174 (2.9938)  Time: 0.584s, 1752.12/s  (2.463s,  415.80/s)  LR: 4.121e-05  Data: 0.019 (1.873)
Train: 78 [ 450/1171 ( 38%)]  Loss:  2.860645 (2.9805)  Time: 0.583s, 1755.48/s  (2.469s,  414.70/s)  LR: 4.121e-05  Data: 0.020 (1.881)
Train: 78 [ 500/1171 ( 43%)]  Loss:  3.345188 (3.0137)  Time: 0.589s, 1738.39/s  (2.484s,  412.18/s)  LR: 4.121e-05  Data: 0.019 (1.895)
Train: 78 [ 550/1171 ( 47%)]  Loss:  2.971738 (3.0102)  Time: 0.585s, 1750.55/s  (2.479s,  413.05/s)  LR: 4.121e-05  Data: 0.020 (1.891)
Train: 78 [ 600/1171 ( 51%)]  Loss:  2.828454 (2.9962)  Time: 0.586s, 1748.79/s  (2.477s,  413.43/s)  LR: 4.121e-05  Data: 0.020 (1.889)
Train: 78 [ 650/1171 ( 56%)]  Loss:  3.025621 (2.9983)  Time: 0.583s, 1757.84/s  (2.453s,  417.46/s)  LR: 4.121e-05  Data: 0.021 (1.865)
Train: 78 [ 700/1171 ( 60%)]  Loss:  3.067552 (3.0029)  Time: 0.584s, 1752.15/s  (2.455s,  417.06/s)  LR: 4.121e-05  Data: 0.020 (1.867)
Train: 78 [ 750/1171 ( 64%)]  Loss:  3.177045 (3.0138)  Time: 0.589s, 1737.19/s  (2.459s,  416.39/s)  LR: 4.121e-05  Data: 0.020 (1.871)
Train: 78 [ 800/1171 ( 68%)]  Loss:  2.712029 (2.9960)  Time: 0.586s, 1746.93/s  (2.464s,  415.62/s)  LR: 4.121e-05  Data: 0.019 (1.876)
Train: 78 [ 850/1171 ( 73%)]  Loss:  3.207057 (3.0078)  Time: 0.584s, 1754.69/s  (2.460s,  416.30/s)  LR: 4.121e-05  Data: 0.019 (1.873)
Train: 78 [ 900/1171 ( 77%)]  Loss:  3.403377 (3.0286)  Time: 0.586s, 1747.64/s  (2.461s,  416.16/s)  LR: 4.121e-05  Data: 0.018 (1.873)
Train: 78 [ 950/1171 ( 81%)]  Loss:  3.336911 (3.0440)  Time: 0.584s, 1752.52/s  (2.447s,  418.39/s)  LR: 4.121e-05  Data: 0.019 (1.860)
Train: 78 [1000/1171 ( 85%)]  Loss:  2.922646 (3.0382)  Time: 0.587s, 1745.32/s  (2.442s,  419.30/s)  LR: 4.121e-05  Data: 0.019 (1.855)
Train: 78 [1050/1171 ( 90%)]  Loss:  2.775219 (3.0263)  Time: 0.586s, 1746.44/s  (2.430s,  421.44/s)  LR: 4.121e-05  Data: 0.020 (1.844)
Train: 78 [1100/1171 ( 94%)]  Loss:  2.663205 (3.0105)  Time: 0.583s, 1755.42/s  (2.443s,  419.16/s)  LR: 4.121e-05  Data: 0.019 (1.855)
Train: 78 [1150/1171 ( 98%)]  Loss:  3.087341 (3.0137)  Time: 0.588s, 1741.96/s  (2.444s,  419.04/s)  LR: 4.121e-05  Data: 0.025 (1.856)
Train: 78 [1170/1171 (100%)]  Loss:  2.415508 (2.9898)  Time: 0.565s, 1812.08/s  (2.447s,  418.46/s)  LR: 4.121e-05  Data: 0.000 (1.859)
Test: [   0/97]  Time: 14.265 (14.265)  Loss:  0.3176 (0.3176)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 1.653 (3.279)  Loss:  0.4902 (0.3994)  Acc@1: 91.6992 (94.6174)  Acc@5: 98.1445 (98.8454)
Test: [  97/97]  Time: 0.119 (3.164)  Loss:  0.3581 (0.4129)  Acc@1: 95.0893 (94.0310)  Acc@5: 99.1071 (98.6300)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-78.pth.tar', 94.03100000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-77.pth.tar', 94.0010000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-76.pth.tar', 93.93099998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-75.pth.tar', 93.9130000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-74.pth.tar', 93.83300002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-73.pth.tar', 93.76200003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-71.pth.tar', 93.6600000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-72.pth.tar', 93.64200001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-70.pth.tar', 93.5789999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-69.pth.tar', 93.5420000390625)

Train: 79 [   0/1171 (  0%)]  Loss:  2.516361 (2.5164)  Time: 10.842s,   94.44/s  (10.842s,   94.44/s)  LR: 3.533e-05  Data: 9.739 (9.739)
Train: 79 [  50/1171 (  4%)]  Loss:  3.139320 (2.8278)  Time: 0.583s, 1757.69/s  (2.410s,  424.87/s)  LR: 3.533e-05  Data: 0.018 (1.815)
Train: 79 [ 100/1171 (  9%)]  Loss:  2.921380 (2.8590)  Time: 0.583s, 1756.34/s  (2.342s,  437.30/s)  LR: 3.533e-05  Data: 0.019 (1.753)
Train: 79 [ 150/1171 ( 13%)]  Loss:  2.414086 (2.7478)  Time: 0.587s, 1744.21/s  (2.339s,  437.79/s)  LR: 3.533e-05  Data: 0.020 (1.744)
Train: 79 [ 200/1171 ( 17%)]  Loss:  2.742468 (2.7467)  Time: 3.003s,  341.04/s  (2.414s,  424.13/s)  LR: 3.533e-05  Data: 2.199 (1.816)
Train: 79 [ 250/1171 ( 21%)]  Loss:  2.814649 (2.7580)  Time: 0.587s, 1743.11/s  (2.429s,  421.55/s)  LR: 3.533e-05  Data: 0.019 (1.833)
Train: 79 [ 300/1171 ( 26%)]  Loss:  3.177541 (2.8180)  Time: 6.296s,  162.64/s  (2.460s,  416.23/s)  LR: 3.533e-05  Data: 5.717 (1.862)
Train: 79 [ 350/1171 ( 30%)]  Loss:  3.208595 (2.8668)  Time: 0.583s, 1757.36/s  (2.446s,  418.71/s)  LR: 3.533e-05  Data: 0.018 (1.848)
Train: 79 [ 400/1171 ( 34%)]  Loss:  3.405500 (2.9267)  Time: 7.745s,  132.22/s  (2.448s,  418.23/s)  LR: 3.533e-05  Data: 7.076 (1.851)
Train: 79 [ 450/1171 ( 38%)]  Loss:  3.029231 (2.9369)  Time: 0.587s, 1745.51/s  (2.443s,  419.20/s)  LR: 3.533e-05  Data: 0.024 (1.846)
Train: 79 [ 500/1171 ( 43%)]  Loss:  2.666702 (2.9123)  Time: 8.374s,  122.28/s  (2.447s,  418.42/s)  LR: 3.533e-05  Data: 7.801 (1.853)
Train: 79 [ 550/1171 ( 47%)]  Loss:  3.026772 (2.9219)  Time: 0.588s, 1742.33/s  (2.506s,  408.56/s)  LR: 3.533e-05  Data: 0.024 (1.913)
Train: 79 [ 600/1171 ( 51%)]  Loss:  2.037830 (2.8539)  Time: 8.876s,  115.36/s  (2.539s,  403.38/s)  LR: 3.533e-05  Data: 8.290 (1.945)
Train: 79 [ 650/1171 ( 56%)]  Loss:  2.620648 (2.8372)  Time: 0.588s, 1741.60/s  (2.534s,  404.15/s)  LR: 3.533e-05  Data: 0.021 (1.940)
Train: 79 [ 700/1171 ( 60%)]  Loss:  2.433251 (2.8103)  Time: 7.637s,  134.08/s  (2.534s,  404.11/s)  LR: 3.533e-05  Data: 7.041 (1.940)
Train: 79 [ 750/1171 ( 64%)]  Loss:  2.512137 (2.7917)  Time: 0.586s, 1746.55/s  (2.519s,  406.45/s)  LR: 3.533e-05  Data: 0.020 (1.926)
Train: 79 [ 800/1171 ( 68%)]  Loss:  3.097182 (2.8096)  Time: 2.497s,  410.09/s  (2.512s,  407.70/s)  LR: 3.533e-05  Data: 1.914 (1.916)
Train: 79 [ 850/1171 ( 73%)]  Loss:  2.816905 (2.8100)  Time: 0.585s, 1751.24/s  (2.493s,  410.71/s)  LR: 3.533e-05  Data: 0.023 (1.898)
Train: 79 [ 900/1171 ( 77%)]  Loss:  3.404504 (2.8413)  Time: 0.587s, 1745.60/s  (2.521s,  406.18/s)  LR: 3.533e-05  Data: 0.021 (1.926)
Train: 79 [ 950/1171 ( 81%)]  Loss:  3.143952 (2.8565)  Time: 1.219s,  839.91/s  (2.523s,  405.80/s)  LR: 3.533e-05  Data: 0.556 (1.928)
Train: 79 [1000/1171 ( 85%)]  Loss:  2.859951 (2.8566)  Time: 0.592s, 1728.73/s  (2.523s,  405.84/s)  LR: 3.533e-05  Data: 0.027 (1.929)
Train: 79 [1050/1171 ( 90%)]  Loss:  3.018927 (2.8640)  Time: 0.587s, 1743.29/s  (2.515s,  407.11/s)  LR: 3.533e-05  Data: 0.024 (1.921)
Train: 79 [1100/1171 ( 94%)]  Loss:  2.863553 (2.8640)  Time: 0.593s, 1727.63/s  (2.512s,  407.70/s)  LR: 3.533e-05  Data: 0.027 (1.919)
Train: 79 [1150/1171 ( 98%)]  Loss:  2.714108 (2.8577)  Time: 0.587s, 1743.15/s  (2.501s,  409.39/s)  LR: 3.533e-05  Data: 0.024 (1.908)
Train: 79 [1170/1171 (100%)]  Loss:  3.277028 (2.8745)  Time: 0.566s, 1809.29/s  (2.498s,  409.96/s)  LR: 3.533e-05  Data: 0.000 (1.905)
Test: [   0/97]  Time: 14.008 (14.008)  Loss:  0.3078 (0.3078)  Acc@1: 96.5820 (96.5820)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (3.288)  Loss:  0.4841 (0.3875)  Acc@1: 91.9922 (94.7170)  Acc@5: 98.1445 (98.8626)
Test: [  97/97]  Time: 0.119 (3.310)  Loss:  0.3471 (0.4016)  Acc@1: 94.9405 (94.1370)  Acc@5: 99.1071 (98.6620)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-79.pth.tar', 94.13700004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-78.pth.tar', 94.03100000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-77.pth.tar', 94.0010000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-76.pth.tar', 93.93099998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-75.pth.tar', 93.9130000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-74.pth.tar', 93.83300002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-73.pth.tar', 93.76200003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-71.pth.tar', 93.6600000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-72.pth.tar', 93.64200001464843)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-70.pth.tar', 93.5789999975586)

Train: 80 [   0/1171 (  0%)]  Loss:  3.267749 (3.2677)  Time: 12.446s,   82.27/s  (12.446s,   82.27/s)  LR: 3.005e-05  Data: 11.877 (11.877)
Train: 80 [  50/1171 (  4%)]  Loss:  2.336220 (2.8020)  Time: 0.592s, 1730.91/s  (2.575s,  397.69/s)  LR: 3.005e-05  Data: 0.028 (1.983)
Train: 80 [ 100/1171 (  9%)]  Loss:  2.713899 (2.7726)  Time: 1.075s,  952.20/s  (2.506s,  408.67/s)  LR: 3.005e-05  Data: 0.322 (1.901)
Train: 80 [ 150/1171 ( 13%)]  Loss:  2.865818 (2.7959)  Time: 0.586s, 1747.58/s  (2.419s,  423.35/s)  LR: 3.005e-05  Data: 0.021 (1.814)
Train: 80 [ 200/1171 ( 17%)]  Loss:  2.693522 (2.7754)  Time: 2.135s,  479.54/s  (2.398s,  427.09/s)  LR: 3.005e-05  Data: 1.494 (1.791)
Train: 80 [ 250/1171 ( 21%)]  Loss:  2.672564 (2.7583)  Time: 3.653s,  280.30/s  (2.378s,  430.61/s)  LR: 3.005e-05  Data: 3.066 (1.771)
Train: 80 [ 300/1171 ( 26%)]  Loss:  2.781726 (2.7616)  Time: 5.000s,  204.81/s  (2.419s,  423.23/s)  LR: 3.005e-05  Data: 4.337 (1.812)
Train: 80 [ 350/1171 ( 30%)]  Loss:  2.769562 (2.7626)  Time: 3.246s,  315.51/s  (2.441s,  419.49/s)  LR: 3.005e-05  Data: 2.561 (1.834)
Train: 80 [ 400/1171 ( 34%)]  Loss:  2.466434 (2.7297)  Time: 4.904s,  208.80/s  (2.457s,  416.74/s)  LR: 3.005e-05  Data: 4.181 (1.850)
Train: 80 [ 450/1171 ( 38%)]  Loss:  2.942007 (2.7510)  Time: 4.693s,  218.21/s  (2.473s,  414.09/s)  LR: 3.005e-05  Data: 4.040 (1.867)
Train: 80 [ 500/1171 ( 43%)]  Loss:  2.857039 (2.7606)  Time: 6.853s,  149.43/s  (2.489s,  411.40/s)  LR: 3.005e-05  Data: 6.275 (1.884)
Train: 80 [ 550/1171 ( 47%)]  Loss:  3.085176 (2.7876)  Time: 3.432s,  298.39/s  (2.494s,  410.61/s)  LR: 3.005e-05  Data: 2.818 (1.889)
Train: 80 [ 600/1171 ( 51%)]  Loss:  2.662816 (2.7780)  Time: 5.227s,  195.90/s  (2.495s,  410.41/s)  LR: 3.005e-05  Data: 4.559 (1.887)
Train: 80 [ 650/1171 ( 56%)]  Loss:  2.807119 (2.7801)  Time: 3.135s,  326.66/s  (2.516s,  407.02/s)  LR: 3.005e-05  Data: 2.103 (1.907)
Train: 80 [ 700/1171 ( 60%)]  Loss:  3.114252 (2.8024)  Time: 5.123s,  199.89/s  (2.520s,  406.27/s)  LR: 3.005e-05  Data: 4.454 (1.912)
Train: 80 [ 750/1171 ( 64%)]  Loss:  2.623053 (2.7912)  Time: 3.024s,  338.61/s  (2.520s,  406.28/s)  LR: 3.005e-05  Data: 2.460 (1.912)
Train: 80 [ 800/1171 ( 68%)]  Loss:  2.573312 (2.7784)  Time: 2.079s,  492.56/s  (2.521s,  406.13/s)  LR: 3.005e-05  Data: 1.505 (1.913)
Train: 80 [ 850/1171 ( 73%)]  Loss:  2.760484 (2.7774)  Time: 5.906s,  173.38/s  (2.519s,  406.45/s)  LR: 3.005e-05  Data: 5.195 (1.911)
Train: 80 [ 900/1171 ( 77%)]  Loss:  2.617444 (2.7690)  Time: 0.584s, 1752.47/s  (2.509s,  408.19/s)  LR: 3.005e-05  Data: 0.020 (1.901)
Train: 80 [ 950/1171 ( 81%)]  Loss:  2.645375 (2.7628)  Time: 3.784s,  270.64/s  (2.502s,  409.20/s)  LR: 3.005e-05  Data: 3.199 (1.896)
Train: 80 [1000/1171 ( 85%)]  Loss:  2.520490 (2.7512)  Time: 1.378s,  742.97/s  (2.506s,  408.62/s)  LR: 3.005e-05  Data: 0.693 (1.900)
Train: 80 [1050/1171 ( 90%)]  Loss:  2.835738 (2.7551)  Time: 6.149s,  166.53/s  (2.514s,  407.27/s)  LR: 3.005e-05  Data: 5.584 (1.908)
Train: 80 [1100/1171 ( 94%)]  Loss:  3.276928 (2.7778)  Time: 3.642s,  281.17/s  (2.515s,  407.12/s)  LR: 3.005e-05  Data: 3.075 (1.910)
Train: 80 [1150/1171 ( 98%)]  Loss:  2.855177 (2.7810)  Time: 5.180s,  197.70/s  (2.518s,  406.63/s)  LR: 3.005e-05  Data: 4.493 (1.913)
Train: 80 [1170/1171 (100%)]  Loss:  2.722896 (2.7787)  Time: 0.563s, 1817.63/s  (2.514s,  407.36/s)  LR: 3.005e-05  Data: 0.000 (1.908)
Test: [   0/97]  Time: 13.364 (13.364)  Loss:  0.3274 (0.3274)  Acc@1: 96.4844 (96.4844)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.197 (3.213)  Loss:  0.4981 (0.4044)  Acc@1: 91.2109 (94.6557)  Acc@5: 98.3398 (98.8454)
Test: [  97/97]  Time: 0.119 (3.101)  Loss:  0.3559 (0.4156)  Acc@1: 94.7917 (94.1080)  Acc@5: 99.1071 (98.6290)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-79.pth.tar', 94.13700004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-80.pth.tar', 94.10800003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-78.pth.tar', 94.03100000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-77.pth.tar', 94.0010000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-76.pth.tar', 93.93099998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-75.pth.tar', 93.9130000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-74.pth.tar', 93.83300002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-73.pth.tar', 93.76200003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-71.pth.tar', 93.6600000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-72.pth.tar', 93.64200001464843)

Train: 81 [   0/1171 (  0%)]  Loss:  2.682566 (2.6826)  Time: 11.124s,   92.05/s  (11.124s,   92.05/s)  LR: 2.538e-05  Data: 9.891 (9.891)
Train: 81 [  50/1171 (  4%)]  Loss:  3.216962 (2.9498)  Time: 0.588s, 1740.63/s  (2.353s,  435.10/s)  LR: 2.538e-05  Data: 0.021 (1.753)
Train: 81 [ 100/1171 (  9%)]  Loss:  2.818602 (2.9060)  Time: 0.585s, 1750.59/s  (2.623s,  390.39/s)  LR: 2.538e-05  Data: 0.021 (2.032)
Train: 81 [ 150/1171 ( 13%)]  Loss:  2.892931 (2.9028)  Time: 0.586s, 1748.53/s  (2.583s,  396.46/s)  LR: 2.538e-05  Data: 0.021 (1.990)
Train: 81 [ 200/1171 ( 17%)]  Loss:  2.942429 (2.9107)  Time: 0.587s, 1744.22/s  (2.592s,  395.08/s)  LR: 2.538e-05  Data: 0.023 (2.000)
Train: 81 [ 250/1171 ( 21%)]  Loss:  3.184019 (2.9563)  Time: 0.585s, 1750.09/s  (2.560s,  400.06/s)  LR: 2.538e-05  Data: 0.020 (1.964)
Train: 81 [ 300/1171 ( 26%)]  Loss:  2.886480 (2.9463)  Time: 2.909s,  352.00/s  (2.550s,  401.52/s)  LR: 2.538e-05  Data: 2.347 (1.954)
Train: 81 [ 350/1171 ( 30%)]  Loss:  3.257580 (2.9852)  Time: 0.588s, 1742.39/s  (2.515s,  407.18/s)  LR: 2.538e-05  Data: 0.021 (1.919)
Train: 81 [ 400/1171 ( 34%)]  Loss:  3.302546 (3.0205)  Time: 0.588s, 1742.77/s  (2.497s,  410.06/s)  LR: 2.538e-05  Data: 0.025 (1.900)
Train: 81 [ 450/1171 ( 38%)]  Loss:  3.091578 (3.0276)  Time: 0.588s, 1740.76/s  (2.525s,  405.57/s)  LR: 2.538e-05  Data: 0.023 (1.925)
Train: 81 [ 500/1171 ( 43%)]  Loss:  3.431630 (3.0643)  Time: 3.620s,  282.90/s  (2.564s,  399.35/s)  LR: 2.538e-05  Data: 3.029 (1.962)
Train: 81 [ 550/1171 ( 47%)]  Loss:  2.794822 (3.0418)  Time: 0.585s, 1749.05/s  (2.575s,  397.70/s)  LR: 2.538e-05  Data: 0.022 (1.971)
Train: 81 [ 600/1171 ( 51%)]  Loss:  3.292979 (3.0612)  Time: 6.559s,  156.12/s  (2.588s,  395.64/s)  LR: 2.538e-05  Data: 5.988 (1.983)
Train: 81 [ 650/1171 ( 56%)]  Loss:  3.163723 (3.0685)  Time: 0.586s, 1746.13/s  (2.577s,  397.37/s)  LR: 2.538e-05  Data: 0.022 (1.970)
Train: 81 [ 700/1171 ( 60%)]  Loss:  2.931235 (3.0593)  Time: 5.049s,  202.80/s  (2.565s,  399.26/s)  LR: 2.538e-05  Data: 4.435 (1.958)
Train: 81 [ 750/1171 ( 64%)]  Loss:  3.074179 (3.0603)  Time: 1.257s,  814.46/s  (2.545s,  402.37/s)  LR: 2.538e-05  Data: 0.617 (1.939)
Train: 81 [ 800/1171 ( 68%)]  Loss:  2.648812 (3.0361)  Time: 6.492s,  157.73/s  (2.570s,  398.45/s)  LR: 2.538e-05  Data: 5.834 (1.962)
Train: 81 [ 850/1171 ( 73%)]  Loss:  2.548917 (3.0090)  Time: 0.588s, 1742.42/s  (2.574s,  397.84/s)  LR: 2.538e-05  Data: 0.022 (1.966)
Train: 81 [ 900/1171 ( 77%)]  Loss:  3.200706 (3.0191)  Time: 8.008s,  127.88/s  (2.575s,  397.71/s)  LR: 2.538e-05  Data: 7.446 (1.968)
Train: 81 [ 950/1171 ( 81%)]  Loss:  2.885585 (3.0124)  Time: 0.588s, 1742.83/s  (2.569s,  398.55/s)  LR: 2.538e-05  Data: 0.018 (1.962)
Train: 81 [1000/1171 ( 85%)]  Loss:  3.013581 (3.0125)  Time: 3.965s,  258.27/s  (2.563s,  399.61/s)  LR: 2.538e-05  Data: 3.209 (1.955)
Train: 81 [1050/1171 ( 90%)]  Loss:  3.195252 (3.0208)  Time: 0.587s, 1744.58/s  (2.551s,  401.34/s)  LR: 2.538e-05  Data: 0.019 (1.944)
Train: 81 [1100/1171 ( 94%)]  Loss:  3.197615 (3.0285)  Time: 3.771s,  271.54/s  (2.546s,  402.24/s)  LR: 2.538e-05  Data: 3.134 (1.939)
Train: 81 [1150/1171 ( 98%)]  Loss:  3.055612 (3.0296)  Time: 0.591s, 1731.30/s  (2.556s,  400.59/s)  LR: 2.538e-05  Data: 0.021 (1.950)
Train: 81 [1170/1171 (100%)]  Loss:  3.261762 (3.0389)  Time: 0.565s, 1812.79/s  (2.556s,  400.68/s)  LR: 2.538e-05  Data: 0.000 (1.950)
Test: [   0/97]  Time: 13.269 (13.269)  Loss:  0.3138 (0.3138)  Acc@1: 96.3867 (96.3867)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (3.311)  Loss:  0.4859 (0.3816)  Acc@1: 91.5039 (94.7266)  Acc@5: 98.1445 (98.8607)
Test: [  97/97]  Time: 0.119 (3.273)  Loss:  0.3455 (0.3947)  Acc@1: 94.0476 (94.1840)  Acc@5: 99.1071 (98.6650)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-81.pth.tar', 94.18400002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-79.pth.tar', 94.13700004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-80.pth.tar', 94.10800003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-78.pth.tar', 94.03100000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-77.pth.tar', 94.0010000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-76.pth.tar', 93.93099998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-75.pth.tar', 93.9130000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-74.pth.tar', 93.83300002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-73.pth.tar', 93.76200003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-71.pth.tar', 93.6600000366211)

Train: 82 [   0/1171 (  0%)]  Loss:  3.127623 (3.1276)  Time: 11.976s,   85.51/s  (11.976s,   85.51/s)  LR: 2.131e-05  Data: 11.174 (11.174)
Train: 82 [  50/1171 (  4%)]  Loss:  3.158573 (3.1431)  Time: 0.585s, 1751.04/s  (2.533s,  404.19/s)  LR: 2.131e-05  Data: 0.023 (1.935)
Train: 82 [ 100/1171 (  9%)]  Loss:  2.827484 (3.0379)  Time: 0.587s, 1745.87/s  (2.453s,  417.38/s)  LR: 2.131e-05  Data: 0.023 (1.860)
Train: 82 [ 150/1171 ( 13%)]  Loss:  2.687837 (2.9504)  Time: 0.587s, 1745.58/s  (2.377s,  430.83/s)  LR: 2.131e-05  Data: 0.024 (1.784)
Train: 82 [ 200/1171 ( 17%)]  Loss:  2.679678 (2.8962)  Time: 0.588s, 1742.80/s  (2.453s,  417.48/s)  LR: 2.131e-05  Data: 0.025 (1.858)
Train: 82 [ 250/1171 ( 21%)]  Loss:  2.819026 (2.8834)  Time: 0.586s, 1746.19/s  (2.457s,  416.76/s)  LR: 2.131e-05  Data: 0.024 (1.860)
Train: 82 [ 300/1171 ( 26%)]  Loss:  2.902150 (2.8861)  Time: 0.586s, 1747.21/s  (2.495s,  410.49/s)  LR: 2.131e-05  Data: 0.023 (1.898)
Train: 82 [ 350/1171 ( 30%)]  Loss:  2.922146 (2.8906)  Time: 0.590s, 1736.39/s  (2.498s,  409.97/s)  LR: 2.131e-05  Data: 0.023 (1.901)
Train: 82 [ 400/1171 ( 34%)]  Loss:  3.092552 (2.9130)  Time: 0.586s, 1748.23/s  (2.480s,  412.91/s)  LR: 2.131e-05  Data: 0.023 (1.884)
Train: 82 [ 450/1171 ( 38%)]  Loss:  3.273779 (2.9491)  Time: 0.586s, 1748.11/s  (2.480s,  412.84/s)  LR: 2.131e-05  Data: 0.021 (1.885)
Train: 82 [ 500/1171 ( 43%)]  Loss:  2.848819 (2.9400)  Time: 0.585s, 1751.66/s  (2.476s,  413.54/s)  LR: 2.131e-05  Data: 0.021 (1.882)
Train: 82 [ 550/1171 ( 47%)]  Loss:  2.751604 (2.9243)  Time: 0.588s, 1740.55/s  (2.517s,  406.84/s)  LR: 2.131e-05  Data: 0.025 (1.923)
Train: 82 [ 600/1171 ( 51%)]  Loss:  2.835453 (2.9174)  Time: 0.586s, 1747.32/s  (2.531s,  404.59/s)  LR: 2.131e-05  Data: 0.021 (1.936)
Train: 82 [ 650/1171 ( 56%)]  Loss:  2.921512 (2.9177)  Time: 1.982s,  516.65/s  (2.549s,  401.75/s)  LR: 2.131e-05  Data: 1.358 (1.952)
Train: 82 [ 700/1171 ( 60%)]  Loss:  3.053280 (2.9268)  Time: 0.589s, 1739.98/s  (2.539s,  403.31/s)  LR: 2.131e-05  Data: 0.025 (1.942)
Train: 82 [ 750/1171 ( 64%)]  Loss:  2.932193 (2.9271)  Time: 0.582s, 1758.42/s  (2.536s,  403.75/s)  LR: 2.131e-05  Data: 0.019 (1.938)
Train: 82 [ 800/1171 ( 68%)]  Loss:  3.136404 (2.9394)  Time: 0.586s, 1747.72/s  (2.525s,  405.51/s)  LR: 2.131e-05  Data: 0.024 (1.926)
Train: 82 [ 850/1171 ( 73%)]  Loss:  3.358833 (2.9627)  Time: 3.227s,  317.35/s  (2.521s,  406.23/s)  LR: 2.131e-05  Data: 2.608 (1.920)
Train: 82 [ 900/1171 ( 77%)]  Loss:  3.048775 (2.9672)  Time: 2.155s,  475.16/s  (2.527s,  405.27/s)  LR: 2.131e-05  Data: 1.485 (1.926)
Train: 82 [ 950/1171 ( 81%)]  Loss:  2.688210 (2.9533)  Time: 1.405s,  729.07/s  (2.535s,  403.87/s)  LR: 2.131e-05  Data: 0.736 (1.933)
Train: 82 [1000/1171 ( 85%)]  Loss:  3.268846 (2.9683)  Time: 4.662s,  219.64/s  (2.541s,  403.02/s)  LR: 2.131e-05  Data: 3.971 (1.939)
Train: 82 [1050/1171 ( 90%)]  Loss:  2.882160 (2.9644)  Time: 0.583s, 1756.64/s  (2.541s,  403.07/s)  LR: 2.131e-05  Data: 0.020 (1.938)
Train: 82 [1100/1171 ( 94%)]  Loss:  2.732800 (2.9543)  Time: 0.585s, 1751.41/s  (2.537s,  403.60/s)  LR: 2.131e-05  Data: 0.019 (1.936)
Train: 82 [1150/1171 ( 98%)]  Loss:  3.251375 (2.9667)  Time: 0.585s, 1751.70/s  (2.532s,  404.36/s)  LR: 2.131e-05  Data: 0.020 (1.931)
Train: 82 [1170/1171 (100%)]  Loss:  2.832018 (2.9613)  Time: 0.563s, 1817.54/s  (2.529s,  404.87/s)  LR: 2.131e-05  Data: 0.000 (1.928)
Test: [   0/97]  Time: 12.777 (12.777)  Loss:  0.3012 (0.3012)  Acc@1: 96.5820 (96.5820)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.196 (3.157)  Loss:  0.4639 (0.3707)  Acc@1: 91.6016 (94.8261)  Acc@5: 98.2422 (98.8722)
Test: [  97/97]  Time: 0.120 (3.265)  Loss:  0.3320 (0.3854)  Acc@1: 94.9405 (94.2360)  Acc@5: 98.9583 (98.6670)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-82.pth.tar', 94.23600004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-81.pth.tar', 94.18400002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-79.pth.tar', 94.13700004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-80.pth.tar', 94.10800003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-78.pth.tar', 94.03100000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-77.pth.tar', 94.0010000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-76.pth.tar', 93.93099998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-75.pth.tar', 93.9130000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-74.pth.tar', 93.83300002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-73.pth.tar', 93.76200003173828)

Train: 83 [   0/1171 (  0%)]  Loss:  2.961033 (2.9610)  Time: 14.477s,   70.74/s  (14.477s,   70.74/s)  LR: 1.786e-05  Data: 13.255 (13.255)
Train: 83 [  50/1171 (  4%)]  Loss:  3.008960 (2.9850)  Time: 0.587s, 1743.91/s  (2.685s,  381.41/s)  LR: 1.786e-05  Data: 0.020 (2.093)
Train: 83 [ 100/1171 (  9%)]  Loss:  3.137779 (3.0359)  Time: 0.590s, 1735.25/s  (2.597s,  394.34/s)  LR: 1.786e-05  Data: 0.024 (2.000)
Train: 83 [ 150/1171 ( 13%)]  Loss:  2.882425 (2.9975)  Time: 0.587s, 1744.60/s  (2.505s,  408.82/s)  LR: 1.786e-05  Data: 0.024 (1.913)
Train: 83 [ 200/1171 ( 17%)]  Loss:  2.463137 (2.8907)  Time: 0.586s, 1746.14/s  (2.497s,  410.16/s)  LR: 1.786e-05  Data: 0.023 (1.906)
Train: 83 [ 250/1171 ( 21%)]  Loss:  3.076709 (2.9217)  Time: 0.584s, 1752.78/s  (2.450s,  418.02/s)  LR: 1.786e-05  Data: 0.022 (1.857)
Train: 83 [ 300/1171 ( 26%)]  Loss:  2.635419 (2.8808)  Time: 0.591s, 1733.65/s  (2.434s,  420.64/s)  LR: 1.786e-05  Data: 0.022 (1.843)
Train: 83 [ 350/1171 ( 30%)]  Loss:  2.690101 (2.8569)  Time: 0.586s, 1746.31/s  (2.501s,  409.37/s)  LR: 1.786e-05  Data: 0.021 (1.909)
Train: 83 [ 400/1171 ( 34%)]  Loss:  3.234138 (2.8989)  Time: 2.849s,  359.39/s  (2.520s,  406.35/s)  LR: 1.786e-05  Data: 2.168 (1.926)
Train: 83 [ 450/1171 ( 38%)]  Loss:  3.212102 (2.9302)  Time: 0.585s, 1751.52/s  (2.513s,  407.54/s)  LR: 1.786e-05  Data: 0.022 (1.917)
Train: 83 [ 500/1171 ( 43%)]  Loss:  2.512589 (2.8922)  Time: 2.489s,  411.48/s  (2.535s,  403.98/s)  LR: 1.786e-05  Data: 1.836 (1.939)
Train: 83 [ 550/1171 ( 47%)]  Loss:  2.836253 (2.8876)  Time: 0.589s, 1739.94/s  (2.537s,  403.64/s)  LR: 1.786e-05  Data: 0.019 (1.939)
Train: 83 [ 600/1171 ( 51%)]  Loss:  3.160553 (2.9086)  Time: 2.615s,  391.64/s  (2.531s,  404.65/s)  LR: 1.786e-05  Data: 2.045 (1.932)
Train: 83 [ 650/1171 ( 56%)]  Loss:  3.039514 (2.9179)  Time: 0.585s, 1750.10/s  (2.538s,  403.45/s)  LR: 1.786e-05  Data: 0.021 (1.938)
Train: 83 [ 700/1171 ( 60%)]  Loss:  3.329354 (2.9453)  Time: 4.967s,  206.16/s  (2.563s,  399.59/s)  LR: 1.786e-05  Data: 4.294 (1.962)
Train: 83 [ 750/1171 ( 64%)]  Loss:  2.951374 (2.9457)  Time: 0.583s, 1757.17/s  (2.554s,  400.95/s)  LR: 1.786e-05  Data: 0.020 (1.952)
Train: 83 [ 800/1171 ( 68%)]  Loss:  2.671588 (2.9296)  Time: 3.214s,  318.63/s  (2.556s,  400.68/s)  LR: 1.786e-05  Data: 2.652 (1.954)
Train: 83 [ 850/1171 ( 73%)]  Loss:  2.856780 (2.9255)  Time: 0.584s, 1754.04/s  (2.548s,  401.86/s)  LR: 1.786e-05  Data: 0.020 (1.945)
Train: 83 [ 900/1171 ( 77%)]  Loss:  3.193063 (2.9396)  Time: 0.584s, 1752.56/s  (2.544s,  402.47/s)  LR: 1.786e-05  Data: 0.021 (1.941)
Train: 83 [ 950/1171 ( 81%)]  Loss:  2.775757 (2.9314)  Time: 0.592s, 1729.30/s  (2.533s,  404.23/s)  LR: 1.786e-05  Data: 0.019 (1.930)
Train: 83 [1000/1171 ( 85%)]  Loss:  2.809164 (2.9256)  Time: 1.173s,  872.99/s  (2.528s,  405.05/s)  LR: 1.786e-05  Data: 0.484 (1.925)
Train: 83 [1050/1171 ( 90%)]  Loss:  2.485000 (2.9056)  Time: 0.587s, 1744.33/s  (2.543s,  402.62/s)  LR: 1.786e-05  Data: 0.018 (1.940)
Train: 83 [1100/1171 ( 94%)]  Loss:  2.827246 (2.9022)  Time: 0.588s, 1741.90/s  (2.544s,  402.53/s)  LR: 1.786e-05  Data: 0.021 (1.940)
Train: 83 [1150/1171 ( 98%)]  Loss:  2.787421 (2.8974)  Time: 0.587s, 1743.97/s  (2.542s,  402.78/s)  LR: 1.786e-05  Data: 0.021 (1.939)
Train: 83 [1170/1171 (100%)]  Loss:  2.723446 (2.8904)  Time: 0.563s, 1817.72/s  (2.542s,  402.80/s)  LR: 1.786e-05  Data: 0.000 (1.938)
Test: [   0/97]  Time: 13.010 (13.010)  Loss:  0.3117 (0.3117)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (3.124)  Loss:  0.4750 (0.3801)  Acc@1: 91.9922 (94.8702)  Acc@5: 98.1445 (98.8434)
Test: [  97/97]  Time: 0.119 (3.045)  Loss:  0.3449 (0.3935)  Acc@1: 94.1964 (94.2840)  Acc@5: 98.8095 (98.6350)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-83.pth.tar', 94.2840000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-82.pth.tar', 94.23600004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-81.pth.tar', 94.18400002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-79.pth.tar', 94.13700004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-80.pth.tar', 94.10800003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-78.pth.tar', 94.03100000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-77.pth.tar', 94.0010000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-76.pth.tar', 93.93099998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-75.pth.tar', 93.9130000390625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-74.pth.tar', 93.83300002685547)

Train: 84 [   0/1171 (  0%)]  Loss:  2.750798 (2.7508)  Time: 9.773s,  104.78/s  (9.773s,  104.78/s)  LR: 1.504e-05  Data: 9.201 (9.201)
Train: 84 [  50/1171 (  4%)]  Loss:  3.170140 (2.9605)  Time: 0.586s, 1747.39/s  (2.389s,  428.60/s)  LR: 1.504e-05  Data: 0.018 (1.776)
Train: 84 [ 100/1171 (  9%)]  Loss:  2.984930 (2.9686)  Time: 0.583s, 1756.13/s  (2.576s,  397.46/s)  LR: 1.504e-05  Data: 0.019 (1.974)
Train: 84 [ 150/1171 ( 13%)]  Loss:  2.611924 (2.8794)  Time: 0.587s, 1745.14/s  (2.563s,  399.47/s)  LR: 1.504e-05  Data: 0.020 (1.965)
Train: 84 [ 200/1171 ( 17%)]  Loss:  2.640080 (2.8316)  Time: 0.583s, 1756.50/s  (2.584s,  396.25/s)  LR: 1.504e-05  Data: 0.019 (1.985)
Train: 84 [ 250/1171 ( 21%)]  Loss:  2.776792 (2.8224)  Time: 0.587s, 1745.10/s  (2.546s,  402.20/s)  LR: 1.504e-05  Data: 0.020 (1.947)
Train: 84 [ 300/1171 ( 26%)]  Loss:  2.725899 (2.8087)  Time: 0.586s, 1746.49/s  (2.526s,  405.42/s)  LR: 1.504e-05  Data: 0.020 (1.930)
Train: 84 [ 350/1171 ( 30%)]  Loss:  3.315799 (2.8720)  Time: 0.586s, 1748.07/s  (2.493s,  410.78/s)  LR: 1.504e-05  Data: 0.020 (1.896)
Train: 84 [ 400/1171 ( 34%)]  Loss:  2.729876 (2.8562)  Time: 0.585s, 1749.70/s  (2.482s,  412.55/s)  LR: 1.504e-05  Data: 0.020 (1.883)
Train: 84 [ 450/1171 ( 38%)]  Loss:  2.972952 (2.8679)  Time: 0.588s, 1742.77/s  (2.504s,  408.95/s)  LR: 1.504e-05  Data: 0.024 (1.906)
Train: 84 [ 500/1171 ( 43%)]  Loss:  2.920333 (2.8727)  Time: 0.584s, 1751.97/s  (2.537s,  403.67/s)  LR: 1.504e-05  Data: 0.020 (1.940)
Train: 84 [ 550/1171 ( 47%)]  Loss:  3.063654 (2.8886)  Time: 0.587s, 1743.48/s  (2.552s,  401.30/s)  LR: 1.504e-05  Data: 0.024 (1.955)
Train: 84 [ 600/1171 ( 51%)]  Loss:  3.412994 (2.9289)  Time: 0.584s, 1752.40/s  (2.560s,  400.03/s)  LR: 1.504e-05  Data: 0.020 (1.964)
Train: 84 [ 650/1171 ( 56%)]  Loss:  3.217372 (2.9495)  Time: 0.587s, 1743.87/s  (2.551s,  401.36/s)  LR: 1.504e-05  Data: 0.024 (1.955)
Train: 84 [ 700/1171 ( 60%)]  Loss:  3.024073 (2.9545)  Time: 0.584s, 1754.72/s  (2.549s,  401.76/s)  LR: 1.504e-05  Data: 0.020 (1.953)
Train: 84 [ 750/1171 ( 64%)]  Loss:  3.150226 (2.9667)  Time: 0.588s, 1740.46/s  (2.530s,  404.72/s)  LR: 1.504e-05  Data: 0.022 (1.935)
Train: 84 [ 800/1171 ( 68%)]  Loss:  2.698735 (2.9510)  Time: 0.587s, 1743.25/s  (2.543s,  402.70/s)  LR: 1.504e-05  Data: 0.020 (1.947)
Train: 84 [ 850/1171 ( 73%)]  Loss:  2.386161 (2.9196)  Time: 0.585s, 1749.04/s  (2.544s,  402.47/s)  LR: 1.504e-05  Data: 0.021 (1.948)
Train: 84 [ 900/1171 ( 77%)]  Loss:  2.677418 (2.9069)  Time: 0.585s, 1749.76/s  (2.551s,  401.48/s)  LR: 1.504e-05  Data: 0.020 (1.954)
Train: 84 [ 950/1171 ( 81%)]  Loss:  3.506114 (2.9368)  Time: 0.588s, 1740.79/s  (2.541s,  403.01/s)  LR: 1.504e-05  Data: 0.022 (1.944)
Train: 84 [1000/1171 ( 85%)]  Loss:  2.671713 (2.9242)  Time: 0.586s, 1746.98/s  (2.539s,  403.33/s)  LR: 1.504e-05  Data: 0.020 (1.942)
Train: 84 [1050/1171 ( 90%)]  Loss:  2.515110 (2.9056)  Time: 0.586s, 1747.61/s  (2.529s,  404.87/s)  LR: 1.504e-05  Data: 0.019 (1.932)
Train: 84 [1100/1171 ( 94%)]  Loss:  3.047863 (2.9118)  Time: 0.587s, 1743.41/s  (2.520s,  406.34/s)  LR: 1.504e-05  Data: 0.020 (1.923)
Train: 84 [1150/1171 ( 98%)]  Loss:  2.785985 (2.9065)  Time: 0.587s, 1743.45/s  (2.530s,  404.78/s)  LR: 1.504e-05  Data: 0.019 (1.931)
Train: 84 [1170/1171 (100%)]  Loss:  3.382745 (2.9256)  Time: 0.564s, 1814.24/s  (2.528s,  405.03/s)  LR: 1.504e-05  Data: 0.000 (1.930)
Test: [   0/97]  Time: 15.535 (15.535)  Loss:  0.3051 (0.3051)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.197 (3.375)  Loss:  0.4761 (0.3769)  Acc@1: 91.6992 (94.8415)  Acc@5: 98.2422 (98.8741)
Test: [  97/97]  Time: 0.119 (3.267)  Loss:  0.3379 (0.3899)  Acc@1: 94.6429 (94.2950)  Acc@5: 98.9583 (98.6800)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-84.pth.tar', 94.29500002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-83.pth.tar', 94.2840000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-82.pth.tar', 94.23600004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-81.pth.tar', 94.18400002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-79.pth.tar', 94.13700004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-80.pth.tar', 94.10800003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-78.pth.tar', 94.03100000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-77.pth.tar', 94.0010000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-76.pth.tar', 93.93099998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-75.pth.tar', 93.9130000390625)

Train: 85 [   0/1171 (  0%)]  Loss:  3.283550 (3.2835)  Time: 11.673s,   87.72/s  (11.673s,   87.72/s)  LR: 1.284e-05  Data: 10.788 (10.788)
Train: 85 [  50/1171 (  4%)]  Loss:  2.240409 (2.7620)  Time: 0.585s, 1750.14/s  (2.534s,  404.16/s)  LR: 1.284e-05  Data: 0.019 (1.914)
Train: 85 [ 100/1171 (  9%)]  Loss:  2.655320 (2.7264)  Time: 0.588s, 1741.10/s  (2.476s,  413.59/s)  LR: 1.284e-05  Data: 0.022 (1.869)
Train: 85 [ 150/1171 ( 13%)]  Loss:  3.114443 (2.8234)  Time: 0.585s, 1749.37/s  (2.375s,  431.19/s)  LR: 1.284e-05  Data: 0.021 (1.769)
Train: 85 [ 200/1171 ( 17%)]  Loss:  3.134859 (2.8857)  Time: 2.374s,  431.30/s  (2.411s,  424.79/s)  LR: 1.284e-05  Data: 1.811 (1.811)
Train: 85 [ 250/1171 ( 21%)]  Loss:  3.059101 (2.9146)  Time: 0.587s, 1743.38/s  (2.477s,  413.41/s)  LR: 1.284e-05  Data: 0.020 (1.871)
Train: 85 [ 300/1171 ( 26%)]  Loss:  2.851805 (2.9056)  Time: 5.898s,  173.61/s  (2.496s,  410.25/s)  LR: 1.284e-05  Data: 4.912 (1.890)
Train: 85 [ 350/1171 ( 30%)]  Loss:  3.087207 (2.9283)  Time: 0.597s, 1716.01/s  (2.491s,  411.15/s)  LR: 1.284e-05  Data: 0.019 (1.884)
Train: 85 [ 400/1171 ( 34%)]  Loss:  2.628571 (2.8950)  Time: 4.759s,  215.19/s  (2.493s,  410.73/s)  LR: 1.284e-05  Data: 4.197 (1.888)
Train: 85 [ 450/1171 ( 38%)]  Loss:  2.857157 (2.8912)  Time: 0.588s, 1741.16/s  (2.480s,  412.97/s)  LR: 1.284e-05  Data: 0.019 (1.874)
Train: 85 [ 500/1171 ( 43%)]  Loss:  2.428342 (2.8492)  Time: 2.831s,  361.65/s  (2.481s,  412.72/s)  LR: 1.284e-05  Data: 2.166 (1.875)
Train: 85 [ 550/1171 ( 47%)]  Loss:  3.292927 (2.8861)  Time: 0.584s, 1754.32/s  (2.475s,  413.73/s)  LR: 1.284e-05  Data: 0.019 (1.869)
Train: 85 [ 600/1171 ( 51%)]  Loss:  2.979554 (2.8933)  Time: 2.652s,  386.06/s  (2.528s,  405.07/s)  LR: 1.284e-05  Data: 2.068 (1.921)
Train: 85 [ 650/1171 ( 56%)]  Loss:  3.212062 (2.9161)  Time: 0.586s, 1746.06/s  (2.526s,  405.45/s)  LR: 1.284e-05  Data: 0.019 (1.919)
Train: 85 [ 700/1171 ( 60%)]  Loss:  2.846501 (2.9115)  Time: 8.472s,  120.87/s  (2.541s,  402.95/s)  LR: 1.284e-05  Data: 7.875 (1.936)
Train: 85 [ 750/1171 ( 64%)]  Loss:  2.369462 (2.8776)  Time: 0.589s, 1737.94/s  (2.536s,  403.83/s)  LR: 1.284e-05  Data: 0.024 (1.932)
Train: 85 [ 800/1171 ( 68%)]  Loss:  3.343944 (2.9050)  Time: 4.932s,  207.63/s  (2.532s,  404.48/s)  LR: 1.284e-05  Data: 4.370 (1.928)
Train: 85 [ 850/1171 ( 73%)]  Loss:  2.952986 (2.9077)  Time: 0.589s, 1737.36/s  (2.521s,  406.26/s)  LR: 1.284e-05  Data: 0.024 (1.916)
Train: 85 [ 900/1171 ( 77%)]  Loss:  3.130646 (2.9194)  Time: 7.417s,  138.06/s  (2.516s,  406.92/s)  LR: 1.284e-05  Data: 6.722 (1.912)
Train: 85 [ 950/1171 ( 81%)]  Loss:  3.286848 (2.9378)  Time: 0.590s, 1736.31/s  (2.531s,  404.56/s)  LR: 1.284e-05  Data: 0.025 (1.927)
Train: 85 [1000/1171 ( 85%)]  Loss:  3.151741 (2.9480)  Time: 8.250s,  124.11/s  (2.541s,  403.04/s)  LR: 1.284e-05  Data: 7.655 (1.937)
Train: 85 [1050/1171 ( 90%)]  Loss:  2.762942 (2.9396)  Time: 0.586s, 1747.16/s  (2.539s,  403.38/s)  LR: 1.284e-05  Data: 0.023 (1.935)
Train: 85 [1100/1171 ( 94%)]  Loss:  3.036407 (2.9438)  Time: 8.434s,  121.41/s  (2.542s,  402.84/s)  LR: 1.284e-05  Data: 7.450 (1.939)
Train: 85 [1150/1171 ( 98%)]  Loss:  3.042491 (2.9479)  Time: 0.586s, 1747.50/s  (2.531s,  404.56/s)  LR: 1.284e-05  Data: 0.019 (1.929)
Train: 85 [1170/1171 (100%)]  Loss:  3.194782 (2.9578)  Time: 0.564s, 1815.48/s  (2.531s,  404.61/s)  LR: 1.284e-05  Data: 0.000 (1.929)
Test: [   0/97]  Time: 14.303 (14.303)  Loss:  0.3023 (0.3023)  Acc@1: 96.8750 (96.8750)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.200 (3.128)  Loss:  0.4716 (0.3786)  Acc@1: 92.0898 (94.8376)  Acc@5: 98.2422 (98.8645)
Test: [  97/97]  Time: 0.120 (3.241)  Loss:  0.3455 (0.3914)  Acc@1: 94.1964 (94.2860)  Acc@5: 98.8095 (98.6670)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-84.pth.tar', 94.29500002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-85.pth.tar', 94.28600003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-83.pth.tar', 94.2840000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-82.pth.tar', 94.23600004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-81.pth.tar', 94.18400002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-79.pth.tar', 94.13700004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-80.pth.tar', 94.10800003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-78.pth.tar', 94.03100000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-77.pth.tar', 94.0010000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-76.pth.tar', 93.93099998535156)

Train: 86 [   0/1171 (  0%)]  Loss:  2.918812 (2.9188)  Time: 13.762s,   74.41/s  (13.762s,   74.41/s)  LR: 1.126e-05  Data: 13.009 (13.009)
Train: 86 [  50/1171 (  4%)]  Loss:  2.906927 (2.9129)  Time: 1.963s,  521.69/s  (2.723s,  376.09/s)  LR: 1.126e-05  Data: 1.291 (2.110)
Train: 86 [ 100/1171 (  9%)]  Loss:  3.085132 (2.9703)  Time: 0.589s, 1737.99/s  (2.685s,  381.37/s)  LR: 1.126e-05  Data: 0.027 (2.081)
Train: 86 [ 150/1171 ( 13%)]  Loss:  2.804889 (2.9289)  Time: 0.588s, 1742.63/s  (2.569s,  398.66/s)  LR: 1.126e-05  Data: 0.024 (1.969)
Train: 86 [ 200/1171 ( 17%)]  Loss:  2.481059 (2.8394)  Time: 2.306s,  444.07/s  (2.550s,  401.56/s)  LR: 1.126e-05  Data: 1.708 (1.956)
Train: 86 [ 250/1171 ( 21%)]  Loss:  3.232831 (2.9049)  Time: 0.584s, 1752.36/s  (2.495s,  410.50/s)  LR: 1.126e-05  Data: 0.018 (1.901)
Train: 86 [ 300/1171 ( 26%)]  Loss:  3.270581 (2.9572)  Time: 0.585s, 1751.40/s  (2.479s,  413.11/s)  LR: 1.126e-05  Data: 0.022 (1.883)
Train: 86 [ 350/1171 ( 30%)]  Loss:  3.410681 (3.0139)  Time: 0.587s, 1745.31/s  (2.500s,  409.59/s)  LR: 1.126e-05  Data: 0.024 (1.903)
Train: 86 [ 400/1171 ( 34%)]  Loss:  3.086977 (3.0220)  Time: 1.542s,  664.07/s  (2.514s,  407.34/s)  LR: 1.126e-05  Data: 0.980 (1.914)
Train: 86 [ 450/1171 ( 38%)]  Loss:  3.046571 (3.0244)  Time: 0.591s, 1733.85/s  (2.524s,  405.64/s)  LR: 1.126e-05  Data: 0.026 (1.922)
Train: 86 [ 500/1171 ( 43%)]  Loss:  3.252236 (3.0452)  Time: 3.560s,  287.66/s  (2.534s,  404.12/s)  LR: 1.126e-05  Data: 2.930 (1.930)
Train: 86 [ 550/1171 ( 47%)]  Loss:  2.588909 (3.0071)  Time: 0.589s, 1737.81/s  (2.534s,  404.14/s)  LR: 1.126e-05  Data: 0.026 (1.931)
Train: 86 [ 600/1171 ( 51%)]  Loss:  2.673571 (2.9815)  Time: 5.856s,  174.88/s  (2.542s,  402.87/s)  LR: 1.126e-05  Data: 5.194 (1.939)
Train: 86 [ 650/1171 ( 56%)]  Loss:  2.316062 (2.9339)  Time: 0.596s, 1718.11/s  (2.522s,  406.05/s)  LR: 1.126e-05  Data: 0.032 (1.920)
Train: 86 [ 700/1171 ( 60%)]  Loss:  2.937893 (2.9342)  Time: 2.890s,  354.34/s  (2.545s,  402.37/s)  LR: 1.126e-05  Data: 2.212 (1.942)
Train: 86 [ 750/1171 ( 64%)]  Loss:  3.275348 (2.9555)  Time: 0.588s, 1741.45/s  (2.552s,  401.21/s)  LR: 1.126e-05  Data: 0.020 (1.949)
Train: 86 [ 800/1171 ( 68%)]  Loss:  3.282692 (2.9748)  Time: 5.253s,  194.93/s  (2.558s,  400.35/s)  LR: 1.126e-05  Data: 4.598 (1.954)
Train: 86 [ 850/1171 ( 73%)]  Loss:  2.427649 (2.9444)  Time: 0.588s, 1740.80/s  (2.550s,  401.63/s)  LR: 1.126e-05  Data: 0.025 (1.945)
Train: 86 [ 900/1171 ( 77%)]  Loss:  2.607847 (2.9267)  Time: 2.149s,  476.39/s  (2.547s,  402.00/s)  LR: 1.126e-05  Data: 1.588 (1.943)
Train: 86 [ 950/1171 ( 81%)]  Loss:  3.387616 (2.9497)  Time: 0.586s, 1746.30/s  (2.536s,  403.86/s)  LR: 1.126e-05  Data: 0.020 (1.931)
Train: 86 [1000/1171 ( 85%)]  Loss:  3.179693 (2.9607)  Time: 3.516s,  291.20/s  (2.527s,  405.30/s)  LR: 1.126e-05  Data: 2.845 (1.922)
Train: 86 [1050/1171 ( 90%)]  Loss:  3.410342 (2.9811)  Time: 0.584s, 1754.74/s  (2.535s,  403.97/s)  LR: 1.126e-05  Data: 0.020 (1.929)
Train: 86 [1100/1171 ( 94%)]  Loss:  2.596555 (2.9644)  Time: 1.272s,  805.12/s  (2.534s,  404.08/s)  LR: 1.126e-05  Data: 0.701 (1.929)
Train: 86 [1150/1171 ( 98%)]  Loss:  3.200562 (2.9742)  Time: 0.584s, 1754.82/s  (2.526s,  405.31/s)  LR: 1.126e-05  Data: 0.022 (1.921)
Train: 86 [1170/1171 (100%)]  Loss:  3.167922 (2.9820)  Time: 0.564s, 1816.33/s  (2.522s,  406.08/s)  LR: 1.126e-05  Data: 0.000 (1.916)
Test: [   0/97]  Time: 12.505 (12.505)  Loss:  0.3062 (0.3062)  Acc@1: 96.5820 (96.5820)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.200 (2.915)  Loss:  0.4782 (0.3785)  Acc@1: 91.6016 (94.8089)  Acc@5: 98.2422 (98.8664)
Test: [  97/97]  Time: 0.119 (2.822)  Loss:  0.3540 (0.3908)  Acc@1: 94.6429 (94.3060)  Acc@5: 98.9583 (98.6860)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-86.pth.tar', 94.30600002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-84.pth.tar', 94.29500002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-85.pth.tar', 94.28600003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-83.pth.tar', 94.2840000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-82.pth.tar', 94.23600004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-81.pth.tar', 94.18400002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-79.pth.tar', 94.13700004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-80.pth.tar', 94.10800003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-78.pth.tar', 94.03100000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-77.pth.tar', 94.0010000366211)

Train: 87 [   0/1171 (  0%)]  Loss:  3.088020 (3.0880)  Time: 10.744s,   95.31/s  (10.744s,   95.31/s)  LR: 1.032e-05  Data: 9.587 (9.587)
Train: 87 [  50/1171 (  4%)]  Loss:  2.628900 (2.8585)  Time: 0.584s, 1753.31/s  (2.137s,  479.14/s)  LR: 1.032e-05  Data: 0.020 (1.536)
Train: 87 [ 100/1171 (  9%)]  Loss:  2.759478 (2.8255)  Time: 0.868s, 1180.10/s  (2.086s,  490.91/s)  LR: 1.032e-05  Data: 0.176 (1.483)
Train: 87 [ 150/1171 ( 13%)]  Loss:  3.379409 (2.9640)  Time: 1.766s,  579.98/s  (2.230s,  459.14/s)  LR: 1.032e-05  Data: 1.196 (1.623)
Train: 87 [ 200/1171 ( 17%)]  Loss:  2.790706 (2.9293)  Time: 1.137s,  900.79/s  (2.194s,  466.63/s)  LR: 1.032e-05  Data: 0.575 (1.586)
Train: 87 [ 250/1171 ( 21%)]  Loss:  2.962470 (2.9348)  Time: 0.914s, 1120.08/s  (2.203s,  464.82/s)  LR: 1.032e-05  Data: 0.350 (1.595)
Train: 87 [ 300/1171 ( 26%)]  Loss:  2.766779 (2.9108)  Time: 2.482s,  412.57/s  (2.198s,  465.84/s)  LR: 1.032e-05  Data: 1.920 (1.588)
Train: 87 [ 350/1171 ( 30%)]  Loss:  2.451658 (2.8534)  Time: 0.586s, 1746.79/s  (2.187s,  468.27/s)  LR: 1.032e-05  Data: 0.022 (1.575)
Train: 87 [ 400/1171 ( 34%)]  Loss:  2.827739 (2.8506)  Time: 1.720s,  595.22/s  (2.178s,  470.13/s)  LR: 1.032e-05  Data: 1.106 (1.568)
Train: 87 [ 450/1171 ( 38%)]  Loss:  3.008560 (2.8664)  Time: 0.589s, 1738.06/s  (2.164s,  473.29/s)  LR: 1.032e-05  Data: 0.021 (1.556)
Train: 87 [ 500/1171 ( 43%)]  Loss:  3.075779 (2.8854)  Time: 3.413s,  300.05/s  (2.168s,  472.41/s)  LR: 1.032e-05  Data: 2.624 (1.561)
Train: 87 [ 550/1171 ( 47%)]  Loss:  2.646691 (2.8655)  Time: 0.587s, 1743.30/s  (2.200s,  465.50/s)  LR: 1.032e-05  Data: 0.020 (1.594)
Train: 87 [ 600/1171 ( 51%)]  Loss:  3.235421 (2.8940)  Time: 2.974s,  344.37/s  (2.216s,  462.04/s)  LR: 1.032e-05  Data: 2.295 (1.609)
Train: 87 [ 650/1171 ( 56%)]  Loss:  3.166431 (2.9134)  Time: 0.590s, 1734.19/s  (2.222s,  460.92/s)  LR: 1.032e-05  Data: 0.024 (1.616)
Train: 87 [ 700/1171 ( 60%)]  Loss:  2.880389 (2.9112)  Time: 7.068s,  144.88/s  (2.230s,  459.25/s)  LR: 1.032e-05  Data: 6.357 (1.623)
Train: 87 [ 750/1171 ( 64%)]  Loss:  3.005190 (2.9171)  Time: 0.589s, 1737.36/s  (2.222s,  460.81/s)  LR: 1.032e-05  Data: 0.020 (1.617)
Train: 87 [ 800/1171 ( 68%)]  Loss:  3.317772 (2.9407)  Time: 7.024s,  145.78/s  (2.224s,  460.42/s)  LR: 1.032e-05  Data: 6.391 (1.620)
Train: 87 [ 850/1171 ( 73%)]  Loss:  2.938662 (2.9406)  Time: 0.590s, 1736.33/s  (2.216s,  462.00/s)  LR: 1.032e-05  Data: 0.025 (1.614)
Train: 87 [ 900/1171 ( 77%)]  Loss:  2.964179 (2.9418)  Time: 6.556s,  156.19/s  (2.210s,  463.45/s)  LR: 1.032e-05  Data: 5.911 (1.608)
Train: 87 [ 950/1171 ( 81%)]  Loss:  2.696714 (2.9295)  Time: 0.589s, 1738.94/s  (2.223s,  460.73/s)  LR: 1.032e-05  Data: 0.021 (1.622)
Train: 87 [1000/1171 ( 85%)]  Loss:  3.075925 (2.9365)  Time: 6.095s,  168.01/s  (2.222s,  460.76/s)  LR: 1.032e-05  Data: 5.481 (1.623)
Train: 87 [1050/1171 ( 90%)]  Loss:  3.082122 (2.9431)  Time: 0.589s, 1739.78/s  (2.222s,  460.93/s)  LR: 1.032e-05  Data: 0.021 (1.623)
Train: 87 [1100/1171 ( 94%)]  Loss:  2.832360 (2.9383)  Time: 7.380s,  138.75/s  (2.223s,  460.55/s)  LR: 1.032e-05  Data: 6.659 (1.625)
Train: 87 [1150/1171 ( 98%)]  Loss:  2.716518 (2.9291)  Time: 0.587s, 1745.55/s  (2.218s,  461.65/s)  LR: 1.032e-05  Data: 0.022 (1.620)
Train: 87 [1170/1171 (100%)]  Loss:  3.266093 (2.9426)  Time: 0.564s, 1815.71/s  (2.217s,  461.86/s)  LR: 1.032e-05  Data: 0.000 (1.620)
Test: [   0/97]  Time: 11.675 (11.675)  Loss:  0.3083 (0.3083)  Acc@1: 96.8750 (96.8750)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.198 (2.838)  Loss:  0.4749 (0.3801)  Acc@1: 91.7969 (94.9104)  Acc@5: 98.2422 (98.8645)
Test: [  97/97]  Time: 0.120 (2.772)  Loss:  0.3436 (0.3930)  Acc@1: 94.4940 (94.3230)  Acc@5: 98.8095 (98.6840)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-87.pth.tar', 94.32300000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-86.pth.tar', 94.30600002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-84.pth.tar', 94.29500002197265)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-85.pth.tar', 94.28600003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-83.pth.tar', 94.2840000366211)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-82.pth.tar', 94.23600004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-81.pth.tar', 94.18400002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-79.pth.tar', 94.13700004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-80.pth.tar', 94.10800003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-78.pth.tar', 94.03100000732422)

*** Best metric: 94.32300000976562 (epoch 87)

wandb: Waiting for W&B process to finish, PID 20289
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210528_145118-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1/logs/debug.log
wandb: Find internal logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210528_145118-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1/logs/debug-internal.log
wandb: Run summary:
wandb:    eval_loss 0.393
wandb:    eval_top1 94.323
wandb:    eval_top5 98.684
wandb:   _timestamp 1622252617
wandb:   train_loss 2.94256
wandb:        _step 87
wandb:        epoch 87
wandb:     _runtime 291997
wandb: Run history:
wandb:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   train_loss ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÅ‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÑ
wandb:    eval_loss ‚ñÉ‚ñÑ‚ñà‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:    eval_top1 ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:    eval_top5 ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñà
wandb:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced PreTraining_vit_deit_tiny_patch16_224_fake_1k: https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
--End--
Sat May 29 10:43:49 JST 2021

--Start--
Wed May 26 15:30:44 JST 2021
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 0
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
wandb: Currently logged in as: gyama_x (use `wandb login --relogin` to force relogin)
wandb: WARNING Tried to auto resume run with id PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k but id PreTraining_vit_deit_tiny_patch16_224_1k is set.
wandb: wandb version 0.10.30 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
wandb: Tracking run with wandb version 0.10.27
wandb: Resuming run PreTraining_vit_deit_tiny_patch16_224_1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gyama_x/pytorch-image-models
wandb: üöÄ View run at https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_1k
wandb: Run data is saved locally in /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210526_153118-PreTraining_vit_deit_tiny_patch16_224_1k
wandb: Run `wandb offline` to turn off syncing.

Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar'
Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_1k/last.pth.tar' (epoch 65)
Using native Torch DistributedDataParallel.
Scheduled epochs: 88
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Train: 66 [   0/1251 (  0%)]  Loss:  4.820934 (4.8209)  Time: 6.887s,  148.70/s  (6.887s,  148.70/s)  LR: 1.550e-04  Data: 5.945 (5.945)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 66 [  50/1251 (  4%)]  Loss:  4.940809 (4.8809)  Time: 0.584s, 1752.92/s  (2.253s,  454.45/s)  LR: 1.550e-04  Data: 0.020 (1.670)
Train: 66 [ 100/1251 (  8%)]  Loss:  4.958048 (4.9066)  Time: 0.585s, 1750.43/s  (2.207s,  464.01/s)  LR: 1.550e-04  Data: 0.018 (1.621)
Train: 66 [ 150/1251 ( 12%)]  Loss:  4.411480 (4.7828)  Time: 0.592s, 1729.00/s  (2.163s,  473.39/s)  LR: 1.550e-04  Data: 0.022 (1.574)
Train: 66 [ 200/1251 ( 16%)]  Loss:  4.508470 (4.7279)  Time: 5.614s,  182.41/s  (2.169s,  472.12/s)  LR: 1.550e-04  Data: 5.048 (1.575)
Train: 66 [ 250/1251 ( 20%)]  Loss:  4.330739 (4.6617)  Time: 0.589s, 1737.93/s  (2.130s,  480.78/s)  LR: 1.550e-04  Data: 0.019 (1.533)
Train: 66 [ 300/1251 ( 24%)]  Loss:  4.896619 (4.6953)  Time: 1.609s,  636.27/s  (2.109s,  485.60/s)  LR: 1.550e-04  Data: 1.041 (1.509)
Train: 66 [ 350/1251 ( 28%)]  Loss:  4.500137 (4.6709)  Time: 0.582s, 1759.03/s  (2.102s,  487.21/s)  LR: 1.550e-04  Data: 0.020 (1.502)
Train: 66 [ 400/1251 ( 32%)]  Loss:  5.042561 (4.7122)  Time: 0.581s, 1761.69/s  (2.137s,  479.08/s)  LR: 1.550e-04  Data: 0.018 (1.538)
Train: 66 [ 450/1251 ( 36%)]  Loss:  4.631765 (4.7042)  Time: 0.584s, 1752.77/s  (2.155s,  475.11/s)  LR: 1.550e-04  Data: 0.020 (1.556)
Train: 66 [ 500/1251 ( 40%)]  Loss:  4.596029 (4.6943)  Time: 0.583s, 1756.86/s  (2.151s,  476.03/s)  LR: 1.550e-04  Data: 0.018 (1.551)
Train: 66 [ 550/1251 ( 44%)]  Loss:  4.772243 (4.7008)  Time: 0.585s, 1751.91/s  (2.153s,  475.59/s)  LR: 1.550e-04  Data: 0.019 (1.553)
Train: 66 [ 600/1251 ( 48%)]  Loss:  5.447539 (4.7583)  Time: 0.586s, 1748.20/s  (2.147s,  476.90/s)  LR: 1.550e-04  Data: 0.021 (1.546)
Train: 66 [ 650/1251 ( 52%)]  Loss:  4.623145 (4.7486)  Time: 2.036s,  503.03/s  (2.146s,  477.16/s)  LR: 1.550e-04  Data: 1.474 (1.545)
Train: 66 [ 700/1251 ( 56%)]  Loss:  4.864677 (4.7563)  Time: 2.509s,  408.05/s  (2.143s,  477.86/s)  LR: 1.550e-04  Data: 1.948 (1.542)
Train: 66 [ 750/1251 ( 60%)]  Loss:  5.252384 (4.7873)  Time: 0.584s, 1752.26/s  (2.149s,  476.51/s)  LR: 1.550e-04  Data: 0.020 (1.546)
Train: 66 [ 800/1251 ( 64%)]  Loss:  4.662956 (4.7800)  Time: 4.064s,  251.97/s  (2.185s,  468.55/s)  LR: 1.550e-04  Data: 3.473 (1.581)
Train: 66 [ 850/1251 ( 68%)]  Loss:  4.837039 (4.7832)  Time: 4.608s,  222.20/s  (2.217s,  461.87/s)  LR: 1.550e-04  Data: 3.921 (1.613)
Train: 66 [ 900/1251 ( 72%)]  Loss:  4.947335 (4.7918)  Time: 0.590s, 1735.22/s  (2.249s,  455.40/s)  LR: 1.550e-04  Data: 0.021 (1.643)
Train: 66 [ 950/1251 ( 76%)]  Loss:  4.487206 (4.7766)  Time: 1.290s,  794.07/s  (2.274s,  450.27/s)  LR: 1.550e-04  Data: 0.720 (1.668)
Train: 66 [1000/1251 ( 80%)]  Loss:  4.396947 (4.7585)  Time: 2.241s,  456.89/s  (2.292s,  446.68/s)  LR: 1.550e-04  Data: 1.454 (1.685)
Train: 66 [1050/1251 ( 84%)]  Loss:  4.779641 (4.7595)  Time: 0.582s, 1759.32/s  (2.310s,  443.32/s)  LR: 1.550e-04  Data: 0.018 (1.702)
Train: 66 [1100/1251 ( 88%)]  Loss:  4.802423 (4.7614)  Time: 2.359s,  434.04/s  (2.319s,  441.64/s)  LR: 1.550e-04  Data: 1.797 (1.711)
Train: 66 [1150/1251 ( 92%)]  Loss:  4.526175 (4.7516)  Time: 0.585s, 1751.71/s  (2.350s,  435.83/s)  LR: 1.550e-04  Data: 0.017 (1.742)
Train: 66 [1200/1251 ( 96%)]  Loss:  4.527219 (4.7426)  Time: 2.365s,  432.99/s  (2.362s,  433.56/s)  LR: 1.550e-04  Data: 1.779 (1.754)
Train: 66 [1250/1251 (100%)]  Loss:  4.689724 (4.7405)  Time: 0.564s, 1816.90/s  (2.372s,  431.65/s)  LR: 1.550e-04  Data: 0.000 (1.765)
Test: [   0/48]  Time: 17.855 (17.855)  Loss:  1.2863 (1.2863)  Acc@1: 73.6328 (73.6328)  Acc@5: 90.5273 (90.5273)
Test: [  48/48]  Time: 0.541 (3.561)  Loss:  1.2475 (2.2723)  Acc@1: 74.0566 (51.3300)  Acc@5: 89.2689 (76.1400)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-66.pth.tar', 51.330000107421874)

Train: 67 [   0/1251 (  0%)]  Loss:  4.888998 (4.8890)  Time: 11.520s,   88.89/s  (11.520s,   88.89/s)  LR: 1.427e-04  Data: 10.575 (10.575)
Train: 67 [  50/1251 (  4%)]  Loss:  4.571805 (4.7304)  Time: 0.590s, 1735.92/s  (2.435s,  420.56/s)  LR: 1.427e-04  Data: 0.024 (1.822)
Train: 67 [ 100/1251 (  8%)]  Loss:  5.145670 (4.8688)  Time: 0.945s, 1084.13/s  (2.393s,  427.85/s)  LR: 1.427e-04  Data: 0.383 (1.783)
Train: 67 [ 150/1251 ( 12%)]  Loss:  4.508314 (4.7787)  Time: 0.585s, 1751.26/s  (2.427s,  421.93/s)  LR: 1.427e-04  Data: 0.022 (1.820)
Train: 67 [ 200/1251 ( 16%)]  Loss:  4.768574 (4.7767)  Time: 0.588s, 1742.05/s  (2.469s,  414.83/s)  LR: 1.427e-04  Data: 0.023 (1.863)
Train: 67 [ 250/1251 ( 20%)]  Loss:  4.272738 (4.6927)  Time: 0.586s, 1748.65/s  (2.464s,  415.58/s)  LR: 1.427e-04  Data: 0.023 (1.860)
Train: 67 [ 300/1251 ( 24%)]  Loss:  5.418820 (4.7964)  Time: 3.021s,  338.97/s  (2.474s,  413.97/s)  LR: 1.427e-04  Data: 2.443 (1.871)
Train: 67 [ 350/1251 ( 28%)]  Loss:  4.492559 (4.7584)  Time: 0.584s, 1753.77/s  (2.453s,  417.45/s)  LR: 1.427e-04  Data: 0.021 (1.848)
Train: 67 [ 400/1251 ( 32%)]  Loss:  4.850210 (4.7686)  Time: 6.119s,  167.35/s  (2.448s,  418.28/s)  LR: 1.427e-04  Data: 5.471 (1.843)
Train: 67 [ 450/1251 ( 36%)]  Loss:  5.202125 (4.8120)  Time: 0.582s, 1758.46/s  (2.426s,  422.16/s)  LR: 1.427e-04  Data: 0.020 (1.821)
Train: 67 [ 500/1251 ( 40%)]  Loss:  4.534692 (4.7868)  Time: 6.629s,  154.47/s  (2.412s,  424.53/s)  LR: 1.427e-04  Data: 6.064 (1.810)
Train: 67 [ 550/1251 ( 44%)]  Loss:  5.038897 (4.8078)  Time: 0.765s, 1339.34/s  (2.436s,  420.40/s)  LR: 1.427e-04  Data: 0.017 (1.835)
Train: 67 [ 600/1251 ( 48%)]  Loss:  4.966730 (4.8200)  Time: 8.798s,  116.39/s  (2.456s,  416.89/s)  LR: 1.427e-04  Data: 8.219 (1.857)
Train: 67 [ 650/1251 ( 52%)]  Loss:  5.152953 (4.8438)  Time: 0.585s, 1751.60/s  (2.459s,  416.49/s)  LR: 1.427e-04  Data: 0.020 (1.860)
Train: 67 [ 700/1251 ( 56%)]  Loss:  4.623873 (4.8291)  Time: 6.631s,  154.42/s  (2.462s,  415.90/s)  LR: 1.427e-04  Data: 6.051 (1.864)
Train: 67 [ 750/1251 ( 60%)]  Loss:  4.997313 (4.8396)  Time: 0.584s, 1754.68/s  (2.450s,  417.94/s)  LR: 1.427e-04  Data: 0.021 (1.851)
Train: 67 [ 800/1251 ( 64%)]  Loss:  5.381921 (4.8715)  Time: 1.116s,  917.92/s  (2.446s,  418.68/s)  LR: 1.427e-04  Data: 0.514 (1.846)
Train: 67 [ 850/1251 ( 68%)]  Loss:  4.928654 (4.8747)  Time: 0.584s, 1753.60/s  (2.433s,  420.92/s)  LR: 1.427e-04  Data: 0.018 (1.832)
Train: 67 [ 900/1251 ( 72%)]  Loss:  5.028726 (4.8828)  Time: 6.242s,  164.05/s  (2.447s,  418.44/s)  LR: 1.427e-04  Data: 5.679 (1.846)
Train: 67 [ 950/1251 ( 76%)]  Loss:  4.598404 (4.8686)  Time: 0.586s, 1748.06/s  (2.455s,  417.17/s)  LR: 1.427e-04  Data: 0.020 (1.855)
Train: 67 [1000/1251 ( 80%)]  Loss:  4.852017 (4.8678)  Time: 7.148s,  143.27/s  (2.458s,  416.67/s)  LR: 1.427e-04  Data: 6.477 (1.858)
Train: 67 [1050/1251 ( 84%)]  Loss:  5.226028 (4.8841)  Time: 0.584s, 1753.29/s  (2.454s,  417.22/s)  LR: 1.427e-04  Data: 0.021 (1.855)
Train: 67 [1100/1251 ( 88%)]  Loss:  5.310110 (4.9026)  Time: 7.738s,  132.33/s  (2.454s,  417.32/s)  LR: 1.427e-04  Data: 7.161 (1.854)
Train: 67 [1150/1251 ( 92%)]  Loss:  5.329119 (4.9204)  Time: 0.584s, 1752.09/s  (2.444s,  419.04/s)  LR: 1.427e-04  Data: 0.022 (1.845)
Train: 67 [1200/1251 ( 96%)]  Loss:  4.820016 (4.9164)  Time: 4.160s,  246.15/s  (2.442s,  419.33/s)  LR: 1.427e-04  Data: 3.598 (1.843)
Train: 67 [1250/1251 (100%)]  Loss:  5.088045 (4.9230)  Time: 0.567s, 1807.47/s  (2.445s,  418.84/s)  LR: 1.427e-04  Data: 0.000 (1.846)
Test: [   0/48]  Time: 16.115 (16.115)  Loss:  1.3456 (1.3456)  Acc@1: 72.7539 (72.7539)  Acc@5: 89.6484 (89.6484)
Test: [  48/48]  Time: 0.149 (3.921)  Loss:  1.2586 (2.2496)  Acc@1: 73.8208 (51.5540)  Acc@5: 89.2689 (76.2200)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-67.pth.tar', 51.55400000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-66.pth.tar', 51.330000107421874)

Train: 68 [   0/1251 (  0%)]  Loss:  5.054840 (5.0548)  Time: 13.950s,   73.40/s  (13.950s,   73.40/s)  LR: 1.309e-04  Data: 13.295 (13.295)
Train: 68 [  50/1251 (  4%)]  Loss:  4.348091 (4.7015)  Time: 1.135s,  901.87/s  (2.848s,  359.52/s)  LR: 1.309e-04  Data: 0.557 (2.243)
Train: 68 [ 100/1251 (  8%)]  Loss:  4.452640 (4.6185)  Time: 0.584s, 1753.15/s  (2.748s,  372.63/s)  LR: 1.309e-04  Data: 0.021 (2.141)
Train: 68 [ 150/1251 ( 12%)]  Loss:  5.164524 (4.7550)  Time: 5.085s,  201.37/s  (2.650s,  386.40/s)  LR: 1.309e-04  Data: 4.512 (2.041)
Train: 68 [ 200/1251 ( 16%)]  Loss:  5.138486 (4.8317)  Time: 0.586s, 1746.61/s  (2.538s,  403.46/s)  LR: 1.309e-04  Data: 0.021 (1.929)
Train: 68 [ 250/1251 ( 20%)]  Loss:  5.145805 (4.8841)  Time: 0.584s, 1754.22/s  (2.476s,  413.54/s)  LR: 1.309e-04  Data: 0.020 (1.871)
Train: 68 [ 300/1251 ( 24%)]  Loss:  5.007199 (4.9017)  Time: 1.861s,  550.34/s  (2.489s,  411.43/s)  LR: 1.309e-04  Data: 1.195 (1.883)
Train: 68 [ 350/1251 ( 28%)]  Loss:  4.857588 (4.8961)  Time: 2.257s,  453.75/s  (2.482s,  412.63/s)  LR: 1.309e-04  Data: 1.610 (1.876)
Train: 68 [ 400/1251 ( 32%)]  Loss:  4.898461 (4.8964)  Time: 6.687s,  153.14/s  (2.491s,  411.14/s)  LR: 1.309e-04  Data: 6.107 (1.886)
Train: 68 [ 450/1251 ( 36%)]  Loss:  4.845433 (4.8913)  Time: 0.586s, 1748.23/s  (2.468s,  414.97/s)  LR: 1.309e-04  Data: 0.017 (1.864)
Train: 68 [ 500/1251 ( 40%)]  Loss:  4.497316 (4.8555)  Time: 1.988s,  514.99/s  (2.455s,  417.06/s)  LR: 1.309e-04  Data: 1.321 (1.851)
Train: 68 [ 550/1251 ( 44%)]  Loss:  4.438540 (4.8207)  Time: 0.586s, 1746.58/s  (2.433s,  420.85/s)  LR: 1.309e-04  Data: 0.020 (1.828)
Train: 68 [ 600/1251 ( 48%)]  Loss:  4.440820 (4.7915)  Time: 5.902s,  173.49/s  (2.427s,  421.86/s)  LR: 1.309e-04  Data: 5.233 (1.823)
Train: 68 [ 650/1251 ( 52%)]  Loss:  4.078784 (4.7406)  Time: 0.587s, 1745.58/s  (2.412s,  424.52/s)  LR: 1.309e-04  Data: 0.020 (1.808)
Train: 68 [ 700/1251 ( 56%)]  Loss:  4.601715 (4.7313)  Time: 5.410s,  189.26/s  (2.441s,  419.45/s)  LR: 1.309e-04  Data: 4.845 (1.837)
Train: 68 [ 750/1251 ( 60%)]  Loss:  5.246261 (4.7635)  Time: 0.588s, 1740.21/s  (2.446s,  418.73/s)  LR: 1.309e-04  Data: 0.021 (1.841)
Train: 68 [ 800/1251 ( 64%)]  Loss:  4.654755 (4.7571)  Time: 5.516s,  185.63/s  (2.451s,  417.85/s)  LR: 1.309e-04  Data: 4.914 (1.847)
Train: 68 [ 850/1251 ( 68%)]  Loss:  3.902439 (4.7096)  Time: 0.589s, 1739.17/s  (2.441s,  419.47/s)  LR: 1.309e-04  Data: 0.022 (1.837)
Train: 68 [ 900/1251 ( 72%)]  Loss:  4.890886 (4.7192)  Time: 3.863s,  265.11/s  (2.438s,  420.04/s)  LR: 1.309e-04  Data: 3.196 (1.834)
Train: 68 [ 950/1251 ( 76%)]  Loss:  5.002026 (4.7333)  Time: 0.584s, 1753.22/s  (2.427s,  421.91/s)  LR: 1.309e-04  Data: 0.021 (1.824)
Train: 68 [1000/1251 ( 80%)]  Loss:  4.628007 (4.7283)  Time: 4.791s,  213.73/s  (2.421s,  423.05/s)  LR: 1.309e-04  Data: 4.124 (1.818)
Train: 68 [1050/1251 ( 84%)]  Loss:  4.519039 (4.7188)  Time: 0.592s, 1730.01/s  (2.420s,  423.13/s)  LR: 1.309e-04  Data: 0.023 (1.817)
Train: 68 [1100/1251 ( 88%)]  Loss:  4.983870 (4.7303)  Time: 8.746s,  117.08/s  (2.432s,  421.04/s)  LR: 1.309e-04  Data: 8.075 (1.831)
Train: 68 [1150/1251 ( 92%)]  Loss:  4.329473 (4.7136)  Time: 0.588s, 1741.21/s  (2.432s,  421.08/s)  LR: 1.309e-04  Data: 0.023 (1.831)
Train: 68 [1200/1251 ( 96%)]  Loss:  4.644474 (4.7109)  Time: 7.525s,  136.08/s  (2.432s,  420.99/s)  LR: 1.309e-04  Data: 6.884 (1.831)
Train: 68 [1250/1251 (100%)]  Loss:  4.787476 (4.7138)  Time: 0.567s, 1806.07/s  (2.424s,  422.38/s)  LR: 1.309e-04  Data: 0.000 (1.824)
Test: [   0/48]  Time: 14.928 (14.928)  Loss:  1.3580 (1.3580)  Acc@1: 72.6562 (72.6562)  Acc@5: 89.2578 (89.2578)
Test: [  48/48]  Time: 0.149 (3.286)  Loss:  1.3155 (2.2415)  Acc@1: 73.3491 (51.8340)  Acc@5: 88.6792 (76.3060)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-68.pth.tar', 51.83400005859375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-67.pth.tar', 51.55400000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-66.pth.tar', 51.330000107421874)

Train: 69 [   0/1251 (  0%)]  Loss:  4.765340 (4.7653)  Time: 10.088s,  101.51/s  (10.088s,  101.51/s)  LR: 1.196e-04  Data: 9.461 (9.461)
Train: 69 [  50/1251 (  4%)]  Loss:  4.549317 (4.6573)  Time: 0.587s, 1743.12/s  (2.284s,  448.40/s)  LR: 1.196e-04  Data: 0.022 (1.691)
Train: 69 [ 100/1251 (  8%)]  Loss:  4.918030 (4.7442)  Time: 6.887s,  148.69/s  (2.369s,  432.30/s)  LR: 1.196e-04  Data: 6.230 (1.777)
Train: 69 [ 150/1251 ( 12%)]  Loss:  4.910434 (4.7858)  Time: 0.588s, 1741.14/s  (2.392s,  428.06/s)  LR: 1.196e-04  Data: 0.021 (1.803)
Train: 69 [ 200/1251 ( 16%)]  Loss:  4.655701 (4.7598)  Time: 7.707s,  132.87/s  (2.407s,  425.36/s)  LR: 1.196e-04  Data: 7.064 (1.821)
Train: 69 [ 250/1251 ( 20%)]  Loss:  4.986446 (4.7975)  Time: 0.589s, 1739.02/s  (2.369s,  432.27/s)  LR: 1.196e-04  Data: 0.021 (1.781)
Train: 69 [ 300/1251 ( 24%)]  Loss:  4.833512 (4.8027)  Time: 7.145s,  143.31/s  (2.363s,  433.39/s)  LR: 1.196e-04  Data: 6.488 (1.776)
Train: 69 [ 350/1251 ( 28%)]  Loss:  4.961985 (4.8226)  Time: 0.586s, 1748.09/s  (2.329s,  439.60/s)  LR: 1.196e-04  Data: 0.021 (1.740)
Train: 69 [ 400/1251 ( 32%)]  Loss:  4.664192 (4.8050)  Time: 7.169s,  142.84/s  (2.316s,  442.08/s)  LR: 1.196e-04  Data: 6.423 (1.728)
Train: 69 [ 450/1251 ( 36%)]  Loss:  4.279844 (4.7525)  Time: 0.585s, 1749.29/s  (2.302s,  444.89/s)  LR: 1.196e-04  Data: 0.023 (1.714)
Train: 69 [ 500/1251 ( 40%)]  Loss:  5.361992 (4.8079)  Time: 7.544s,  135.74/s  (2.340s,  437.70/s)  LR: 1.196e-04  Data: 6.891 (1.749)
Train: 69 [ 550/1251 ( 44%)]  Loss:  5.050051 (4.8281)  Time: 0.596s, 1718.58/s  (2.355s,  434.74/s)  LR: 1.196e-04  Data: 0.022 (1.765)
Train: 69 [ 600/1251 ( 48%)]  Loss:  5.129728 (4.8513)  Time: 7.179s,  142.65/s  (2.373s,  431.52/s)  LR: 1.196e-04  Data: 6.580 (1.783)
Train: 69 [ 650/1251 ( 52%)]  Loss:  5.045033 (4.8651)  Time: 0.587s, 1743.10/s  (2.375s,  431.20/s)  LR: 1.196e-04  Data: 0.021 (1.786)
Train: 69 [ 700/1251 ( 56%)]  Loss:  5.188830 (4.8867)  Time: 7.478s,  136.93/s  (2.381s,  430.05/s)  LR: 1.196e-04  Data: 6.815 (1.792)
Train: 69 [ 750/1251 ( 60%)]  Loss:  5.077216 (4.8986)  Time: 0.589s, 1737.11/s  (2.376s,  430.93/s)  LR: 1.196e-04  Data: 0.024 (1.787)
Train: 69 [ 800/1251 ( 64%)]  Loss:  4.942228 (4.9012)  Time: 7.457s,  137.32/s  (2.372s,  431.67/s)  LR: 1.196e-04  Data: 6.803 (1.782)
Train: 69 [ 850/1251 ( 68%)]  Loss:  4.275849 (4.8664)  Time: 0.586s, 1747.23/s  (2.379s,  430.47/s)  LR: 1.196e-04  Data: 0.020 (1.789)
Train: 69 [ 900/1251 ( 72%)]  Loss:  4.329733 (4.8382)  Time: 3.450s,  296.84/s  (2.397s,  427.25/s)  LR: 1.196e-04  Data: 2.766 (1.806)
Train: 69 [ 950/1251 ( 76%)]  Loss:  4.518713 (4.8222)  Time: 0.585s, 1749.23/s  (2.394s,  427.73/s)  LR: 1.196e-04  Data: 0.020 (1.804)
Train: 69 [1000/1251 ( 80%)]  Loss:  4.402737 (4.8022)  Time: 0.583s, 1755.63/s  (2.399s,  426.84/s)  LR: 1.196e-04  Data: 0.020 (1.808)
Train: 69 [1050/1251 ( 84%)]  Loss:  5.076939 (4.8147)  Time: 0.584s, 1754.04/s  (2.396s,  427.35/s)  LR: 1.196e-04  Data: 0.021 (1.805)
Train: 69 [1100/1251 ( 88%)]  Loss:  4.919209 (4.8193)  Time: 5.711s,  179.30/s  (2.397s,  427.14/s)  LR: 1.196e-04  Data: 5.049 (1.806)
Train: 69 [1150/1251 ( 92%)]  Loss:  4.924386 (4.8236)  Time: 0.590s, 1735.10/s  (2.387s,  429.04/s)  LR: 1.196e-04  Data: 0.021 (1.795)
Train: 69 [1200/1251 ( 96%)]  Loss:  4.433517 (4.8080)  Time: 6.859s,  149.30/s  (2.383s,  429.67/s)  LR: 1.196e-04  Data: 6.273 (1.791)
Train: 69 [1250/1251 (100%)]  Loss:  4.624435 (4.8010)  Time: 0.563s, 1819.95/s  (2.393s,  427.91/s)  LR: 1.196e-04  Data: 0.000 (1.799)
Test: [   0/48]  Time: 15.172 (15.172)  Loss:  1.2817 (1.2817)  Acc@1: 74.1211 (74.1211)  Acc@5: 90.6250 (90.6250)
Test: [  48/48]  Time: 0.149 (3.523)  Loss:  1.2176 (2.2225)  Acc@1: 74.6462 (52.1820)  Acc@5: 89.6226 (76.6800)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-69.pth.tar', 52.18200010498047)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-68.pth.tar', 51.83400005859375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-67.pth.tar', 51.55400000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-66.pth.tar', 51.330000107421874)

Train: 70 [   0/1251 (  0%)]  Loss:  4.685768 (4.6858)  Time: 13.697s,   74.76/s  (13.697s,   74.76/s)  LR: 1.087e-04  Data: 12.077 (12.077)
Train: 70 [  50/1251 (  4%)]  Loss:  4.489792 (4.5878)  Time: 0.584s, 1753.17/s  (2.570s,  398.48/s)  LR: 1.087e-04  Data: 0.019 (1.962)
Train: 70 [ 100/1251 (  8%)]  Loss:  5.109801 (4.7618)  Time: 2.716s,  377.00/s  (2.488s,  411.63/s)  LR: 1.087e-04  Data: 1.947 (1.879)
Train: 70 [ 150/1251 ( 12%)]  Loss:  4.077374 (4.5907)  Time: 0.747s, 1371.66/s  (2.387s,  429.00/s)  LR: 1.087e-04  Data: 0.175 (1.772)
Train: 70 [ 200/1251 ( 16%)]  Loss:  5.120131 (4.6966)  Time: 0.585s, 1750.43/s  (2.361s,  433.64/s)  LR: 1.087e-04  Data: 0.023 (1.750)
Train: 70 [ 250/1251 ( 20%)]  Loss:  5.197013 (4.7800)  Time: 1.808s,  566.23/s  (2.326s,  440.26/s)  LR: 1.087e-04  Data: 1.087 (1.718)
Train: 70 [ 300/1251 ( 24%)]  Loss:  4.916819 (4.7995)  Time: 0.583s, 1755.19/s  (2.395s,  427.56/s)  LR: 1.087e-04  Data: 0.020 (1.787)
Train: 70 [ 350/1251 ( 28%)]  Loss:  4.458651 (4.7569)  Time: 0.587s, 1744.86/s  (2.403s,  426.20/s)  LR: 1.087e-04  Data: 0.019 (1.797)
Train: 70 [ 400/1251 ( 32%)]  Loss:  5.443464 (4.8332)  Time: 0.584s, 1754.51/s  (2.412s,  424.52/s)  LR: 1.087e-04  Data: 0.021 (1.806)
Train: 70 [ 450/1251 ( 36%)]  Loss:  4.454244 (4.7953)  Time: 0.584s, 1753.76/s  (2.413s,  424.40/s)  LR: 1.087e-04  Data: 0.018 (1.806)
Train: 70 [ 500/1251 ( 40%)]  Loss:  4.508973 (4.7693)  Time: 0.585s, 1750.01/s  (2.394s,  427.79/s)  LR: 1.087e-04  Data: 0.022 (1.786)
Train: 70 [ 550/1251 ( 44%)]  Loss:  4.406187 (4.7390)  Time: 1.649s,  621.01/s  (2.391s,  428.21/s)  LR: 1.087e-04  Data: 1.012 (1.781)
Train: 70 [ 600/1251 ( 48%)]  Loss:  5.106810 (4.7673)  Time: 0.582s, 1760.06/s  (2.382s,  429.92/s)  LR: 1.087e-04  Data: 0.018 (1.771)
Train: 70 [ 650/1251 ( 52%)]  Loss:  4.547071 (4.7516)  Time: 2.575s,  397.65/s  (2.407s,  425.40/s)  LR: 1.087e-04  Data: 2.013 (1.796)
Train: 70 [ 700/1251 ( 56%)]  Loss:  4.359290 (4.7254)  Time: 0.582s, 1759.80/s  (2.417s,  423.63/s)  LR: 1.087e-04  Data: 0.017 (1.804)
Train: 70 [ 750/1251 ( 60%)]  Loss:  4.672428 (4.7221)  Time: 1.923s,  532.47/s  (2.428s,  421.73/s)  LR: 1.087e-04  Data: 1.284 (1.816)
Train: 70 [ 800/1251 ( 64%)]  Loss:  4.843201 (4.7292)  Time: 0.586s, 1746.26/s  (2.431s,  421.24/s)  LR: 1.087e-04  Data: 0.018 (1.818)
Train: 70 [ 850/1251 ( 68%)]  Loss:  4.725024 (4.7290)  Time: 3.390s,  302.06/s  (2.428s,  421.81/s)  LR: 1.087e-04  Data: 2.709 (1.814)
Train: 70 [ 900/1251 ( 72%)]  Loss:  4.442373 (4.7139)  Time: 0.582s, 1760.59/s  (2.422s,  422.71/s)  LR: 1.087e-04  Data: 0.017 (1.810)
Train: 70 [ 950/1251 ( 76%)]  Loss:  4.884028 (4.7224)  Time: 2.687s,  381.03/s  (2.416s,  423.82/s)  LR: 1.087e-04  Data: 2.125 (1.804)
Train: 70 [1000/1251 ( 80%)]  Loss:  5.022262 (4.7367)  Time: 0.591s, 1731.22/s  (2.413s,  424.29/s)  LR: 1.087e-04  Data: 0.028 (1.800)
Train: 70 [1050/1251 ( 84%)]  Loss:  4.421187 (4.7224)  Time: 0.587s, 1744.29/s  (2.426s,  422.14/s)  LR: 1.087e-04  Data: 0.019 (1.813)
Train: 70 [1100/1251 ( 88%)]  Loss:  4.544199 (4.7146)  Time: 0.586s, 1747.23/s  (2.433s,  420.86/s)  LR: 1.087e-04  Data: 0.019 (1.821)
Train: 70 [1150/1251 ( 92%)]  Loss:  5.310520 (4.7394)  Time: 5.950s,  172.09/s  (2.438s,  419.96/s)  LR: 1.087e-04  Data: 5.318 (1.826)
Train: 70 [1200/1251 ( 96%)]  Loss:  5.325542 (4.7629)  Time: 0.586s, 1746.50/s  (2.434s,  420.72/s)  LR: 1.087e-04  Data: 0.018 (1.823)
Train: 70 [1250/1251 (100%)]  Loss:  4.842962 (4.7660)  Time: 0.562s, 1821.94/s  (2.426s,  422.02/s)  LR: 1.087e-04  Data: 0.000 (1.816)
Test: [   0/48]  Time: 14.851 (14.851)  Loss:  1.2468 (1.2468)  Acc@1: 75.0977 (75.0977)  Acc@5: 91.3086 (91.3086)
Test: [  48/48]  Time: 0.148 (3.332)  Loss:  1.2185 (2.2071)  Acc@1: 74.5283 (52.1380)  Acc@5: 88.7972 (76.9060)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-69.pth.tar', 52.18200010498047)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-70.pth.tar', 52.137999924316404)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-68.pth.tar', 51.83400005859375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-67.pth.tar', 51.55400000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-66.pth.tar', 51.330000107421874)

Train: 71 [   0/1251 (  0%)]  Loss:  4.813636 (4.8136)  Time: 10.415s,   98.32/s  (10.415s,   98.32/s)  LR: 9.840e-05  Data: 9.734 (9.734)
Train: 71 [  50/1251 (  4%)]  Loss:  5.229833 (5.0217)  Time: 0.582s, 1760.64/s  (2.469s,  414.75/s)  LR: 9.840e-05  Data: 0.019 (1.869)
Train: 71 [ 100/1251 (  8%)]  Loss:  4.424743 (4.8227)  Time: 0.852s, 1201.96/s  (2.589s,  395.48/s)  LR: 9.840e-05  Data: 0.175 (1.985)
Train: 71 [ 150/1251 ( 12%)]  Loss:  4.710579 (4.7947)  Time: 0.586s, 1746.43/s  (2.549s,  401.71/s)  LR: 9.840e-05  Data: 0.023 (1.950)
Train: 71 [ 200/1251 ( 16%)]  Loss:  4.742430 (4.7842)  Time: 0.584s, 1753.21/s  (2.521s,  406.26/s)  LR: 9.840e-05  Data: 0.022 (1.924)
Train: 71 [ 250/1251 ( 20%)]  Loss:  4.876320 (4.7996)  Time: 0.635s, 1613.21/s  (2.477s,  413.46/s)  LR: 9.840e-05  Data: 0.019 (1.877)
Train: 71 [ 300/1251 ( 24%)]  Loss:  4.587631 (4.7693)  Time: 2.901s,  352.99/s  (2.465s,  415.35/s)  LR: 9.840e-05  Data: 2.255 (1.862)
Train: 71 [ 350/1251 ( 28%)]  Loss:  4.451788 (4.7296)  Time: 0.586s, 1748.37/s  (2.431s,  421.25/s)  LR: 9.840e-05  Data: 0.019 (1.830)
Train: 71 [ 400/1251 ( 32%)]  Loss:  5.171134 (4.7787)  Time: 3.808s,  268.90/s  (2.412s,  424.54/s)  LR: 9.840e-05  Data: 3.104 (1.810)
Train: 71 [ 450/1251 ( 36%)]  Loss:  5.159949 (4.8168)  Time: 0.583s, 1757.06/s  (2.431s,  421.27/s)  LR: 9.840e-05  Data: 0.018 (1.828)
Train: 71 [ 500/1251 ( 40%)]  Loss:  4.549712 (4.7925)  Time: 5.877s,  174.24/s  (2.455s,  417.04/s)  LR: 9.840e-05  Data: 5.198 (1.851)
Train: 71 [ 550/1251 ( 44%)]  Loss:  4.945457 (4.8053)  Time: 0.583s, 1757.24/s  (2.450s,  417.88/s)  LR: 9.840e-05  Data: 0.018 (1.845)
Train: 71 [ 600/1251 ( 48%)]  Loss:  5.130805 (4.8303)  Time: 5.231s,  195.75/s  (2.465s,  415.43/s)  LR: 9.840e-05  Data: 4.592 (1.860)
Train: 71 [ 650/1251 ( 52%)]  Loss:  5.049007 (4.8459)  Time: 0.585s, 1751.69/s  (2.461s,  416.07/s)  LR: 9.840e-05  Data: 0.019 (1.857)
Train: 71 [ 700/1251 ( 56%)]  Loss:  5.094293 (4.8625)  Time: 4.028s,  254.23/s  (2.462s,  415.84/s)  LR: 9.840e-05  Data: 3.355 (1.858)
Train: 71 [ 750/1251 ( 60%)]  Loss:  5.017741 (4.8722)  Time: 0.582s, 1759.23/s  (2.451s,  417.82/s)  LR: 9.840e-05  Data: 0.020 (1.846)
Train: 71 [ 800/1251 ( 64%)]  Loss:  4.918594 (4.8749)  Time: 6.050s,  169.27/s  (2.470s,  414.65/s)  LR: 9.840e-05  Data: 5.472 (1.865)
Train: 71 [ 850/1251 ( 68%)]  Loss:  4.874974 (4.8749)  Time: 0.590s, 1734.24/s  (2.475s,  413.77/s)  LR: 9.840e-05  Data: 0.021 (1.870)
Train: 71 [ 900/1251 ( 72%)]  Loss:  4.860640 (4.8742)  Time: 4.026s,  254.36/s  (2.476s,  413.49/s)  LR: 9.840e-05  Data: 3.462 (1.870)
Train: 71 [ 950/1251 ( 76%)]  Loss:  4.850327 (4.8730)  Time: 0.584s, 1753.38/s  (2.476s,  413.59/s)  LR: 9.840e-05  Data: 0.021 (1.868)
Train: 71 [1000/1251 ( 80%)]  Loss:  4.123247 (4.8373)  Time: 3.425s,  299.02/s  (2.476s,  413.59/s)  LR: 9.840e-05  Data: 2.544 (1.868)
Train: 71 [1050/1251 ( 84%)]  Loss:  4.403613 (4.8176)  Time: 0.756s, 1353.94/s  (2.467s,  415.06/s)  LR: 9.840e-05  Data: 0.156 (1.859)
Train: 71 [1100/1251 ( 88%)]  Loss:  5.197715 (4.8341)  Time: 2.685s,  381.31/s  (2.462s,  415.86/s)  LR: 9.840e-05  Data: 2.081 (1.855)
Train: 71 [1150/1251 ( 92%)]  Loss:  4.686092 (4.8279)  Time: 1.702s,  601.70/s  (2.467s,  415.10/s)  LR: 9.840e-05  Data: 1.025 (1.857)
Train: 71 [1200/1251 ( 96%)]  Loss:  4.646554 (4.8207)  Time: 2.100s,  487.53/s  (2.473s,  414.12/s)  LR: 9.840e-05  Data: 1.510 (1.863)
Train: 71 [1250/1251 (100%)]  Loss:  4.611111 (4.8126)  Time: 0.562s, 1821.74/s  (2.472s,  414.29/s)  LR: 9.840e-05  Data: 0.000 (1.862)
Test: [   0/48]  Time: 16.051 (16.051)  Loss:  1.2566 (1.2566)  Acc@1: 74.7070 (74.7070)  Acc@5: 90.8203 (90.8203)
Test: [  48/48]  Time: 0.148 (3.582)  Loss:  1.2368 (2.1977)  Acc@1: 74.0566 (52.6080)  Acc@5: 89.2689 (77.1280)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-71.pth.tar', 52.60800010742187)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-69.pth.tar', 52.18200010498047)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-70.pth.tar', 52.137999924316404)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-68.pth.tar', 51.83400005859375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-67.pth.tar', 51.55400000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-66.pth.tar', 51.330000107421874)

Train: 72 [   0/1251 (  0%)]  Loss:  4.577348 (4.5773)  Time: 12.966s,   78.98/s  (12.966s,   78.98/s)  LR: 8.858e-05  Data: 11.682 (11.682)
Train: 72 [  50/1251 (  4%)]  Loss:  4.821450 (4.6994)  Time: 0.584s, 1753.48/s  (2.504s,  408.95/s)  LR: 8.858e-05  Data: 0.017 (1.898)
Train: 72 [ 100/1251 (  8%)]  Loss:  4.666141 (4.6883)  Time: 0.582s, 1760.14/s  (2.399s,  426.86/s)  LR: 8.858e-05  Data: 0.016 (1.792)
Train: 72 [ 150/1251 ( 12%)]  Loss:  4.904675 (4.7424)  Time: 0.579s, 1768.88/s  (2.334s,  438.70/s)  LR: 8.858e-05  Data: 0.017 (1.734)
Train: 72 [ 200/1251 ( 16%)]  Loss:  5.076033 (4.8091)  Time: 0.883s, 1159.79/s  (2.393s,  427.84/s)  LR: 8.858e-05  Data: 0.200 (1.792)
Train: 72 [ 250/1251 ( 20%)]  Loss:  4.941857 (4.8313)  Time: 0.585s, 1749.57/s  (2.409s,  425.11/s)  LR: 8.858e-05  Data: 0.022 (1.805)
Train: 72 [ 300/1251 ( 24%)]  Loss:  5.058197 (4.8637)  Time: 0.783s, 1307.82/s  (2.414s,  424.27/s)  LR: 8.858e-05  Data: 0.167 (1.805)
Train: 72 [ 350/1251 ( 28%)]  Loss:  4.794303 (4.8550)  Time: 0.587s, 1745.86/s  (2.421s,  422.89/s)  LR: 8.858e-05  Data: 0.019 (1.810)
Train: 72 [ 400/1251 ( 32%)]  Loss:  4.653426 (4.8326)  Time: 4.279s,  239.33/s  (2.415s,  424.00/s)  LR: 8.858e-05  Data: 3.672 (1.802)
Train: 72 [ 450/1251 ( 36%)]  Loss:  5.017444 (4.8511)  Time: 0.582s, 1758.13/s  (2.390s,  428.44/s)  LR: 8.858e-05  Data: 0.019 (1.777)
Train: 72 [ 500/1251 ( 40%)]  Loss:  4.604936 (4.8287)  Time: 0.587s, 1743.59/s  (2.382s,  429.82/s)  LR: 8.858e-05  Data: 0.020 (1.768)
Train: 72 [ 550/1251 ( 44%)]  Loss:  4.393947 (4.7925)  Time: 0.746s, 1372.04/s  (2.368s,  432.37/s)  LR: 8.858e-05  Data: 0.024 (1.756)
Train: 72 [ 600/1251 ( 48%)]  Loss:  5.255569 (4.8281)  Time: 1.081s,  947.67/s  (2.393s,  427.97/s)  LR: 8.858e-05  Data: 0.404 (1.780)
Train: 72 [ 650/1251 ( 52%)]  Loss:  4.552470 (4.8084)  Time: 0.584s, 1752.48/s  (2.416s,  423.86/s)  LR: 8.858e-05  Data: 0.020 (1.802)
Train: 72 [ 700/1251 ( 56%)]  Loss:  5.043187 (4.8241)  Time: 3.052s,  335.56/s  (2.435s,  420.56/s)  LR: 8.858e-05  Data: 2.376 (1.822)
Train: 72 [ 750/1251 ( 60%)]  Loss:  4.068579 (4.7768)  Time: 0.614s, 1668.57/s  (2.432s,  421.12/s)  LR: 8.858e-05  Data: 0.034 (1.820)
Train: 72 [ 800/1251 ( 64%)]  Loss:  4.437892 (4.7569)  Time: 5.617s,  182.31/s  (2.435s,  420.58/s)  LR: 8.858e-05  Data: 4.974 (1.822)
Train: 72 [ 850/1251 ( 68%)]  Loss:  4.303636 (4.7317)  Time: 3.357s,  305.03/s  (2.426s,  422.04/s)  LR: 8.858e-05  Data: 2.756 (1.815)
Train: 72 [ 900/1251 ( 72%)]  Loss:  5.326313 (4.7630)  Time: 3.348s,  305.83/s  (2.420s,  423.06/s)  LR: 8.858e-05  Data: 2.691 (1.809)
Train: 72 [ 950/1251 ( 76%)]  Loss:  4.832947 (4.7665)  Time: 5.949s,  172.13/s  (2.428s,  421.80/s)  LR: 8.858e-05  Data: 5.257 (1.816)
Train: 72 [1000/1251 ( 80%)]  Loss:  4.772274 (4.7668)  Time: 1.245s,  822.77/s  (2.433s,  420.90/s)  LR: 8.858e-05  Data: 0.659 (1.820)
Train: 72 [1050/1251 ( 84%)]  Loss:  4.367022 (4.7486)  Time: 0.897s, 1141.05/s  (2.438s,  420.07/s)  LR: 8.858e-05  Data: 0.335 (1.825)
Train: 72 [1100/1251 ( 88%)]  Loss:  4.429727 (4.7348)  Time: 3.859s,  265.33/s  (2.439s,  419.89/s)  LR: 8.858e-05  Data: 3.278 (1.827)
Train: 72 [1150/1251 ( 92%)]  Loss:  3.976866 (4.7032)  Time: 0.584s, 1754.78/s  (2.436s,  420.42/s)  LR: 8.858e-05  Data: 0.019 (1.824)
Train: 72 [1200/1251 ( 96%)]  Loss:  5.276591 (4.7261)  Time: 7.250s,  141.25/s  (2.434s,  420.63/s)  LR: 8.858e-05  Data: 6.670 (1.823)
Train: 72 [1250/1251 (100%)]  Loss:  4.959684 (4.7351)  Time: 0.565s, 1811.46/s  (2.425s,  422.23/s)  LR: 8.858e-05  Data: 0.000 (1.815)
Test: [   0/48]  Time: 13.455 (13.455)  Loss:  1.2667 (1.2667)  Acc@1: 74.4141 (74.4141)  Acc@5: 90.2344 (90.2344)
Test: [  48/48]  Time: 0.150 (3.491)  Loss:  1.2273 (2.1838)  Acc@1: 74.4104 (52.9520)  Acc@5: 89.6227 (77.4200)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-72.pth.tar', 52.95200000244141)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-71.pth.tar', 52.60800010742187)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-69.pth.tar', 52.18200010498047)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-70.pth.tar', 52.137999924316404)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-68.pth.tar', 51.83400005859375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-67.pth.tar', 51.55400000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-66.pth.tar', 51.330000107421874)

Train: 73 [   0/1251 (  0%)]  Loss:  4.942190 (4.9422)  Time: 12.723s,   80.49/s  (12.723s,   80.49/s)  LR: 7.929e-05  Data: 11.917 (11.917)
Train: 73 [  50/1251 (  4%)]  Loss:  4.708444 (4.8253)  Time: 0.584s, 1753.10/s  (2.747s,  372.71/s)  LR: 7.929e-05  Data: 0.020 (2.128)
Train: 73 [ 100/1251 (  8%)]  Loss:  5.290902 (4.9805)  Time: 5.336s,  191.91/s  (2.627s,  389.76/s)  LR: 7.929e-05  Data: 4.665 (2.012)
Train: 73 [ 150/1251 ( 12%)]  Loss:  4.556506 (4.8745)  Time: 0.584s, 1754.02/s  (2.537s,  403.69/s)  LR: 7.929e-05  Data: 0.018 (1.931)
Train: 73 [ 200/1251 ( 16%)]  Loss:  4.932044 (4.8860)  Time: 7.390s,  138.56/s  (2.492s,  410.92/s)  LR: 7.929e-05  Data: 6.725 (1.890)
Train: 73 [ 250/1251 ( 20%)]  Loss:  4.534500 (4.8274)  Time: 0.585s, 1750.77/s  (2.451s,  417.84/s)  LR: 7.929e-05  Data: 0.019 (1.853)
Train: 73 [ 300/1251 ( 24%)]  Loss:  4.749616 (4.8163)  Time: 7.000s,  146.28/s  (2.436s,  420.42/s)  LR: 7.929e-05  Data: 6.438 (1.841)
Train: 73 [ 350/1251 ( 28%)]  Loss:  4.683164 (4.7997)  Time: 0.588s, 1742.98/s  (2.436s,  420.31/s)  LR: 7.929e-05  Data: 0.022 (1.838)
Train: 73 [ 400/1251 ( 32%)]  Loss:  4.977835 (4.8195)  Time: 7.961s,  128.63/s  (2.464s,  415.58/s)  LR: 7.929e-05  Data: 7.306 (1.865)
Train: 73 [ 450/1251 ( 36%)]  Loss:  5.129642 (4.8505)  Time: 0.586s, 1748.84/s  (2.463s,  415.77/s)  LR: 7.929e-05  Data: 0.021 (1.865)
Train: 73 [ 500/1251 ( 40%)]  Loss:  4.261991 (4.7970)  Time: 8.479s,  120.77/s  (2.469s,  414.74/s)  LR: 7.929e-05  Data: 7.916 (1.872)
Train: 73 [ 550/1251 ( 44%)]  Loss:  4.203902 (4.7476)  Time: 0.584s, 1752.75/s  (2.457s,  416.77/s)  LR: 7.929e-05  Data: 0.022 (1.861)
Train: 73 [ 600/1251 ( 48%)]  Loss:  4.436987 (4.7237)  Time: 8.901s,  115.05/s  (2.469s,  414.79/s)  LR: 7.929e-05  Data: 8.239 (1.873)
Train: 73 [ 650/1251 ( 52%)]  Loss:  4.721901 (4.7235)  Time: 0.586s, 1747.86/s  (2.458s,  416.68/s)  LR: 7.929e-05  Data: 0.023 (1.863)
Train: 73 [ 700/1251 ( 56%)]  Loss:  5.051541 (4.7454)  Time: 6.906s,  148.29/s  (2.455s,  417.04/s)  LR: 7.929e-05  Data: 6.242 (1.860)
Train: 73 [ 750/1251 ( 60%)]  Loss:  5.014449 (4.7622)  Time: 0.589s, 1739.94/s  (2.477s,  413.46/s)  LR: 7.929e-05  Data: 0.022 (1.882)
Train: 73 [ 800/1251 ( 64%)]  Loss:  4.713602 (4.7594)  Time: 7.565s,  135.36/s  (2.484s,  412.18/s)  LR: 7.929e-05  Data: 6.893 (1.889)
Train: 73 [ 850/1251 ( 68%)]  Loss:  4.975660 (4.7714)  Time: 0.587s, 1743.83/s  (2.482s,  412.61/s)  LR: 7.929e-05  Data: 0.019 (1.886)
Train: 73 [ 900/1251 ( 72%)]  Loss:  4.876319 (4.7769)  Time: 9.590s,  106.78/s  (2.486s,  411.84/s)  LR: 7.929e-05  Data: 8.996 (1.892)
Train: 73 [ 950/1251 ( 76%)]  Loss:  4.278056 (4.7520)  Time: 0.587s, 1745.63/s  (2.480s,  412.90/s)  LR: 7.929e-05  Data: 0.019 (1.885)
Train: 73 [1000/1251 ( 80%)]  Loss:  4.595247 (4.7445)  Time: 7.696s,  133.06/s  (2.478s,  413.16/s)  LR: 7.929e-05  Data: 7.094 (1.884)
Train: 73 [1050/1251 ( 84%)]  Loss:  4.997493 (4.7560)  Time: 0.584s, 1752.06/s  (2.469s,  414.68/s)  LR: 7.929e-05  Data: 0.021 (1.875)
Train: 73 [1100/1251 ( 88%)]  Loss:  4.656968 (4.7517)  Time: 8.465s,  120.96/s  (2.485s,  412.14/s)  LR: 7.929e-05  Data: 7.894 (1.891)
Train: 73 [1150/1251 ( 92%)]  Loss:  4.851259 (4.7558)  Time: 0.586s, 1747.58/s  (2.485s,  412.07/s)  LR: 7.929e-05  Data: 0.021 (1.891)
Train: 73 [1200/1251 ( 96%)]  Loss:  4.863963 (4.7602)  Time: 6.374s,  160.66/s  (2.490s,  411.18/s)  LR: 7.929e-05  Data: 5.812 (1.897)
Train: 73 [1250/1251 (100%)]  Loss:  5.290858 (4.7806)  Time: 0.563s, 1817.31/s  (2.488s,  411.51/s)  LR: 7.929e-05  Data: 0.000 (1.895)
Test: [   0/48]  Time: 13.636 (13.636)  Loss:  1.3011 (1.3011)  Acc@1: 74.2188 (74.2188)  Acc@5: 90.1367 (90.1367)
Test: [  48/48]  Time: 0.149 (3.267)  Loss:  1.2877 (2.2046)  Acc@1: 73.9387 (52.8040)  Acc@5: 88.2076 (77.3960)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-72.pth.tar', 52.95200000244141)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-73.pth.tar', 52.80400005615235)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-71.pth.tar', 52.60800010742187)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-69.pth.tar', 52.18200010498047)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-70.pth.tar', 52.137999924316404)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-68.pth.tar', 51.83400005859375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-67.pth.tar', 51.55400000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-66.pth.tar', 51.330000107421874)

Train: 74 [   0/1251 (  0%)]  Loss:  4.836756 (4.8368)  Time: 11.089s,   92.35/s  (11.089s,   92.35/s)  LR: 7.055e-05  Data: 10.170 (10.170)
Train: 74 [  50/1251 (  4%)]  Loss:  5.106034 (4.9714)  Time: 0.589s, 1739.39/s  (2.439s,  419.84/s)  LR: 7.055e-05  Data: 0.021 (1.815)
Train: 74 [ 100/1251 (  8%)]  Loss:  5.014274 (4.9857)  Time: 0.584s, 1753.26/s  (2.396s,  427.45/s)  LR: 7.055e-05  Data: 0.017 (1.785)
Train: 74 [ 150/1251 ( 12%)]  Loss:  4.607908 (4.8912)  Time: 0.589s, 1738.32/s  (2.534s,  404.09/s)  LR: 7.055e-05  Data: 0.020 (1.912)
Train: 74 [ 200/1251 ( 16%)]  Loss:  3.996869 (4.7124)  Time: 0.583s, 1755.42/s  (2.533s,  404.30/s)  LR: 7.055e-05  Data: 0.017 (1.907)
Train: 74 [ 250/1251 ( 20%)]  Loss:  5.101390 (4.7772)  Time: 0.588s, 1741.86/s  (2.513s,  407.56/s)  LR: 7.055e-05  Data: 0.021 (1.888)
Train: 74 [ 300/1251 ( 24%)]  Loss:  5.120675 (4.8263)  Time: 0.584s, 1753.39/s  (2.485s,  412.02/s)  LR: 7.055e-05  Data: 0.017 (1.864)
Train: 74 [ 350/1251 ( 28%)]  Loss:  4.861039 (4.8306)  Time: 0.589s, 1739.46/s  (2.465s,  415.50/s)  LR: 7.055e-05  Data: 0.020 (1.847)
Train: 74 [ 400/1251 ( 32%)]  Loss:  4.469501 (4.7905)  Time: 0.584s, 1754.29/s  (2.443s,  419.08/s)  LR: 7.055e-05  Data: 0.018 (1.826)
Train: 74 [ 450/1251 ( 36%)]  Loss:  4.801421 (4.7916)  Time: 0.588s, 1742.60/s  (2.424s,  422.46/s)  LR: 7.055e-05  Data: 0.020 (1.808)
Train: 74 [ 500/1251 ( 40%)]  Loss:  5.359693 (4.8432)  Time: 0.586s, 1748.82/s  (2.448s,  418.25/s)  LR: 7.055e-05  Data: 0.019 (1.831)
Train: 74 [ 550/1251 ( 44%)]  Loss:  5.000339 (4.8563)  Time: 0.586s, 1748.63/s  (2.467s,  415.10/s)  LR: 7.055e-05  Data: 0.017 (1.851)
Train: 74 [ 600/1251 ( 48%)]  Loss:  4.294057 (4.8131)  Time: 0.587s, 1745.29/s  (2.473s,  414.00/s)  LR: 7.055e-05  Data: 0.025 (1.858)
Train: 74 [ 650/1251 ( 52%)]  Loss:  4.609448 (4.7985)  Time: 0.589s, 1739.87/s  (2.477s,  413.43/s)  LR: 7.055e-05  Data: 0.023 (1.863)
Train: 74 [ 700/1251 ( 56%)]  Loss:  4.540327 (4.7813)  Time: 0.584s, 1753.72/s  (2.472s,  414.22/s)  LR: 7.055e-05  Data: 0.019 (1.860)
Train: 74 [ 750/1251 ( 60%)]  Loss:  4.432623 (4.7595)  Time: 0.587s, 1744.58/s  (2.473s,  414.14/s)  LR: 7.055e-05  Data: 0.022 (1.862)
Train: 74 [ 800/1251 ( 64%)]  Loss:  5.190496 (4.7849)  Time: 4.714s,  217.21/s  (2.464s,  415.51/s)  LR: 7.055e-05  Data: 4.152 (1.854)
Train: 74 [ 850/1251 ( 68%)]  Loss:  4.602001 (4.7747)  Time: 0.585s, 1749.52/s  (2.474s,  413.99/s)  LR: 7.055e-05  Data: 0.023 (1.862)
Train: 74 [ 900/1251 ( 72%)]  Loss:  4.935951 (4.7832)  Time: 8.300s,  123.38/s  (2.494s,  410.51/s)  LR: 7.055e-05  Data: 7.069 (1.883)
Train: 74 [ 950/1251 ( 76%)]  Loss:  4.357829 (4.7619)  Time: 0.584s, 1752.96/s  (2.496s,  410.32/s)  LR: 7.055e-05  Data: 0.020 (1.884)
Train: 74 [1000/1251 ( 80%)]  Loss:  4.703174 (4.7591)  Time: 7.914s,  129.39/s  (2.497s,  410.02/s)  LR: 7.055e-05  Data: 7.349 (1.887)
Train: 74 [1050/1251 ( 84%)]  Loss:  4.614409 (4.7526)  Time: 0.584s, 1754.03/s  (2.494s,  410.58/s)  LR: 7.055e-05  Data: 0.020 (1.884)
Train: 74 [1100/1251 ( 88%)]  Loss:  5.377425 (4.7797)  Time: 7.771s,  131.77/s  (2.492s,  411.00/s)  LR: 7.055e-05  Data: 7.208 (1.883)
Train: 74 [1150/1251 ( 92%)]  Loss:  4.663908 (4.7749)  Time: 0.586s, 1748.77/s  (2.481s,  412.65/s)  LR: 7.055e-05  Data: 0.020 (1.874)
Train: 74 [1200/1251 ( 96%)]  Loss:  5.154567 (4.7901)  Time: 8.678s,  118.00/s  (2.490s,  411.26/s)  LR: 7.055e-05  Data: 8.083 (1.881)
Train: 74 [1250/1251 (100%)]  Loss:  4.757023 (4.7888)  Time: 0.564s, 1815.81/s  (2.494s,  410.54/s)  LR: 7.055e-05  Data: 0.000 (1.886)
Test: [   0/48]  Time: 15.362 (15.362)  Loss:  1.2288 (1.2288)  Acc@1: 74.9023 (74.9023)  Acc@5: 91.3086 (91.3086)
Test: [  48/48]  Time: 0.149 (3.568)  Loss:  1.2251 (2.1543)  Acc@1: 74.2925 (53.6520)  Acc@5: 89.7406 (77.8480)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-74.pth.tar', 53.65200008056641)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-72.pth.tar', 52.95200000244141)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-73.pth.tar', 52.80400005615235)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-71.pth.tar', 52.60800010742187)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-69.pth.tar', 52.18200010498047)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-70.pth.tar', 52.137999924316404)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-68.pth.tar', 51.83400005859375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-67.pth.tar', 51.55400000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-66.pth.tar', 51.330000107421874)

Train: 75 [   0/1251 (  0%)]  Loss:  4.853954 (4.8540)  Time: 12.279s,   83.39/s  (12.279s,   83.39/s)  LR: 6.236e-05  Data: 11.341 (11.341)
Train: 75 [  50/1251 (  4%)]  Loss:  4.625264 (4.7396)  Time: 0.584s, 1754.45/s  (2.560s,  400.02/s)  LR: 6.236e-05  Data: 0.021 (1.944)
Train: 75 [ 100/1251 (  8%)]  Loss:  4.316739 (4.5987)  Time: 0.750s, 1364.68/s  (2.512s,  407.58/s)  LR: 6.236e-05  Data: 0.148 (1.902)
Train: 75 [ 150/1251 ( 12%)]  Loss:  4.703099 (4.6248)  Time: 0.588s, 1742.60/s  (2.442s,  419.38/s)  LR: 6.236e-05  Data: 0.019 (1.833)
Train: 75 [ 200/1251 ( 16%)]  Loss:  4.462960 (4.5924)  Time: 0.585s, 1751.25/s  (2.401s,  426.46/s)  LR: 6.236e-05  Data: 0.019 (1.792)
Train: 75 [ 250/1251 ( 20%)]  Loss:  4.871307 (4.6389)  Time: 0.757s, 1352.18/s  (2.419s,  423.35/s)  LR: 6.236e-05  Data: 0.187 (1.807)
Train: 75 [ 300/1251 ( 24%)]  Loss:  4.557462 (4.6273)  Time: 7.247s,  141.30/s  (2.442s,  419.34/s)  LR: 6.236e-05  Data: 6.682 (1.834)
Train: 75 [ 350/1251 ( 28%)]  Loss:  5.238229 (4.7036)  Time: 0.586s, 1748.24/s  (2.424s,  422.36/s)  LR: 6.236e-05  Data: 0.021 (1.817)
Train: 75 [ 400/1251 ( 32%)]  Loss:  5.121268 (4.7500)  Time: 4.066s,  251.87/s  (2.419s,  423.37/s)  LR: 6.236e-05  Data: 3.360 (1.807)
Train: 75 [ 450/1251 ( 36%)]  Loss:  4.462474 (4.7213)  Time: 0.585s, 1751.72/s  (2.404s,  425.95/s)  LR: 6.236e-05  Data: 0.019 (1.794)
Train: 75 [ 500/1251 ( 40%)]  Loss:  4.945472 (4.7417)  Time: 7.210s,  142.03/s  (2.402s,  426.25/s)  LR: 6.236e-05  Data: 6.629 (1.793)
Train: 75 [ 550/1251 ( 44%)]  Loss:  4.175484 (4.6945)  Time: 0.583s, 1757.83/s  (2.378s,  430.65/s)  LR: 6.236e-05  Data: 0.019 (1.770)
Train: 75 [ 600/1251 ( 48%)]  Loss:  5.209270 (4.7341)  Time: 7.957s,  128.69/s  (2.375s,  431.08/s)  LR: 6.236e-05  Data: 7.295 (1.768)
Train: 75 [ 650/1251 ( 52%)]  Loss:  4.899794 (4.7459)  Time: 0.583s, 1755.16/s  (2.396s,  427.39/s)  LR: 6.236e-05  Data: 0.018 (1.789)
Train: 75 [ 700/1251 ( 56%)]  Loss:  4.998448 (4.7627)  Time: 8.111s,  126.25/s  (2.410s,  424.92/s)  LR: 6.236e-05  Data: 7.417 (1.803)
Train: 75 [ 750/1251 ( 60%)]  Loss:  4.510252 (4.7470)  Time: 0.583s, 1757.42/s  (2.409s,  425.02/s)  LR: 6.236e-05  Data: 0.020 (1.802)
Train: 75 [ 800/1251 ( 64%)]  Loss:  4.899135 (4.7559)  Time: 7.174s,  142.73/s  (2.413s,  424.33/s)  LR: 6.236e-05  Data: 6.582 (1.806)
Train: 75 [ 850/1251 ( 68%)]  Loss:  4.784679 (4.7575)  Time: 0.584s, 1754.17/s  (2.404s,  426.00/s)  LR: 6.236e-05  Data: 0.018 (1.799)
Train: 75 [ 900/1251 ( 72%)]  Loss:  4.139647 (4.7250)  Time: 7.403s,  138.32/s  (2.397s,  427.23/s)  LR: 6.236e-05  Data: 6.813 (1.793)
Train: 75 [ 950/1251 ( 76%)]  Loss:  4.631278 (4.7203)  Time: 0.584s, 1754.31/s  (2.386s,  429.09/s)  LR: 6.236e-05  Data: 0.021 (1.782)
Train: 75 [1000/1251 ( 80%)]  Loss:  4.382949 (4.7042)  Time: 7.526s,  136.06/s  (2.394s,  427.83/s)  LR: 6.236e-05  Data: 6.863 (1.789)
Train: 75 [1050/1251 ( 84%)]  Loss:  4.770946 (4.7073)  Time: 0.586s, 1746.81/s  (2.396s,  427.38/s)  LR: 6.236e-05  Data: 0.024 (1.792)
Train: 75 [1100/1251 ( 88%)]  Loss:  4.799138 (4.7113)  Time: 0.594s, 1724.09/s  (2.397s,  427.15/s)  LR: 6.236e-05  Data: 0.022 (1.793)
Train: 75 [1150/1251 ( 92%)]  Loss:  4.911415 (4.7196)  Time: 0.585s, 1751.33/s  (2.402s,  426.24/s)  LR: 6.236e-05  Data: 0.021 (1.799)
Train: 75 [1200/1251 ( 96%)]  Loss:  4.593683 (4.7146)  Time: 0.962s, 1064.46/s  (2.398s,  427.10/s)  LR: 6.236e-05  Data: 0.400 (1.794)
Train: 75 [1250/1251 (100%)]  Loss:  4.847236 (4.7197)  Time: 0.566s, 1809.32/s  (2.395s,  427.58/s)  LR: 6.236e-05  Data: 0.000 (1.792)
Test: [   0/48]  Time: 14.164 (14.164)  Loss:  1.2434 (1.2434)  Acc@1: 75.0977 (75.0977)  Acc@5: 90.6250 (90.6250)
Test: [  48/48]  Time: 0.148 (3.231)  Loss:  1.2135 (2.1396)  Acc@1: 73.7028 (53.5720)  Acc@5: 89.3868 (78.0500)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-74.pth.tar', 53.65200008056641)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-75.pth.tar', 53.572000083007815)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-72.pth.tar', 52.95200000244141)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-73.pth.tar', 52.80400005615235)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-71.pth.tar', 52.60800010742187)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-69.pth.tar', 52.18200010498047)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-70.pth.tar', 52.137999924316404)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-68.pth.tar', 51.83400005859375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-67.pth.tar', 51.55400000488281)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-66.pth.tar', 51.330000107421874)

Train: 76 [   0/1251 (  0%)]  Loss:  4.788665 (4.7887)  Time: 11.287s,   90.72/s  (11.287s,   90.72/s)  LR: 5.473e-05  Data: 10.241 (10.241)
Train: 76 [  50/1251 (  4%)]  Loss:  4.702255 (4.7455)  Time: 0.587s, 1745.68/s  (2.509s,  408.10/s)  LR: 5.473e-05  Data: 0.022 (1.905)
Train: 76 [ 100/1251 (  8%)]  Loss:  4.901232 (4.7974)  Time: 1.476s,  693.80/s  (2.512s,  407.68/s)  LR: 5.473e-05  Data: 0.889 (1.915)
Train: 76 [ 150/1251 ( 12%)]  Loss:  4.311979 (4.6760)  Time: 0.585s, 1749.64/s  (2.448s,  418.28/s)  LR: 5.473e-05  Data: 0.020 (1.850)
Train: 76 [ 200/1251 ( 16%)]  Loss:  5.007648 (4.7424)  Time: 0.585s, 1751.49/s  (2.461s,  416.10/s)  LR: 5.473e-05  Data: 0.019 (1.869)
Train: 76 [ 250/1251 ( 20%)]  Loss:  4.564242 (4.7127)  Time: 0.585s, 1749.46/s  (2.419s,  423.26/s)  LR: 5.473e-05  Data: 0.020 (1.830)
Train: 76 [ 300/1251 ( 24%)]  Loss:  5.149550 (4.7751)  Time: 0.586s, 1747.94/s  (2.395s,  427.50/s)  LR: 5.473e-05  Data: 0.022 (1.806)
Train: 76 [ 350/1251 ( 28%)]  Loss:  4.323564 (4.7186)  Time: 0.579s, 1768.92/s  (2.364s,  433.25/s)  LR: 5.473e-05  Data: 0.017 (1.773)
Train: 76 [ 400/1251 ( 32%)]  Loss:  4.811423 (4.7290)  Time: 0.586s, 1747.12/s  (2.345s,  436.74/s)  LR: 5.473e-05  Data: 0.020 (1.753)
Train: 76 [ 450/1251 ( 36%)]  Loss:  4.140476 (4.6701)  Time: 0.581s, 1762.34/s  (2.349s,  436.00/s)  LR: 5.473e-05  Data: 0.018 (1.758)
Train: 76 [ 500/1251 ( 40%)]  Loss:  4.925883 (4.6934)  Time: 0.587s, 1743.84/s  (2.368s,  432.49/s)  LR: 5.473e-05  Data: 0.019 (1.776)
Train: 76 [ 550/1251 ( 44%)]  Loss:  4.789641 (4.7014)  Time: 0.583s, 1756.44/s  (2.375s,  431.21/s)  LR: 5.473e-05  Data: 0.020 (1.784)
Train: 76 [ 600/1251 ( 48%)]  Loss:  5.178472 (4.7381)  Time: 0.588s, 1741.12/s  (2.381s,  430.00/s)  LR: 5.473e-05  Data: 0.018 (1.791)
Train: 76 [ 650/1251 ( 52%)]  Loss:  4.714573 (4.7364)  Time: 0.582s, 1758.75/s  (2.370s,  432.07/s)  LR: 5.473e-05  Data: 0.018 (1.780)
Train: 76 [ 700/1251 ( 56%)]  Loss:  4.633435 (4.7295)  Time: 0.586s, 1747.21/s  (2.369s,  432.27/s)  LR: 5.473e-05  Data: 0.019 (1.777)
Train: 76 [ 750/1251 ( 60%)]  Loss:  4.771877 (4.7322)  Time: 0.583s, 1755.95/s  (2.361s,  433.76/s)  LR: 5.473e-05  Data: 0.018 (1.768)
Train: 76 [ 800/1251 ( 64%)]  Loss:  4.978909 (4.7467)  Time: 3.684s,  277.98/s  (2.357s,  434.52/s)  LR: 5.473e-05  Data: 3.101 (1.763)
Train: 76 [ 850/1251 ( 68%)]  Loss:  5.096043 (4.7661)  Time: 0.584s, 1754.83/s  (2.377s,  430.76/s)  LR: 5.473e-05  Data: 0.018 (1.782)
Train: 76 [ 900/1251 ( 72%)]  Loss:  4.890515 (4.7727)  Time: 7.759s,  131.98/s  (2.390s,  428.36/s)  LR: 5.473e-05  Data: 7.068 (1.795)
Train: 76 [ 950/1251 ( 76%)]  Loss:  4.445756 (4.7563)  Time: 0.586s, 1746.59/s  (2.388s,  428.76/s)  LR: 5.473e-05  Data: 0.020 (1.793)
Train: 76 [1000/1251 ( 80%)]  Loss:  4.877977 (4.7621)  Time: 1.081s,  947.23/s  (2.384s,  429.44/s)  LR: 5.473e-05  Data: 0.519 (1.788)
Train: 76 [1050/1251 ( 84%)]  Loss:  4.583811 (4.7540)  Time: 0.583s, 1757.20/s  (2.376s,  430.93/s)  LR: 5.473e-05  Data: 0.019 (1.779)
Train: 76 [1100/1251 ( 88%)]  Loss:  4.505970 (4.7432)  Time: 2.860s,  358.10/s  (2.368s,  432.47/s)  LR: 5.473e-05  Data: 2.162 (1.770)
Train: 76 [1150/1251 ( 92%)]  Loss:  4.480878 (4.7323)  Time: 0.584s, 1754.17/s  (2.353s,  435.21/s)  LR: 5.473e-05  Data: 0.021 (1.755)
Train: 76 [1200/1251 ( 96%)]  Loss:  4.783396 (4.7343)  Time: 2.362s,  433.49/s  (2.351s,  435.61/s)  LR: 5.473e-05  Data: 1.687 (1.753)
Train: 76 [1250/1251 (100%)]  Loss:  4.863869 (4.7393)  Time: 0.563s, 1819.70/s  (2.361s,  433.62/s)  LR: 5.473e-05  Data: 0.000 (1.763)
Test: [   0/48]  Time: 14.201 (14.201)  Loss:  1.2611 (1.2611)  Acc@1: 75.0977 (75.0977)  Acc@5: 91.0156 (91.0156)
Test: [  48/48]  Time: 0.149 (3.549)  Loss:  1.2398 (2.1514)  Acc@1: 74.7642 (53.7080)  Acc@5: 89.7406 (78.1140)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-76.pth.tar', 53.70800015625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-74.pth.tar', 53.65200008056641)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-75.pth.tar', 53.572000083007815)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-72.pth.tar', 52.95200000244141)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-73.pth.tar', 52.80400005615235)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-71.pth.tar', 52.60800010742187)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-69.pth.tar', 52.18200010498047)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-70.pth.tar', 52.137999924316404)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-68.pth.tar', 51.83400005859375)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-67.pth.tar', 51.55400000488281)

Train: 77 [   0/1251 (  0%)]  Loss:  4.833870 (4.8339)  Time: 11.528s,   88.83/s  (11.528s,   88.83/s)  LR: 4.768e-05  Data: 10.156 (10.156)
Train: 77 [  50/1251 (  4%)]  Loss:  4.512446 (4.6732)  Time: 1.772s,  578.04/s  (2.527s,  405.19/s)  LR: 4.768e-05  Data: 1.209 (1.916)
Train: 77 [ 100/1251 (  8%)]  Loss:  4.660571 (4.6690)  Time: 0.587s, 1745.88/s  (2.488s,  411.52/s)  LR: 4.768e-05  Data: 0.019 (1.875)
Train: 77 [ 150/1251 ( 12%)]  Loss:  5.277937 (4.8212)  Time: 0.585s, 1750.73/s  (2.398s,  427.11/s)  LR: 4.768e-05  Data: 0.023 (1.785)
Train: 77 [ 200/1251 ( 16%)]  Loss:  4.437515 (4.7445)  Time: 0.589s, 1739.02/s  (2.345s,  436.69/s)  LR: 4.768e-05  Data: 0.024 (1.737)
Train: 77 [ 250/1251 ( 20%)]  Loss:  4.283650 (4.6677)  Time: 5.625s,  182.03/s  (2.359s,  434.08/s)  LR: 4.768e-05  Data: 5.029 (1.756)
Train: 77 [ 300/1251 ( 24%)]  Loss:  3.930568 (4.5624)  Time: 0.588s, 1740.17/s  (2.376s,  430.94/s)  LR: 4.768e-05  Data: 0.025 (1.773)
Train: 77 [ 350/1251 ( 28%)]  Loss:  5.041719 (4.6223)  Time: 2.218s,  461.69/s  (2.373s,  431.60/s)  LR: 4.768e-05  Data: 1.399 (1.766)
Train: 77 [ 400/1251 ( 32%)]  Loss:  4.893706 (4.6524)  Time: 0.587s, 1744.64/s  (2.360s,  433.97/s)  LR: 4.768e-05  Data: 0.021 (1.752)
Train: 77 [ 450/1251 ( 36%)]  Loss:  4.902581 (4.6775)  Time: 6.964s,  147.04/s  (2.358s,  434.26/s)  LR: 4.768e-05  Data: 6.401 (1.749)
Train: 77 [ 500/1251 ( 40%)]  Loss:  4.838881 (4.6921)  Time: 0.583s, 1755.70/s  (2.347s,  436.22/s)  LR: 4.768e-05  Data: 0.019 (1.741)
Train: 77 [ 550/1251 ( 44%)]  Loss:  4.625620 (4.6866)  Time: 6.665s,  153.64/s  (2.341s,  437.48/s)  LR: 4.768e-05  Data: 6.084 (1.736)
Train: 77 [ 600/1251 ( 48%)]  Loss:  4.518861 (4.6737)  Time: 0.585s, 1751.74/s  (2.327s,  440.10/s)  LR: 4.768e-05  Data: 0.020 (1.723)
Train: 77 [ 650/1251 ( 52%)]  Loss:  4.936861 (4.6925)  Time: 7.134s,  143.54/s  (2.350s,  435.74/s)  LR: 4.768e-05  Data: 6.461 (1.747)
Train: 77 [ 700/1251 ( 56%)]  Loss:  4.576177 (4.6847)  Time: 0.585s, 1750.46/s  (2.365s,  433.06/s)  LR: 4.768e-05  Data: 0.022 (1.762)
Train: 77 [ 750/1251 ( 60%)]  Loss:  4.451132 (4.6701)  Time: 7.902s,  129.60/s  (2.374s,  431.28/s)  LR: 4.768e-05  Data: 7.246 (1.772)
Train: 77 [ 800/1251 ( 64%)]  Loss:  4.637322 (4.6682)  Time: 0.586s, 1746.27/s  (2.370s,  432.02/s)  LR: 4.768e-05  Data: 0.024 (1.769)
Train: 77 [ 850/1251 ( 68%)]  Loss:  4.204716 (4.6425)  Time: 7.880s,  129.95/s  (2.374s,  431.27/s)  LR: 4.768e-05  Data: 7.318 (1.774)
Train: 77 [ 900/1251 ( 72%)]  Loss:  4.111433 (4.6145)  Time: 0.585s, 1750.27/s  (2.361s,  433.73/s)  LR: 4.768e-05  Data: 0.024 (1.762)
Train: 77 [ 950/1251 ( 76%)]  Loss:  4.777184 (4.6226)  Time: 8.334s,  122.88/s  (2.360s,  433.85/s)  LR: 4.768e-05  Data: 6.797 (1.761)
Train: 77 [1000/1251 ( 80%)]  Loss:  4.371317 (4.6107)  Time: 0.584s, 1753.83/s  (2.350s,  435.80/s)  LR: 4.768e-05  Data: 0.021 (1.751)
Train: 77 [1050/1251 ( 84%)]  Loss:  4.920509 (4.6248)  Time: 9.138s,  112.05/s  (2.369s,  432.32/s)  LR: 4.768e-05  Data: 8.533 (1.770)
Train: 77 [1100/1251 ( 88%)]  Loss:  4.917999 (4.6375)  Time: 0.586s, 1748.76/s  (2.368s,  432.43/s)  LR: 4.768e-05  Data: 0.022 (1.770)
Train: 77 [1150/1251 ( 92%)]  Loss:  4.773193 (4.6432)  Time: 6.877s,  148.89/s  (2.373s,  431.52/s)  LR: 4.768e-05  Data: 6.315 (1.775)
Train: 77 [1200/1251 ( 96%)]  Loss:  4.835667 (4.6509)  Time: 0.586s, 1747.34/s  (2.374s,  431.38/s)  LR: 4.768e-05  Data: 0.022 (1.776)
Train: 77 [1250/1251 (100%)]  Loss:  4.435867 (4.6426)  Time: 0.565s, 1811.04/s  (2.367s,  432.60/s)  LR: 4.768e-05  Data: 0.000 (1.770)
Test: [   0/48]  Time: 13.170 (13.170)  Loss:  1.2340 (1.2340)  Acc@1: 75.8789 (75.8789)  Acc@5: 91.4062 (91.4062)
Test: [  48/48]  Time: 0.148 (3.239)  Loss:  1.2136 (2.1342)  Acc@1: 75.7075 (54.2780)  Acc@5: 89.0330 (78.3740)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-77.pth.tar', 54.2779999194336)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-76.pth.tar', 53.70800015625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-74.pth.tar', 53.65200008056641)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-75.pth.tar', 53.572000083007815)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-72.pth.tar', 52.95200000244141)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-73.pth.tar', 52.80400005615235)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-71.pth.tar', 52.60800010742187)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-69.pth.tar', 52.18200010498047)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-70.pth.tar', 52.137999924316404)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-68.pth.tar', 51.83400005859375)

Train: 78 [   0/1251 (  0%)]  Loss:  4.858701 (4.8587)  Time: 10.679s,   95.89/s  (10.679s,   95.89/s)  LR: 4.121e-05  Data: 9.833 (9.833)
Train: 78 [  50/1251 (  4%)]  Loss:  4.577421 (4.7181)  Time: 0.584s, 1753.97/s  (2.273s,  450.42/s)  LR: 4.121e-05  Data: 0.020 (1.684)
Train: 78 [ 100/1251 (  8%)]  Loss:  4.816197 (4.7508)  Time: 0.587s, 1745.48/s  (2.388s,  428.75/s)  LR: 4.121e-05  Data: 0.021 (1.797)
Train: 78 [ 150/1251 ( 12%)]  Loss:  4.589537 (4.7105)  Time: 4.637s,  220.82/s  (2.402s,  426.31/s)  LR: 4.121e-05  Data: 3.972 (1.790)
Train: 78 [ 200/1251 ( 16%)]  Loss:  4.963465 (4.7611)  Time: 0.588s, 1741.78/s  (2.380s,  430.18/s)  LR: 4.121e-05  Data: 0.019 (1.767)
Train: 78 [ 250/1251 ( 20%)]  Loss:  4.782840 (4.7647)  Time: 6.133s,  166.97/s  (2.376s,  430.94/s)  LR: 4.121e-05  Data: 5.564 (1.759)
Train: 78 [ 300/1251 ( 24%)]  Loss:  5.084768 (4.8104)  Time: 0.588s, 1742.96/s  (2.341s,  437.50/s)  LR: 4.121e-05  Data: 0.021 (1.726)
Train: 78 [ 350/1251 ( 28%)]  Loss:  4.912123 (4.8231)  Time: 3.095s,  330.85/s  (2.325s,  440.49/s)  LR: 4.121e-05  Data: 2.533 (1.707)
Train: 78 [ 400/1251 ( 32%)]  Loss:  5.019994 (4.8450)  Time: 3.194s,  320.58/s  (2.312s,  442.84/s)  LR: 4.121e-05  Data: 2.632 (1.695)
Train: 78 [ 450/1251 ( 36%)]  Loss:  4.820413 (4.8425)  Time: 0.585s, 1750.13/s  (2.296s,  446.06/s)  LR: 4.121e-05  Data: 0.020 (1.679)
Train: 78 [ 500/1251 ( 40%)]  Loss:  4.549158 (4.8159)  Time: 9.137s,  112.07/s  (2.337s,  438.16/s)  LR: 4.121e-05  Data: 8.383 (1.721)
Train: 78 [ 550/1251 ( 44%)]  Loss:  4.503228 (4.7898)  Time: 0.584s, 1753.20/s  (2.342s,  437.21/s)  LR: 4.121e-05  Data: 0.019 (1.729)
Train: 78 [ 600/1251 ( 48%)]  Loss:  4.668873 (4.7805)  Time: 8.336s,  122.85/s  (2.355s,  434.73/s)  LR: 4.121e-05  Data: 7.771 (1.744)
Train: 78 [ 650/1251 ( 52%)]  Loss:  4.624517 (4.7694)  Time: 0.587s, 1743.29/s  (2.350s,  435.70/s)  LR: 4.121e-05  Data: 0.019 (1.741)
Train: 78 [ 700/1251 ( 56%)]  Loss:  4.569233 (4.7560)  Time: 2.319s,  441.63/s  (2.351s,  435.52/s)  LR: 4.121e-05  Data: 1.587 (1.744)
Train: 78 [ 750/1251 ( 60%)]  Loss:  4.875419 (4.7635)  Time: 3.167s,  323.33/s  (2.349s,  435.86/s)  LR: 4.121e-05  Data: 2.585 (1.742)
Train: 78 [ 800/1251 ( 64%)]  Loss:  4.241564 (4.7328)  Time: 3.497s,  292.82/s  (2.341s,  437.36/s)  LR: 4.121e-05  Data: 2.846 (1.735)
Train: 78 [ 850/1251 ( 68%)]  Loss:  4.728844 (4.7326)  Time: 7.613s,  134.50/s  (2.354s,  435.10/s)  LR: 4.121e-05  Data: 6.943 (1.748)
Train: 78 [ 900/1251 ( 72%)]  Loss:  4.332014 (4.7115)  Time: 0.637s, 1606.65/s  (2.359s,  434.13/s)  LR: 4.121e-05  Data: 0.018 (1.754)
Train: 78 [ 950/1251 ( 76%)]  Loss:  4.595014 (4.7057)  Time: 8.681s,  117.95/s  (2.371s,  431.84/s)  LR: 4.121e-05  Data: 8.084 (1.767)
Train: 78 [1000/1251 ( 80%)]  Loss:  4.721996 (4.7064)  Time: 0.587s, 1743.34/s  (2.372s,  431.68/s)  LR: 4.121e-05  Data: 0.021 (1.767)
Train: 78 [1050/1251 ( 84%)]  Loss:  4.598602 (4.7015)  Time: 6.913s,  148.14/s  (2.369s,  432.28/s)  LR: 4.121e-05  Data: 6.350 (1.765)
Train: 78 [1100/1251 ( 88%)]  Loss:  4.946864 (4.7122)  Time: 6.214s,  164.79/s  (2.364s,  433.12/s)  LR: 4.121e-05  Data: 5.539 (1.761)
Train: 78 [1150/1251 ( 92%)]  Loss:  4.772595 (4.7147)  Time: 3.754s,  272.81/s  (2.356s,  434.58/s)  LR: 4.121e-05  Data: 3.179 (1.752)
Train: 78 [1200/1251 ( 96%)]  Loss:  4.471034 (4.7050)  Time: 2.622s,  390.50/s  (2.347s,  436.29/s)  LR: 4.121e-05  Data: 1.990 (1.743)
Train: 78 [1250/1251 (100%)]  Loss:  5.059355 (4.7186)  Time: 0.565s, 1812.65/s  (2.355s,  434.89/s)  LR: 4.121e-05  Data: 0.000 (1.751)
Test: [   0/48]  Time: 16.135 (16.135)  Loss:  1.2005 (1.2005)  Acc@1: 75.6836 (75.6836)  Acc@5: 91.2109 (91.2109)
Test: [  48/48]  Time: 0.149 (3.587)  Loss:  1.1745 (2.1006)  Acc@1: 75.3538 (54.4920)  Acc@5: 89.1509 (78.5480)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-78.pth.tar', 54.49200002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-77.pth.tar', 54.2779999194336)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-76.pth.tar', 53.70800015625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-74.pth.tar', 53.65200008056641)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-75.pth.tar', 53.572000083007815)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-72.pth.tar', 52.95200000244141)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-73.pth.tar', 52.80400005615235)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-71.pth.tar', 52.60800010742187)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-69.pth.tar', 52.18200010498047)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-70.pth.tar', 52.137999924316404)

Train: 79 [   0/1251 (  0%)]  Loss:  4.963300 (4.9633)  Time: 11.814s,   86.68/s  (11.814s,   86.68/s)  LR: 3.533e-05  Data: 11.091 (11.091)
Train: 79 [  50/1251 (  4%)]  Loss:  5.016792 (4.9900)  Time: 0.585s, 1749.03/s  (2.499s,  409.70/s)  LR: 3.533e-05  Data: 0.022 (1.914)
Train: 79 [ 100/1251 (  8%)]  Loss:  4.441940 (4.8073)  Time: 0.583s, 1755.20/s  (2.446s,  418.56/s)  LR: 3.533e-05  Data: 0.018 (1.858)
Train: 79 [ 150/1251 ( 12%)]  Loss:  4.679306 (4.7753)  Time: 0.585s, 1751.51/s  (2.348s,  436.14/s)  LR: 3.533e-05  Data: 0.022 (1.757)
Train: 79 [ 200/1251 ( 16%)]  Loss:  4.436434 (4.7076)  Time: 0.584s, 1754.08/s  (2.326s,  440.19/s)  LR: 3.533e-05  Data: 0.018 (1.738)
Train: 79 [ 250/1251 ( 20%)]  Loss:  4.289421 (4.6379)  Time: 2.929s,  349.58/s  (2.281s,  448.92/s)  LR: 3.533e-05  Data: 2.351 (1.690)
Train: 79 [ 300/1251 ( 24%)]  Loss:  4.198696 (4.5751)  Time: 0.583s, 1756.19/s  (2.321s,  441.19/s)  LR: 3.533e-05  Data: 0.018 (1.726)
Train: 79 [ 350/1251 ( 28%)]  Loss:  4.812664 (4.6048)  Time: 1.349s,  759.17/s  (2.340s,  437.64/s)  LR: 3.533e-05  Data: 0.693 (1.737)
Train: 79 [ 400/1251 ( 32%)]  Loss:  5.066493 (4.6561)  Time: 0.583s, 1755.35/s  (2.341s,  437.40/s)  LR: 3.533e-05  Data: 0.018 (1.739)
Train: 79 [ 450/1251 ( 36%)]  Loss:  5.007806 (4.6913)  Time: 3.155s,  324.58/s  (2.336s,  438.37/s)  LR: 3.533e-05  Data: 2.540 (1.732)
Train: 79 [ 500/1251 ( 40%)]  Loss:  4.747693 (4.6964)  Time: 0.583s, 1756.77/s  (2.331s,  439.23/s)  LR: 3.533e-05  Data: 0.019 (1.727)
Train: 79 [ 550/1251 ( 44%)]  Loss:  4.606829 (4.6889)  Time: 0.584s, 1752.68/s  (2.330s,  439.56/s)  LR: 3.533e-05  Data: 0.021 (1.725)
Train: 79 [ 600/1251 ( 48%)]  Loss:  4.293849 (4.6586)  Time: 0.584s, 1753.85/s  (2.316s,  442.20/s)  LR: 3.533e-05  Data: 0.018 (1.712)
Train: 79 [ 650/1251 ( 52%)]  Loss:  4.750165 (4.6651)  Time: 0.583s, 1756.66/s  (2.314s,  442.57/s)  LR: 3.533e-05  Data: 0.020 (1.711)
Train: 79 [ 700/1251 ( 56%)]  Loss:  4.290526 (4.6401)  Time: 0.584s, 1753.99/s  (2.361s,  433.74/s)  LR: 3.533e-05  Data: 0.019 (1.758)
Train: 79 [ 750/1251 ( 60%)]  Loss:  5.061529 (4.6665)  Time: 0.584s, 1753.18/s  (2.373s,  431.58/s)  LR: 3.533e-05  Data: 0.022 (1.770)
Train: 79 [ 800/1251 ( 64%)]  Loss:  4.784541 (4.6734)  Time: 0.585s, 1750.09/s  (2.383s,  429.73/s)  LR: 3.533e-05  Data: 0.020 (1.779)
Train: 79 [ 850/1251 ( 68%)]  Loss:  4.813827 (4.6812)  Time: 0.587s, 1743.71/s  (2.378s,  430.58/s)  LR: 3.533e-05  Data: 0.020 (1.775)
Train: 79 [ 900/1251 ( 72%)]  Loss:  4.702553 (4.6823)  Time: 0.585s, 1749.19/s  (2.377s,  430.84/s)  LR: 3.533e-05  Data: 0.019 (1.774)
Train: 79 [ 950/1251 ( 76%)]  Loss:  5.222288 (4.7093)  Time: 6.151s,  166.48/s  (2.369s,  432.21/s)  LR: 3.533e-05  Data: 5.589 (1.767)
Train: 79 [1000/1251 ( 80%)]  Loss:  4.960687 (4.7213)  Time: 0.583s, 1755.17/s  (2.361s,  433.80/s)  LR: 3.533e-05  Data: 0.018 (1.758)
Train: 79 [1050/1251 ( 84%)]  Loss:  4.776194 (4.7238)  Time: 4.614s,  221.92/s  (2.369s,  432.18/s)  LR: 3.533e-05  Data: 4.049 (1.768)
Train: 79 [1100/1251 ( 88%)]  Loss:  5.128224 (4.7414)  Time: 0.586s, 1747.13/s  (2.372s,  431.67/s)  LR: 3.533e-05  Data: 0.022 (1.771)
Train: 79 [1150/1251 ( 92%)]  Loss:  4.815230 (4.7445)  Time: 0.721s, 1420.21/s  (2.370s,  431.98/s)  LR: 3.533e-05  Data: 0.125 (1.769)
Train: 79 [1200/1251 ( 96%)]  Loss:  5.167709 (4.7614)  Time: 0.587s, 1744.92/s  (2.373s,  431.59/s)  LR: 3.533e-05  Data: 0.022 (1.771)
Train: 79 [1250/1251 (100%)]  Loss:  5.052061 (4.7726)  Time: 0.567s, 1805.41/s  (2.370s,  432.01/s)  LR: 3.533e-05  Data: 0.000 (1.769)
Test: [   0/48]  Time: 14.129 (14.129)  Loss:  1.1984 (1.1984)  Acc@1: 75.6836 (75.6836)  Acc@5: 91.1133 (91.1133)
Test: [  48/48]  Time: 0.149 (3.239)  Loss:  1.1984 (2.0920)  Acc@1: 75.7076 (54.6860)  Acc@5: 89.6226 (78.7700)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-79.pth.tar', 54.68600004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-78.pth.tar', 54.49200002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-77.pth.tar', 54.2779999194336)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-76.pth.tar', 53.70800015625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-74.pth.tar', 53.65200008056641)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-75.pth.tar', 53.572000083007815)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-72.pth.tar', 52.95200000244141)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-73.pth.tar', 52.80400005615235)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-71.pth.tar', 52.60800010742187)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-69.pth.tar', 52.18200010498047)

Train: 80 [   0/1251 (  0%)]  Loss:  4.942369 (4.9424)  Time: 10.250s,   99.91/s  (10.250s,   99.91/s)  LR: 3.005e-05  Data: 9.663 (9.663)
Train: 80 [  50/1251 (  4%)]  Loss:  4.417891 (4.6801)  Time: 0.585s, 1751.29/s  (2.312s,  442.82/s)  LR: 3.005e-05  Data: 0.016 (1.719)
Train: 80 [ 100/1251 (  8%)]  Loss:  4.915743 (4.7587)  Time: 0.585s, 1749.16/s  (2.423s,  422.67/s)  LR: 3.005e-05  Data: 0.019 (1.835)
Train: 80 [ 150/1251 ( 12%)]  Loss:  4.693203 (4.7423)  Time: 0.588s, 1742.25/s  (2.386s,  429.09/s)  LR: 3.005e-05  Data: 0.021 (1.801)
Train: 80 [ 200/1251 ( 16%)]  Loss:  4.621492 (4.7181)  Time: 0.586s, 1746.81/s  (2.405s,  425.84/s)  LR: 3.005e-05  Data: 0.020 (1.818)
Train: 80 [ 250/1251 ( 20%)]  Loss:  4.795949 (4.7311)  Time: 0.582s, 1760.63/s  (2.375s,  431.15/s)  LR: 3.005e-05  Data: 0.019 (1.792)
Train: 80 [ 300/1251 ( 24%)]  Loss:  4.850630 (4.7482)  Time: 0.585s, 1750.69/s  (2.371s,  431.89/s)  LR: 3.005e-05  Data: 0.019 (1.785)
Train: 80 [ 350/1251 ( 28%)]  Loss:  4.603230 (4.7301)  Time: 0.585s, 1750.73/s  (2.338s,  437.91/s)  LR: 3.005e-05  Data: 0.019 (1.752)
Train: 80 [ 400/1251 ( 32%)]  Loss:  5.072536 (4.7681)  Time: 0.594s, 1724.90/s  (2.338s,  437.90/s)  LR: 3.005e-05  Data: 0.029 (1.751)
Train: 80 [ 450/1251 ( 36%)]  Loss:  5.179305 (4.8092)  Time: 0.584s, 1753.46/s  (2.323s,  440.84/s)  LR: 3.005e-05  Data: 0.020 (1.736)
Train: 80 [ 500/1251 ( 40%)]  Loss:  5.077012 (4.8336)  Time: 0.584s, 1753.11/s  (2.348s,  436.02/s)  LR: 3.005e-05  Data: 0.019 (1.762)
Train: 80 [ 550/1251 ( 44%)]  Loss:  5.110343 (4.8566)  Time: 0.584s, 1754.59/s  (2.365s,  433.03/s)  LR: 3.005e-05  Data: 0.018 (1.778)
Train: 80 [ 600/1251 ( 48%)]  Loss:  4.677237 (4.8428)  Time: 0.583s, 1755.25/s  (2.387s,  429.06/s)  LR: 3.005e-05  Data: 0.021 (1.799)
Train: 80 [ 650/1251 ( 52%)]  Loss:  4.889431 (4.8462)  Time: 0.583s, 1756.28/s  (2.376s,  431.05/s)  LR: 3.005e-05  Data: 0.020 (1.788)
Train: 80 [ 700/1251 ( 56%)]  Loss:  4.993118 (4.8560)  Time: 0.586s, 1746.62/s  (2.371s,  431.84/s)  LR: 3.005e-05  Data: 0.019 (1.784)
Train: 80 [ 750/1251 ( 60%)]  Loss:  4.554100 (4.8371)  Time: 0.586s, 1748.90/s  (2.366s,  432.86/s)  LR: 3.005e-05  Data: 0.022 (1.778)
Train: 80 [ 800/1251 ( 64%)]  Loss:  4.797456 (4.8348)  Time: 0.588s, 1741.30/s  (2.363s,  433.34/s)  LR: 3.005e-05  Data: 0.021 (1.776)
Train: 80 [ 850/1251 ( 68%)]  Loss:  4.280634 (4.8040)  Time: 3.215s,  318.53/s  (2.367s,  432.69/s)  LR: 3.005e-05  Data: 2.546 (1.777)
Train: 80 [ 900/1251 ( 72%)]  Loss:  4.367169 (4.7810)  Time: 0.588s, 1742.67/s  (2.384s,  429.58/s)  LR: 3.005e-05  Data: 0.019 (1.794)
Train: 80 [ 950/1251 ( 76%)]  Loss:  5.119350 (4.7979)  Time: 0.586s, 1748.51/s  (2.389s,  428.66/s)  LR: 3.005e-05  Data: 0.020 (1.800)
Train: 80 [1000/1251 ( 80%)]  Loss:  4.606712 (4.7888)  Time: 0.925s, 1106.44/s  (2.402s,  426.30/s)  LR: 3.005e-05  Data: 0.194 (1.812)
Train: 80 [1050/1251 ( 84%)]  Loss:  4.734162 (4.7863)  Time: 1.345s,  761.26/s  (2.396s,  427.29/s)  LR: 3.005e-05  Data: 0.700 (1.806)
Train: 80 [1100/1251 ( 88%)]  Loss:  4.892633 (4.7909)  Time: 0.586s, 1746.53/s  (2.391s,  428.26/s)  LR: 3.005e-05  Data: 0.020 (1.800)
Train: 80 [1150/1251 ( 92%)]  Loss:  5.172704 (4.8069)  Time: 6.737s,  151.99/s  (2.386s,  429.12/s)  LR: 3.005e-05  Data: 6.053 (1.795)
Train: 80 [1200/1251 ( 96%)]  Loss:  4.805829 (4.8068)  Time: 0.585s, 1750.23/s  (2.377s,  430.75/s)  LR: 3.005e-05  Data: 0.021 (1.785)
Train: 80 [1250/1251 (100%)]  Loss:  4.709623 (4.8031)  Time: 0.565s, 1812.87/s  (2.388s,  428.78/s)  LR: 3.005e-05  Data: 0.000 (1.796)
Test: [   0/48]  Time: 15.872 (15.872)  Loss:  1.1618 (1.1618)  Acc@1: 77.1484 (77.1484)  Acc@5: 91.7969 (91.7969)
Test: [  48/48]  Time: 0.169 (3.630)  Loss:  1.1812 (2.0770)  Acc@1: 74.5283 (54.8780)  Acc@5: 89.8585 (79.0500)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-80.pth.tar', 54.878000053710934)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-79.pth.tar', 54.68600004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-78.pth.tar', 54.49200002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-77.pth.tar', 54.2779999194336)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-76.pth.tar', 53.70800015625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-74.pth.tar', 53.65200008056641)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-75.pth.tar', 53.572000083007815)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-72.pth.tar', 52.95200000244141)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-73.pth.tar', 52.80400005615235)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-71.pth.tar', 52.60800010742187)

Train: 81 [   0/1251 (  0%)]  Loss:  4.822293 (4.8223)  Time: 12.294s,   83.29/s  (12.294s,   83.29/s)  LR: 2.538e-05  Data: 11.627 (11.627)
Train: 81 [  50/1251 (  4%)]  Loss:  4.907336 (4.8648)  Time: 0.585s, 1750.85/s  (2.614s,  391.71/s)  LR: 2.538e-05  Data: 0.022 (2.001)
Train: 81 [ 100/1251 (  8%)]  Loss:  4.714932 (4.8149)  Time: 1.043s,  981.61/s  (2.675s,  382.82/s)  LR: 2.538e-05  Data: 0.231 (2.063)
Train: 81 [ 150/1251 ( 12%)]  Loss:  5.137914 (4.8956)  Time: 0.583s, 1757.66/s  (2.565s,  399.27/s)  LR: 2.538e-05  Data: 0.018 (1.954)
Train: 81 [ 200/1251 ( 16%)]  Loss:  5.119161 (4.9403)  Time: 6.019s,  170.12/s  (2.515s,  407.19/s)  LR: 2.538e-05  Data: 5.353 (1.907)
Train: 81 [ 250/1251 ( 20%)]  Loss:  4.562262 (4.8773)  Time: 0.587s, 1743.20/s  (2.440s,  419.69/s)  LR: 2.538e-05  Data: 0.017 (1.834)
Train: 81 [ 300/1251 ( 24%)]  Loss:  4.814443 (4.8683)  Time: 7.142s,  143.37/s  (2.518s,  406.70/s)  LR: 2.538e-05  Data: 6.581 (1.914)
Train: 81 [ 350/1251 ( 28%)]  Loss:  4.334814 (4.8016)  Time: 0.866s, 1182.66/s  (2.511s,  407.80/s)  LR: 2.538e-05  Data: 0.256 (1.907)
Train: 81 [ 400/1251 ( 32%)]  Loss:  5.041197 (4.8283)  Time: 6.522s,  157.01/s  (2.513s,  407.43/s)  LR: 2.538e-05  Data: 5.832 (1.903)
Train: 81 [ 450/1251 ( 36%)]  Loss:  4.095204 (4.7550)  Time: 0.583s, 1755.97/s  (2.497s,  410.13/s)  LR: 2.538e-05  Data: 0.018 (1.888)
Train: 81 [ 500/1251 ( 40%)]  Loss:  4.294313 (4.7131)  Time: 4.399s,  232.79/s  (2.491s,  411.07/s)  LR: 2.538e-05  Data: 3.836 (1.884)
Train: 81 [ 550/1251 ( 44%)]  Loss:  4.499919 (4.6953)  Time: 0.588s, 1742.61/s  (2.471s,  414.36/s)  LR: 2.538e-05  Data: 0.017 (1.865)
Train: 81 [ 600/1251 ( 48%)]  Loss:  5.027425 (4.7209)  Time: 5.926s,  172.81/s  (2.469s,  414.74/s)  LR: 2.538e-05  Data: 5.261 (1.862)
Train: 81 [ 650/1251 ( 52%)]  Loss:  4.690586 (4.7187)  Time: 0.584s, 1752.80/s  (2.493s,  410.73/s)  LR: 2.538e-05  Data: 0.018 (1.886)
Train: 81 [ 700/1251 ( 56%)]  Loss:  4.584429 (4.7097)  Time: 4.451s,  230.07/s  (2.514s,  407.38/s)  LR: 2.538e-05  Data: 3.889 (1.906)
Train: 81 [ 750/1251 ( 60%)]  Loss:  3.972820 (4.6637)  Time: 0.584s, 1754.73/s  (2.515s,  407.11/s)  LR: 2.538e-05  Data: 0.021 (1.907)
Train: 81 [ 800/1251 ( 64%)]  Loss:  4.504787 (4.6543)  Time: 6.326s,  161.86/s  (2.517s,  406.83/s)  LR: 2.538e-05  Data: 5.630 (1.909)
Train: 81 [ 850/1251 ( 68%)]  Loss:  4.939981 (4.6702)  Time: 0.588s, 1740.37/s  (2.511s,  407.73/s)  LR: 2.538e-05  Data: 0.019 (1.904)
Train: 81 [ 900/1251 ( 72%)]  Loss:  4.641625 (4.6687)  Time: 7.136s,  143.50/s  (2.506s,  408.54/s)  LR: 2.538e-05  Data: 5.790 (1.899)
Train: 81 [ 950/1251 ( 76%)]  Loss:  4.993100 (4.6849)  Time: 0.584s, 1754.73/s  (2.495s,  410.34/s)  LR: 2.538e-05  Data: 0.020 (1.887)
Train: 81 [1000/1251 ( 80%)]  Loss:  4.979236 (4.6989)  Time: 5.039s,  203.21/s  (2.509s,  408.07/s)  LR: 2.538e-05  Data: 4.446 (1.902)
Train: 81 [1050/1251 ( 84%)]  Loss:  4.351394 (4.6831)  Time: 0.583s, 1757.85/s  (2.512s,  407.69/s)  LR: 2.538e-05  Data: 0.020 (1.905)
Train: 81 [1100/1251 ( 88%)]  Loss:  4.614252 (4.6801)  Time: 6.334s,  161.66/s  (2.517s,  406.83/s)  LR: 2.538e-05  Data: 5.685 (1.910)
Train: 81 [1150/1251 ( 92%)]  Loss:  4.794551 (4.6849)  Time: 1.325s,  772.83/s  (2.514s,  407.25/s)  LR: 2.538e-05  Data: 0.763 (1.907)
Train: 81 [1200/1251 ( 96%)]  Loss:  4.635759 (4.6829)  Time: 4.098s,  249.91/s  (2.509s,  408.18/s)  LR: 2.538e-05  Data: 3.522 (1.901)
Train: 81 [1250/1251 (100%)]  Loss:  4.416114 (4.6727)  Time: 0.563s, 1818.13/s  (2.503s,  409.15/s)  LR: 2.538e-05  Data: 0.000 (1.893)
Test: [   0/48]  Time: 13.450 (13.450)  Loss:  1.1800 (1.1800)  Acc@1: 76.9531 (76.9531)  Acc@5: 91.5039 (91.5039)
Test: [  48/48]  Time: 0.149 (3.278)  Loss:  1.1740 (2.0859)  Acc@1: 75.9434 (54.8340)  Acc@5: 89.9764 (79.0340)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-80.pth.tar', 54.878000053710934)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-81.pth.tar', 54.834000021972656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-79.pth.tar', 54.68600004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-78.pth.tar', 54.49200002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-77.pth.tar', 54.2779999194336)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-76.pth.tar', 53.70800015625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-74.pth.tar', 53.65200008056641)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-75.pth.tar', 53.572000083007815)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-72.pth.tar', 52.95200000244141)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-73.pth.tar', 52.80400005615235)

Train: 82 [   0/1251 (  0%)]  Loss:  3.924078 (3.9241)  Time: 12.457s,   82.20/s  (12.457s,   82.20/s)  LR: 2.131e-05  Data: 10.794 (10.794)
Train: 82 [  50/1251 (  4%)]  Loss:  4.279331 (4.1017)  Time: 0.585s, 1751.92/s  (2.734s,  374.50/s)  LR: 2.131e-05  Data: 0.021 (2.111)
Train: 82 [ 100/1251 (  8%)]  Loss:  4.766294 (4.3232)  Time: 0.586s, 1746.25/s  (2.704s,  378.66/s)  LR: 2.131e-05  Data: 0.023 (2.098)
Train: 82 [ 150/1251 ( 12%)]  Loss:  4.830344 (4.4500)  Time: 0.585s, 1750.93/s  (2.608s,  392.58/s)  LR: 2.131e-05  Data: 0.019 (2.008)
Train: 82 [ 200/1251 ( 16%)]  Loss:  4.316276 (4.4233)  Time: 0.592s, 1728.89/s  (2.571s,  398.28/s)  LR: 2.131e-05  Data: 0.023 (1.970)
Train: 82 [ 250/1251 ( 20%)]  Loss:  4.564462 (4.4468)  Time: 0.587s, 1743.71/s  (2.519s,  406.53/s)  LR: 2.131e-05  Data: 0.022 (1.921)
Train: 82 [ 300/1251 ( 24%)]  Loss:  5.020933 (4.5288)  Time: 0.583s, 1755.25/s  (2.489s,  411.38/s)  LR: 2.131e-05  Data: 0.019 (1.893)
Train: 82 [ 350/1251 ( 28%)]  Loss:  4.919178 (4.5776)  Time: 0.586s, 1748.07/s  (2.451s,  417.81/s)  LR: 2.131e-05  Data: 0.021 (1.854)
Train: 82 [ 400/1251 ( 32%)]  Loss:  4.251585 (4.5414)  Time: 5.996s,  170.78/s  (2.470s,  414.51/s)  LR: 2.131e-05  Data: 5.417 (1.872)
Train: 82 [ 450/1251 ( 36%)]  Loss:  4.563975 (4.5436)  Time: 0.584s, 1752.86/s  (2.464s,  415.62/s)  LR: 2.131e-05  Data: 0.020 (1.864)
Train: 82 [ 500/1251 ( 40%)]  Loss:  4.421340 (4.5325)  Time: 6.470s,  158.26/s  (2.460s,  416.25/s)  LR: 2.131e-05  Data: 5.819 (1.862)
Train: 82 [ 550/1251 ( 44%)]  Loss:  4.905675 (4.5636)  Time: 0.584s, 1754.84/s  (2.445s,  418.87/s)  LR: 2.131e-05  Data: 0.020 (1.844)
Train: 82 [ 600/1251 ( 48%)]  Loss:  4.757186 (4.5785)  Time: 6.004s,  170.56/s  (2.436s,  420.36/s)  LR: 2.131e-05  Data: 5.339 (1.835)
Train: 82 [ 650/1251 ( 52%)]  Loss:  4.783195 (4.5931)  Time: 0.585s, 1751.28/s  (2.424s,  422.39/s)  LR: 2.131e-05  Data: 0.019 (1.824)
Train: 82 [ 700/1251 ( 56%)]  Loss:  4.692896 (4.5998)  Time: 2.623s,  390.38/s  (2.416s,  423.92/s)  LR: 2.131e-05  Data: 1.955 (1.815)
Train: 82 [ 750/1251 ( 60%)]  Loss:  4.465349 (4.5914)  Time: 0.583s, 1756.59/s  (2.403s,  426.05/s)  LR: 2.131e-05  Data: 0.018 (1.802)
Train: 82 [ 800/1251 ( 64%)]  Loss:  4.487446 (4.5853)  Time: 8.025s,  127.60/s  (2.418s,  423.57/s)  LR: 2.131e-05  Data: 7.372 (1.816)
Train: 82 [ 850/1251 ( 68%)]  Loss:  4.742183 (4.5940)  Time: 0.584s, 1752.14/s  (2.416s,  423.76/s)  LR: 2.131e-05  Data: 0.018 (1.815)
Train: 82 [ 900/1251 ( 72%)]  Loss:  4.414988 (4.5846)  Time: 6.803s,  150.53/s  (2.416s,  423.80/s)  LR: 2.131e-05  Data: 6.097 (1.816)
Train: 82 [ 950/1251 ( 76%)]  Loss:  4.485297 (4.5796)  Time: 0.585s, 1751.22/s  (2.404s,  425.91/s)  LR: 2.131e-05  Data: 0.018 (1.803)
Train: 82 [1000/1251 ( 80%)]  Loss:  4.343978 (4.5684)  Time: 7.218s,  141.88/s  (2.404s,  425.96/s)  LR: 2.131e-05  Data: 6.648 (1.803)
Train: 82 [1050/1251 ( 84%)]  Loss:  4.995903 (4.5878)  Time: 0.585s, 1749.05/s  (2.393s,  427.92/s)  LR: 2.131e-05  Data: 0.020 (1.793)
Train: 82 [1100/1251 ( 88%)]  Loss:  4.963584 (4.6042)  Time: 6.740s,  151.92/s  (2.388s,  428.79/s)  LR: 2.131e-05  Data: 6.177 (1.788)
Train: 82 [1150/1251 ( 92%)]  Loss:  4.309082 (4.5919)  Time: 0.587s, 1745.27/s  (2.376s,  430.97/s)  LR: 2.131e-05  Data: 0.019 (1.777)
Train: 82 [1200/1251 ( 96%)]  Loss:  4.672701 (4.5951)  Time: 7.695s,  133.07/s  (2.392s,  428.08/s)  LR: 2.131e-05  Data: 7.055 (1.793)
Train: 82 [1250/1251 (100%)]  Loss:  4.704233 (4.5993)  Time: 0.565s, 1812.21/s  (2.394s,  427.68/s)  LR: 2.131e-05  Data: 0.000 (1.795)
Test: [   0/48]  Time: 15.389 (15.389)  Loss:  1.1662 (1.1662)  Acc@1: 76.5625 (76.5625)  Acc@5: 91.6016 (91.6016)
Test: [  48/48]  Time: 0.148 (3.388)  Loss:  1.1689 (2.0660)  Acc@1: 75.8255 (55.0360)  Acc@5: 90.2123 (79.0820)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-82.pth.tar', 55.03599997070312)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-80.pth.tar', 54.878000053710934)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-81.pth.tar', 54.834000021972656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-79.pth.tar', 54.68600004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-78.pth.tar', 54.49200002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-77.pth.tar', 54.2779999194336)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-76.pth.tar', 53.70800015625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-74.pth.tar', 53.65200008056641)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-75.pth.tar', 53.572000083007815)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-72.pth.tar', 52.95200000244141)

Train: 83 [   0/1251 (  0%)]  Loss:  4.658371 (4.6584)  Time: 11.447s,   89.46/s  (11.447s,   89.46/s)  LR: 1.786e-05  Data: 10.529 (10.529)
Train: 83 [  50/1251 (  4%)]  Loss:  4.548325 (4.6033)  Time: 0.585s, 1751.11/s  (2.381s,  430.10/s)  LR: 1.786e-05  Data: 0.021 (1.773)
Train: 83 [ 100/1251 (  8%)]  Loss:  5.007893 (4.7382)  Time: 0.586s, 1747.86/s  (2.320s,  441.39/s)  LR: 1.786e-05  Data: 0.021 (1.725)
Train: 83 [ 150/1251 ( 12%)]  Loss:  4.548234 (4.6907)  Time: 0.587s, 1743.84/s  (2.247s,  455.75/s)  LR: 1.786e-05  Data: 0.018 (1.645)
Train: 83 [ 200/1251 ( 16%)]  Loss:  5.085734 (4.7697)  Time: 2.028s,  505.04/s  (2.232s,  458.69/s)  LR: 1.786e-05  Data: 1.352 (1.631)
Train: 83 [ 250/1251 ( 20%)]  Loss:  4.284457 (4.6888)  Time: 0.585s, 1751.70/s  (2.275s,  450.21/s)  LR: 1.786e-05  Data: 0.021 (1.673)
Train: 83 [ 300/1251 ( 24%)]  Loss:  4.795182 (4.7040)  Time: 4.565s,  224.31/s  (2.313s,  442.74/s)  LR: 1.786e-05  Data: 3.878 (1.707)
Train: 83 [ 350/1251 ( 28%)]  Loss:  4.688813 (4.7021)  Time: 0.583s, 1756.94/s  (2.293s,  446.65/s)  LR: 1.786e-05  Data: 0.017 (1.687)
Train: 83 [ 400/1251 ( 32%)]  Loss:  5.123155 (4.7489)  Time: 2.782s,  368.10/s  (2.327s,  440.09/s)  LR: 1.786e-05  Data: 1.994 (1.721)
Train: 83 [ 450/1251 ( 36%)]  Loss:  4.282579 (4.7023)  Time: 0.590s, 1736.05/s  (2.312s,  442.84/s)  LR: 1.786e-05  Data: 0.022 (1.705)
Train: 83 [ 500/1251 ( 40%)]  Loss:  4.500720 (4.6840)  Time: 2.054s,  498.58/s  (2.306s,  444.11/s)  LR: 1.786e-05  Data: 1.445 (1.699)
Train: 83 [ 550/1251 ( 44%)]  Loss:  4.821567 (4.6954)  Time: 0.589s, 1738.13/s  (2.282s,  448.71/s)  LR: 1.786e-05  Data: 0.026 (1.676)
Train: 83 [ 600/1251 ( 48%)]  Loss:  4.931355 (4.7136)  Time: 0.586s, 1747.96/s  (2.275s,  450.11/s)  LR: 1.786e-05  Data: 0.021 (1.670)
Train: 83 [ 650/1251 ( 52%)]  Loss:  4.371288 (4.6891)  Time: 4.621s,  221.59/s  (2.306s,  444.15/s)  LR: 1.786e-05  Data: 4.016 (1.699)
Train: 83 [ 700/1251 ( 56%)]  Loss:  4.161093 (4.6539)  Time: 0.585s, 1750.44/s  (2.316s,  442.09/s)  LR: 1.786e-05  Data: 0.021 (1.711)
Train: 83 [ 750/1251 ( 60%)]  Loss:  4.632661 (4.6526)  Time: 2.839s,  360.67/s  (2.318s,  441.83/s)  LR: 1.786e-05  Data: 2.256 (1.711)
Train: 83 [ 800/1251 ( 64%)]  Loss:  4.964650 (4.6709)  Time: 0.585s, 1749.24/s  (2.320s,  441.39/s)  LR: 1.786e-05  Data: 0.021 (1.712)
Train: 83 [ 850/1251 ( 68%)]  Loss:  4.588907 (4.6664)  Time: 2.107s,  485.92/s  (2.315s,  442.36/s)  LR: 1.786e-05  Data: 1.372 (1.706)
Train: 83 [ 900/1251 ( 72%)]  Loss:  5.124385 (4.6905)  Time: 0.583s, 1756.23/s  (2.311s,  443.19/s)  LR: 1.786e-05  Data: 0.019 (1.703)
Train: 83 [ 950/1251 ( 76%)]  Loss:  4.565285 (4.6842)  Time: 0.585s, 1749.84/s  (2.302s,  444.86/s)  LR: 1.786e-05  Data: 0.019 (1.694)
Train: 83 [1000/1251 ( 80%)]  Loss:  5.288794 (4.7130)  Time: 0.583s, 1756.71/s  (2.314s,  442.61/s)  LR: 1.786e-05  Data: 0.020 (1.705)
Train: 83 [1050/1251 ( 84%)]  Loss:  4.216440 (4.6904)  Time: 3.636s,  281.61/s  (2.317s,  442.02/s)  LR: 1.786e-05  Data: 3.031 (1.709)
Train: 83 [1100/1251 ( 88%)]  Loss:  4.102409 (4.6649)  Time: 0.583s, 1757.00/s  (2.325s,  440.37/s)  LR: 1.786e-05  Data: 0.018 (1.717)
Train: 83 [1150/1251 ( 92%)]  Loss:  4.680417 (4.6655)  Time: 0.587s, 1743.19/s  (2.322s,  440.98/s)  LR: 1.786e-05  Data: 0.018 (1.715)
Train: 83 [1200/1251 ( 96%)]  Loss:  4.460627 (4.6573)  Time: 0.582s, 1760.83/s  (2.320s,  441.33/s)  LR: 1.786e-05  Data: 0.018 (1.714)
Train: 83 [1250/1251 (100%)]  Loss:  5.301295 (4.6821)  Time: 0.562s, 1821.78/s  (2.311s,  443.10/s)  LR: 1.786e-05  Data: 0.000 (1.705)
Test: [   0/48]  Time: 13.181 (13.181)  Loss:  1.1780 (1.1780)  Acc@1: 76.5625 (76.5625)  Acc@5: 91.2109 (91.2109)
Test: [  48/48]  Time: 0.149 (3.258)  Loss:  1.1641 (2.0689)  Acc@1: 76.1792 (55.1240)  Acc@5: 90.4481 (79.1240)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-83.pth.tar', 55.12399999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-82.pth.tar', 55.03599997070312)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-80.pth.tar', 54.878000053710934)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-81.pth.tar', 54.834000021972656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-79.pth.tar', 54.68600004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-78.pth.tar', 54.49200002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-77.pth.tar', 54.2779999194336)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-76.pth.tar', 53.70800015625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-74.pth.tar', 53.65200008056641)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-75.pth.tar', 53.572000083007815)

Train: 84 [   0/1251 (  0%)]  Loss:  5.128624 (5.1286)  Time: 17.070s,   59.99/s  (17.070s,   59.99/s)  LR: 1.504e-05  Data: 15.719 (15.719)
Train: 84 [  50/1251 (  4%)]  Loss:  4.886315 (5.0075)  Time: 0.583s, 1757.72/s  (2.338s,  438.05/s)  LR: 1.504e-05  Data: 0.019 (1.733)
Train: 84 [ 100/1251 (  8%)]  Loss:  5.221993 (5.0790)  Time: 0.588s, 1740.87/s  (2.476s,  413.65/s)  LR: 1.504e-05  Data: 0.022 (1.876)
Train: 84 [ 150/1251 ( 12%)]  Loss:  4.921781 (5.0397)  Time: 0.582s, 1758.51/s  (2.435s,  420.55/s)  LR: 1.504e-05  Data: 0.020 (1.842)
Train: 84 [ 200/1251 ( 16%)]  Loss:  5.053161 (5.0424)  Time: 0.589s, 1739.43/s  (2.413s,  424.37/s)  LR: 1.504e-05  Data: 0.018 (1.820)
Train: 84 [ 250/1251 ( 20%)]  Loss:  4.391141 (4.9338)  Time: 0.586s, 1747.11/s  (2.361s,  433.75/s)  LR: 1.504e-05  Data: 0.021 (1.768)
Train: 84 [ 300/1251 ( 24%)]  Loss:  4.860847 (4.9234)  Time: 0.588s, 1740.38/s  (2.330s,  439.48/s)  LR: 1.504e-05  Data: 0.017 (1.733)
Train: 84 [ 350/1251 ( 28%)]  Loss:  4.952528 (4.9270)  Time: 1.023s, 1001.28/s  (2.291s,  446.96/s)  LR: 1.504e-05  Data: 0.358 (1.694)
Train: 84 [ 400/1251 ( 32%)]  Loss:  4.627666 (4.8938)  Time: 0.590s, 1736.45/s  (2.277s,  449.79/s)  LR: 1.504e-05  Data: 0.020 (1.676)
Train: 84 [ 450/1251 ( 36%)]  Loss:  4.993579 (4.9038)  Time: 2.511s,  407.74/s  (2.253s,  454.51/s)  LR: 1.504e-05  Data: 1.949 (1.652)
Train: 84 [ 500/1251 ( 40%)]  Loss:  5.036506 (4.9158)  Time: 0.588s, 1742.11/s  (2.291s,  447.00/s)  LR: 1.504e-05  Data: 0.017 (1.687)
Train: 84 [ 550/1251 ( 44%)]  Loss:  4.554231 (4.8857)  Time: 0.586s, 1747.50/s  (2.285s,  448.24/s)  LR: 1.504e-05  Data: 0.020 (1.682)
Train: 84 [ 600/1251 ( 48%)]  Loss:  4.697720 (4.8712)  Time: 1.280s,  799.74/s  (2.295s,  446.20/s)  LR: 1.504e-05  Data: 0.631 (1.691)
Train: 84 [ 650/1251 ( 52%)]  Loss:  4.040600 (4.8119)  Time: 0.582s, 1758.19/s  (2.282s,  448.80/s)  LR: 1.504e-05  Data: 0.019 (1.678)
Train: 84 [ 700/1251 ( 56%)]  Loss:  4.975129 (4.8228)  Time: 2.392s,  428.08/s  (2.287s,  447.75/s)  LR: 1.504e-05  Data: 1.797 (1.683)
Train: 84 [ 750/1251 ( 60%)]  Loss:  4.459774 (4.8001)  Time: 0.585s, 1749.69/s  (2.276s,  449.95/s)  LR: 1.504e-05  Data: 0.018 (1.670)
Train: 84 [ 800/1251 ( 64%)]  Loss:  4.840170 (4.8025)  Time: 1.141s,  897.69/s  (2.271s,  450.90/s)  LR: 1.504e-05  Data: 0.579 (1.666)
Train: 84 [ 850/1251 ( 68%)]  Loss:  5.130148 (4.8207)  Time: 0.589s, 1739.13/s  (2.260s,  453.16/s)  LR: 1.504e-05  Data: 0.018 (1.655)
Train: 84 [ 900/1251 ( 72%)]  Loss:  4.935292 (4.8267)  Time: 0.589s, 1737.25/s  (2.277s,  449.62/s)  LR: 1.504e-05  Data: 0.023 (1.673)
Train: 84 [ 950/1251 ( 76%)]  Loss:  4.558409 (4.8133)  Time: 0.584s, 1753.12/s  (2.276s,  449.99/s)  LR: 1.504e-05  Data: 0.022 (1.671)
Train: 84 [1000/1251 ( 80%)]  Loss:  4.268512 (4.7873)  Time: 0.716s, 1429.78/s  (2.282s,  448.71/s)  LR: 1.504e-05  Data: 0.019 (1.678)
Train: 84 [1050/1251 ( 84%)]  Loss:  4.579425 (4.7779)  Time: 0.588s, 1740.87/s  (2.281s,  448.91/s)  LR: 1.504e-05  Data: 0.023 (1.674)
Train: 84 [1100/1251 ( 88%)]  Loss:  5.144198 (4.7938)  Time: 6.335s,  161.65/s  (2.277s,  449.79/s)  LR: 1.504e-05  Data: 5.754 (1.670)
Train: 84 [1150/1251 ( 92%)]  Loss:  4.956511 (4.8006)  Time: 0.790s, 1295.95/s  (2.270s,  451.15/s)  LR: 1.504e-05  Data: 0.181 (1.664)
Train: 84 [1200/1251 ( 96%)]  Loss:  4.925880 (4.8056)  Time: 5.711s,  179.31/s  (2.264s,  452.28/s)  LR: 1.504e-05  Data: 5.104 (1.659)
Train: 84 [1250/1251 (100%)]  Loss:  5.033215 (4.8144)  Time: 0.562s, 1821.11/s  (2.251s,  454.96/s)  LR: 1.504e-05  Data: 0.000 (1.647)
Test: [   0/48]  Time: 22.535 (22.535)  Loss:  1.1673 (1.1673)  Acc@1: 76.6602 (76.6602)  Acc@5: 91.4062 (91.4062)
Test: [  48/48]  Time: 0.148 (3.665)  Loss:  1.1731 (2.0676)  Acc@1: 75.5896 (55.2820)  Acc@5: 90.4481 (79.2040)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-84.pth.tar', 55.28200012695312)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-83.pth.tar', 55.12399999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-82.pth.tar', 55.03599997070312)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-80.pth.tar', 54.878000053710934)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-81.pth.tar', 54.834000021972656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-79.pth.tar', 54.68600004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-78.pth.tar', 54.49200002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-77.pth.tar', 54.2779999194336)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-76.pth.tar', 53.70800015625)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-74.pth.tar', 53.65200008056641)

Train: 85 [   0/1251 (  0%)]  Loss:  5.159058 (5.1591)  Time: 11.150s,   91.84/s  (11.150s,   91.84/s)  LR: 1.284e-05  Data: 9.892 (9.892)
Train: 85 [  50/1251 (  4%)]  Loss:  5.022694 (5.0909)  Time: 0.585s, 1749.81/s  (2.468s,  414.91/s)  LR: 1.284e-05  Data: 0.023 (1.848)
Train: 85 [ 100/1251 (  8%)]  Loss:  4.805016 (4.9956)  Time: 3.666s,  279.32/s  (2.398s,  426.93/s)  LR: 1.284e-05  Data: 3.026 (1.779)
Train: 85 [ 150/1251 ( 12%)]  Loss:  4.757620 (4.9361)  Time: 0.585s, 1750.08/s  (2.320s,  441.47/s)  LR: 1.284e-05  Data: 0.019 (1.702)
Train: 85 [ 200/1251 ( 16%)]  Loss:  4.356238 (4.8201)  Time: 4.914s,  208.37/s  (2.289s,  447.35/s)  LR: 1.284e-05  Data: 4.342 (1.673)
Train: 85 [ 250/1251 ( 20%)]  Loss:  4.662880 (4.7939)  Time: 0.585s, 1751.36/s  (2.234s,  458.42/s)  LR: 1.284e-05  Data: 0.020 (1.624)
Train: 85 [ 300/1251 ( 24%)]  Loss:  4.038915 (4.6861)  Time: 4.696s,  218.07/s  (2.208s,  463.81/s)  LR: 1.284e-05  Data: 4.118 (1.599)
Train: 85 [ 350/1251 ( 28%)]  Loss:  5.028702 (4.7289)  Time: 0.585s, 1751.89/s  (2.222s,  460.88/s)  LR: 1.284e-05  Data: 0.019 (1.614)
Train: 85 [ 400/1251 ( 32%)]  Loss:  5.051117 (4.7647)  Time: 7.950s,  128.81/s  (2.254s,  454.33/s)  LR: 1.284e-05  Data: 7.202 (1.646)
Train: 85 [ 450/1251 ( 36%)]  Loss:  4.388906 (4.7271)  Time: 0.584s, 1752.21/s  (2.251s,  454.92/s)  LR: 1.284e-05  Data: 0.021 (1.643)
Train: 85 [ 500/1251 ( 40%)]  Loss:  4.210123 (4.6801)  Time: 6.643s,  154.15/s  (2.254s,  454.29/s)  LR: 1.284e-05  Data: 5.967 (1.645)
Train: 85 [ 550/1251 ( 44%)]  Loss:  4.818029 (4.6916)  Time: 0.584s, 1753.69/s  (2.247s,  455.81/s)  LR: 1.284e-05  Data: 0.021 (1.640)
Train: 85 [ 600/1251 ( 48%)]  Loss:  5.031374 (4.7177)  Time: 11.896s,   86.08/s  (2.252s,  454.65/s)  LR: 1.284e-05  Data: 10.749 (1.646)
Train: 85 [ 650/1251 ( 52%)]  Loss:  4.898896 (4.7307)  Time: 0.584s, 1754.22/s  (2.241s,  456.93/s)  LR: 1.284e-05  Data: 0.020 (1.636)
Train: 85 [ 700/1251 ( 56%)]  Loss:  5.231377 (4.7641)  Time: 3.287s,  311.57/s  (2.238s,  457.56/s)  LR: 1.284e-05  Data: 2.702 (1.632)
Train: 85 [ 750/1251 ( 60%)]  Loss:  4.497342 (4.7474)  Time: 0.583s, 1755.70/s  (2.251s,  454.93/s)  LR: 1.284e-05  Data: 0.020 (1.646)
Train: 85 [ 800/1251 ( 64%)]  Loss:  5.029232 (4.7640)  Time: 0.588s, 1742.84/s  (2.261s,  452.99/s)  LR: 1.284e-05  Data: 0.021 (1.657)
Train: 85 [ 850/1251 ( 68%)]  Loss:  4.824008 (4.7673)  Time: 0.582s, 1760.96/s  (2.260s,  453.07/s)  LR: 1.284e-05  Data: 0.018 (1.656)
Train: 85 [ 900/1251 ( 72%)]  Loss:  4.384718 (4.7472)  Time: 0.587s, 1744.79/s  (2.264s,  452.39/s)  LR: 1.284e-05  Data: 0.022 (1.660)
Train: 85 [ 950/1251 ( 76%)]  Loss:  4.778085 (4.7487)  Time: 0.585s, 1750.78/s  (2.256s,  453.84/s)  LR: 1.284e-05  Data: 0.023 (1.654)
Train: 85 [1000/1251 ( 80%)]  Loss:  5.136482 (4.7672)  Time: 0.585s, 1751.61/s  (2.247s,  455.66/s)  LR: 1.284e-05  Data: 0.022 (1.645)
Train: 85 [1050/1251 ( 84%)]  Loss:  4.013627 (4.7329)  Time: 0.584s, 1752.46/s  (2.237s,  457.81/s)  LR: 1.284e-05  Data: 0.019 (1.634)
Train: 85 [1100/1251 ( 88%)]  Loss:  4.889476 (4.7397)  Time: 0.591s, 1733.83/s  (2.232s,  458.71/s)  LR: 1.284e-05  Data: 0.023 (1.631)
Train: 85 [1150/1251 ( 92%)]  Loss:  3.930283 (4.7060)  Time: 0.582s, 1758.67/s  (2.233s,  458.63/s)  LR: 1.284e-05  Data: 0.021 (1.632)
Train: 85 [1200/1251 ( 96%)]  Loss:  4.444639 (4.6956)  Time: 0.584s, 1754.00/s  (2.237s,  457.76/s)  LR: 1.284e-05  Data: 0.021 (1.637)
Train: 85 [1250/1251 (100%)]  Loss:  4.983424 (4.7066)  Time: 0.562s, 1822.27/s  (2.234s,  458.31/s)  LR: 1.284e-05  Data: 0.000 (1.635)
Test: [   0/48]  Time: 13.755 (13.755)  Loss:  1.1649 (1.1649)  Acc@1: 76.4648 (76.4648)  Acc@5: 91.9922 (91.9922)
Test: [  48/48]  Time: 0.152 (3.135)  Loss:  1.1735 (2.0649)  Acc@1: 76.1792 (55.2980)  Acc@5: 90.0943 (79.3900)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-85.pth.tar', 55.29799999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-84.pth.tar', 55.28200012695312)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-83.pth.tar', 55.12399999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-82.pth.tar', 55.03599997070312)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-80.pth.tar', 54.878000053710934)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-81.pth.tar', 54.834000021972656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-79.pth.tar', 54.68600004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-78.pth.tar', 54.49200002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-77.pth.tar', 54.2779999194336)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-76.pth.tar', 53.70800015625)

Train: 86 [   0/1251 (  0%)]  Loss:  4.535927 (4.5359)  Time: 9.749s,  105.04/s  (9.749s,  105.04/s)  LR: 1.126e-05  Data: 9.155 (9.155)
Train: 86 [  50/1251 (  4%)]  Loss:  4.776719 (4.6563)  Time: 0.583s, 1757.68/s  (2.095s,  488.75/s)  LR: 1.126e-05  Data: 0.020 (1.500)
Train: 86 [ 100/1251 (  8%)]  Loss:  4.670466 (4.6610)  Time: 0.584s, 1753.00/s  (2.048s,  499.88/s)  LR: 1.126e-05  Data: 0.020 (1.457)
Train: 86 [ 150/1251 ( 12%)]  Loss:  5.156058 (4.7848)  Time: 0.585s, 1751.51/s  (1.995s,  513.31/s)  LR: 1.126e-05  Data: 0.020 (1.405)
Train: 86 [ 200/1251 ( 16%)]  Loss:  5.229602 (4.8738)  Time: 3.661s,  279.68/s  (1.988s,  514.98/s)  LR: 1.126e-05  Data: 3.099 (1.396)
Train: 86 [ 250/1251 ( 20%)]  Loss:  4.628257 (4.8328)  Time: 0.588s, 1742.65/s  (1.993s,  513.76/s)  LR: 1.126e-05  Data: 0.019 (1.397)
Train: 86 [ 300/1251 ( 24%)]  Loss:  4.418095 (4.7736)  Time: 6.554s,  156.25/s  (2.061s,  496.86/s)  LR: 1.126e-05  Data: 5.863 (1.464)
Train: 86 [ 350/1251 ( 28%)]  Loss:  5.149691 (4.8206)  Time: 0.587s, 1743.11/s  (2.041s,  501.61/s)  LR: 1.126e-05  Data: 0.022 (1.445)
Train: 86 [ 400/1251 ( 32%)]  Loss:  4.576767 (4.7935)  Time: 2.487s,  411.82/s  (2.050s,  499.54/s)  LR: 1.126e-05  Data: 1.800 (1.451)
Train: 86 [ 450/1251 ( 36%)]  Loss:  4.170888 (4.7312)  Time: 0.585s, 1750.23/s  (2.040s,  501.98/s)  LR: 1.126e-05  Data: 0.019 (1.440)
Train: 86 [ 500/1251 ( 40%)]  Loss:  4.673514 (4.7260)  Time: 0.585s, 1749.92/s  (2.034s,  503.33/s)  LR: 1.126e-05  Data: 0.018 (1.435)
Train: 86 [ 550/1251 ( 44%)]  Loss:  4.695374 (4.7234)  Time: 1.841s,  556.31/s  (2.022s,  506.53/s)  LR: 1.126e-05  Data: 1.248 (1.422)
Train: 86 [ 600/1251 ( 48%)]  Loss:  4.970438 (4.7424)  Time: 0.584s, 1753.85/s  (2.013s,  508.74/s)  LR: 1.126e-05  Data: 0.019 (1.414)
Train: 86 [ 650/1251 ( 52%)]  Loss:  4.611686 (4.7331)  Time: 0.583s, 1755.10/s  (2.001s,  511.75/s)  LR: 1.126e-05  Data: 0.018 (1.402)
Train: 86 [ 700/1251 ( 56%)]  Loss:  4.781024 (4.7363)  Time: 0.585s, 1749.04/s  (2.011s,  509.30/s)  LR: 1.126e-05  Data: 0.019 (1.411)
Train: 86 [ 750/1251 ( 60%)]  Loss:  4.630004 (4.7297)  Time: 0.587s, 1745.30/s  (2.015s,  508.10/s)  LR: 1.126e-05  Data: 0.017 (1.416)
Train: 86 [ 800/1251 ( 64%)]  Loss:  5.035587 (4.7477)  Time: 0.583s, 1755.33/s  (2.015s,  508.19/s)  LR: 1.126e-05  Data: 0.018 (1.416)
Train: 86 [ 850/1251 ( 68%)]  Loss:  4.759629 (4.7483)  Time: 0.586s, 1746.78/s  (2.017s,  507.65/s)  LR: 1.126e-05  Data: 0.018 (1.418)
Train: 86 [ 900/1251 ( 72%)]  Loss:  4.064119 (4.7123)  Time: 0.588s, 1741.90/s  (2.014s,  508.51/s)  LR: 1.126e-05  Data: 0.025 (1.412)
Train: 86 [ 950/1251 ( 76%)]  Loss:  4.439854 (4.6987)  Time: 5.913s,  173.18/s  (2.012s,  508.87/s)  LR: 1.126e-05  Data: 5.351 (1.412)
Train: 86 [1000/1251 ( 80%)]  Loss:  4.811549 (4.7041)  Time: 0.586s, 1747.32/s  (2.007s,  510.20/s)  LR: 1.126e-05  Data: 0.022 (1.406)
Train: 86 [1050/1251 ( 84%)]  Loss:  4.434498 (4.6918)  Time: 3.513s,  291.50/s  (2.005s,  510.81/s)  LR: 1.126e-05  Data: 2.846 (1.402)
Train: 86 [1100/1251 ( 88%)]  Loss:  4.670316 (4.6909)  Time: 3.713s,  275.76/s  (2.001s,  511.69/s)  LR: 1.126e-05  Data: 2.549 (1.398)
Train: 86 [1150/1251 ( 92%)]  Loss:  4.443796 (4.6806)  Time: 3.394s,  301.70/s  (2.008s,  510.07/s)  LR: 1.126e-05  Data: 2.804 (1.404)
Train: 86 [1200/1251 ( 96%)]  Loss:  4.944149 (4.6911)  Time: 4.215s,  242.94/s  (2.016s,  507.83/s)  LR: 1.126e-05  Data: 3.251 (1.414)
Train: 86 [1250/1251 (100%)]  Loss:  5.226152 (4.7117)  Time: 0.567s, 1804.91/s  (2.021s,  506.58/s)  LR: 1.126e-05  Data: 0.000 (1.418)
Test: [   0/48]  Time: 16.015 (16.015)  Loss:  1.1511 (1.1511)  Acc@1: 76.5625 (76.5625)  Acc@5: 92.0898 (92.0898)
Test: [  48/48]  Time: 0.148 (3.266)  Loss:  1.1700 (2.0641)  Acc@1: 75.7076 (55.2600)  Acc@5: 90.0943 (79.3400)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-85.pth.tar', 55.29799999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-84.pth.tar', 55.28200012695312)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-86.pth.tar', 55.260000048828125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-83.pth.tar', 55.12399999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-82.pth.tar', 55.03599997070312)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-80.pth.tar', 54.878000053710934)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-81.pth.tar', 54.834000021972656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-79.pth.tar', 54.68600004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-78.pth.tar', 54.49200002441406)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-77.pth.tar', 54.2779999194336)

Train: 87 [   0/1251 (  0%)]  Loss:  5.084037 (5.0840)  Time: 10.716s,   95.55/s  (10.716s,   95.55/s)  LR: 1.032e-05  Data: 10.047 (10.047)
Train: 87 [  50/1251 (  4%)]  Loss:  4.767491 (4.9258)  Time: 0.586s, 1747.75/s  (2.514s,  407.26/s)  LR: 1.032e-05  Data: 0.019 (1.925)
Train: 87 [ 100/1251 (  8%)]  Loss:  4.801629 (4.8844)  Time: 0.586s, 1748.36/s  (2.289s,  447.41/s)  LR: 1.032e-05  Data: 0.021 (1.701)
Train: 87 [ 150/1251 ( 12%)]  Loss:  4.722411 (4.8439)  Time: 0.716s, 1429.22/s  (2.146s,  477.18/s)  LR: 1.032e-05  Data: 0.072 (1.558)
Train: 87 [ 200/1251 ( 16%)]  Loss:  4.361353 (4.7474)  Time: 0.588s, 1742.79/s  (2.111s,  485.02/s)  LR: 1.032e-05  Data: 0.020 (1.524)
Train: 87 [ 250/1251 ( 20%)]  Loss:  4.702840 (4.7400)  Time: 2.344s,  436.94/s  (2.154s,  475.29/s)  LR: 1.032e-05  Data: 1.641 (1.564)
Train: 87 [ 300/1251 ( 24%)]  Loss:  4.706377 (4.7352)  Time: 0.582s, 1759.10/s  (2.173s,  471.24/s)  LR: 1.032e-05  Data: 0.020 (1.579)
Train: 87 [ 350/1251 ( 28%)]  Loss:  4.571166 (4.7147)  Time: 0.581s, 1763.09/s  (2.170s,  471.87/s)  LR: 1.032e-05  Data: 0.016 (1.577)
Train: 87 [ 400/1251 ( 32%)]  Loss:  4.859352 (4.7307)  Time: 0.582s, 1760.77/s  (2.181s,  469.54/s)  LR: 1.032e-05  Data: 0.018 (1.587)
Train: 87 [ 450/1251 ( 36%)]  Loss:  5.014882 (4.7592)  Time: 0.789s, 1297.09/s  (2.163s,  473.34/s)  LR: 1.032e-05  Data: 0.228 (1.569)
Train: 87 [ 500/1251 ( 40%)]  Loss:  4.429511 (4.7292)  Time: 0.585s, 1751.53/s  (2.151s,  475.97/s)  LR: 1.032e-05  Data: 0.017 (1.557)
Train: 87 [ 550/1251 ( 44%)]  Loss:  4.771598 (4.7327)  Time: 4.471s,  229.02/s  (2.142s,  478.12/s)  LR: 1.032e-05  Data: 3.809 (1.548)
Train: 87 [ 600/1251 ( 48%)]  Loss:  5.111568 (4.7619)  Time: 0.584s, 1753.63/s  (2.139s,  478.67/s)  LR: 1.032e-05  Data: 0.018 (1.545)
Train: 87 [ 650/1251 ( 52%)]  Loss:  4.352514 (4.7326)  Time: 11.226s,   91.22/s  (2.144s,  477.72/s)  LR: 1.032e-05  Data: 10.576 (1.547)
Train: 87 [ 700/1251 ( 56%)]  Loss:  4.505804 (4.7175)  Time: 0.583s, 1756.13/s  (2.168s,  472.27/s)  LR: 1.032e-05  Data: 0.017 (1.571)
Train: 87 [ 750/1251 ( 60%)]  Loss:  4.716176 (4.7174)  Time: 4.089s,  250.41/s  (2.169s,  472.04/s)  LR: 1.032e-05  Data: 3.528 (1.571)
Train: 87 [ 800/1251 ( 64%)]  Loss:  4.924007 (4.7296)  Time: 0.584s, 1752.25/s  (2.171s,  471.71/s)  LR: 1.032e-05  Data: 0.018 (1.572)
Train: 87 [ 850/1251 ( 68%)]  Loss:  3.995063 (4.6888)  Time: 6.132s,  166.99/s  (2.166s,  472.80/s)  LR: 1.032e-05  Data: 5.532 (1.566)
Train: 87 [ 900/1251 ( 72%)]  Loss:  3.961963 (4.6505)  Time: 0.596s, 1718.76/s  (2.161s,  473.78/s)  LR: 1.032e-05  Data: 0.019 (1.561)
Train: 87 [ 950/1251 ( 76%)]  Loss:  4.581758 (4.6471)  Time: 5.351s,  191.35/s  (2.157s,  474.83/s)  LR: 1.032e-05  Data: 4.644 (1.555)
Train: 87 [1000/1251 ( 80%)]  Loss:  4.879302 (4.6581)  Time: 0.586s, 1747.42/s  (2.149s,  476.47/s)  LR: 1.032e-05  Data: 0.021 (1.548)
Train: 87 [1050/1251 ( 84%)]  Loss:  4.884510 (4.6684)  Time: 4.244s,  241.28/s  (2.152s,  475.83/s)  LR: 1.032e-05  Data: 3.681 (1.551)
Train: 87 [1100/1251 ( 88%)]  Loss:  4.718510 (4.6706)  Time: 1.072s,  955.29/s  (2.163s,  473.43/s)  LR: 1.032e-05  Data: 0.425 (1.560)
Train: 87 [1150/1251 ( 92%)]  Loss:  4.899652 (4.6801)  Time: 2.785s,  367.69/s  (2.156s,  474.92/s)  LR: 1.032e-05  Data: 2.223 (1.553)
Train: 87 [1200/1251 ( 96%)]  Loss:  4.957471 (4.6912)  Time: 0.586s, 1746.24/s  (2.159s,  474.25/s)  LR: 1.032e-05  Data: 0.021 (1.556)
Train: 87 [1250/1251 (100%)]  Loss:  4.873949 (4.6983)  Time: 0.565s, 1812.58/s  (2.153s,  475.53/s)  LR: 1.032e-05  Data: 0.000 (1.550)
Test: [   0/48]  Time: 13.893 (13.893)  Loss:  1.1566 (1.1566)  Acc@1: 76.2695 (76.2695)  Acc@5: 92.1875 (92.1875)
Test: [  48/48]  Time: 0.150 (3.025)  Loss:  1.1574 (2.0601)  Acc@1: 76.2972 (55.4340)  Acc@5: 90.4481 (79.3760)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-87.pth.tar', 55.43400004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-85.pth.tar', 55.29799999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-84.pth.tar', 55.28200012695312)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-86.pth.tar', 55.260000048828125)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-83.pth.tar', 55.12399999511719)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-82.pth.tar', 55.03599997070312)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-80.pth.tar', 54.878000053710934)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-81.pth.tar', 54.834000021972656)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-79.pth.tar', 54.68600004882813)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_1k/checkpoint-78.pth.tar', 54.49200002441406)

*** Best metric: 55.43400004638672 (epoch 87)

wandb: Waiting for W&B process to finish, PID 6814
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210526_153118-PreTraining_vit_deit_tiny_patch16_224_1k/logs/debug.log
wandb: Find internal logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210526_153118-PreTraining_vit_deit_tiny_patch16_224_1k/logs/debug-internal.log
wandb: Run summary:
wandb:    eval_top1 55.434
wandb:    eval_top5 79.376
wandb:   _timestamp 1622079561
wandb:   train_loss 4.69827
wandb:        _step 87
wandb:        epoch 87
wandb:     _runtime 293021
wandb:    eval_loss 2.06012
wandb: Run history:
wandb:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   train_loss ‚ñÑ‚ñà‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÉ
wandb:    eval_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:    eval_top1 ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:    eval_top5 ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà
wandb:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced PreTraining_vit_deit_tiny_patch16_224_1k: https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_1k
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
--End--
Thu May 27 10:39:32 JST 2021

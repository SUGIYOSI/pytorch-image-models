--Start--
Fri Jun 4 14:08:43 JST 2021
Added key: store_based_barrier_key:1 to store for rank: 3
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 1
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
wandb: Currently logged in as: gyama_x (use `wandb login --relogin` to force relogin)
wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
wandb: Tracking run with wandb version 0.10.27
wandb: Resuming run PreTraining_vit_deit_tiny_patch16_224_fake_1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gyama_x/pytorch-image-models
wandb: üöÄ View run at https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run data is saved locally in /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210604_140951-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run `wandb offline` to turn off syncing.

Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar' (epoch 165)
Using native Torch DistributedDataParallel.
Scheduled epochs: 188
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Train: 166 [   0/1171 (  0%)]  Loss:  2.926656 (2.9267)  Time: 14.951s,   68.49/s  (14.951s,   68.49/s)  LR: 4.308e-05  Data: 13.829 (13.829)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 166 [  50/1171 (  4%)]  Loss:  2.976283 (2.9515)  Time: 0.583s, 1756.21/s  (2.316s,  442.07/s)  LR: 4.308e-05  Data: 0.021 (1.725)
Train: 166 [ 100/1171 (  9%)]  Loss:  2.953907 (2.9523)  Time: 0.587s, 1745.30/s  (2.254s,  454.38/s)  LR: 4.308e-05  Data: 0.023 (1.666)
Train: 166 [ 150/1171 ( 13%)]  Loss:  2.513397 (2.8426)  Time: 0.586s, 1747.84/s  (2.296s,  446.05/s)  LR: 4.308e-05  Data: 0.023 (1.709)
Train: 166 [ 200/1171 ( 17%)]  Loss:  2.627082 (2.7995)  Time: 0.585s, 1749.39/s  (2.279s,  449.23/s)  LR: 4.308e-05  Data: 0.021 (1.692)
Train: 166 [ 250/1171 ( 21%)]  Loss:  2.322077 (2.7199)  Time: 0.585s, 1751.76/s  (2.223s,  460.68/s)  LR: 4.308e-05  Data: 0.018 (1.634)
Train: 166 [ 300/1171 ( 26%)]  Loss:  3.024461 (2.7634)  Time: 0.605s, 1693.18/s  (2.201s,  465.25/s)  LR: 4.308e-05  Data: 0.039 (1.613)
Train: 166 [ 350/1171 ( 30%)]  Loss:  2.579340 (2.7404)  Time: 0.588s, 1742.03/s  (2.161s,  473.80/s)  LR: 4.308e-05  Data: 0.021 (1.574)
Train: 166 [ 400/1171 ( 34%)]  Loss:  2.921948 (2.7606)  Time: 0.811s, 1261.90/s  (2.144s,  477.68/s)  LR: 4.308e-05  Data: 0.248 (1.555)
Train: 166 [ 450/1171 ( 38%)]  Loss:  2.530944 (2.7376)  Time: 0.623s, 1643.58/s  (2.124s,  482.18/s)  LR: 4.308e-05  Data: 0.060 (1.534)
Train: 166 [ 500/1171 ( 43%)]  Loss:  2.661488 (2.7307)  Time: 1.662s,  616.00/s  (2.115s,  484.06/s)  LR: 4.308e-05  Data: 1.022 (1.524)
Train: 166 [ 550/1171 ( 47%)]  Loss:  2.765248 (2.7336)  Time: 2.887s,  354.68/s  (2.107s,  485.91/s)  LR: 4.308e-05  Data: 2.204 (1.514)
Train: 166 [ 600/1171 ( 51%)]  Loss:  3.340928 (2.7803)  Time: 0.583s, 1755.68/s  (2.119s,  483.27/s)  LR: 4.308e-05  Data: 0.021 (1.525)
Train: 166 [ 650/1171 ( 56%)]  Loss:  2.660182 (2.7717)  Time: 1.777s,  576.11/s  (2.120s,  482.92/s)  LR: 4.308e-05  Data: 1.209 (1.527)
Train: 166 [ 700/1171 ( 60%)]  Loss:  2.952790 (2.7838)  Time: 3.244s,  315.64/s  (2.135s,  479.57/s)  LR: 4.308e-05  Data: 2.682 (1.541)
Train: 166 [ 750/1171 ( 64%)]  Loss:  3.173856 (2.8082)  Time: 0.587s, 1745.92/s  (2.143s,  477.80/s)  LR: 4.308e-05  Data: 0.020 (1.548)
Train: 166 [ 800/1171 ( 68%)]  Loss:  2.748444 (2.8046)  Time: 6.892s,  148.57/s  (2.153s,  475.53/s)  LR: 4.308e-05  Data: 6.301 (1.557)
Train: 166 [ 850/1171 ( 73%)]  Loss:  2.773309 (2.8029)  Time: 1.340s,  764.01/s  (2.154s,  475.49/s)  LR: 4.308e-05  Data: 0.778 (1.556)
Train: 166 [ 900/1171 ( 77%)]  Loss:  3.090852 (2.8181)  Time: 5.447s,  187.98/s  (2.156s,  475.01/s)  LR: 4.308e-05  Data: 4.876 (1.558)
Train: 166 [ 950/1171 ( 81%)]  Loss:  2.579257 (2.8061)  Time: 3.558s,  287.79/s  (2.153s,  475.53/s)  LR: 4.308e-05  Data: 2.966 (1.556)
Train: 166 [1000/1171 ( 85%)]  Loss:  2.360593 (2.7849)  Time: 4.421s,  231.61/s  (2.171s,  471.77/s)  LR: 4.308e-05  Data: 3.756 (1.572)
Train: 166 [1050/1171 ( 90%)]  Loss:  2.865602 (2.7886)  Time: 3.441s,  297.59/s  (2.183s,  469.06/s)  LR: 4.308e-05  Data: 2.781 (1.585)
Train: 166 [1100/1171 ( 94%)]  Loss:  2.780988 (2.7882)  Time: 6.936s,  147.63/s  (2.189s,  467.80/s)  LR: 4.308e-05  Data: 6.358 (1.591)
Train: 166 [1150/1171 ( 98%)]  Loss:  2.440472 (2.7738)  Time: 3.356s,  305.17/s  (2.189s,  467.70/s)  LR: 4.308e-05  Data: 2.664 (1.591)
Train: 166 [1170/1171 (100%)]  Loss:  2.830082 (2.7760)  Time: 0.564s, 1816.63/s  (2.186s,  468.36/s)  LR: 4.308e-05  Data: 0.000 (1.588)
Test: [   0/97]  Time: 13.342 (13.342)  Loss:  0.2870 (0.2870)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.197 (2.929)  Loss:  0.4123 (0.3465)  Acc@1: 93.5547 (95.2474)  Acc@5: 98.4375 (98.9392)
Test: [  97/97]  Time: 0.484 (2.832)  Loss:  0.2904 (0.3592)  Acc@1: 95.0893 (94.7080)  Acc@5: 99.2560 (98.7950)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-166.pth.tar', 94.70800000732422)

Train: 167 [   0/1171 (  0%)]  Loss:  2.770082 (2.7701)  Time: 9.904s,  103.40/s  (9.904s,  103.40/s)  LR: 4.017e-05  Data: 8.805 (8.805)
Train: 167 [  50/1171 (  4%)]  Loss:  3.221725 (2.9959)  Time: 0.586s, 1747.14/s  (2.167s,  472.59/s)  LR: 4.017e-05  Data: 0.023 (1.576)
Train: 167 [ 100/1171 (  9%)]  Loss:  2.906456 (2.9661)  Time: 0.587s, 1745.63/s  (2.340s,  437.65/s)  LR: 4.017e-05  Data: 0.023 (1.755)
Train: 167 [ 150/1171 ( 13%)]  Loss:  2.679065 (2.8943)  Time: 0.585s, 1749.31/s  (2.254s,  454.40/s)  LR: 4.017e-05  Data: 0.020 (1.669)
Train: 167 [ 200/1171 ( 17%)]  Loss:  2.639287 (2.8433)  Time: 0.589s, 1739.83/s  (2.229s,  459.36/s)  LR: 4.017e-05  Data: 0.020 (1.646)
Train: 167 [ 250/1171 ( 21%)]  Loss:  2.536395 (2.7922)  Time: 0.589s, 1739.79/s  (2.178s,  470.21/s)  LR: 4.017e-05  Data: 0.019 (1.593)
Train: 167 [ 300/1171 ( 26%)]  Loss:  2.632876 (2.7694)  Time: 0.591s, 1731.51/s  (2.162s,  473.57/s)  LR: 4.017e-05  Data: 0.025 (1.578)
Train: 167 [ 350/1171 ( 30%)]  Loss:  2.741221 (2.7659)  Time: 0.587s, 1744.62/s  (2.128s,  481.20/s)  LR: 4.017e-05  Data: 0.024 (1.544)
Train: 167 [ 400/1171 ( 34%)]  Loss:  3.277825 (2.8228)  Time: 0.586s, 1748.18/s  (2.113s,  484.65/s)  LR: 4.017e-05  Data: 0.018 (1.529)
Train: 167 [ 450/1171 ( 38%)]  Loss:  2.622287 (2.8027)  Time: 0.586s, 1746.13/s  (2.090s,  489.97/s)  LR: 4.017e-05  Data: 0.024 (1.507)
Train: 167 [ 500/1171 ( 43%)]  Loss:  2.882777 (2.8100)  Time: 0.585s, 1750.75/s  (2.121s,  482.69/s)  LR: 4.017e-05  Data: 0.019 (1.537)
Train: 167 [ 550/1171 ( 47%)]  Loss:  2.754754 (2.8054)  Time: 0.656s, 1561.36/s  (2.137s,  479.20/s)  LR: 4.017e-05  Data: 0.045 (1.552)
Train: 167 [ 600/1171 ( 51%)]  Loss:  3.000056 (2.8204)  Time: 0.584s, 1752.16/s  (2.164s,  473.22/s)  LR: 4.017e-05  Data: 0.019 (1.578)
Train: 167 [ 650/1171 ( 56%)]  Loss:  2.862733 (2.8234)  Time: 1.497s,  684.14/s  (2.183s,  469.17/s)  LR: 4.017e-05  Data: 0.934 (1.595)
Train: 167 [ 700/1171 ( 60%)]  Loss:  2.769509 (2.8198)  Time: 0.585s, 1749.95/s  (2.189s,  467.75/s)  LR: 4.017e-05  Data: 0.019 (1.601)
Train: 167 [ 750/1171 ( 64%)]  Loss:  2.687285 (2.8115)  Time: 3.761s,  272.30/s  (2.186s,  468.51/s)  LR: 4.017e-05  Data: 3.067 (1.596)
Train: 167 [ 800/1171 ( 68%)]  Loss:  2.625052 (2.8006)  Time: 0.822s, 1246.39/s  (2.184s,  468.94/s)  LR: 4.017e-05  Data: 0.248 (1.594)
Train: 167 [ 850/1171 ( 73%)]  Loss:  2.824309 (2.8019)  Time: 2.222s,  460.88/s  (2.180s,  469.82/s)  LR: 4.017e-05  Data: 1.650 (1.590)
Train: 167 [ 900/1171 ( 77%)]  Loss:  2.868210 (2.8054)  Time: 0.590s, 1734.12/s  (2.201s,  465.19/s)  LR: 4.017e-05  Data: 0.025 (1.611)
Train: 167 [ 950/1171 ( 81%)]  Loss:  2.874828 (2.8088)  Time: 2.142s,  477.96/s  (2.206s,  464.27/s)  LR: 4.017e-05  Data: 1.580 (1.615)
Train: 167 [1000/1171 ( 85%)]  Loss:  3.298304 (2.8321)  Time: 0.589s, 1738.51/s  (2.210s,  463.30/s)  LR: 4.017e-05  Data: 0.021 (1.619)
Train: 167 [1050/1171 ( 90%)]  Loss:  3.193763 (2.8486)  Time: 3.450s,  296.79/s  (2.216s,  462.11/s)  LR: 4.017e-05  Data: 2.888 (1.623)
Train: 167 [1100/1171 ( 94%)]  Loss:  2.868037 (2.8494)  Time: 0.587s, 1745.35/s  (2.218s,  461.70/s)  LR: 4.017e-05  Data: 0.018 (1.625)
Train: 167 [1150/1171 ( 98%)]  Loss:  2.781862 (2.8466)  Time: 0.589s, 1739.51/s  (2.213s,  462.62/s)  LR: 4.017e-05  Data: 0.023 (1.620)
Train: 167 [1170/1171 (100%)]  Loss:  2.754920 (2.8429)  Time: 0.564s, 1817.20/s  (2.211s,  463.14/s)  LR: 4.017e-05  Data: 0.000 (1.618)
Test: [   0/97]  Time: 11.816 (11.816)  Loss:  0.2812 (0.2812)  Acc@1: 96.5820 (96.5820)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (2.784)  Loss:  0.4297 (0.3415)  Acc@1: 92.2852 (95.1938)  Acc@5: 98.4375 (98.9239)
Test: [  97/97]  Time: 0.120 (2.865)  Loss:  0.3036 (0.3523)  Acc@1: 94.9405 (94.7080)  Acc@5: 99.4048 (98.7840)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-166.pth.tar', 94.70800000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-167.pth.tar', 94.70799999511719)

Train: 168 [   0/1171 (  0%)]  Loss:  2.733917 (2.7339)  Time: 10.808s,   94.74/s  (10.808s,   94.74/s)  LR: 3.739e-05  Data: 10.116 (10.116)
Train: 168 [  50/1171 (  4%)]  Loss:  2.674670 (2.7043)  Time: 0.586s, 1747.16/s  (2.335s,  438.56/s)  LR: 3.739e-05  Data: 0.022 (1.760)
Train: 168 [ 100/1171 (  9%)]  Loss:  3.341086 (2.9166)  Time: 0.584s, 1753.10/s  (2.292s,  446.78/s)  LR: 3.739e-05  Data: 0.018 (1.709)
Train: 168 [ 150/1171 ( 13%)]  Loss:  2.329761 (2.7699)  Time: 0.586s, 1746.81/s  (2.216s,  462.08/s)  LR: 3.739e-05  Data: 0.023 (1.634)
Train: 168 [ 200/1171 ( 17%)]  Loss:  2.638186 (2.7435)  Time: 0.586s, 1747.95/s  (2.196s,  466.20/s)  LR: 3.739e-05  Data: 0.021 (1.612)
Train: 168 [ 250/1171 ( 21%)]  Loss:  2.652849 (2.7284)  Time: 0.585s, 1750.23/s  (2.161s,  473.81/s)  LR: 3.739e-05  Data: 0.023 (1.576)
Train: 168 [ 300/1171 ( 26%)]  Loss:  2.402822 (2.6819)  Time: 0.587s, 1745.68/s  (2.152s,  475.88/s)  LR: 3.739e-05  Data: 0.022 (1.564)
Train: 168 [ 350/1171 ( 30%)]  Loss:  2.660279 (2.6792)  Time: 0.823s, 1244.55/s  (2.134s,  479.94/s)  LR: 3.739e-05  Data: 0.140 (1.543)
Train: 168 [ 400/1171 ( 34%)]  Loss:  2.856944 (2.6989)  Time: 0.584s, 1752.85/s  (2.148s,  476.73/s)  LR: 3.739e-05  Data: 0.020 (1.558)
Train: 168 [ 450/1171 ( 38%)]  Loss:  2.779526 (2.7070)  Time: 0.587s, 1744.57/s  (2.162s,  473.61/s)  LR: 3.739e-05  Data: 0.022 (1.573)
Train: 168 [ 500/1171 ( 43%)]  Loss:  2.968593 (2.7308)  Time: 0.585s, 1749.35/s  (2.181s,  469.49/s)  LR: 3.739e-05  Data: 0.021 (1.592)
Train: 168 [ 550/1171 ( 47%)]  Loss:  2.794848 (2.7361)  Time: 0.585s, 1750.66/s  (2.189s,  467.73/s)  LR: 3.739e-05  Data: 0.022 (1.599)
Train: 168 [ 600/1171 ( 51%)]  Loss:  2.613244 (2.7267)  Time: 0.586s, 1747.35/s  (2.197s,  466.06/s)  LR: 3.739e-05  Data: 0.022 (1.606)
Train: 168 [ 650/1171 ( 56%)]  Loss:  2.947506 (2.7424)  Time: 0.585s, 1751.52/s  (2.199s,  465.73/s)  LR: 3.739e-05  Data: 0.019 (1.606)
Train: 168 [ 700/1171 ( 60%)]  Loss:  2.980227 (2.7583)  Time: 0.585s, 1751.65/s  (2.194s,  466.75/s)  LR: 3.739e-05  Data: 0.021 (1.600)
Train: 168 [ 750/1171 ( 64%)]  Loss:  2.890966 (2.7666)  Time: 2.118s,  483.58/s  (2.191s,  467.47/s)  LR: 3.739e-05  Data: 1.532 (1.595)
Train: 168 [ 800/1171 ( 68%)]  Loss:  2.816035 (2.7695)  Time: 0.586s, 1747.94/s  (2.194s,  466.80/s)  LR: 3.739e-05  Data: 0.019 (1.598)
Train: 168 [ 850/1171 ( 73%)]  Loss:  2.280491 (2.7423)  Time: 0.749s, 1366.38/s  (2.204s,  464.68/s)  LR: 3.739e-05  Data: 0.177 (1.608)
Train: 168 [ 900/1171 ( 77%)]  Loss:  2.596913 (2.7347)  Time: 0.585s, 1750.40/s  (2.207s,  464.04/s)  LR: 3.739e-05  Data: 0.022 (1.611)
Train: 168 [ 950/1171 ( 81%)]  Loss:  3.126571 (2.7543)  Time: 1.250s,  818.88/s  (2.208s,  463.72/s)  LR: 3.739e-05  Data: 0.680 (1.613)
Train: 168 [1000/1171 ( 85%)]  Loss:  2.743977 (2.7538)  Time: 0.591s, 1733.09/s  (2.207s,  463.90/s)  LR: 3.739e-05  Data: 0.024 (1.612)
Train: 168 [1050/1171 ( 90%)]  Loss:  2.974125 (2.7638)  Time: 0.587s, 1744.11/s  (2.208s,  463.67/s)  LR: 3.739e-05  Data: 0.019 (1.614)
Train: 168 [1100/1171 ( 94%)]  Loss:  3.159673 (2.7810)  Time: 0.588s, 1740.38/s  (2.200s,  465.43/s)  LR: 3.739e-05  Data: 0.024 (1.606)
Train: 168 [1150/1171 ( 98%)]  Loss:  2.796372 (2.7816)  Time: 0.590s, 1735.86/s  (2.197s,  466.09/s)  LR: 3.739e-05  Data: 0.020 (1.603)
Train: 168 [1170/1171 (100%)]  Loss:  3.050903 (2.7924)  Time: 0.565s, 1812.28/s  (2.194s,  466.64/s)  LR: 3.739e-05  Data: 0.000 (1.601)
Test: [   0/97]  Time: 11.448 (11.448)  Loss:  0.3128 (0.3128)  Acc@1: 96.4844 (96.4844)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.202 (3.037)  Loss:  0.4714 (0.3741)  Acc@1: 92.2852 (95.2780)  Acc@5: 98.3398 (98.9526)
Test: [  97/97]  Time: 0.119 (3.022)  Loss:  0.3529 (0.3854)  Acc@1: 94.4940 (94.7790)  Acc@5: 99.4048 (98.8220)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-168.pth.tar', 94.77900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-166.pth.tar', 94.70800000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-167.pth.tar', 94.70799999511719)

Train: 169 [   0/1171 (  0%)]  Loss:  3.134744 (3.1347)  Time: 11.502s,   89.03/s  (11.502s,   89.03/s)  LR: 3.474e-05  Data: 10.654 (10.654)
Train: 169 [  50/1171 (  4%)]  Loss:  2.927571 (3.0312)  Time: 0.587s, 1745.83/s  (2.356s,  434.68/s)  LR: 3.474e-05  Data: 0.020 (1.767)
Train: 169 [ 100/1171 (  9%)]  Loss:  2.692365 (2.9182)  Time: 0.591s, 1733.62/s  (2.282s,  448.69/s)  LR: 3.474e-05  Data: 0.022 (1.680)
Train: 169 [ 150/1171 ( 13%)]  Loss:  2.833177 (2.8970)  Time: 0.586s, 1748.41/s  (2.203s,  464.80/s)  LR: 3.474e-05  Data: 0.022 (1.601)
Train: 169 [ 200/1171 ( 17%)]  Loss:  2.378801 (2.7933)  Time: 0.586s, 1748.38/s  (2.172s,  471.41/s)  LR: 3.474e-05  Data: 0.019 (1.575)
Train: 169 [ 250/1171 ( 21%)]  Loss:  2.779183 (2.7910)  Time: 0.586s, 1745.96/s  (2.126s,  481.61/s)  LR: 3.474e-05  Data: 0.020 (1.531)
Train: 169 [ 300/1171 ( 26%)]  Loss:  3.117575 (2.8376)  Time: 0.589s, 1737.84/s  (2.106s,  486.23/s)  LR: 3.474e-05  Data: 0.022 (1.511)
Train: 169 [ 350/1171 ( 30%)]  Loss:  3.166708 (2.8788)  Time: 0.586s, 1746.93/s  (2.143s,  477.94/s)  LR: 3.474e-05  Data: 0.023 (1.547)
Train: 169 [ 400/1171 ( 34%)]  Loss:  2.788282 (2.8687)  Time: 0.584s, 1751.98/s  (2.145s,  477.42/s)  LR: 3.474e-05  Data: 0.021 (1.550)
Train: 169 [ 450/1171 ( 38%)]  Loss:  3.118101 (2.8937)  Time: 0.589s, 1739.41/s  (2.164s,  473.10/s)  LR: 3.474e-05  Data: 0.025 (1.570)
Train: 169 [ 500/1171 ( 43%)]  Loss:  2.719708 (2.8778)  Time: 2.294s,  446.43/s  (2.168s,  472.35/s)  LR: 3.474e-05  Data: 1.707 (1.573)
Train: 169 [ 550/1171 ( 47%)]  Loss:  2.587801 (2.8537)  Time: 0.592s, 1730.79/s  (2.169s,  472.17/s)  LR: 3.474e-05  Data: 0.025 (1.570)
Train: 169 [ 600/1171 ( 51%)]  Loss:  2.457372 (2.8232)  Time: 0.587s, 1744.03/s  (2.170s,  471.90/s)  LR: 3.474e-05  Data: 0.020 (1.571)
Train: 169 [ 650/1171 ( 56%)]  Loss:  3.038654 (2.8386)  Time: 0.590s, 1735.39/s  (2.169s,  472.18/s)  LR: 3.474e-05  Data: 0.021 (1.570)
Train: 169 [ 700/1171 ( 60%)]  Loss:  2.678620 (2.8279)  Time: 0.585s, 1750.68/s  (2.161s,  473.91/s)  LR: 3.474e-05  Data: 0.018 (1.563)
Train: 169 [ 750/1171 ( 64%)]  Loss:  2.928285 (2.8342)  Time: 0.643s, 1592.33/s  (2.180s,  469.74/s)  LR: 3.474e-05  Data: 0.025 (1.582)
Train: 169 [ 800/1171 ( 68%)]  Loss:  3.078680 (2.8486)  Time: 0.586s, 1747.87/s  (2.179s,  469.92/s)  LR: 3.474e-05  Data: 0.023 (1.582)
Train: 169 [ 850/1171 ( 73%)]  Loss:  3.060578 (2.8603)  Time: 0.587s, 1743.28/s  (2.187s,  468.29/s)  LR: 3.474e-05  Data: 0.021 (1.589)
Train: 169 [ 900/1171 ( 77%)]  Loss:  2.921726 (2.8636)  Time: 0.582s, 1759.05/s  (2.188s,  467.90/s)  LR: 3.474e-05  Data: 0.019 (1.592)
Train: 169 [ 950/1171 ( 81%)]  Loss:  2.690039 (2.8549)  Time: 0.588s, 1742.53/s  (2.190s,  467.61/s)  LR: 3.474e-05  Data: 0.020 (1.594)
Train: 169 [1000/1171 ( 85%)]  Loss:  2.663356 (2.8458)  Time: 0.587s, 1744.59/s  (2.184s,  468.81/s)  LR: 3.474e-05  Data: 0.022 (1.589)
Train: 169 [1050/1171 ( 90%)]  Loss:  2.878824 (2.8473)  Time: 0.585s, 1751.09/s  (2.185s,  468.72/s)  LR: 3.474e-05  Data: 0.019 (1.589)
Train: 169 [1100/1171 ( 94%)]  Loss:  2.863641 (2.8480)  Time: 0.588s, 1740.39/s  (2.177s,  470.42/s)  LR: 3.474e-05  Data: 0.020 (1.581)
Train: 169 [1150/1171 ( 98%)]  Loss:  3.044182 (2.8562)  Time: 0.585s, 1750.34/s  (2.188s,  468.11/s)  LR: 3.474e-05  Data: 0.019 (1.592)
Train: 169 [1170/1171 (100%)]  Loss:  3.082816 (2.8652)  Time: 0.565s, 1812.34/s  (2.188s,  468.10/s)  LR: 3.474e-05  Data: 0.000 (1.592)
Test: [   0/97]  Time: 11.569 (11.569)  Loss:  0.2703 (0.2703)  Acc@1: 97.0703 (97.0703)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (2.976)  Loss:  0.4403 (0.3389)  Acc@1: 92.5781 (95.1880)  Acc@5: 98.5352 (98.9622)
Test: [  97/97]  Time: 0.121 (2.927)  Loss:  0.2998 (0.3506)  Acc@1: 95.0893 (94.7300)  Acc@5: 99.2560 (98.8090)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-168.pth.tar', 94.77900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-169.pth.tar', 94.73000000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-166.pth.tar', 94.70800000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-167.pth.tar', 94.70799999511719)

Train: 170 [   0/1171 (  0%)]  Loss:  2.661634 (2.6616)  Time: 10.358s,   98.86/s  (10.358s,   98.86/s)  LR: 3.222e-05  Data: 9.278 (9.278)
Train: 170 [  50/1171 (  4%)]  Loss:  2.693917 (2.6778)  Time: 0.585s, 1751.72/s  (2.221s,  460.99/s)  LR: 3.222e-05  Data: 0.020 (1.625)
Train: 170 [ 100/1171 (  9%)]  Loss:  2.619722 (2.6584)  Time: 0.589s, 1739.26/s  (2.170s,  471.80/s)  LR: 3.222e-05  Data: 0.019 (1.576)
Train: 170 [ 150/1171 ( 13%)]  Loss:  2.739454 (2.6787)  Time: 0.592s, 1730.39/s  (2.113s,  484.66/s)  LR: 3.222e-05  Data: 0.025 (1.525)
Train: 170 [ 200/1171 ( 17%)]  Loss:  3.246051 (2.7922)  Time: 0.589s, 1737.46/s  (2.096s,  488.51/s)  LR: 3.222e-05  Data: 0.021 (1.507)
Train: 170 [ 250/1171 ( 21%)]  Loss:  2.722037 (2.7805)  Time: 2.012s,  508.98/s  (2.066s,  495.75/s)  LR: 3.222e-05  Data: 1.363 (1.478)
Train: 170 [ 300/1171 ( 26%)]  Loss:  3.080184 (2.8233)  Time: 0.588s, 1740.77/s  (2.128s,  481.18/s)  LR: 3.222e-05  Data: 0.020 (1.538)
Train: 170 [ 350/1171 ( 30%)]  Loss:  2.577530 (2.7926)  Time: 0.589s, 1739.86/s  (2.138s,  479.02/s)  LR: 3.222e-05  Data: 0.025 (1.543)
Train: 170 [ 400/1171 ( 34%)]  Loss:  3.030267 (2.8190)  Time: 0.588s, 1742.62/s  (2.143s,  477.73/s)  LR: 3.222e-05  Data: 0.025 (1.547)
Train: 170 [ 450/1171 ( 38%)]  Loss:  2.720872 (2.8092)  Time: 2.215s,  462.27/s  (2.141s,  478.24/s)  LR: 3.222e-05  Data: 1.554 (1.542)
Train: 170 [ 500/1171 ( 43%)]  Loss:  2.812193 (2.8094)  Time: 0.585s, 1751.13/s  (2.146s,  477.18/s)  LR: 3.222e-05  Data: 0.020 (1.547)
Train: 170 [ 550/1171 ( 47%)]  Loss:  2.849186 (2.8128)  Time: 5.210s,  196.53/s  (2.149s,  476.51/s)  LR: 3.222e-05  Data: 4.492 (1.549)
Train: 170 [ 600/1171 ( 51%)]  Loss:  3.081258 (2.8334)  Time: 0.586s, 1747.26/s  (2.147s,  476.93/s)  LR: 3.222e-05  Data: 0.023 (1.546)
Train: 170 [ 650/1171 ( 56%)]  Loss:  3.095891 (2.8522)  Time: 3.330s,  307.52/s  (2.145s,  477.50/s)  LR: 3.222e-05  Data: 2.764 (1.544)
Train: 170 [ 700/1171 ( 60%)]  Loss:  2.980897 (2.8607)  Time: 0.588s, 1741.03/s  (2.171s,  471.70/s)  LR: 3.222e-05  Data: 0.023 (1.571)
Train: 170 [ 750/1171 ( 64%)]  Loss:  2.590147 (2.8438)  Time: 3.724s,  274.97/s  (2.180s,  469.73/s)  LR: 3.222e-05  Data: 3.080 (1.580)
Train: 170 [ 800/1171 ( 68%)]  Loss:  3.117926 (2.8600)  Time: 0.584s, 1752.33/s  (2.184s,  468.91/s)  LR: 3.222e-05  Data: 0.021 (1.584)
Train: 170 [ 850/1171 ( 73%)]  Loss:  2.824612 (2.8580)  Time: 2.997s,  341.67/s  (2.185s,  468.63/s)  LR: 3.222e-05  Data: 2.314 (1.585)
Train: 170 [ 900/1171 ( 77%)]  Loss:  3.005683 (2.8658)  Time: 0.586s, 1748.89/s  (2.185s,  468.75/s)  LR: 3.222e-05  Data: 0.023 (1.584)
Train: 170 [ 950/1171 ( 81%)]  Loss:  3.223708 (2.8837)  Time: 2.853s,  358.90/s  (2.179s,  470.01/s)  LR: 3.222e-05  Data: 2.228 (1.579)
Train: 170 [1000/1171 ( 85%)]  Loss:  2.557388 (2.8681)  Time: 0.591s, 1733.10/s  (2.174s,  471.05/s)  LR: 3.222e-05  Data: 0.024 (1.574)
Train: 170 [1050/1171 ( 90%)]  Loss:  2.241973 (2.8397)  Time: 2.718s,  376.71/s  (2.169s,  472.09/s)  LR: 3.222e-05  Data: 2.155 (1.570)
Train: 170 [1100/1171 ( 94%)]  Loss:  3.076776 (2.8500)  Time: 0.586s, 1746.61/s  (2.182s,  469.36/s)  LR: 3.222e-05  Data: 0.023 (1.583)
Train: 170 [1150/1171 ( 98%)]  Loss:  3.028196 (2.8574)  Time: 4.042s,  253.34/s  (2.184s,  468.78/s)  LR: 3.222e-05  Data: 3.479 (1.586)
Train: 170 [1170/1171 (100%)]  Loss:  2.665187 (2.8497)  Time: 0.564s, 1816.66/s  (2.185s,  468.72/s)  LR: 3.222e-05  Data: 0.000 (1.586)
Test: [   0/97]  Time: 12.137 (12.137)  Loss:  0.2976 (0.2976)  Acc@1: 96.3867 (96.3867)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (3.036)  Loss:  0.4727 (0.3659)  Acc@1: 91.6992 (95.2225)  Acc@5: 98.3398 (98.9334)
Test: [  97/97]  Time: 0.119 (2.949)  Loss:  0.3385 (0.3767)  Acc@1: 94.4940 (94.7390)  Acc@5: 99.4048 (98.8070)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-168.pth.tar', 94.77900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-170.pth.tar', 94.73900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-169.pth.tar', 94.73000000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-166.pth.tar', 94.70800000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-167.pth.tar', 94.70799999511719)

Train: 171 [   0/1171 (  0%)]  Loss:  2.640512 (2.6405)  Time: 9.225s,  111.01/s  (9.225s,  111.01/s)  LR: 2.984e-05  Data: 8.628 (8.628)
Train: 171 [  50/1171 (  4%)]  Loss:  2.430101 (2.5353)  Time: 0.587s, 1745.35/s  (2.126s,  481.70/s)  LR: 2.984e-05  Data: 0.023 (1.537)
Train: 171 [ 100/1171 (  9%)]  Loss:  2.899850 (2.6568)  Time: 3.839s,  266.76/s  (2.122s,  482.58/s)  LR: 2.984e-05  Data: 3.167 (1.530)
Train: 171 [ 150/1171 ( 13%)]  Loss:  2.942230 (2.7282)  Time: 0.589s, 1737.83/s  (2.076s,  493.31/s)  LR: 2.984e-05  Data: 0.023 (1.486)
Train: 171 [ 200/1171 ( 17%)]  Loss:  2.458769 (2.6743)  Time: 4.935s,  207.50/s  (2.163s,  473.42/s)  LR: 2.984e-05  Data: 4.370 (1.571)
Train: 171 [ 250/1171 ( 21%)]  Loss:  2.603125 (2.6624)  Time: 0.582s, 1758.47/s  (2.165s,  473.01/s)  LR: 2.984e-05  Data: 0.020 (1.572)
Train: 171 [ 300/1171 ( 26%)]  Loss:  3.206264 (2.7401)  Time: 7.198s,  142.27/s  (2.201s,  465.17/s)  LR: 2.984e-05  Data: 6.633 (1.608)
Train: 171 [ 350/1171 ( 30%)]  Loss:  3.426285 (2.8259)  Time: 2.871s,  356.61/s  (2.209s,  463.66/s)  LR: 2.984e-05  Data: 2.200 (1.615)
Train: 171 [ 400/1171 ( 34%)]  Loss:  2.837572 (2.8272)  Time: 4.918s,  208.21/s  (2.221s,  461.10/s)  LR: 2.984e-05  Data: 4.254 (1.626)
Train: 171 [ 450/1171 ( 38%)]  Loss:  3.039502 (2.8484)  Time: 2.298s,  445.62/s  (2.210s,  463.44/s)  LR: 2.984e-05  Data: 1.724 (1.615)
Train: 171 [ 500/1171 ( 43%)]  Loss:  2.567586 (2.8229)  Time: 6.750s,  151.71/s  (2.208s,  463.79/s)  LR: 2.984e-05  Data: 6.071 (1.612)
Train: 171 [ 550/1171 ( 47%)]  Loss:  2.755570 (2.8173)  Time: 0.634s, 1615.62/s  (2.206s,  464.16/s)  LR: 2.984e-05  Data: 0.018 (1.610)
Train: 171 [ 600/1171 ( 51%)]  Loss:  2.919032 (2.8251)  Time: 7.754s,  132.06/s  (2.240s,  457.19/s)  LR: 2.984e-05  Data: 7.142 (1.644)
Train: 171 [ 650/1171 ( 56%)]  Loss:  2.856287 (2.8273)  Time: 0.588s, 1741.44/s  (2.249s,  455.36/s)  LR: 2.984e-05  Data: 0.023 (1.654)
Train: 171 [ 700/1171 ( 60%)]  Loss:  2.611243 (2.8129)  Time: 7.256s,  141.12/s  (2.260s,  453.16/s)  LR: 2.984e-05  Data: 6.601 (1.665)
Train: 171 [ 750/1171 ( 64%)]  Loss:  2.651365 (2.8028)  Time: 0.588s, 1742.55/s  (2.258s,  453.53/s)  LR: 2.984e-05  Data: 0.024 (1.665)
Train: 171 [ 800/1171 ( 68%)]  Loss:  3.041266 (2.8169)  Time: 5.504s,  186.06/s  (2.262s,  452.79/s)  LR: 2.984e-05  Data: 4.859 (1.668)
Train: 171 [ 850/1171 ( 73%)]  Loss:  3.074175 (2.8312)  Time: 0.586s, 1746.52/s  (2.252s,  454.64/s)  LR: 2.984e-05  Data: 0.023 (1.659)
Train: 171 [ 900/1171 ( 77%)]  Loss:  2.617257 (2.8199)  Time: 3.574s,  286.48/s  (2.249s,  455.33/s)  LR: 2.984e-05  Data: 2.909 (1.654)
Train: 171 [ 950/1171 ( 81%)]  Loss:  3.050399 (2.8314)  Time: 2.326s,  440.29/s  (2.239s,  457.29/s)  LR: 2.984e-05  Data: 1.660 (1.643)
Train: 171 [1000/1171 ( 85%)]  Loss:  3.114573 (2.8449)  Time: 2.971s,  344.63/s  (2.255s,  454.10/s)  LR: 2.984e-05  Data: 2.391 (1.657)
Train: 171 [1050/1171 ( 90%)]  Loss:  3.151313 (2.8588)  Time: 3.468s,  295.28/s  (2.259s,  453.35/s)  LR: 2.984e-05  Data: 2.905 (1.660)
Train: 171 [1100/1171 ( 94%)]  Loss:  3.098389 (2.8692)  Time: 3.030s,  338.01/s  (2.265s,  452.17/s)  LR: 2.984e-05  Data: 2.370 (1.666)
Train: 171 [1150/1171 ( 98%)]  Loss:  3.006497 (2.8750)  Time: 0.585s, 1749.83/s  (2.259s,  453.36/s)  LR: 2.984e-05  Data: 0.022 (1.660)
Train: 171 [1170/1171 (100%)]  Loss:  3.338196 (2.8935)  Time: 0.564s, 1816.26/s  (2.257s,  453.61/s)  LR: 2.984e-05  Data: 0.000 (1.659)
Test: [   0/97]  Time: 12.356 (12.356)  Loss:  0.2980 (0.2980)  Acc@1: 96.2891 (96.2891)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.200 (2.952)  Loss:  0.4362 (0.3539)  Acc@1: 92.7734 (95.2665)  Acc@5: 98.2422 (98.9468)
Test: [  97/97]  Time: 0.119 (2.846)  Loss:  0.3209 (0.3649)  Acc@1: 94.9405 (94.7520)  Acc@5: 99.5536 (98.8360)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-168.pth.tar', 94.77900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-171.pth.tar', 94.75200004638671)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-170.pth.tar', 94.73900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-169.pth.tar', 94.73000000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-166.pth.tar', 94.70800000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-167.pth.tar', 94.70799999511719)

Train: 172 [   0/1171 (  0%)]  Loss:  3.303827 (3.3038)  Time: 9.134s,  112.11/s  (9.134s,  112.11/s)  LR: 2.759e-05  Data: 8.558 (8.558)
Train: 172 [  50/1171 (  4%)]  Loss:  2.736229 (3.0200)  Time: 0.718s, 1425.66/s  (2.151s,  475.98/s)  LR: 2.759e-05  Data: 0.127 (1.570)
Train: 172 [ 100/1171 (  9%)]  Loss:  2.990686 (3.0102)  Time: 0.589s, 1739.40/s  (2.351s,  435.51/s)  LR: 2.759e-05  Data: 0.019 (1.761)
Train: 172 [ 150/1171 ( 13%)]  Loss:  3.140358 (3.0428)  Time: 0.589s, 1738.25/s  (2.327s,  439.98/s)  LR: 2.759e-05  Data: 0.020 (1.737)
Train: 172 [ 200/1171 ( 17%)]  Loss:  2.947865 (3.0238)  Time: 0.589s, 1739.46/s  (2.318s,  441.74/s)  LR: 2.759e-05  Data: 0.020 (1.729)
Train: 172 [ 250/1171 ( 21%)]  Loss:  3.025779 (3.0241)  Time: 0.583s, 1755.85/s  (2.290s,  447.23/s)  LR: 2.759e-05  Data: 0.020 (1.700)
Train: 172 [ 300/1171 ( 26%)]  Loss:  2.343776 (2.9269)  Time: 0.590s, 1735.39/s  (2.277s,  449.73/s)  LR: 2.759e-05  Data: 0.024 (1.689)
Train: 172 [ 350/1171 ( 30%)]  Loss:  2.529874 (2.8773)  Time: 0.662s, 1546.95/s  (2.243s,  456.51/s)  LR: 2.759e-05  Data: 0.022 (1.655)
Train: 172 [ 400/1171 ( 34%)]  Loss:  2.845276 (2.8737)  Time: 0.586s, 1746.67/s  (2.234s,  458.27/s)  LR: 2.759e-05  Data: 0.024 (1.644)
Train: 172 [ 450/1171 ( 38%)]  Loss:  3.110211 (2.8974)  Time: 1.777s,  576.15/s  (2.212s,  462.84/s)  LR: 2.759e-05  Data: 1.115 (1.617)
Train: 172 [ 500/1171 ( 43%)]  Loss:  2.933313 (2.9007)  Time: 0.590s, 1736.20/s  (2.242s,  456.83/s)  LR: 2.759e-05  Data: 0.024 (1.643)
Train: 172 [ 550/1171 ( 47%)]  Loss:  2.775406 (2.8902)  Time: 3.502s,  292.37/s  (2.266s,  451.89/s)  LR: 2.759e-05  Data: 2.819 (1.668)
Train: 172 [ 600/1171 ( 51%)]  Loss:  3.191948 (2.9134)  Time: 0.587s, 1745.42/s  (2.285s,  448.19/s)  LR: 2.759e-05  Data: 0.022 (1.684)
Train: 172 [ 650/1171 ( 56%)]  Loss:  2.713755 (2.8992)  Time: 0.770s, 1330.31/s  (2.289s,  447.27/s)  LR: 2.759e-05  Data: 0.123 (1.688)
Train: 172 [ 700/1171 ( 60%)]  Loss:  2.740972 (2.8886)  Time: 0.584s, 1752.91/s  (2.289s,  447.32/s)  LR: 2.759e-05  Data: 0.021 (1.686)
Train: 172 [ 750/1171 ( 64%)]  Loss:  2.837736 (2.8854)  Time: 0.588s, 1741.49/s  (2.283s,  448.59/s)  LR: 2.759e-05  Data: 0.023 (1.681)
Train: 172 [ 800/1171 ( 68%)]  Loss:  2.871429 (2.8846)  Time: 0.588s, 1742.69/s  (2.272s,  450.65/s)  LR: 2.759e-05  Data: 0.021 (1.670)
Train: 172 [ 850/1171 ( 73%)]  Loss:  2.714197 (2.8751)  Time: 0.589s, 1737.98/s  (2.261s,  452.90/s)  LR: 2.759e-05  Data: 0.024 (1.658)
Train: 172 [ 900/1171 ( 77%)]  Loss:  3.095013 (2.8867)  Time: 0.590s, 1735.36/s  (2.275s,  450.14/s)  LR: 2.759e-05  Data: 0.021 (1.672)
Train: 172 [ 950/1171 ( 81%)]  Loss:  3.049529 (2.8949)  Time: 0.584s, 1754.84/s  (2.287s,  447.76/s)  LR: 2.759e-05  Data: 0.019 (1.683)
Train: 172 [1000/1171 ( 85%)]  Loss:  2.392698 (2.8709)  Time: 0.585s, 1750.56/s  (2.289s,  447.35/s)  LR: 2.759e-05  Data: 0.021 (1.686)
Train: 172 [1050/1171 ( 90%)]  Loss:  2.942702 (2.8742)  Time: 0.586s, 1747.49/s  (2.291s,  447.03/s)  LR: 2.759e-05  Data: 0.021 (1.687)
Train: 172 [1100/1171 ( 94%)]  Loss:  2.827363 (2.8722)  Time: 0.584s, 1754.01/s  (2.288s,  447.59/s)  LR: 2.759e-05  Data: 0.021 (1.684)
Train: 172 [1150/1171 ( 98%)]  Loss:  2.696670 (2.8649)  Time: 0.884s, 1158.55/s  (2.281s,  448.98/s)  LR: 2.759e-05  Data: 0.321 (1.676)
Train: 172 [1170/1171 (100%)]  Loss:  3.241596 (2.8799)  Time: 0.564s, 1815.91/s  (2.278s,  449.53/s)  LR: 2.759e-05  Data: 0.000 (1.674)
Test: [   0/97]  Time: 12.626 (12.626)  Loss:  0.2990 (0.2990)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (2.769)  Loss:  0.4318 (0.3679)  Acc@1: 93.4570 (95.2493)  Acc@5: 98.4375 (98.9430)
Test: [  97/97]  Time: 0.120 (2.790)  Loss:  0.3269 (0.3763)  Acc@1: 94.3452 (94.7840)  Acc@5: 99.2560 (98.8120)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-172.pth.tar', 94.7839999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-168.pth.tar', 94.77900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-171.pth.tar', 94.75200004638671)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-170.pth.tar', 94.73900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-169.pth.tar', 94.73000000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-166.pth.tar', 94.70800000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-167.pth.tar', 94.70799999511719)

Train: 173 [   0/1171 (  0%)]  Loss:  3.169273 (3.1693)  Time: 9.889s,  103.55/s  (9.889s,  103.55/s)  LR: 2.547e-05  Data: 8.879 (8.879)
Train: 173 [  50/1171 (  4%)]  Loss:  2.820349 (2.9948)  Time: 0.585s, 1751.57/s  (2.321s,  441.27/s)  LR: 2.547e-05  Data: 0.021 (1.724)
Train: 173 [ 100/1171 (  9%)]  Loss:  2.678364 (2.8893)  Time: 5.382s,  190.25/s  (2.294s,  446.45/s)  LR: 2.547e-05  Data: 4.800 (1.693)
Train: 173 [ 150/1171 ( 13%)]  Loss:  2.391125 (2.7648)  Time: 0.586s, 1747.32/s  (2.224s,  460.35/s)  LR: 2.547e-05  Data: 0.019 (1.624)
Train: 173 [ 200/1171 ( 17%)]  Loss:  3.323062 (2.8764)  Time: 6.687s,  153.14/s  (2.222s,  460.80/s)  LR: 2.547e-05  Data: 6.098 (1.624)
Train: 173 [ 250/1171 ( 21%)]  Loss:  2.754660 (2.8561)  Time: 0.583s, 1755.40/s  (2.184s,  468.94/s)  LR: 2.547e-05  Data: 0.019 (1.587)
Train: 173 [ 300/1171 ( 26%)]  Loss:  3.211390 (2.9069)  Time: 5.918s,  173.03/s  (2.179s,  469.89/s)  LR: 2.547e-05  Data: 5.330 (1.581)
Train: 173 [ 350/1171 ( 30%)]  Loss:  3.354980 (2.9629)  Time: 0.584s, 1754.84/s  (2.152s,  475.85/s)  LR: 2.547e-05  Data: 0.019 (1.557)
Train: 173 [ 400/1171 ( 34%)]  Loss:  2.545016 (2.9165)  Time: 7.705s,  132.90/s  (2.205s,  464.49/s)  LR: 2.547e-05  Data: 7.035 (1.609)
Train: 173 [ 450/1171 ( 38%)]  Loss:  3.064626 (2.9313)  Time: 0.586s, 1746.08/s  (2.215s,  462.24/s)  LR: 2.547e-05  Data: 0.021 (1.621)
Train: 173 [ 500/1171 ( 43%)]  Loss:  2.968122 (2.9346)  Time: 8.350s,  122.63/s  (2.242s,  456.72/s)  LR: 2.547e-05  Data: 7.772 (1.648)
Train: 173 [ 550/1171 ( 47%)]  Loss:  2.844619 (2.9271)  Time: 0.585s, 1750.87/s  (2.247s,  455.78/s)  LR: 2.547e-05  Data: 0.020 (1.654)
Train: 173 [ 600/1171 ( 51%)]  Loss:  2.935231 (2.9278)  Time: 7.198s,  142.27/s  (2.260s,  453.17/s)  LR: 2.547e-05  Data: 6.509 (1.668)
Train: 173 [ 650/1171 ( 56%)]  Loss:  2.989588 (2.9322)  Time: 0.585s, 1750.63/s  (2.257s,  453.64/s)  LR: 2.547e-05  Data: 0.021 (1.666)
Train: 173 [ 700/1171 ( 60%)]  Loss:  2.675757 (2.9151)  Time: 7.122s,  143.78/s  (2.255s,  454.12/s)  LR: 2.547e-05  Data: 6.538 (1.664)
Train: 173 [ 750/1171 ( 64%)]  Loss:  2.909903 (2.9148)  Time: 0.583s, 1756.11/s  (2.243s,  456.59/s)  LR: 2.547e-05  Data: 0.019 (1.652)
Train: 173 [ 800/1171 ( 68%)]  Loss:  2.903490 (2.9141)  Time: 6.758s,  151.53/s  (2.261s,  452.80/s)  LR: 2.547e-05  Data: 6.177 (1.671)
Train: 173 [ 850/1171 ( 73%)]  Loss:  3.076746 (2.9231)  Time: 2.899s,  353.22/s  (2.264s,  452.38/s)  LR: 2.547e-05  Data: 2.320 (1.674)
Train: 173 [ 900/1171 ( 77%)]  Loss:  2.664900 (2.9095)  Time: 4.619s,  221.71/s  (2.267s,  451.69/s)  LR: 2.547e-05  Data: 3.993 (1.677)
Train: 173 [ 950/1171 ( 81%)]  Loss:  3.086977 (2.9184)  Time: 5.166s,  198.23/s  (2.265s,  452.12/s)  LR: 2.547e-05  Data: 4.573 (1.675)
Train: 173 [1000/1171 ( 85%)]  Loss:  3.006622 (2.9226)  Time: 5.322s,  192.42/s  (2.263s,  452.49/s)  LR: 2.547e-05  Data: 4.696 (1.672)
Train: 173 [1050/1171 ( 90%)]  Loss:  2.785792 (2.9164)  Time: 2.095s,  488.89/s  (2.255s,  454.16/s)  LR: 2.547e-05  Data: 1.475 (1.664)
Train: 173 [1100/1171 ( 94%)]  Loss:  3.055232 (2.9224)  Time: 6.768s,  151.29/s  (2.250s,  455.04/s)  LR: 2.547e-05  Data: 5.483 (1.659)
Train: 173 [1150/1171 ( 98%)]  Loss:  2.643746 (2.9108)  Time: 0.586s, 1748.09/s  (2.241s,  456.91/s)  LR: 2.547e-05  Data: 0.021 (1.650)
Train: 173 [1170/1171 (100%)]  Loss:  3.001392 (2.9144)  Time: 0.565s, 1812.80/s  (2.250s,  455.10/s)  LR: 2.547e-05  Data: 0.000 (1.659)
Test: [   0/97]  Time: 16.235 (16.235)  Loss:  0.2961 (0.2961)  Acc@1: 96.4844 (96.4844)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (3.157)  Loss:  0.4489 (0.3605)  Acc@1: 92.7734 (95.3278)  Acc@5: 98.4375 (98.9334)
Test: [  97/97]  Time: 0.120 (3.057)  Loss:  0.3130 (0.3697)  Acc@1: 95.2381 (94.8960)  Acc@5: 99.4048 (98.8330)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-173.pth.tar', 94.89600001953124)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-172.pth.tar', 94.7839999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-168.pth.tar', 94.77900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-171.pth.tar', 94.75200004638671)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-170.pth.tar', 94.73900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-169.pth.tar', 94.73000000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-166.pth.tar', 94.70800000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-167.pth.tar', 94.70799999511719)

Train: 174 [   0/1171 (  0%)]  Loss:  2.547845 (2.5478)  Time: 10.884s,   94.08/s  (10.884s,   94.08/s)  LR: 2.348e-05  Data: 9.713 (9.713)
Train: 174 [  50/1171 (  4%)]  Loss:  2.767090 (2.6575)  Time: 0.586s, 1748.50/s  (2.276s,  449.87/s)  LR: 2.348e-05  Data: 0.022 (1.665)
Train: 174 [ 100/1171 (  9%)]  Loss:  2.640679 (2.6519)  Time: 1.576s,  649.72/s  (2.227s,  459.73/s)  LR: 2.348e-05  Data: 0.947 (1.618)
Train: 174 [ 150/1171 ( 13%)]  Loss:  2.842797 (2.6996)  Time: 0.589s, 1739.05/s  (2.164s,  473.17/s)  LR: 2.348e-05  Data: 0.024 (1.562)
Train: 174 [ 200/1171 ( 17%)]  Loss:  2.796987 (2.7191)  Time: 0.584s, 1754.08/s  (2.147s,  476.86/s)  LR: 2.348e-05  Data: 0.019 (1.547)
Train: 174 [ 250/1171 ( 21%)]  Loss:  2.982270 (2.7629)  Time: 0.588s, 1742.88/s  (2.116s,  483.90/s)  LR: 2.348e-05  Data: 0.019 (1.515)
Train: 174 [ 300/1171 ( 26%)]  Loss:  2.634117 (2.7445)  Time: 1.348s,  759.42/s  (2.181s,  469.45/s)  LR: 2.348e-05  Data: 0.558 (1.579)
Train: 174 [ 350/1171 ( 30%)]  Loss:  2.743259 (2.7444)  Time: 0.589s, 1739.65/s  (2.176s,  470.48/s)  LR: 2.348e-05  Data: 0.020 (1.575)
Train: 174 [ 400/1171 ( 34%)]  Loss:  2.419126 (2.7082)  Time: 0.586s, 1747.11/s  (2.199s,  465.59/s)  LR: 2.348e-05  Data: 0.018 (1.600)
Train: 174 [ 450/1171 ( 38%)]  Loss:  2.700279 (2.7074)  Time: 0.588s, 1741.59/s  (2.203s,  464.77/s)  LR: 2.348e-05  Data: 0.024 (1.605)
Train: 174 [ 500/1171 ( 43%)]  Loss:  3.050364 (2.7386)  Time: 0.587s, 1743.89/s  (2.219s,  461.42/s)  LR: 2.348e-05  Data: 0.019 (1.622)
Train: 174 [ 550/1171 ( 47%)]  Loss:  2.965115 (2.7575)  Time: 0.585s, 1749.79/s  (2.226s,  460.05/s)  LR: 2.348e-05  Data: 0.022 (1.630)
Train: 174 [ 600/1171 ( 51%)]  Loss:  2.789380 (2.7599)  Time: 0.584s, 1754.54/s  (2.237s,  457.82/s)  LR: 2.348e-05  Data: 0.019 (1.641)
Train: 174 [ 650/1171 ( 56%)]  Loss:  3.070849 (2.7822)  Time: 0.591s, 1733.56/s  (2.230s,  459.17/s)  LR: 2.348e-05  Data: 0.027 (1.636)
Train: 174 [ 700/1171 ( 60%)]  Loss:  2.973501 (2.7949)  Time: 1.449s,  706.74/s  (2.270s,  451.19/s)  LR: 2.348e-05  Data: 0.774 (1.675)
Train: 174 [ 750/1171 ( 64%)]  Loss:  2.781567 (2.7941)  Time: 0.585s, 1750.28/s  (2.285s,  448.14/s)  LR: 2.348e-05  Data: 0.020 (1.690)
Train: 174 [ 800/1171 ( 68%)]  Loss:  2.814700 (2.7953)  Time: 0.587s, 1745.24/s  (2.305s,  444.32/s)  LR: 2.348e-05  Data: 0.023 (1.710)
Train: 174 [ 850/1171 ( 73%)]  Loss:  3.120251 (2.8133)  Time: 0.587s, 1744.21/s  (2.305s,  444.23/s)  LR: 2.348e-05  Data: 0.024 (1.711)
Train: 174 [ 900/1171 ( 77%)]  Loss:  2.775703 (2.8114)  Time: 0.585s, 1751.59/s  (2.310s,  443.19/s)  LR: 2.348e-05  Data: 0.020 (1.716)
Train: 174 [ 950/1171 ( 81%)]  Loss:  3.046411 (2.8231)  Time: 0.588s, 1742.77/s  (2.305s,  444.23/s)  LR: 2.348e-05  Data: 0.024 (1.711)
Train: 174 [1000/1171 ( 85%)]  Loss:  3.235376 (2.8427)  Time: 0.588s, 1740.12/s  (2.302s,  444.89/s)  LR: 2.348e-05  Data: 0.025 (1.707)
Train: 174 [1050/1171 ( 90%)]  Loss:  2.432091 (2.8241)  Time: 0.589s, 1739.95/s  (2.306s,  444.03/s)  LR: 2.348e-05  Data: 0.023 (1.712)
Train: 174 [1100/1171 ( 94%)]  Loss:  2.944688 (2.8293)  Time: 0.587s, 1745.73/s  (2.307s,  443.80/s)  LR: 2.348e-05  Data: 0.021 (1.714)
Train: 174 [1150/1171 ( 98%)]  Loss:  2.919004 (2.8331)  Time: 0.585s, 1749.38/s  (2.304s,  444.48/s)  LR: 2.348e-05  Data: 0.022 (1.711)
Train: 174 [1170/1171 (100%)]  Loss:  2.679348 (2.8269)  Time: 0.565s, 1811.52/s  (2.304s,  444.39/s)  LR: 2.348e-05  Data: 0.000 (1.711)
Test: [   0/97]  Time: 13.699 (13.699)  Loss:  0.2885 (0.2885)  Acc@1: 96.9727 (96.9727)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (3.063)  Loss:  0.4171 (0.3366)  Acc@1: 92.8711 (95.4695)  Acc@5: 98.3398 (98.9545)
Test: [  97/97]  Time: 0.120 (2.926)  Loss:  0.3031 (0.3473)  Acc@1: 94.1964 (94.9390)  Acc@5: 99.4048 (98.8550)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-174.pth.tar', 94.93899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-173.pth.tar', 94.89600001953124)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-172.pth.tar', 94.7839999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-168.pth.tar', 94.77900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-171.pth.tar', 94.75200004638671)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-170.pth.tar', 94.73900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-169.pth.tar', 94.73000000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-166.pth.tar', 94.70800000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-167.pth.tar', 94.70799999511719)

Train: 175 [   0/1171 (  0%)]  Loss:  2.674666 (2.6747)  Time: 10.206s,  100.34/s  (10.206s,  100.34/s)  LR: 2.163e-05  Data: 9.146 (9.146)
Train: 175 [  50/1171 (  4%)]  Loss:  2.692617 (2.6836)  Time: 0.586s, 1747.86/s  (2.154s,  475.43/s)  LR: 2.163e-05  Data: 0.022 (1.551)
Train: 175 [ 100/1171 (  9%)]  Loss:  3.048727 (2.8053)  Time: 1.929s,  530.80/s  (2.118s,  483.56/s)  LR: 2.163e-05  Data: 1.366 (1.519)
Train: 175 [ 150/1171 ( 13%)]  Loss:  2.829041 (2.8113)  Time: 0.586s, 1745.98/s  (2.171s,  471.67/s)  LR: 2.163e-05  Data: 0.020 (1.566)
Train: 175 [ 200/1171 ( 17%)]  Loss:  2.736675 (2.7963)  Time: 5.301s,  193.16/s  (2.190s,  467.55/s)  LR: 2.163e-05  Data: 4.490 (1.588)
Train: 175 [ 250/1171 ( 21%)]  Loss:  2.646445 (2.7714)  Time: 0.586s, 1747.41/s  (2.191s,  467.44/s)  LR: 2.163e-05  Data: 0.020 (1.588)
Train: 175 [ 300/1171 ( 26%)]  Loss:  2.749150 (2.7682)  Time: 3.259s,  314.17/s  (2.202s,  464.97/s)  LR: 2.163e-05  Data: 2.601 (1.599)
Train: 175 [ 350/1171 ( 30%)]  Loss:  2.736036 (2.7642)  Time: 0.588s, 1742.92/s  (2.191s,  467.34/s)  LR: 2.163e-05  Data: 0.020 (1.587)
Train: 175 [ 400/1171 ( 34%)]  Loss:  2.686045 (2.7555)  Time: 1.739s,  588.71/s  (2.181s,  469.52/s)  LR: 2.163e-05  Data: 1.087 (1.577)
Train: 175 [ 450/1171 ( 38%)]  Loss:  2.978494 (2.7778)  Time: 0.589s, 1739.87/s  (2.174s,  470.92/s)  LR: 2.163e-05  Data: 0.020 (1.569)
Train: 175 [ 500/1171 ( 43%)]  Loss:  2.580104 (2.7598)  Time: 1.601s,  639.66/s  (2.167s,  472.45/s)  LR: 2.163e-05  Data: 0.943 (1.559)
Train: 175 [ 550/1171 ( 47%)]  Loss:  2.887716 (2.7705)  Time: 0.588s, 1741.99/s  (2.177s,  470.30/s)  LR: 2.163e-05  Data: 0.023 (1.569)
Train: 175 [ 600/1171 ( 51%)]  Loss:  3.263799 (2.8084)  Time: 1.592s,  643.29/s  (2.201s,  465.33/s)  LR: 2.163e-05  Data: 1.029 (1.592)
Train: 175 [ 650/1171 ( 56%)]  Loss:  3.026879 (2.8240)  Time: 0.590s, 1736.93/s  (2.217s,  461.80/s)  LR: 2.163e-05  Data: 0.023 (1.608)
Train: 175 [ 700/1171 ( 60%)]  Loss:  2.828042 (2.8243)  Time: 6.201s,  165.15/s  (2.233s,  458.66/s)  LR: 2.163e-05  Data: 5.589 (1.624)
Train: 175 [ 750/1171 ( 64%)]  Loss:  3.408487 (2.8608)  Time: 0.589s, 1738.01/s  (2.230s,  459.11/s)  LR: 2.163e-05  Data: 0.024 (1.622)
Train: 175 [ 800/1171 ( 68%)]  Loss:  2.786429 (2.8564)  Time: 6.215s,  164.75/s  (2.228s,  459.60/s)  LR: 2.163e-05  Data: 5.652 (1.621)
Train: 175 [ 850/1171 ( 73%)]  Loss:  2.849232 (2.8560)  Time: 0.589s, 1738.10/s  (2.215s,  462.24/s)  LR: 2.163e-05  Data: 0.024 (1.610)
Train: 175 [ 900/1171 ( 77%)]  Loss:  2.939655 (2.8604)  Time: 4.225s,  242.34/s  (2.210s,  463.43/s)  LR: 2.163e-05  Data: 3.555 (1.605)
Train: 175 [ 950/1171 ( 81%)]  Loss:  3.048326 (2.8698)  Time: 0.584s, 1753.45/s  (2.207s,  463.98/s)  LR: 2.163e-05  Data: 0.019 (1.602)
Train: 175 [1000/1171 ( 85%)]  Loss:  3.097728 (2.8807)  Time: 0.969s, 1056.29/s  (2.217s,  461.92/s)  LR: 2.163e-05  Data: 0.407 (1.611)
Train: 175 [1050/1171 ( 90%)]  Loss:  2.714091 (2.8731)  Time: 0.586s, 1747.35/s  (2.228s,  459.68/s)  LR: 2.163e-05  Data: 0.022 (1.621)
Train: 175 [1100/1171 ( 94%)]  Loss:  3.186848 (2.8867)  Time: 0.586s, 1746.46/s  (2.229s,  459.41/s)  LR: 2.163e-05  Data: 0.023 (1.624)
Train: 175 [1150/1171 ( 98%)]  Loss:  2.923710 (2.8883)  Time: 0.588s, 1740.50/s  (2.231s,  459.08/s)  LR: 2.163e-05  Data: 0.021 (1.626)
Train: 175 [1170/1171 (100%)]  Loss:  2.535573 (2.8742)  Time: 0.567s, 1806.42/s  (2.228s,  459.70/s)  LR: 2.163e-05  Data: 0.000 (1.624)
Test: [   0/97]  Time: 12.163 (12.163)  Loss:  0.2835 (0.2835)  Acc@1: 96.8750 (96.8750)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (2.880)  Loss:  0.4428 (0.3507)  Acc@1: 92.2852 (95.3948)  Acc@5: 98.4375 (98.9526)
Test: [  97/97]  Time: 0.120 (2.789)  Loss:  0.3093 (0.3621)  Acc@1: 95.3869 (94.9210)  Acc@5: 99.4048 (98.8180)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-174.pth.tar', 94.93899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-175.pth.tar', 94.92100003173829)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-173.pth.tar', 94.89600001953124)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-172.pth.tar', 94.7839999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-168.pth.tar', 94.77900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-171.pth.tar', 94.75200004638671)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-170.pth.tar', 94.73900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-169.pth.tar', 94.73000000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-166.pth.tar', 94.70800000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-167.pth.tar', 94.70799999511719)

Train: 176 [   0/1171 (  0%)]  Loss:  2.641159 (2.6412)  Time: 9.168s,  111.69/s  (9.168s,  111.69/s)  LR: 1.992e-05  Data: 8.547 (8.547)
Train: 176 [  50/1171 (  4%)]  Loss:  2.398838 (2.5200)  Time: 0.588s, 1742.06/s  (2.090s,  489.97/s)  LR: 1.992e-05  Data: 0.022 (1.508)
Train: 176 [ 100/1171 (  9%)]  Loss:  2.353810 (2.4646)  Time: 0.588s, 1742.50/s  (2.262s,  452.69/s)  LR: 1.992e-05  Data: 0.019 (1.680)
Train: 176 [ 150/1171 ( 13%)]  Loss:  2.699199 (2.5233)  Time: 0.585s, 1751.44/s  (2.254s,  454.31/s)  LR: 1.992e-05  Data: 0.022 (1.669)
Train: 176 [ 200/1171 ( 17%)]  Loss:  2.919911 (2.6026)  Time: 0.583s, 1755.25/s  (2.257s,  453.63/s)  LR: 1.992e-05  Data: 0.019 (1.672)
Train: 176 [ 250/1171 ( 21%)]  Loss:  3.021358 (2.6724)  Time: 0.584s, 1753.06/s  (2.228s,  459.58/s)  LR: 1.992e-05  Data: 0.021 (1.644)
Train: 176 [ 300/1171 ( 26%)]  Loss:  2.578108 (2.6589)  Time: 0.587s, 1744.13/s  (2.225s,  460.20/s)  LR: 1.992e-05  Data: 0.020 (1.641)
Train: 176 [ 350/1171 ( 30%)]  Loss:  2.973962 (2.6983)  Time: 0.586s, 1746.74/s  (2.191s,  467.41/s)  LR: 1.992e-05  Data: 0.023 (1.607)
Train: 176 [ 400/1171 ( 34%)]  Loss:  2.971917 (2.7287)  Time: 0.587s, 1745.36/s  (2.182s,  469.31/s)  LR: 1.992e-05  Data: 0.020 (1.598)
Train: 176 [ 450/1171 ( 38%)]  Loss:  2.666692 (2.7225)  Time: 0.587s, 1745.10/s  (2.156s,  475.04/s)  LR: 1.992e-05  Data: 0.021 (1.572)
Train: 176 [ 500/1171 ( 43%)]  Loss:  2.721823 (2.7224)  Time: 0.586s, 1748.80/s  (2.188s,  467.98/s)  LR: 1.992e-05  Data: 0.021 (1.604)
Train: 176 [ 550/1171 ( 47%)]  Loss:  2.461022 (2.7006)  Time: 0.586s, 1746.97/s  (2.198s,  465.97/s)  LR: 1.992e-05  Data: 0.022 (1.614)
Train: 176 [ 600/1171 ( 51%)]  Loss:  2.877689 (2.7143)  Time: 0.587s, 1743.66/s  (2.208s,  463.79/s)  LR: 1.992e-05  Data: 0.020 (1.623)
Train: 176 [ 650/1171 ( 56%)]  Loss:  2.635176 (2.7086)  Time: 0.585s, 1751.07/s  (2.208s,  463.77/s)  LR: 1.992e-05  Data: 0.020 (1.623)
Train: 176 [ 700/1171 ( 60%)]  Loss:  2.924812 (2.7230)  Time: 0.587s, 1745.19/s  (2.209s,  463.63/s)  LR: 1.992e-05  Data: 0.019 (1.624)
Train: 176 [ 750/1171 ( 64%)]  Loss:  2.854104 (2.7312)  Time: 0.589s, 1738.40/s  (2.201s,  465.25/s)  LR: 1.992e-05  Data: 0.022 (1.616)
Train: 176 [ 800/1171 ( 68%)]  Loss:  2.883564 (2.7402)  Time: 0.584s, 1752.70/s  (2.202s,  465.09/s)  LR: 1.992e-05  Data: 0.021 (1.616)
Train: 176 [ 850/1171 ( 73%)]  Loss:  2.773282 (2.7420)  Time: 0.587s, 1743.89/s  (2.187s,  468.13/s)  LR: 1.992e-05  Data: 0.025 (1.602)
Train: 176 [ 900/1171 ( 77%)]  Loss:  2.944294 (2.7527)  Time: 0.588s, 1742.21/s  (2.203s,  464.83/s)  LR: 1.992e-05  Data: 0.019 (1.618)
Train: 176 [ 950/1171 ( 81%)]  Loss:  2.353455 (2.7327)  Time: 0.587s, 1743.74/s  (2.201s,  465.27/s)  LR: 1.992e-05  Data: 0.024 (1.615)
Train: 176 [1000/1171 ( 85%)]  Loss:  3.133946 (2.7518)  Time: 0.586s, 1748.26/s  (2.204s,  464.59/s)  LR: 1.992e-05  Data: 0.018 (1.618)
Train: 176 [1050/1171 ( 90%)]  Loss:  2.867843 (2.7571)  Time: 0.585s, 1750.89/s  (2.204s,  464.64/s)  LR: 1.992e-05  Data: 0.019 (1.618)
Train: 176 [1100/1171 ( 94%)]  Loss:  3.145309 (2.7740)  Time: 0.587s, 1743.76/s  (2.205s,  464.49/s)  LR: 1.992e-05  Data: 0.025 (1.618)
Train: 176 [1150/1171 ( 98%)]  Loss:  2.349913 (2.7563)  Time: 0.584s, 1752.12/s  (2.197s,  466.01/s)  LR: 1.992e-05  Data: 0.017 (1.611)
Train: 176 [1170/1171 (100%)]  Loss:  2.683298 (2.7534)  Time: 0.564s, 1816.45/s  (2.195s,  466.44/s)  LR: 1.992e-05  Data: 0.000 (1.609)
Test: [   0/97]  Time: 11.877 (11.877)  Loss:  0.2917 (0.2917)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.200 (2.817)  Loss:  0.4235 (0.3531)  Acc@1: 92.8711 (95.4044)  Acc@5: 98.4375 (98.9392)
Test: [  97/97]  Time: 0.119 (2.842)  Loss:  0.3259 (0.3636)  Acc@1: 94.6429 (94.8950)  Acc@5: 99.2560 (98.8160)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-174.pth.tar', 94.93899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-175.pth.tar', 94.92100003173829)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-173.pth.tar', 94.89600001953124)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-176.pth.tar', 94.89500002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-172.pth.tar', 94.7839999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-168.pth.tar', 94.77900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-171.pth.tar', 94.75200004638671)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-170.pth.tar', 94.73900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-169.pth.tar', 94.73000000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-166.pth.tar', 94.70800000732422)

Train: 177 [   0/1171 (  0%)]  Loss:  2.715972 (2.7160)  Time: 11.872s,   86.25/s  (11.872s,   86.25/s)  LR: 1.834e-05  Data: 10.743 (10.743)
Train: 177 [  50/1171 (  4%)]  Loss:  2.120193 (2.4181)  Time: 0.702s, 1457.98/s  (2.282s,  448.67/s)  LR: 1.834e-05  Data: 0.134 (1.690)
Train: 177 [ 100/1171 (  9%)]  Loss:  2.695375 (2.5105)  Time: 0.587s, 1743.73/s  (2.256s,  453.82/s)  LR: 1.834e-05  Data: 0.020 (1.673)
Train: 177 [ 150/1171 ( 13%)]  Loss:  2.658519 (2.5475)  Time: 4.462s,  229.47/s  (2.230s,  459.09/s)  LR: 1.834e-05  Data: 3.895 (1.640)
Train: 177 [ 200/1171 ( 17%)]  Loss:  2.715541 (2.5811)  Time: 0.587s, 1744.95/s  (2.203s,  464.76/s)  LR: 1.834e-05  Data: 0.019 (1.612)
Train: 177 [ 250/1171 ( 21%)]  Loss:  2.340797 (2.5411)  Time: 2.635s,  388.65/s  (2.170s,  472.00/s)  LR: 1.834e-05  Data: 1.926 (1.575)
Train: 177 [ 300/1171 ( 26%)]  Loss:  2.920114 (2.5952)  Time: 1.063s,  963.13/s  (2.150s,  476.38/s)  LR: 1.834e-05  Data: 0.417 (1.553)
Train: 177 [ 350/1171 ( 30%)]  Loss:  2.909710 (2.6345)  Time: 0.585s, 1751.05/s  (2.123s,  482.38/s)  LR: 1.834e-05  Data: 0.020 (1.523)
Train: 177 [ 400/1171 ( 34%)]  Loss:  2.248425 (2.5916)  Time: 4.735s,  216.24/s  (2.113s,  484.54/s)  LR: 1.834e-05  Data: 4.050 (1.513)
Train: 177 [ 450/1171 ( 38%)]  Loss:  3.189229 (2.6514)  Time: 0.587s, 1743.12/s  (2.141s,  478.27/s)  LR: 1.834e-05  Data: 0.019 (1.539)
Train: 177 [ 500/1171 ( 43%)]  Loss:  3.376581 (2.7173)  Time: 4.572s,  224.00/s  (2.171s,  471.71/s)  LR: 1.834e-05  Data: 3.983 (1.569)
Train: 177 [ 550/1171 ( 47%)]  Loss:  2.302627 (2.6828)  Time: 0.588s, 1740.04/s  (2.175s,  470.72/s)  LR: 1.834e-05  Data: 0.021 (1.575)
Train: 177 [ 600/1171 ( 51%)]  Loss:  3.200235 (2.7226)  Time: 5.186s,  197.45/s  (2.191s,  467.45/s)  LR: 1.834e-05  Data: 4.542 (1.590)
Train: 177 [ 650/1171 ( 56%)]  Loss:  2.603197 (2.7140)  Time: 0.586s, 1748.66/s  (2.186s,  468.43/s)  LR: 1.834e-05  Data: 0.022 (1.586)
Train: 177 [ 700/1171 ( 60%)]  Loss:  3.249170 (2.7497)  Time: 0.590s, 1735.26/s  (2.186s,  468.39/s)  LR: 1.834e-05  Data: 0.025 (1.586)
Train: 177 [ 750/1171 ( 64%)]  Loss:  2.940733 (2.7617)  Time: 0.587s, 1744.00/s  (2.174s,  470.96/s)  LR: 1.834e-05  Data: 0.019 (1.575)
Train: 177 [ 800/1171 ( 68%)]  Loss:  2.412148 (2.7411)  Time: 0.585s, 1750.60/s  (2.170s,  471.89/s)  LR: 1.834e-05  Data: 0.020 (1.571)
Train: 177 [ 850/1171 ( 73%)]  Loss:  2.665439 (2.7369)  Time: 0.586s, 1748.28/s  (2.182s,  469.25/s)  LR: 1.834e-05  Data: 0.021 (1.584)
Train: 177 [ 900/1171 ( 77%)]  Loss:  2.703276 (2.7351)  Time: 0.585s, 1750.95/s  (2.189s,  467.70/s)  LR: 1.834e-05  Data: 0.022 (1.592)
Train: 177 [ 950/1171 ( 81%)]  Loss:  2.968154 (2.7468)  Time: 0.586s, 1746.12/s  (2.189s,  467.90/s)  LR: 1.834e-05  Data: 0.021 (1.592)
Train: 177 [1000/1171 ( 85%)]  Loss:  3.008585 (2.7592)  Time: 0.583s, 1755.38/s  (2.190s,  467.63/s)  LR: 1.834e-05  Data: 0.020 (1.593)
Train: 177 [1050/1171 ( 90%)]  Loss:  2.577764 (2.7510)  Time: 0.586s, 1748.84/s  (2.184s,  468.96/s)  LR: 1.834e-05  Data: 0.020 (1.588)
Train: 177 [1100/1171 ( 94%)]  Loss:  3.185140 (2.7699)  Time: 0.586s, 1748.46/s  (2.181s,  469.51/s)  LR: 1.834e-05  Data: 0.023 (1.585)
Train: 177 [1150/1171 ( 98%)]  Loss:  3.103420 (2.7838)  Time: 0.585s, 1750.40/s  (2.173s,  471.16/s)  LR: 1.834e-05  Data: 0.020 (1.578)
Train: 177 [1170/1171 (100%)]  Loss:  2.790847 (2.7840)  Time: 0.564s, 1817.11/s  (2.171s,  471.71/s)  LR: 1.834e-05  Data: 0.000 (1.576)
Test: [   0/97]  Time: 11.322 (11.322)  Loss:  0.2965 (0.2965)  Acc@1: 96.8750 (96.8750)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.202 (2.961)  Loss:  0.4469 (0.3574)  Acc@1: 93.0664 (95.4312)  Acc@5: 98.6328 (98.9622)
Test: [  97/97]  Time: 0.120 (2.959)  Loss:  0.3414 (0.3681)  Acc@1: 93.7500 (94.8980)  Acc@5: 99.2560 (98.8240)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-174.pth.tar', 94.93899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-175.pth.tar', 94.92100003173829)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-177.pth.tar', 94.89800005126953)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-173.pth.tar', 94.89600001953124)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-176.pth.tar', 94.89500002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-172.pth.tar', 94.7839999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-168.pth.tar', 94.77900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-171.pth.tar', 94.75200004638671)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-170.pth.tar', 94.73900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-169.pth.tar', 94.73000000732422)

Train: 178 [   0/1171 (  0%)]  Loss:  2.413462 (2.4135)  Time: 10.581s,   96.78/s  (10.581s,   96.78/s)  LR: 1.690e-05  Data: 10.019 (10.019)
Train: 178 [  50/1171 (  4%)]  Loss:  3.056024 (2.7347)  Time: 0.721s, 1420.29/s  (2.264s,  452.37/s)  LR: 1.690e-05  Data: 0.093 (1.668)
Train: 178 [ 100/1171 (  9%)]  Loss:  3.093465 (2.8543)  Time: 2.080s,  492.37/s  (2.218s,  461.64/s)  LR: 1.690e-05  Data: 1.460 (1.621)
Train: 178 [ 150/1171 ( 13%)]  Loss:  2.487055 (2.7625)  Time: 3.608s,  283.79/s  (2.201s,  465.19/s)  LR: 1.690e-05  Data: 3.033 (1.596)
Train: 178 [ 200/1171 ( 17%)]  Loss:  2.904053 (2.7908)  Time: 0.995s, 1029.03/s  (2.175s,  470.75/s)  LR: 1.690e-05  Data: 0.383 (1.568)
Train: 178 [ 250/1171 ( 21%)]  Loss:  3.108034 (2.8437)  Time: 2.179s,  470.01/s  (2.151s,  475.95/s)  LR: 1.690e-05  Data: 1.488 (1.543)
Train: 178 [ 300/1171 ( 26%)]  Loss:  3.140319 (2.8861)  Time: 2.184s,  468.87/s  (2.125s,  481.95/s)  LR: 1.690e-05  Data: 1.519 (1.517)
Train: 178 [ 350/1171 ( 30%)]  Loss:  2.655311 (2.8572)  Time: 1.856s,  551.63/s  (2.151s,  476.12/s)  LR: 1.690e-05  Data: 1.284 (1.543)
Train: 178 [ 400/1171 ( 34%)]  Loss:  2.792954 (2.8501)  Time: 2.653s,  385.94/s  (2.144s,  477.50/s)  LR: 1.690e-05  Data: 2.072 (1.535)
Train: 178 [ 450/1171 ( 38%)]  Loss:  2.738143 (2.8389)  Time: 2.954s,  346.71/s  (2.166s,  472.79/s)  LR: 1.690e-05  Data: 2.279 (1.557)
Train: 178 [ 500/1171 ( 43%)]  Loss:  3.091079 (2.8618)  Time: 3.829s,  267.45/s  (2.171s,  471.69/s)  LR: 1.690e-05  Data: 3.247 (1.561)
Train: 178 [ 550/1171 ( 47%)]  Loss:  2.832528 (2.8594)  Time: 2.380s,  430.24/s  (2.173s,  471.30/s)  LR: 1.690e-05  Data: 1.738 (1.564)
Train: 178 [ 600/1171 ( 51%)]  Loss:  2.694069 (2.8467)  Time: 4.112s,  249.03/s  (2.176s,  470.65/s)  LR: 1.690e-05  Data: 3.462 (1.564)
Train: 178 [ 650/1171 ( 56%)]  Loss:  2.783943 (2.8422)  Time: 0.584s, 1752.56/s  (2.169s,  472.10/s)  LR: 1.690e-05  Data: 0.022 (1.557)
Train: 178 [ 700/1171 ( 60%)]  Loss:  2.926441 (2.8478)  Time: 4.048s,  252.99/s  (2.164s,  473.23/s)  LR: 1.690e-05  Data: 3.345 (1.553)
Train: 178 [ 750/1171 ( 64%)]  Loss:  2.989516 (2.8566)  Time: 0.587s, 1743.45/s  (2.154s,  475.48/s)  LR: 1.690e-05  Data: 0.021 (1.543)
Train: 178 [ 800/1171 ( 68%)]  Loss:  2.656200 (2.8449)  Time: 4.220s,  242.67/s  (2.167s,  472.48/s)  LR: 1.690e-05  Data: 3.628 (1.558)
Train: 178 [ 850/1171 ( 73%)]  Loss:  3.076323 (2.8577)  Time: 0.584s, 1754.44/s  (2.174s,  470.99/s)  LR: 1.690e-05  Data: 0.019 (1.565)
Train: 178 [ 900/1171 ( 77%)]  Loss:  3.245683 (2.8781)  Time: 4.717s,  217.10/s  (2.178s,  470.25/s)  LR: 1.690e-05  Data: 4.072 (1.569)
Train: 178 [ 950/1171 ( 81%)]  Loss:  3.153480 (2.8919)  Time: 0.585s, 1749.23/s  (2.174s,  471.09/s)  LR: 1.690e-05  Data: 0.019 (1.565)
Train: 178 [1000/1171 ( 85%)]  Loss:  2.739756 (2.8847)  Time: 1.590s,  643.93/s  (2.166s,  472.72/s)  LR: 1.690e-05  Data: 0.917 (1.559)
Train: 178 [1050/1171 ( 90%)]  Loss:  2.632562 (2.8732)  Time: 0.584s, 1753.64/s  (2.162s,  473.64/s)  LR: 1.690e-05  Data: 0.020 (1.555)
Train: 178 [1100/1171 ( 94%)]  Loss:  2.474633 (2.8559)  Time: 2.469s,  414.69/s  (2.153s,  475.53/s)  LR: 1.690e-05  Data: 1.712 (1.547)
Train: 178 [1150/1171 ( 98%)]  Loss:  2.855826 (2.8559)  Time: 0.585s, 1749.07/s  (2.150s,  476.32/s)  LR: 1.690e-05  Data: 0.018 (1.544)
Train: 178 [1170/1171 (100%)]  Loss:  2.318681 (2.8344)  Time: 0.565s, 1813.18/s  (2.147s,  476.88/s)  LR: 1.690e-05  Data: 0.000 (1.541)
Test: [   0/97]  Time: 17.662 (17.662)  Loss:  0.2839 (0.2839)  Acc@1: 97.2656 (97.2656)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.200 (3.140)  Loss:  0.4367 (0.3528)  Acc@1: 92.7734 (95.4044)  Acc@5: 98.5352 (98.9660)
Test: [  97/97]  Time: 0.119 (2.969)  Loss:  0.3264 (0.3637)  Acc@1: 94.7917 (94.9090)  Acc@5: 99.5536 (98.8510)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-174.pth.tar', 94.93899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-175.pth.tar', 94.92100003173829)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-178.pth.tar', 94.90899998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-177.pth.tar', 94.89800005126953)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-173.pth.tar', 94.89600001953124)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-176.pth.tar', 94.89500002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-172.pth.tar', 94.7839999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-168.pth.tar', 94.77900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-171.pth.tar', 94.75200004638671)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-170.pth.tar', 94.73900000976562)

Train: 179 [   0/1171 (  0%)]  Loss:  2.390603 (2.3906)  Time: 10.511s,   97.42/s  (10.511s,   97.42/s)  LR: 1.559e-05  Data: 9.523 (9.523)
Train: 179 [  50/1171 (  4%)]  Loss:  3.009790 (2.7002)  Time: 0.584s, 1752.43/s  (2.137s,  479.22/s)  LR: 1.559e-05  Data: 0.019 (1.535)
Train: 179 [ 100/1171 (  9%)]  Loss:  2.738602 (2.7130)  Time: 3.564s,  287.31/s  (2.121s,  482.90/s)  LR: 1.559e-05  Data: 2.996 (1.517)
Train: 179 [ 150/1171 ( 13%)]  Loss:  2.277754 (2.6042)  Time: 0.587s, 1743.97/s  (2.059s,  497.24/s)  LR: 1.559e-05  Data: 0.018 (1.452)
Train: 179 [ 200/1171 ( 17%)]  Loss:  2.636358 (2.6106)  Time: 3.521s,  290.84/s  (2.051s,  499.38/s)  LR: 1.559e-05  Data: 2.841 (1.441)
Train: 179 [ 250/1171 ( 21%)]  Loss:  2.750620 (2.6340)  Time: 0.585s, 1751.72/s  (2.011s,  509.30/s)  LR: 1.559e-05  Data: 0.019 (1.406)
Train: 179 [ 300/1171 ( 26%)]  Loss:  2.995782 (2.6856)  Time: 2.813s,  364.00/s  (1.996s,  513.03/s)  LR: 1.559e-05  Data: 2.245 (1.393)
Train: 179 [ 350/1171 ( 30%)]  Loss:  3.073485 (2.7341)  Time: 0.585s, 1751.28/s  (2.031s,  504.30/s)  LR: 1.559e-05  Data: 0.021 (1.428)
Train: 179 [ 400/1171 ( 34%)]  Loss:  3.126513 (2.7777)  Time: 4.186s,  244.60/s  (2.058s,  497.63/s)  LR: 1.559e-05  Data: 3.522 (1.455)
Train: 179 [ 450/1171 ( 38%)]  Loss:  2.863786 (2.7863)  Time: 1.678s,  610.23/s  (2.062s,  496.64/s)  LR: 1.559e-05  Data: 1.048 (1.458)
Train: 179 [ 500/1171 ( 43%)]  Loss:  2.481511 (2.7586)  Time: 2.104s,  486.65/s  (2.069s,  494.83/s)  LR: 1.559e-05  Data: 1.537 (1.466)
Train: 179 [ 550/1171 ( 47%)]  Loss:  2.799372 (2.7620)  Time: 2.199s,  465.74/s  (2.074s,  493.79/s)  LR: 1.559e-05  Data: 1.562 (1.471)
Train: 179 [ 600/1171 ( 51%)]  Loss:  2.085994 (2.7100)  Time: 4.799s,  213.36/s  (2.081s,  492.11/s)  LR: 1.559e-05  Data: 4.200 (1.478)
Train: 179 [ 650/1171 ( 56%)]  Loss:  2.688573 (2.7085)  Time: 3.373s,  303.59/s  (2.079s,  492.47/s)  LR: 1.559e-05  Data: 2.808 (1.476)
Train: 179 [ 700/1171 ( 60%)]  Loss:  2.370080 (2.6859)  Time: 4.123s,  248.36/s  (2.079s,  492.60/s)  LR: 1.559e-05  Data: 3.560 (1.476)
Train: 179 [ 750/1171 ( 64%)]  Loss:  2.457760 (2.6717)  Time: 3.439s,  297.74/s  (2.097s,  488.22/s)  LR: 1.559e-05  Data: 2.787 (1.495)
Train: 179 [ 800/1171 ( 68%)]  Loss:  2.820207 (2.6804)  Time: 2.925s,  350.07/s  (2.100s,  487.72/s)  LR: 1.559e-05  Data: 2.264 (1.496)
Train: 179 [ 850/1171 ( 73%)]  Loss:  2.724065 (2.6828)  Time: 0.590s, 1736.97/s  (2.109s,  485.51/s)  LR: 1.559e-05  Data: 0.025 (1.506)
Train: 179 [ 900/1171 ( 77%)]  Loss:  3.285662 (2.7146)  Time: 5.967s,  171.60/s  (2.119s,  483.23/s)  LR: 1.559e-05  Data: 5.389 (1.517)
Train: 179 [ 950/1171 ( 81%)]  Loss:  2.989417 (2.7283)  Time: 0.588s, 1741.05/s  (2.117s,  483.73/s)  LR: 1.559e-05  Data: 0.025 (1.515)
Train: 179 [1000/1171 ( 85%)]  Loss:  2.811553 (2.7323)  Time: 6.593s,  155.32/s  (2.118s,  483.49/s)  LR: 1.559e-05  Data: 5.852 (1.517)
Train: 179 [1050/1171 ( 90%)]  Loss:  3.001608 (2.7445)  Time: 0.587s, 1743.07/s  (2.110s,  485.26/s)  LR: 1.559e-05  Data: 0.021 (1.510)
Train: 179 [1100/1171 ( 94%)]  Loss:  2.770657 (2.7456)  Time: 6.018s,  170.15/s  (2.107s,  485.90/s)  LR: 1.559e-05  Data: 5.307 (1.508)
Train: 179 [1150/1171 ( 98%)]  Loss:  2.682997 (2.7430)  Time: 1.116s,  917.47/s  (2.101s,  487.34/s)  LR: 1.559e-05  Data: 0.358 (1.501)
Train: 179 [1170/1171 (100%)]  Loss:  3.073820 (2.7563)  Time: 0.567s, 1807.22/s  (2.109s,  485.63/s)  LR: 1.559e-05  Data: 0.000 (1.509)
Test: [   0/97]  Time: 11.580 (11.580)  Loss:  0.2926 (0.2926)  Acc@1: 96.8750 (96.8750)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (2.828)  Loss:  0.4177 (0.3525)  Acc@1: 93.3594 (95.4561)  Acc@5: 98.5352 (98.9756)
Test: [  97/97]  Time: 0.120 (2.821)  Loss:  0.3360 (0.3625)  Acc@1: 94.4940 (94.9640)  Acc@5: 99.4048 (98.8560)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-179.pth.tar', 94.96400000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-174.pth.tar', 94.93899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-175.pth.tar', 94.92100003173829)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-178.pth.tar', 94.90899998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-177.pth.tar', 94.89800005126953)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-173.pth.tar', 94.89600001953124)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-176.pth.tar', 94.89500002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-172.pth.tar', 94.7839999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-168.pth.tar', 94.77900000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-171.pth.tar', 94.75200004638671)

Train: 180 [   0/1171 (  0%)]  Loss:  3.098104 (3.0981)  Time: 10.662s,   96.04/s  (10.662s,   96.04/s)  LR: 1.442e-05  Data: 9.676 (9.676)
Train: 180 [  50/1171 (  4%)]  Loss:  2.306053 (2.7021)  Time: 0.589s, 1738.70/s  (2.168s,  472.29/s)  LR: 1.442e-05  Data: 0.025 (1.571)
Train: 180 [ 100/1171 (  9%)]  Loss:  2.529257 (2.6445)  Time: 1.864s,  549.27/s  (2.121s,  482.74/s)  LR: 1.442e-05  Data: 1.296 (1.520)
Train: 180 [ 150/1171 ( 13%)]  Loss:  2.770435 (2.6760)  Time: 0.587s, 1743.95/s  (2.050s,  499.51/s)  LR: 1.442e-05  Data: 0.023 (1.449)
Train: 180 [ 200/1171 ( 17%)]  Loss:  2.597891 (2.6603)  Time: 0.932s, 1098.17/s  (2.029s,  504.58/s)  LR: 1.442e-05  Data: 0.249 (1.426)
Train: 180 [ 250/1171 ( 21%)]  Loss:  2.522866 (2.6374)  Time: 0.586s, 1748.15/s  (1.996s,  512.99/s)  LR: 1.442e-05  Data: 0.023 (1.395)
Train: 180 [ 300/1171 ( 26%)]  Loss:  2.641769 (2.6381)  Time: 0.588s, 1741.68/s  (2.049s,  499.72/s)  LR: 1.442e-05  Data: 0.022 (1.450)
Train: 180 [ 350/1171 ( 30%)]  Loss:  2.582571 (2.6311)  Time: 0.586s, 1748.50/s  (2.044s,  500.98/s)  LR: 1.442e-05  Data: 0.023 (1.448)
Train: 180 [ 400/1171 ( 34%)]  Loss:  2.430235 (2.6088)  Time: 0.587s, 1743.94/s  (2.069s,  494.96/s)  LR: 1.442e-05  Data: 0.019 (1.473)
Train: 180 [ 450/1171 ( 38%)]  Loss:  2.812948 (2.6292)  Time: 0.586s, 1748.00/s  (2.071s,  494.46/s)  LR: 1.442e-05  Data: 0.020 (1.475)
Train: 180 [ 500/1171 ( 43%)]  Loss:  2.638699 (2.6301)  Time: 0.589s, 1739.19/s  (2.086s,  490.97/s)  LR: 1.442e-05  Data: 0.018 (1.491)
Train: 180 [ 550/1171 ( 47%)]  Loss:  2.929746 (2.6550)  Time: 0.588s, 1741.08/s  (2.091s,  489.81/s)  LR: 1.442e-05  Data: 0.021 (1.497)
Train: 180 [ 600/1171 ( 51%)]  Loss:  2.570041 (2.6485)  Time: 0.587s, 1743.08/s  (2.099s,  487.74/s)  LR: 1.442e-05  Data: 0.021 (1.505)
Train: 180 [ 650/1171 ( 56%)]  Loss:  2.632573 (2.6474)  Time: 0.583s, 1756.10/s  (2.093s,  489.28/s)  LR: 1.442e-05  Data: 0.022 (1.499)
Train: 180 [ 700/1171 ( 60%)]  Loss:  3.042959 (2.6737)  Time: 0.590s, 1736.61/s  (2.101s,  487.33/s)  LR: 1.442e-05  Data: 0.021 (1.507)
Train: 180 [ 750/1171 ( 64%)]  Loss:  2.425964 (2.6583)  Time: 0.585s, 1751.12/s  (2.107s,  486.09/s)  LR: 1.442e-05  Data: 0.020 (1.514)
Train: 180 [ 800/1171 ( 68%)]  Loss:  2.493180 (2.6485)  Time: 0.586s, 1746.71/s  (2.119s,  483.16/s)  LR: 1.442e-05  Data: 0.018 (1.527)
Train: 180 [ 850/1171 ( 73%)]  Loss:  2.565485 (2.6439)  Time: 0.584s, 1754.87/s  (2.119s,  483.28/s)  LR: 1.442e-05  Data: 0.019 (1.527)
Train: 180 [ 900/1171 ( 77%)]  Loss:  2.531855 (2.6380)  Time: 0.586s, 1746.35/s  (2.121s,  482.78/s)  LR: 1.442e-05  Data: 0.019 (1.530)
Train: 180 [ 950/1171 ( 81%)]  Loss:  2.623363 (2.6373)  Time: 0.587s, 1743.58/s  (2.120s,  482.98/s)  LR: 1.442e-05  Data: 0.020 (1.529)
Train: 180 [1000/1171 ( 85%)]  Loss:  2.380488 (2.6251)  Time: 0.586s, 1746.23/s  (2.120s,  482.98/s)  LR: 1.442e-05  Data: 0.020 (1.530)
Train: 180 [1050/1171 ( 90%)]  Loss:  2.694146 (2.6282)  Time: 0.587s, 1745.32/s  (2.112s,  484.96/s)  LR: 1.442e-05  Data: 0.022 (1.521)
Train: 180 [1100/1171 ( 94%)]  Loss:  3.059005 (2.6469)  Time: 0.589s, 1737.33/s  (2.107s,  486.00/s)  LR: 1.442e-05  Data: 0.020 (1.517)
Train: 180 [1150/1171 ( 98%)]  Loss:  2.721781 (2.6501)  Time: 0.586s, 1746.88/s  (2.114s,  484.43/s)  LR: 1.442e-05  Data: 0.023 (1.524)
Train: 180 [1170/1171 (100%)]  Loss:  2.602124 (2.6481)  Time: 0.565s, 1811.22/s  (2.112s,  484.87/s)  LR: 1.442e-05  Data: 0.000 (1.523)
Test: [   0/97]  Time: 10.737 (10.737)  Loss:  0.3032 (0.3032)  Acc@1: 96.8750 (96.8750)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (2.900)  Loss:  0.4534 (0.3680)  Acc@1: 92.9688 (95.3948)  Acc@5: 98.5352 (98.9507)
Test: [  97/97]  Time: 0.119 (2.890)  Loss:  0.3461 (0.3779)  Acc@1: 94.7917 (94.9060)  Acc@5: 99.4048 (98.8390)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-179.pth.tar', 94.96400000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-174.pth.tar', 94.93899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-175.pth.tar', 94.92100003173829)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-178.pth.tar', 94.90899998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-180.pth.tar', 94.90600003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-177.pth.tar', 94.89800005126953)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-173.pth.tar', 94.89600001953124)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-176.pth.tar', 94.89500002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-172.pth.tar', 94.7839999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-168.pth.tar', 94.77900000976562)

Train: 181 [   0/1171 (  0%)]  Loss:  2.615493 (2.6155)  Time: 10.144s,  100.95/s  (10.144s,  100.95/s)  LR: 1.338e-05  Data: 9.267 (9.267)
Train: 181 [  50/1171 (  4%)]  Loss:  3.059916 (2.8377)  Time: 0.585s, 1749.58/s  (2.176s,  470.68/s)  LR: 1.338e-05  Data: 0.019 (1.569)
Train: 181 [ 100/1171 (  9%)]  Loss:  2.728030 (2.8011)  Time: 0.584s, 1753.82/s  (2.107s,  485.91/s)  LR: 1.338e-05  Data: 0.020 (1.508)
Train: 181 [ 150/1171 ( 13%)]  Loss:  2.754998 (2.7896)  Time: 0.584s, 1752.08/s  (2.044s,  500.87/s)  LR: 1.338e-05  Data: 0.021 (1.448)
Train: 181 [ 200/1171 ( 17%)]  Loss:  2.770585 (2.7858)  Time: 1.764s,  580.40/s  (2.048s,  499.91/s)  LR: 1.338e-05  Data: 1.112 (1.453)
Train: 181 [ 250/1171 ( 21%)]  Loss:  3.105318 (2.8391)  Time: 0.585s, 1751.39/s  (2.039s,  502.13/s)  LR: 1.338e-05  Data: 0.019 (1.441)
Train: 181 [ 300/1171 ( 26%)]  Loss:  2.771702 (2.8294)  Time: 4.809s,  212.94/s  (2.086s,  490.85/s)  LR: 1.338e-05  Data: 4.246 (1.489)
Train: 181 [ 350/1171 ( 30%)]  Loss:  3.080719 (2.8608)  Time: 0.589s, 1738.71/s  (2.099s,  487.95/s)  LR: 1.338e-05  Data: 0.018 (1.501)
Train: 181 [ 400/1171 ( 34%)]  Loss:  3.147927 (2.8927)  Time: 6.747s,  151.76/s  (2.115s,  484.24/s)  LR: 1.338e-05  Data: 6.185 (1.518)
Train: 181 [ 450/1171 ( 38%)]  Loss:  3.096572 (2.9131)  Time: 0.586s, 1745.97/s  (2.101s,  487.37/s)  LR: 1.338e-05  Data: 0.019 (1.505)
Train: 181 [ 500/1171 ( 43%)]  Loss:  3.228506 (2.9418)  Time: 6.767s,  151.31/s  (2.106s,  486.32/s)  LR: 1.338e-05  Data: 6.205 (1.511)
Train: 181 [ 550/1171 ( 47%)]  Loss:  2.631689 (2.9160)  Time: 0.586s, 1746.62/s  (2.101s,  487.37/s)  LR: 1.338e-05  Data: 0.022 (1.507)
Train: 181 [ 600/1171 ( 51%)]  Loss:  3.041888 (2.9256)  Time: 5.524s,  185.39/s  (2.103s,  486.91/s)  LR: 1.338e-05  Data: 4.948 (1.509)
Train: 181 [ 650/1171 ( 56%)]  Loss:  2.956887 (2.9279)  Time: 0.588s, 1741.91/s  (2.095s,  488.83/s)  LR: 1.338e-05  Data: 0.019 (1.500)
Train: 181 [ 700/1171 ( 60%)]  Loss:  2.598456 (2.9059)  Time: 6.479s,  158.04/s  (2.118s,  483.38/s)  LR: 1.338e-05  Data: 5.849 (1.524)
Train: 181 [ 750/1171 ( 64%)]  Loss:  2.945759 (2.9084)  Time: 0.588s, 1741.77/s  (2.121s,  482.73/s)  LR: 1.338e-05  Data: 0.020 (1.527)
Train: 181 [ 800/1171 ( 68%)]  Loss:  2.517860 (2.8854)  Time: 6.294s,  162.71/s  (2.130s,  480.73/s)  LR: 1.338e-05  Data: 5.656 (1.535)
Train: 181 [ 850/1171 ( 73%)]  Loss:  2.427011 (2.8600)  Time: 0.586s, 1748.54/s  (2.130s,  480.80/s)  LR: 1.338e-05  Data: 0.022 (1.536)
Train: 181 [ 900/1171 ( 77%)]  Loss:  3.210934 (2.8784)  Time: 4.929s,  207.75/s  (2.134s,  479.84/s)  LR: 1.338e-05  Data: 4.367 (1.541)
Train: 181 [ 950/1171 ( 81%)]  Loss:  2.773123 (2.8732)  Time: 0.589s, 1739.07/s  (2.131s,  480.58/s)  LR: 1.338e-05  Data: 0.020 (1.537)
Train: 181 [1000/1171 ( 85%)]  Loss:  2.912578 (2.8750)  Time: 5.065s,  202.16/s  (2.173s,  471.24/s)  LR: 1.338e-05  Data: 4.446 (1.576)
Train: 181 [1050/1171 ( 90%)]  Loss:  2.940526 (2.8780)  Time: 0.595s, 1720.61/s  (2.167s,  472.60/s)  LR: 1.338e-05  Data: 0.020 (1.570)
Train: 181 [1100/1171 ( 94%)]  Loss:  3.120401 (2.8886)  Time: 4.096s,  249.97/s  (2.180s,  469.69/s)  LR: 1.338e-05  Data: 3.438 (1.583)
Train: 181 [1150/1171 ( 98%)]  Loss:  2.806309 (2.8851)  Time: 0.587s, 1744.74/s  (2.181s,  469.49/s)  LR: 1.338e-05  Data: 0.020 (1.583)
Train: 181 [1170/1171 (100%)]  Loss:  3.118564 (2.8945)  Time: 0.565s, 1811.20/s  (2.180s,  469.71/s)  LR: 1.338e-05  Data: 0.000 (1.581)
Test: [   0/97]  Time: 11.959 (11.959)  Loss:  0.2748 (0.2748)  Acc@1: 96.5820 (96.5820)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.273 (2.985)  Loss:  0.4177 (0.3431)  Acc@1: 93.2617 (95.4236)  Acc@5: 98.4375 (98.9602)
Test: [  97/97]  Time: 0.119 (2.876)  Loss:  0.3208 (0.3541)  Acc@1: 94.7917 (94.9550)  Acc@5: 99.4048 (98.8350)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-179.pth.tar', 94.96400000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-181.pth.tar', 94.95500003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-174.pth.tar', 94.93899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-175.pth.tar', 94.92100003173829)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-178.pth.tar', 94.90899998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-180.pth.tar', 94.90600003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-177.pth.tar', 94.89800005126953)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-173.pth.tar', 94.89600001953124)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-176.pth.tar', 94.89500002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-172.pth.tar', 94.7839999975586)

Train: 182 [   0/1171 (  0%)]  Loss:  2.923399 (2.9234)  Time: 9.419s,  108.72/s  (9.419s,  108.72/s)  LR: 1.249e-05  Data: 8.695 (8.695)
Train: 182 [  50/1171 (  4%)]  Loss:  2.956222 (2.9398)  Time: 0.589s, 1737.67/s  (2.132s,  480.19/s)  LR: 1.249e-05  Data: 0.026 (1.538)
Train: 182 [ 100/1171 (  9%)]  Loss:  2.671759 (2.8505)  Time: 4.302s,  238.01/s  (2.108s,  485.85/s)  LR: 1.249e-05  Data: 3.684 (1.510)
Train: 182 [ 150/1171 ( 13%)]  Loss:  2.520777 (2.7680)  Time: 0.587s, 1744.98/s  (2.035s,  503.11/s)  LR: 1.249e-05  Data: 0.023 (1.437)
Train: 182 [ 200/1171 ( 17%)]  Loss:  2.516840 (2.7178)  Time: 6.430s,  159.25/s  (2.120s,  483.04/s)  LR: 1.249e-05  Data: 5.173 (1.516)
Train: 182 [ 250/1171 ( 21%)]  Loss:  2.770908 (2.7267)  Time: 0.588s, 1742.62/s  (2.097s,  488.38/s)  LR: 1.249e-05  Data: 0.019 (1.494)
Train: 182 [ 300/1171 ( 26%)]  Loss:  2.829467 (2.7413)  Time: 2.428s,  421.66/s  (2.119s,  483.19/s)  LR: 1.249e-05  Data: 1.864 (1.517)
Train: 182 [ 350/1171 ( 30%)]  Loss:  2.861899 (2.7564)  Time: 0.589s, 1739.91/s  (2.125s,  481.82/s)  LR: 1.249e-05  Data: 0.024 (1.525)
Train: 182 [ 400/1171 ( 34%)]  Loss:  2.948274 (2.7777)  Time: 2.559s,  400.08/s  (2.128s,  481.13/s)  LR: 1.249e-05  Data: 1.911 (1.528)
Train: 182 [ 450/1171 ( 38%)]  Loss:  3.238202 (2.8238)  Time: 0.590s, 1735.55/s  (2.126s,  481.60/s)  LR: 1.249e-05  Data: 0.024 (1.524)
Train: 182 [ 500/1171 ( 43%)]  Loss:  2.682914 (2.8110)  Time: 1.014s, 1010.17/s  (2.116s,  483.89/s)  LR: 1.249e-05  Data: 0.442 (1.511)
Train: 182 [ 550/1171 ( 47%)]  Loss:  2.585819 (2.7922)  Time: 0.587s, 1744.33/s  (2.114s,  484.49/s)  LR: 1.249e-05  Data: 0.022 (1.510)
Train: 182 [ 600/1171 ( 51%)]  Loss:  2.728216 (2.7873)  Time: 0.586s, 1746.81/s  (2.108s,  485.67/s)  LR: 1.249e-05  Data: 0.023 (1.505)
Train: 182 [ 650/1171 ( 56%)]  Loss:  2.761483 (2.7854)  Time: 0.585s, 1749.19/s  (2.222s,  460.78/s)  LR: 1.249e-05  Data: 0.020 (1.615)
Train: 182 [ 700/1171 ( 60%)]  Loss:  2.904348 (2.7934)  Time: 0.584s, 1753.56/s  (2.218s,  461.59/s)  LR: 1.249e-05  Data: 0.021 (1.612)
Train: 182 [ 750/1171 ( 64%)]  Loss:  2.892425 (2.7996)  Time: 0.588s, 1742.26/s  (2.220s,  461.16/s)  LR: 1.249e-05  Data: 0.018 (1.614)
Train: 182 [ 800/1171 ( 68%)]  Loss:  3.089847 (2.8166)  Time: 0.592s, 1729.93/s  (2.212s,  462.95/s)  LR: 1.249e-05  Data: 0.020 (1.607)
Train: 182 [ 850/1171 ( 73%)]  Loss:  3.210764 (2.8385)  Time: 1.133s,  903.50/s  (2.206s,  464.13/s)  LR: 1.249e-05  Data: 0.571 (1.601)
Train: 182 [ 900/1171 ( 77%)]  Loss:  2.957358 (2.8448)  Time: 1.110s,  922.91/s  (2.195s,  466.46/s)  LR: 1.249e-05  Data: 0.448 (1.590)
Train: 182 [ 950/1171 ( 81%)]  Loss:  2.644901 (2.8348)  Time: 1.261s,  812.21/s  (2.189s,  467.86/s)  LR: 1.249e-05  Data: 0.585 (1.582)
Train: 182 [1000/1171 ( 85%)]  Loss:  3.101764 (2.8475)  Time: 0.588s, 1742.04/s  (2.181s,  469.54/s)  LR: 1.249e-05  Data: 0.022 (1.576)
Train: 182 [1050/1171 ( 90%)]  Loss:  2.735466 (2.8424)  Time: 2.353s,  435.16/s  (2.186s,  468.34/s)  LR: 1.249e-05  Data: 1.701 (1.582)
Train: 182 [1100/1171 ( 94%)]  Loss:  2.458761 (2.8257)  Time: 0.588s, 1741.82/s  (2.183s,  469.03/s)  LR: 1.249e-05  Data: 0.019 (1.579)
Train: 182 [1150/1171 ( 98%)]  Loss:  3.167711 (2.8400)  Time: 3.834s,  267.05/s  (2.178s,  470.17/s)  LR: 1.249e-05  Data: 3.239 (1.573)
Train: 182 [1170/1171 (100%)]  Loss:  2.848489 (2.8403)  Time: 0.565s, 1811.94/s  (2.175s,  470.87/s)  LR: 1.249e-05  Data: 0.000 (1.570)
Test: [   0/97]  Time: 11.946 (11.946)  Loss:  0.2750 (0.2750)  Acc@1: 97.0703 (97.0703)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (2.804)  Loss:  0.4170 (0.3360)  Acc@1: 92.8711 (95.4370)  Acc@5: 98.5352 (98.9392)
Test: [  97/97]  Time: 0.120 (2.695)  Loss:  0.3114 (0.3458)  Acc@1: 94.9405 (95.0150)  Acc@5: 99.4048 (98.8420)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-182.pth.tar', 95.0149999951172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-179.pth.tar', 94.96400000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-181.pth.tar', 94.95500003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-174.pth.tar', 94.93899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-175.pth.tar', 94.92100003173829)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-178.pth.tar', 94.90899998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-180.pth.tar', 94.90600003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-177.pth.tar', 94.89800005126953)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-173.pth.tar', 94.89600001953124)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-176.pth.tar', 94.89500002197266)

Train: 183 [   0/1171 (  0%)]  Loss:  2.788490 (2.7885)  Time: 9.669s,  105.90/s  (9.669s,  105.90/s)  LR: 1.173e-05  Data: 8.427 (8.427)
Train: 183 [  50/1171 (  4%)]  Loss:  2.893270 (2.8409)  Time: 0.585s, 1748.95/s  (2.019s,  507.15/s)  LR: 1.173e-05  Data: 0.020 (1.408)
Train: 183 [ 100/1171 (  9%)]  Loss:  3.015016 (2.8989)  Time: 0.583s, 1755.02/s  (1.980s,  517.22/s)  LR: 1.173e-05  Data: 0.020 (1.380)
Train: 183 [ 150/1171 ( 13%)]  Loss:  2.812059 (2.8772)  Time: 0.587s, 1744.00/s  (2.048s,  500.00/s)  LR: 1.173e-05  Data: 0.023 (1.451)
Train: 183 [ 200/1171 ( 17%)]  Loss:  2.330823 (2.7679)  Time: 0.585s, 1749.38/s  (2.055s,  498.21/s)  LR: 1.173e-05  Data: 0.022 (1.462)
Train: 183 [ 250/1171 ( 21%)]  Loss:  3.037810 (2.8129)  Time: 0.586s, 1747.65/s  (2.051s,  499.37/s)  LR: 1.173e-05  Data: 0.023 (1.460)
Train: 183 [ 300/1171 ( 26%)]  Loss:  2.536869 (2.7735)  Time: 0.847s, 1209.44/s  (2.050s,  499.45/s)  LR: 1.173e-05  Data: 0.279 (1.455)
Train: 183 [ 350/1171 ( 30%)]  Loss:  2.574706 (2.7486)  Time: 0.586s, 1748.39/s  (2.162s,  473.63/s)  LR: 1.173e-05  Data: 0.022 (1.558)
Train: 183 [ 400/1171 ( 34%)]  Loss:  2.946579 (2.7706)  Time: 3.711s,  275.94/s  (2.148s,  476.70/s)  LR: 1.173e-05  Data: 3.078 (1.545)
Train: 183 [ 450/1171 ( 38%)]  Loss:  3.123209 (2.8059)  Time: 0.586s, 1747.89/s  (2.119s,  483.21/s)  LR: 1.173e-05  Data: 0.019 (1.516)
Train: 183 [ 500/1171 ( 43%)]  Loss:  2.367440 (2.7660)  Time: 1.522s,  672.74/s  (2.110s,  485.27/s)  LR: 1.173e-05  Data: 0.851 (1.507)
Train: 183 [ 550/1171 ( 47%)]  Loss:  2.770605 (2.7664)  Time: 0.587s, 1743.37/s  (2.095s,  488.72/s)  LR: 1.173e-05  Data: 0.022 (1.493)
Train: 183 [ 600/1171 ( 51%)]  Loss:  3.013103 (2.7854)  Time: 3.759s,  272.43/s  (2.134s,  479.75/s)  LR: 1.173e-05  Data: 3.181 (1.532)
Train: 183 [ 650/1171 ( 56%)]  Loss:  2.863017 (2.7909)  Time: 0.587s, 1743.67/s  (2.129s,  480.89/s)  LR: 1.173e-05  Data: 0.024 (1.527)
Train: 183 [ 700/1171 ( 60%)]  Loss:  3.133101 (2.8137)  Time: 4.178s,  245.10/s  (2.136s,  479.33/s)  LR: 1.173e-05  Data: 3.446 (1.534)
Train: 183 [ 750/1171 ( 64%)]  Loss:  2.799089 (2.8128)  Time: 0.586s, 1746.87/s  (2.124s,  482.19/s)  LR: 1.173e-05  Data: 0.020 (1.522)
Train: 183 [ 800/1171 ( 68%)]  Loss:  2.547642 (2.7972)  Time: 2.438s,  420.00/s  (2.121s,  482.75/s)  LR: 1.173e-05  Data: 1.876 (1.519)
Train: 183 [ 850/1171 ( 73%)]  Loss:  2.704505 (2.7921)  Time: 0.584s, 1753.20/s  (2.110s,  485.39/s)  LR: 1.173e-05  Data: 0.019 (1.508)
Train: 183 [ 900/1171 ( 77%)]  Loss:  3.049385 (2.8056)  Time: 5.362s,  190.97/s  (2.105s,  486.41/s)  LR: 1.173e-05  Data: 4.699 (1.504)
Train: 183 [ 950/1171 ( 81%)]  Loss:  2.605571 (2.7956)  Time: 0.589s, 1737.44/s  (2.093s,  489.19/s)  LR: 1.173e-05  Data: 0.020 (1.492)
Train: 183 [1000/1171 ( 85%)]  Loss:  2.692727 (2.7907)  Time: 7.568s,  135.30/s  (2.109s,  485.64/s)  LR: 1.173e-05  Data: 6.324 (1.507)
Train: 183 [1050/1171 ( 90%)]  Loss:  2.439940 (2.7748)  Time: 0.587s, 1744.44/s  (2.102s,  487.18/s)  LR: 1.173e-05  Data: 0.021 (1.500)
Train: 183 [1100/1171 ( 94%)]  Loss:  2.779991 (2.7750)  Time: 6.305s,  162.42/s  (2.117s,  483.81/s)  LR: 1.173e-05  Data: 5.730 (1.515)
Train: 183 [1150/1171 ( 98%)]  Loss:  2.647959 (2.7697)  Time: 0.589s, 1738.06/s  (2.109s,  485.50/s)  LR: 1.173e-05  Data: 0.018 (1.508)
Train: 183 [1170/1171 (100%)]  Loss:  2.712079 (2.7674)  Time: 0.565s, 1812.88/s  (2.109s,  485.55/s)  LR: 1.173e-05  Data: 0.000 (1.508)
Test: [   0/97]  Time: 11.464 (11.464)  Loss:  0.2817 (0.2817)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (2.752)  Loss:  0.4164 (0.3446)  Acc@1: 93.5547 (95.5155)  Acc@5: 98.5352 (98.9545)
Test: [  97/97]  Time: 0.119 (2.697)  Loss:  0.3110 (0.3544)  Acc@1: 95.0893 (95.0290)  Acc@5: 99.4048 (98.8480)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-183.pth.tar', 95.02900000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-182.pth.tar', 95.0149999951172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-179.pth.tar', 94.96400000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-181.pth.tar', 94.95500003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-174.pth.tar', 94.93899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-175.pth.tar', 94.92100003173829)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-178.pth.tar', 94.90899998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-180.pth.tar', 94.90600003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-177.pth.tar', 94.89800005126953)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-173.pth.tar', 94.89600001953124)

Train: 184 [   0/1171 (  0%)]  Loss:  2.585902 (2.5859)  Time: 8.828s,  116.00/s  (8.828s,  116.00/s)  LR: 1.111e-05  Data: 8.198 (8.198)
Train: 184 [  50/1171 (  4%)]  Loss:  3.071866 (2.8289)  Time: 1.667s,  614.31/s  (2.825s,  362.43/s)  LR: 1.111e-05  Data: 1.104 (2.225)
Train: 184 [ 100/1171 (  9%)]  Loss:  2.956733 (2.8715)  Time: 0.589s, 1738.73/s  (2.553s,  401.06/s)  LR: 1.111e-05  Data: 0.018 (1.956)
Train: 184 [ 150/1171 ( 13%)]  Loss:  2.456500 (2.7678)  Time: 0.588s, 1742.20/s  (2.397s,  427.19/s)  LR: 1.111e-05  Data: 0.021 (1.800)
Train: 184 [ 200/1171 ( 17%)]  Loss:  2.495775 (2.7134)  Time: 0.584s, 1753.10/s  (2.326s,  440.27/s)  LR: 1.111e-05  Data: 0.021 (1.732)
Train: 184 [ 250/1171 ( 21%)]  Loss:  2.597788 (2.6941)  Time: 0.588s, 1741.23/s  (2.266s,  451.90/s)  LR: 1.111e-05  Data: 0.021 (1.673)
Train: 184 [ 300/1171 ( 26%)]  Loss:  2.559458 (2.6749)  Time: 0.789s, 1298.02/s  (2.236s,  458.03/s)  LR: 1.111e-05  Data: 0.129 (1.645)
Train: 184 [ 350/1171 ( 30%)]  Loss:  3.136173 (2.7325)  Time: 0.746s, 1372.70/s  (2.191s,  467.38/s)  LR: 1.111e-05  Data: 0.184 (1.595)
Train: 184 [ 400/1171 ( 34%)]  Loss:  2.730744 (2.7323)  Time: 0.588s, 1741.43/s  (2.166s,  472.65/s)  LR: 1.111e-05  Data: 0.022 (1.568)
Train: 184 [ 450/1171 ( 38%)]  Loss:  2.975246 (2.7566)  Time: 0.587s, 1745.27/s  (2.134s,  479.94/s)  LR: 1.111e-05  Data: 0.024 (1.537)
Train: 184 [ 500/1171 ( 43%)]  Loss:  2.796021 (2.7602)  Time: 0.587s, 1745.94/s  (2.118s,  483.48/s)  LR: 1.111e-05  Data: 0.021 (1.522)
Train: 184 [ 550/1171 ( 47%)]  Loss:  2.876281 (2.7699)  Time: 0.932s, 1098.71/s  (2.135s,  479.72/s)  LR: 1.111e-05  Data: 0.287 (1.538)
Train: 184 [ 600/1171 ( 51%)]  Loss:  3.219949 (2.8045)  Time: 0.587s, 1745.18/s  (2.140s,  478.56/s)  LR: 1.111e-05  Data: 0.019 (1.543)
Train: 184 [ 650/1171 ( 56%)]  Loss:  3.028264 (2.8205)  Time: 0.590s, 1734.43/s  (2.143s,  477.92/s)  LR: 1.111e-05  Data: 0.024 (1.547)
Train: 184 [ 700/1171 ( 60%)]  Loss:  2.950652 (2.8292)  Time: 0.856s, 1196.60/s  (2.141s,  478.26/s)  LR: 1.111e-05  Data: 0.079 (1.546)
Train: 184 [ 750/1171 ( 64%)]  Loss:  3.053735 (2.8432)  Time: 1.535s,  667.02/s  (2.134s,  479.89/s)  LR: 1.111e-05  Data: 0.972 (1.538)
Train: 184 [ 800/1171 ( 68%)]  Loss:  2.598855 (2.8288)  Time: 3.081s,  332.37/s  (2.130s,  480.72/s)  LR: 1.111e-05  Data: 2.518 (1.535)
Train: 184 [ 850/1171 ( 73%)]  Loss:  2.268729 (2.7977)  Time: 0.585s, 1750.72/s  (2.122s,  482.55/s)  LR: 1.111e-05  Data: 0.022 (1.527)
Train: 184 [ 900/1171 ( 77%)]  Loss:  2.514673 (2.7828)  Time: 5.140s,  199.21/s  (2.117s,  483.66/s)  LR: 1.111e-05  Data: 4.559 (1.522)
Train: 184 [ 950/1171 ( 81%)]  Loss:  3.280935 (2.8077)  Time: 0.586s, 1747.21/s  (2.104s,  486.69/s)  LR: 1.111e-05  Data: 0.021 (1.509)
Train: 184 [1000/1171 ( 85%)]  Loss:  2.691182 (2.8022)  Time: 6.042s,  169.49/s  (2.121s,  482.90/s)  LR: 1.111e-05  Data: 5.377 (1.526)
Train: 184 [1050/1171 ( 90%)]  Loss:  2.438410 (2.7856)  Time: 0.584s, 1752.54/s  (2.156s,  474.87/s)  LR: 1.111e-05  Data: 0.019 (1.561)
Train: 184 [1100/1171 ( 94%)]  Loss:  2.858064 (2.7888)  Time: 4.128s,  248.08/s  (2.163s,  473.36/s)  LR: 1.111e-05  Data: 3.464 (1.567)
Train: 184 [1150/1171 ( 98%)]  Loss:  2.718199 (2.7858)  Time: 0.614s, 1667.98/s  (2.160s,  474.02/s)  LR: 1.111e-05  Data: 0.048 (1.563)
Train: 184 [1170/1171 (100%)]  Loss:  3.223238 (2.8033)  Time: 0.564s, 1816.43/s  (2.158s,  474.61/s)  LR: 1.111e-05  Data: 0.000 (1.561)
Test: [   0/97]  Time: 11.265 (11.265)  Loss:  0.2881 (0.2881)  Acc@1: 96.9727 (96.9727)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (2.764)  Loss:  0.4293 (0.3529)  Acc@1: 93.5547 (95.4504)  Acc@5: 98.4375 (98.9717)
Test: [  97/97]  Time: 0.119 (2.698)  Loss:  0.3276 (0.3625)  Acc@1: 95.0893 (94.9860)  Acc@5: 99.4048 (98.8530)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-183.pth.tar', 95.02900000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-182.pth.tar', 95.0149999951172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-184.pth.tar', 94.98600000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-179.pth.tar', 94.96400000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-181.pth.tar', 94.95500003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-174.pth.tar', 94.93899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-175.pth.tar', 94.92100003173829)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-178.pth.tar', 94.90899998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-180.pth.tar', 94.90600003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-177.pth.tar', 94.89800005126953)

Train: 185 [   0/1171 (  0%)]  Loss:  3.134726 (3.1347)  Time: 9.187s,  111.46/s  (9.187s,  111.46/s)  LR: 1.062e-05  Data: 8.299 (8.299)
Train: 185 [  50/1171 (  4%)]  Loss:  2.129328 (2.6320)  Time: 0.588s, 1740.45/s  (2.008s,  510.01/s)  LR: 1.062e-05  Data: 0.021 (1.420)
Train: 185 [ 100/1171 (  9%)]  Loss:  2.615743 (2.6266)  Time: 0.587s, 1744.25/s  (2.204s,  464.63/s)  LR: 1.062e-05  Data: 0.024 (1.618)
Train: 185 [ 150/1171 ( 13%)]  Loss:  2.938653 (2.7046)  Time: 3.404s,  300.80/s  (2.110s,  485.26/s)  LR: 1.062e-05  Data: 2.841 (1.513)
Train: 185 [ 200/1171 ( 17%)]  Loss:  3.017436 (2.7672)  Time: 0.764s, 1339.99/s  (2.145s,  477.43/s)  LR: 1.062e-05  Data: 0.168 (1.549)
Train: 185 [ 250/1171 ( 21%)]  Loss:  2.881391 (2.7862)  Time: 0.586s, 1746.35/s  (2.109s,  485.50/s)  LR: 1.062e-05  Data: 0.021 (1.511)
Train: 185 [ 300/1171 ( 26%)]  Loss:  2.773098 (2.7843)  Time: 0.586s, 1747.70/s  (2.117s,  483.69/s)  LR: 1.062e-05  Data: 0.019 (1.522)
Train: 185 [ 350/1171 ( 30%)]  Loss:  2.952505 (2.8054)  Time: 0.586s, 1746.84/s  (2.101s,  487.47/s)  LR: 1.062e-05  Data: 0.019 (1.508)
Train: 185 [ 400/1171 ( 34%)]  Loss:  2.504910 (2.7720)  Time: 0.588s, 1741.91/s  (2.100s,  487.63/s)  LR: 1.062e-05  Data: 0.024 (1.508)
Train: 185 [ 450/1171 ( 38%)]  Loss:  2.783098 (2.7731)  Time: 0.585s, 1749.18/s  (2.080s,  492.19/s)  LR: 1.062e-05  Data: 0.020 (1.490)
Train: 185 [ 500/1171 ( 43%)]  Loss:  2.449871 (2.7437)  Time: 0.588s, 1741.88/s  (2.097s,  488.22/s)  LR: 1.062e-05  Data: 0.025 (1.507)
Train: 185 [ 550/1171 ( 47%)]  Loss:  3.111533 (2.7744)  Time: 0.588s, 1741.89/s  (2.115s,  484.14/s)  LR: 1.062e-05  Data: 0.022 (1.524)
Train: 185 [ 600/1171 ( 51%)]  Loss:  2.911548 (2.7849)  Time: 3.303s,  309.99/s  (2.136s,  479.31/s)  LR: 1.062e-05  Data: 2.569 (1.543)
Train: 185 [ 650/1171 ( 56%)]  Loss:  3.144302 (2.8106)  Time: 0.586s, 1746.90/s  (2.142s,  478.16/s)  LR: 1.062e-05  Data: 0.023 (1.549)
Train: 185 [ 700/1171 ( 60%)]  Loss:  2.714288 (2.8042)  Time: 0.587s, 1744.93/s  (2.150s,  476.27/s)  LR: 1.062e-05  Data: 0.023 (1.557)
Train: 185 [ 750/1171 ( 64%)]  Loss:  2.236699 (2.7687)  Time: 0.584s, 1752.54/s  (2.200s,  465.46/s)  LR: 1.062e-05  Data: 0.021 (1.606)
Train: 185 [ 800/1171 ( 68%)]  Loss:  3.174230 (2.7926)  Time: 1.845s,  554.93/s  (2.193s,  466.86/s)  LR: 1.062e-05  Data: 1.282 (1.600)
Train: 185 [ 850/1171 ( 73%)]  Loss:  2.758938 (2.7907)  Time: 0.588s, 1740.51/s  (2.180s,  469.73/s)  LR: 1.062e-05  Data: 0.019 (1.586)
Train: 185 [ 900/1171 ( 77%)]  Loss:  2.908083 (2.7969)  Time: 2.458s,  416.58/s  (2.191s,  467.43/s)  LR: 1.062e-05  Data: 1.780 (1.596)
Train: 185 [ 950/1171 ( 81%)]  Loss:  2.956276 (2.8048)  Time: 0.584s, 1753.99/s  (2.183s,  469.05/s)  LR: 1.062e-05  Data: 0.021 (1.588)
Train: 185 [1000/1171 ( 85%)]  Loss:  3.036149 (2.8158)  Time: 3.114s,  328.88/s  (2.184s,  468.76/s)  LR: 1.062e-05  Data: 2.529 (1.590)
Train: 185 [1050/1171 ( 90%)]  Loss:  2.647688 (2.8082)  Time: 0.585s, 1751.14/s  (2.180s,  469.76/s)  LR: 1.062e-05  Data: 0.021 (1.584)
Train: 185 [1100/1171 ( 94%)]  Loss:  3.061526 (2.8192)  Time: 4.801s,  213.29/s  (2.181s,  469.47/s)  LR: 1.062e-05  Data: 4.239 (1.585)
Train: 185 [1150/1171 ( 98%)]  Loss:  2.993717 (2.8265)  Time: 0.584s, 1752.26/s  (2.176s,  470.66/s)  LR: 1.062e-05  Data: 0.021 (1.579)
Train: 185 [1170/1171 (100%)]  Loss:  3.057832 (2.8357)  Time: 0.564s, 1816.50/s  (2.172s,  471.35/s)  LR: 1.062e-05  Data: 0.000 (1.576)
Test: [   0/97]  Time: 11.238 (11.238)  Loss:  0.2724 (0.2724)  Acc@1: 96.8750 (96.8750)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (2.709)  Loss:  0.4312 (0.3468)  Acc@1: 92.9688 (95.4867)  Acc@5: 98.5352 (98.9622)
Test: [  97/97]  Time: 0.119 (2.615)  Loss:  0.3131 (0.3568)  Acc@1: 94.7917 (95.0170)  Acc@5: 99.5536 (98.8520)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-183.pth.tar', 95.02900000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-185.pth.tar', 95.01699998291015)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-182.pth.tar', 95.0149999951172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-184.pth.tar', 94.98600000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-179.pth.tar', 94.96400000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-181.pth.tar', 94.95500003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-174.pth.tar', 94.93899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-175.pth.tar', 94.92100003173829)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-178.pth.tar', 94.90899998291016)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-180.pth.tar', 94.90600003417968)

Train: 186 [   0/1171 (  0%)]  Loss:  2.818623 (2.8186)  Time: 9.028s,  113.43/s  (9.028s,  113.43/s)  LR: 1.028e-05  Data: 8.105 (8.105)
Train: 186 [  50/1171 (  4%)]  Loss:  2.747032 (2.7828)  Time: 0.588s, 1742.87/s  (2.400s,  426.59/s)  LR: 1.028e-05  Data: 0.022 (1.815)
Train: 186 [ 100/1171 (  9%)]  Loss:  2.881586 (2.8157)  Time: 0.583s, 1756.52/s  (2.195s,  466.47/s)  LR: 1.028e-05  Data: 0.019 (1.606)
Train: 186 [ 150/1171 ( 13%)]  Loss:  2.628014 (2.7688)  Time: 0.589s, 1739.13/s  (2.174s,  470.93/s)  LR: 1.028e-05  Data: 0.020 (1.586)
Train: 186 [ 200/1171 ( 17%)]  Loss:  2.447954 (2.7046)  Time: 0.589s, 1739.08/s  (2.129s,  480.94/s)  LR: 1.028e-05  Data: 0.022 (1.543)
Train: 186 [ 250/1171 ( 21%)]  Loss:  3.183136 (2.7844)  Time: 0.588s, 1742.11/s  (2.099s,  487.91/s)  LR: 1.028e-05  Data: 0.020 (1.514)
Train: 186 [ 300/1171 ( 26%)]  Loss:  3.061365 (2.8240)  Time: 0.583s, 1755.61/s  (2.081s,  491.99/s)  LR: 1.028e-05  Data: 0.020 (1.496)
Train: 186 [ 350/1171 ( 30%)]  Loss:  3.142651 (2.8638)  Time: 0.587s, 1743.26/s  (2.054s,  498.47/s)  LR: 1.028e-05  Data: 0.021 (1.468)
Train: 186 [ 400/1171 ( 34%)]  Loss:  2.866877 (2.8641)  Time: 0.588s, 1742.54/s  (2.048s,  500.10/s)  LR: 1.028e-05  Data: 0.020 (1.462)
Train: 186 [ 450/1171 ( 38%)]  Loss:  2.880599 (2.8658)  Time: 0.586s, 1748.33/s  (2.130s,  480.79/s)  LR: 1.028e-05  Data: 0.020 (1.533)
Train: 186 [ 500/1171 ( 43%)]  Loss:  3.022634 (2.8800)  Time: 4.241s,  241.44/s  (2.132s,  480.35/s)  LR: 1.028e-05  Data: 3.611 (1.535)
Train: 186 [ 550/1171 ( 47%)]  Loss:  2.375440 (2.8380)  Time: 0.587s, 1744.25/s  (2.120s,  482.98/s)  LR: 1.028e-05  Data: 0.018 (1.523)
Train: 186 [ 600/1171 ( 51%)]  Loss:  2.550814 (2.8159)  Time: 6.791s,  150.80/s  (2.132s,  480.32/s)  LR: 1.028e-05  Data: 6.207 (1.535)
Train: 186 [ 650/1171 ( 56%)]  Loss:  2.245642 (2.7752)  Time: 0.586s, 1746.34/s  (2.123s,  482.37/s)  LR: 1.028e-05  Data: 0.019 (1.528)
Train: 186 [ 700/1171 ( 60%)]  Loss:  2.875906 (2.7819)  Time: 6.448s,  158.82/s  (2.125s,  481.93/s)  LR: 1.028e-05  Data: 5.794 (1.530)
Train: 186 [ 750/1171 ( 64%)]  Loss:  2.983999 (2.7945)  Time: 0.592s, 1729.01/s  (2.113s,  484.69/s)  LR: 1.028e-05  Data: 0.020 (1.518)
Train: 186 [ 800/1171 ( 68%)]  Loss:  3.291517 (2.8238)  Time: 6.267s,  163.40/s  (2.111s,  485.04/s)  LR: 1.028e-05  Data: 5.443 (1.516)
Train: 186 [ 850/1171 ( 73%)]  Loss:  2.326180 (2.7961)  Time: 0.588s, 1741.13/s  (2.100s,  487.68/s)  LR: 1.028e-05  Data: 0.024 (1.505)
Train: 186 [ 900/1171 ( 77%)]  Loss:  2.624131 (2.7871)  Time: 6.567s,  155.94/s  (2.120s,  483.11/s)  LR: 1.028e-05  Data: 5.972 (1.525)
Train: 186 [ 950/1171 ( 81%)]  Loss:  3.244225 (2.8099)  Time: 0.587s, 1745.49/s  (2.110s,  485.20/s)  LR: 1.028e-05  Data: 0.018 (1.517)
Train: 186 [1000/1171 ( 85%)]  Loss:  3.158056 (2.8265)  Time: 7.117s,  143.89/s  (2.118s,  483.41/s)  LR: 1.028e-05  Data: 6.464 (1.525)
Train: 186 [1050/1171 ( 90%)]  Loss:  3.305383 (2.8483)  Time: 0.587s, 1745.18/s  (2.111s,  485.06/s)  LR: 1.028e-05  Data: 0.020 (1.518)
Train: 186 [1100/1171 ( 94%)]  Loss:  2.501330 (2.8332)  Time: 6.368s,  160.81/s  (2.113s,  484.61/s)  LR: 1.028e-05  Data: 5.802 (1.521)
Train: 186 [1150/1171 ( 98%)]  Loss:  3.119834 (2.8451)  Time: 0.586s, 1748.13/s  (2.108s,  485.68/s)  LR: 1.028e-05  Data: 0.020 (1.516)
Train: 186 [1170/1171 (100%)]  Loss:  3.053492 (2.8535)  Time: 0.564s, 1815.90/s  (2.107s,  486.03/s)  LR: 1.028e-05  Data: 0.000 (1.515)
Test: [   0/97]  Time: 11.513 (11.513)  Loss:  0.2855 (0.2855)  Acc@1: 96.8750 (96.8750)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.201 (2.773)  Loss:  0.4151 (0.3454)  Acc@1: 93.1641 (95.4733)  Acc@5: 98.5352 (98.9660)
Test: [  97/97]  Time: 0.120 (2.787)  Loss:  0.3213 (0.3558)  Acc@1: 94.4940 (95.0210)  Acc@5: 99.4048 (98.8460)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-183.pth.tar', 95.02900000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-186.pth.tar', 95.02100000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-185.pth.tar', 95.01699998291015)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-182.pth.tar', 95.0149999951172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-184.pth.tar', 94.98600000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-179.pth.tar', 94.96400000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-181.pth.tar', 94.95500003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-174.pth.tar', 94.93899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-175.pth.tar', 94.92100003173829)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-178.pth.tar', 94.90899998291016)

Train: 187 [   0/1171 (  0%)]  Loss:  2.899481 (2.8995)  Time: 16.078s,   63.69/s  (16.078s,   63.69/s)  LR: 1.007e-05  Data: 14.911 (14.911)
Train: 187 [  50/1171 (  4%)]  Loss:  2.579343 (2.7394)  Time: 0.587s, 1745.78/s  (2.439s,  419.86/s)  LR: 1.007e-05  Data: 0.021 (1.833)
Train: 187 [ 100/1171 (  9%)]  Loss:  2.665663 (2.7148)  Time: 0.588s, 1741.25/s  (2.260s,  453.02/s)  LR: 1.007e-05  Data: 0.020 (1.667)
Train: 187 [ 150/1171 ( 13%)]  Loss:  3.211309 (2.8389)  Time: 0.588s, 1741.38/s  (2.550s,  401.52/s)  LR: 1.007e-05  Data: 0.019 (1.949)
Train: 187 [ 200/1171 ( 17%)]  Loss:  2.672046 (2.8056)  Time: 0.589s, 1739.11/s  (2.464s,  415.57/s)  LR: 1.007e-05  Data: 0.020 (1.866)
Train: 187 [ 250/1171 ( 21%)]  Loss:  2.845852 (2.8123)  Time: 0.588s, 1742.43/s  (2.363s,  433.26/s)  LR: 1.007e-05  Data: 0.022 (1.767)
Train: 187 [ 300/1171 ( 26%)]  Loss:  2.632653 (2.7866)  Time: 0.586s, 1746.50/s  (2.321s,  441.20/s)  LR: 1.007e-05  Data: 0.021 (1.723)
Train: 187 [ 350/1171 ( 30%)]  Loss:  2.438205 (2.7431)  Time: 0.590s, 1735.44/s  (2.262s,  452.69/s)  LR: 1.007e-05  Data: 0.019 (1.663)
Train: 187 [ 400/1171 ( 34%)]  Loss:  2.662398 (2.7341)  Time: 1.171s,  874.15/s  (2.255s,  454.14/s)  LR: 1.007e-05  Data: 0.563 (1.651)
Train: 187 [ 450/1171 ( 38%)]  Loss:  2.918760 (2.7526)  Time: 0.586s, 1746.32/s  (2.261s,  452.93/s)  LR: 1.007e-05  Data: 0.019 (1.652)
Train: 187 [ 500/1171 ( 43%)]  Loss:  3.045447 (2.7792)  Time: 3.121s,  328.13/s  (2.244s,  456.29/s)  LR: 1.007e-05  Data: 2.462 (1.634)
Train: 187 [ 550/1171 ( 47%)]  Loss:  2.553017 (2.7603)  Time: 0.588s, 1740.10/s  (2.247s,  455.66/s)  LR: 1.007e-05  Data: 0.018 (1.639)
Train: 187 [ 600/1171 ( 51%)]  Loss:  3.037891 (2.7817)  Time: 0.589s, 1738.57/s  (2.240s,  457.15/s)  LR: 1.007e-05  Data: 0.024 (1.632)
Train: 187 [ 650/1171 ( 56%)]  Loss:  2.968101 (2.7950)  Time: 0.587s, 1745.95/s  (2.238s,  457.54/s)  LR: 1.007e-05  Data: 0.023 (1.631)
Train: 187 [ 700/1171 ( 60%)]  Loss:  2.808807 (2.7959)  Time: 0.588s, 1741.96/s  (2.226s,  460.02/s)  LR: 1.007e-05  Data: 0.021 (1.619)
Train: 187 [ 750/1171 ( 64%)]  Loss:  2.890295 (2.8018)  Time: 0.586s, 1748.76/s  (2.209s,  463.59/s)  LR: 1.007e-05  Data: 0.022 (1.602)
Train: 187 [ 800/1171 ( 68%)]  Loss:  3.129877 (2.8211)  Time: 0.586s, 1747.08/s  (2.194s,  466.72/s)  LR: 1.007e-05  Data: 0.020 (1.588)
Train: 187 [ 850/1171 ( 73%)]  Loss:  2.774550 (2.8185)  Time: 0.586s, 1746.23/s  (2.209s,  463.54/s)  LR: 1.007e-05  Data: 0.023 (1.603)
Train: 187 [ 900/1171 ( 77%)]  Loss:  2.776924 (2.8163)  Time: 0.587s, 1744.77/s  (2.195s,  466.46/s)  LR: 1.007e-05  Data: 0.019 (1.590)
Train: 187 [ 950/1171 ( 81%)]  Loss:  2.589099 (2.8050)  Time: 0.586s, 1747.34/s  (2.196s,  466.21/s)  LR: 1.007e-05  Data: 0.020 (1.591)
Train: 187 [1000/1171 ( 85%)]  Loss:  2.985499 (2.8136)  Time: 0.584s, 1754.32/s  (2.186s,  468.46/s)  LR: 1.007e-05  Data: 0.019 (1.580)
Train: 187 [1050/1171 ( 90%)]  Loss:  2.959824 (2.8202)  Time: 0.585s, 1750.38/s  (2.184s,  468.94/s)  LR: 1.007e-05  Data: 0.021 (1.578)
Train: 187 [1100/1171 ( 94%)]  Loss:  2.764422 (2.8178)  Time: 0.585s, 1750.04/s  (2.190s,  467.60/s)  LR: 1.007e-05  Data: 0.019 (1.584)
Train: 187 [1150/1171 ( 98%)]  Loss:  2.656285 (2.8111)  Time: 0.587s, 1743.00/s  (2.209s,  463.55/s)  LR: 1.007e-05  Data: 0.024 (1.603)
Train: 187 [1170/1171 (100%)]  Loss:  3.138229 (2.8242)  Time: 0.568s, 1803.35/s  (2.207s,  463.98/s)  LR: 1.007e-05  Data: 0.000 (1.601)
Test: [   0/97]  Time: 11.416 (11.416)  Loss:  0.2797 (0.2797)  Acc@1: 96.8750 (96.8750)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.198 (3.083)  Loss:  0.4234 (0.3415)  Acc@1: 93.3594 (95.4523)  Acc@5: 98.6328 (98.9717)
Test: [  97/97]  Time: 0.120 (2.935)  Loss:  0.3180 (0.3526)  Acc@1: 94.7917 (94.9950)  Acc@5: 99.4048 (98.8540)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-183.pth.tar', 95.02900000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-186.pth.tar', 95.02100000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-185.pth.tar', 95.01699998291015)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-182.pth.tar', 95.0149999951172)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-187.pth.tar', 94.99500003417968)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-184.pth.tar', 94.98600000732422)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-179.pth.tar', 94.96400000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-181.pth.tar', 94.95500003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-174.pth.tar', 94.93899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-175.pth.tar', 94.92100003173829)

*** Best metric: 95.02900000732421 (epoch 183)

wandb: Waiting for W&B process to finish, PID 44219
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)
wandb: \ 0.00MB of 0.01MB uploaded (0.00MB deduped)
wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)
wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)
wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)
wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)
wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)
wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)
wandb:                                                                                
wandb: Find user logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210604_140951-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1/logs/debug.log
wandb: Find internal logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210604_140951-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1/logs/debug-internal.log
wandb: Run summary:
wandb:        _step 187
wandb:        epoch 187
wandb:     _runtime 588847
wandb:    eval_loss 0.35264
wandb:    eval_top1 94.995
wandb:    eval_top5 98.854
wandb:   _timestamp 1622845991
wandb:   train_loss 2.82416
wandb: Run history:
wandb:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   train_loss ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÅ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ
wandb:    eval_loss ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ
wandb:    eval_top1 ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá
wandb:    eval_top5 ‚ñÇ‚ñÅ‚ñÖ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÜ‚ñà‚ñÑ‚ñÑ‚ñÖ‚ñà‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà
wandb:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced PreTraining_vit_deit_tiny_patch16_224_fake_1k: https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
--End--
Sat Jun 5 07:33:22 JST 2021

--Start--
Mon May 31 14:02:09 JST 2021
Added key: store_based_barrier_key:1 to store for rank: 1
Added key: store_based_barrier_key:1 to store for rank: 0
Added key: store_based_barrier_key:1 to store for rank: 2
Added key: store_based_barrier_key:1 to store for rank: 3
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
wandb: Currently logged in as: gyama_x (use `wandb login --relogin` to force relogin)
wandb: WARNING Tried to auto resume run with id PreTraining_vit_deit_tiny_patch16_224_fake_v2_1k but id PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1 is set.
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
wandb: wandb version 0.10.31 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.10.27
wandb: Resuming run PreTraining_vit_deit_tiny_patch16_224_fake_1k
wandb: ‚≠êÔ∏è View project at https://wandb.ai/gyama_x/pytorch-image-models
wandb: üöÄ View run at https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run data is saved locally in /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210531_140256-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
wandb: Run `wandb offline` to turn off syncing.

Loaded state_dict from checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar'
Model vit_deit_tiny_patch16_224 created, param count:5717416
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bicubic
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.9
AMP not enabled. Training in float32.
Restoring model state from checkpoint...
Restoring optimizer state from checkpoint...
Loaded checkpoint '/gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/last.pth.tar' (epoch 99)
Using native Torch DistributedDataParallel.
Scheduled epochs: 122
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
/gs/hs0/tga-i/sugiyama.y.al/TIMM/TIMM_386/lib/python3.8/site-packages/torchvision/transforms/functional.py:364: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.
  warnings.warn(
Train: 100 [   0/1171 (  0%)]  Loss:  3.027115 (3.0271)  Time: 17.317s,   59.13/s  (17.317s,   59.13/s)  LR: 8.733e-05  Data: 16.004 (16.004)
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Reducer buckets have been rebuilt in this iteration.
Train: 100 [  50/1171 (  4%)]  Loss:  3.100762 (3.0639)  Time: 0.585s, 1749.35/s  (2.814s,  363.86/s)  LR: 8.733e-05  Data: 0.020 (2.211)
Train: 100 [ 100/1171 (  9%)]  Loss:  3.083169 (3.0703)  Time: 0.589s, 1737.85/s  (2.609s,  392.43/s)  LR: 8.733e-05  Data: 0.019 (2.008)
Train: 100 [ 150/1171 ( 13%)]  Loss:  2.521231 (2.9331)  Time: 0.585s, 1751.59/s  (2.452s,  417.54/s)  LR: 8.733e-05  Data: 0.019 (1.854)
Train: 100 [ 200/1171 ( 17%)]  Loss:  2.749751 (2.8964)  Time: 0.588s, 1741.24/s  (2.379s,  430.40/s)  LR: 8.733e-05  Data: 0.018 (1.781)
Train: 100 [ 250/1171 ( 21%)]  Loss:  2.453258 (2.8225)  Time: 0.588s, 1740.47/s  (2.322s,  440.96/s)  LR: 8.733e-05  Data: 0.019 (1.723)
Train: 100 [ 300/1171 ( 26%)]  Loss:  3.092609 (2.8611)  Time: 0.586s, 1748.86/s  (2.344s,  436.82/s)  LR: 8.733e-05  Data: 0.020 (1.747)
Train: 100 [ 350/1171 ( 30%)]  Loss:  2.591837 (2.8275)  Time: 0.585s, 1750.04/s  (2.301s,  444.98/s)  LR: 8.733e-05  Data: 0.020 (1.705)
Train: 100 [ 400/1171 ( 34%)]  Loss:  3.108338 (2.8587)  Time: 0.589s, 1738.26/s  (2.308s,  443.58/s)  LR: 8.733e-05  Data: 0.020 (1.711)
Train: 100 [ 450/1171 ( 38%)]  Loss:  2.668512 (2.8397)  Time: 3.279s,  312.33/s  (2.296s,  446.01/s)  LR: 8.733e-05  Data: 2.371 (1.697)
Train: 100 [ 500/1171 ( 43%)]  Loss:  2.783531 (2.8346)  Time: 2.510s,  407.95/s  (2.281s,  449.01/s)  LR: 8.733e-05  Data: 1.842 (1.680)
Train: 100 [ 550/1171 ( 47%)]  Loss:  2.795821 (2.8313)  Time: 1.233s,  830.36/s  (2.263s,  452.56/s)  LR: 8.733e-05  Data: 0.573 (1.660)
Train: 100 [ 600/1171 ( 51%)]  Loss:  3.378625 (2.8734)  Time: 2.472s,  414.17/s  (2.253s,  454.42/s)  LR: 8.733e-05  Data: 1.808 (1.650)
Train: 100 [ 650/1171 ( 56%)]  Loss:  2.720786 (2.8625)  Time: 3.269s,  313.27/s  (2.244s,  456.42/s)  LR: 8.733e-05  Data: 2.570 (1.640)
Train: 100 [ 700/1171 ( 60%)]  Loss:  2.964786 (2.8693)  Time: 3.943s,  259.69/s  (2.271s,  450.98/s)  LR: 8.733e-05  Data: 3.378 (1.667)
Train: 100 [ 750/1171 ( 64%)]  Loss:  3.135029 (2.8859)  Time: 6.576s,  155.71/s  (2.294s,  446.45/s)  LR: 8.733e-05  Data: 5.995 (1.688)
Train: 100 [ 800/1171 ( 68%)]  Loss:  2.868338 (2.8849)  Time: 2.787s,  367.44/s  (2.300s,  445.15/s)  LR: 8.733e-05  Data: 2.222 (1.694)
Train: 100 [ 850/1171 ( 73%)]  Loss:  2.913846 (2.8865)  Time: 8.324s,  123.01/s  (2.317s,  442.03/s)  LR: 8.733e-05  Data: 7.696 (1.712)
Train: 100 [ 900/1171 ( 77%)]  Loss:  3.194721 (2.9027)  Time: 0.942s, 1086.71/s  (2.319s,  441.64/s)  LR: 8.733e-05  Data: 0.230 (1.714)
Train: 100 [ 950/1171 ( 81%)]  Loss:  2.786218 (2.8969)  Time: 4.683s,  218.65/s  (2.317s,  442.03/s)  LR: 8.733e-05  Data: 4.104 (1.713)
Train: 100 [1000/1171 ( 85%)]  Loss:  2.466452 (2.8764)  Time: 1.275s,  802.83/s  (2.313s,  442.63/s)  LR: 8.733e-05  Data: 0.636 (1.709)
Train: 100 [1050/1171 ( 90%)]  Loss:  2.980328 (2.8811)  Time: 7.380s,  138.75/s  (2.314s,  442.51/s)  LR: 8.733e-05  Data: 6.775 (1.709)
Train: 100 [1100/1171 ( 94%)]  Loss:  3.039741 (2.8880)  Time: 0.586s, 1746.52/s  (2.325s,  440.43/s)  LR: 8.733e-05  Data: 0.019 (1.721)
Train: 100 [1150/1171 ( 98%)]  Loss:  2.555259 (2.8742)  Time: 5.925s,  172.83/s  (2.330s,  439.49/s)  LR: 8.733e-05  Data: 5.229 (1.725)
Train: 100 [1170/1171 (100%)]  Loss:  2.911914 (2.8757)  Time: 0.565s, 1810.87/s  (2.325s,  440.48/s)  LR: 8.733e-05  Data: 0.000 (1.720)
Test: [   0/97]  Time: 14.427 (14.427)  Loss:  0.2994 (0.2994)  Acc@1: 95.8984 (95.8984)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (3.145)  Loss:  0.4898 (0.3757)  Acc@1: 90.5273 (94.4087)  Acc@5: 97.9492 (98.8377)
Test: [  97/97]  Time: 0.736 (3.040)  Loss:  0.3377 (0.3880)  Acc@1: 94.7917 (93.9380)  Acc@5: 98.9583 (98.6420)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-100.pth.tar', 93.9380000341797)

Train: 101 [   0/1171 (  0%)]  Loss:  2.891371 (2.8914)  Time: 9.527s,  107.49/s  (9.527s,  107.49/s)  LR: 8.063e-05  Data: 8.074 (8.074)
Train: 101 [  50/1171 (  4%)]  Loss:  3.218180 (3.0548)  Time: 0.588s, 1741.30/s  (2.312s,  442.84/s)  LR: 8.063e-05  Data: 0.024 (1.696)
Train: 101 [ 100/1171 (  9%)]  Loss:  3.035085 (3.0482)  Time: 0.587s, 1745.94/s  (2.236s,  458.00/s)  LR: 8.063e-05  Data: 0.022 (1.635)
Train: 101 [ 150/1171 ( 13%)]  Loss:  2.831900 (2.9941)  Time: 0.591s, 1733.67/s  (2.180s,  469.83/s)  LR: 8.063e-05  Data: 0.023 (1.580)
Train: 101 [ 200/1171 ( 17%)]  Loss:  2.820560 (2.9594)  Time: 0.587s, 1743.23/s  (2.305s,  444.28/s)  LR: 8.063e-05  Data: 0.021 (1.709)
Train: 101 [ 250/1171 ( 21%)]  Loss:  2.635740 (2.9055)  Time: 0.589s, 1737.46/s  (2.315s,  442.32/s)  LR: 8.063e-05  Data: 0.024 (1.722)
Train: 101 [ 300/1171 ( 26%)]  Loss:  2.828297 (2.8944)  Time: 0.585s, 1749.27/s  (2.331s,  439.38/s)  LR: 8.063e-05  Data: 0.019 (1.738)
Train: 101 [ 350/1171 ( 30%)]  Loss:  2.771227 (2.8790)  Time: 0.587s, 1744.09/s  (2.318s,  441.68/s)  LR: 8.063e-05  Data: 0.022 (1.728)
Train: 101 [ 400/1171 ( 34%)]  Loss:  3.354522 (2.9319)  Time: 0.589s, 1737.74/s  (2.327s,  440.01/s)  LR: 8.063e-05  Data: 0.020 (1.736)
Train: 101 [ 450/1171 ( 38%)]  Loss:  2.703699 (2.9091)  Time: 0.588s, 1740.02/s  (2.312s,  442.97/s)  LR: 8.063e-05  Data: 0.020 (1.721)
Train: 101 [ 500/1171 ( 43%)]  Loss:  3.053176 (2.9222)  Time: 0.589s, 1739.95/s  (2.315s,  442.32/s)  LR: 8.063e-05  Data: 0.022 (1.725)
Train: 101 [ 550/1171 ( 47%)]  Loss:  2.849898 (2.9161)  Time: 0.585s, 1749.62/s  (2.354s,  435.06/s)  LR: 8.063e-05  Data: 0.019 (1.763)
Train: 101 [ 600/1171 ( 51%)]  Loss:  3.172144 (2.9358)  Time: 0.592s, 1730.31/s  (2.374s,  431.40/s)  LR: 8.063e-05  Data: 0.021 (1.783)
Train: 101 [ 650/1171 ( 56%)]  Loss:  3.027546 (2.9424)  Time: 0.587s, 1744.45/s  (2.369s,  432.20/s)  LR: 8.063e-05  Data: 0.021 (1.777)
Train: 101 [ 700/1171 ( 60%)]  Loss:  2.759398 (2.9302)  Time: 0.588s, 1740.93/s  (2.372s,  431.63/s)  LR: 8.063e-05  Data: 0.022 (1.781)
Train: 101 [ 750/1171 ( 64%)]  Loss:  2.781922 (2.9209)  Time: 0.590s, 1736.73/s  (2.361s,  433.76/s)  LR: 8.063e-05  Data: 0.024 (1.770)
Train: 101 [ 800/1171 ( 68%)]  Loss:  2.805732 (2.9141)  Time: 0.589s, 1739.38/s  (2.357s,  434.53/s)  LR: 8.063e-05  Data: 0.023 (1.766)
Train: 101 [ 850/1171 ( 73%)]  Loss:  2.941721 (2.9157)  Time: 0.588s, 1741.03/s  (2.347s,  436.37/s)  LR: 8.063e-05  Data: 0.023 (1.756)
Train: 101 [ 900/1171 ( 77%)]  Loss:  3.053196 (2.9229)  Time: 0.589s, 1738.13/s  (2.340s,  437.59/s)  LR: 8.063e-05  Data: 0.020 (1.749)
Train: 101 [ 950/1171 ( 81%)]  Loss:  3.024372 (2.9280)  Time: 0.591s, 1732.23/s  (2.357s,  434.51/s)  LR: 8.063e-05  Data: 0.020 (1.764)
Train: 101 [1000/1171 ( 85%)]  Loss:  3.406787 (2.9508)  Time: 0.587s, 1743.56/s  (2.364s,  433.18/s)  LR: 8.063e-05  Data: 0.019 (1.770)
Train: 101 [1050/1171 ( 90%)]  Loss:  3.253389 (2.9645)  Time: 0.585s, 1749.91/s  (2.363s,  433.27/s)  LR: 8.063e-05  Data: 0.019 (1.769)
Train: 101 [1100/1171 ( 94%)]  Loss:  3.023948 (2.9671)  Time: 0.590s, 1735.83/s  (2.359s,  434.00/s)  LR: 8.063e-05  Data: 0.021 (1.765)
Train: 101 [1150/1171 ( 98%)]  Loss:  2.847260 (2.9621)  Time: 0.589s, 1739.45/s  (2.355s,  434.74/s)  LR: 8.063e-05  Data: 0.022 (1.760)
Train: 101 [1170/1171 (100%)]  Loss:  2.935343 (2.9611)  Time: 0.566s, 1809.91/s  (2.353s,  435.21/s)  LR: 8.063e-05  Data: 0.000 (1.757)
Test: [   0/97]  Time: 12.219 (12.219)  Loss:  0.2897 (0.2897)  Acc@1: 96.4844 (96.4844)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.200 (2.936)  Loss:  0.4537 (0.3617)  Acc@1: 91.8945 (94.6481)  Acc@5: 98.4375 (98.8798)
Test: [  97/97]  Time: 0.120 (2.945)  Loss:  0.3169 (0.3781)  Acc@1: 94.9405 (94.1000)  Acc@5: 99.2560 (98.6760)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-101.pth.tar', 94.10000004638673)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-100.pth.tar', 93.9380000341797)

Train: 102 [   0/1171 (  0%)]  Loss:  2.822266 (2.8223)  Time: 15.870s,   64.53/s  (15.870s,   64.53/s)  LR: 7.421e-05  Data: 14.696 (14.696)
Train: 102 [  50/1171 (  4%)]  Loss:  2.749717 (2.7860)  Time: 0.588s, 1741.43/s  (2.709s,  378.07/s)  LR: 7.421e-05  Data: 0.020 (2.119)
Train: 102 [ 100/1171 (  9%)]  Loss:  3.439355 (3.0038)  Time: 0.585s, 1749.04/s  (2.579s,  396.99/s)  LR: 7.421e-05  Data: 0.021 (1.985)
Train: 102 [ 150/1171 ( 13%)]  Loss:  2.392380 (2.8509)  Time: 0.592s, 1728.53/s  (2.463s,  415.72/s)  LR: 7.421e-05  Data: 0.027 (1.875)
Train: 102 [ 200/1171 ( 17%)]  Loss:  2.736391 (2.8280)  Time: 0.585s, 1748.96/s  (2.430s,  421.47/s)  LR: 7.421e-05  Data: 0.019 (1.841)
Train: 102 [ 250/1171 ( 21%)]  Loss:  2.732229 (2.8121)  Time: 0.589s, 1738.89/s  (2.364s,  433.23/s)  LR: 7.421e-05  Data: 0.023 (1.774)
Train: 102 [ 300/1171 ( 26%)]  Loss:  2.603396 (2.7822)  Time: 0.858s, 1193.41/s  (2.342s,  437.17/s)  LR: 7.421e-05  Data: 0.290 (1.753)
Train: 102 [ 350/1171 ( 30%)]  Loss:  2.837721 (2.7892)  Time: 0.588s, 1742.79/s  (2.309s,  443.54/s)  LR: 7.421e-05  Data: 0.022 (1.718)
Train: 102 [ 400/1171 ( 34%)]  Loss:  2.961803 (2.8084)  Time: 2.178s,  470.17/s  (2.331s,  439.30/s)  LR: 7.421e-05  Data: 1.613 (1.741)
Train: 102 [ 450/1171 ( 38%)]  Loss:  2.800496 (2.8076)  Time: 0.589s, 1739.02/s  (2.304s,  444.54/s)  LR: 7.421e-05  Data: 0.023 (1.710)
Train: 102 [ 500/1171 ( 43%)]  Loss:  2.963051 (2.8217)  Time: 0.587s, 1744.30/s  (2.327s,  439.96/s)  LR: 7.421e-05  Data: 0.020 (1.735)
Train: 102 [ 550/1171 ( 47%)]  Loss:  2.824552 (2.8219)  Time: 0.587s, 1743.04/s  (2.333s,  438.97/s)  LR: 7.421e-05  Data: 0.022 (1.741)
Train: 102 [ 600/1171 ( 51%)]  Loss:  2.744227 (2.8160)  Time: 2.408s,  425.33/s  (2.344s,  436.94/s)  LR: 7.421e-05  Data: 1.843 (1.751)
Train: 102 [ 650/1171 ( 56%)]  Loss:  3.024458 (2.8309)  Time: 0.590s, 1736.36/s  (2.330s,  439.51/s)  LR: 7.421e-05  Data: 0.024 (1.737)
Train: 102 [ 700/1171 ( 60%)]  Loss:  3.111791 (2.8496)  Time: 0.589s, 1739.71/s  (2.324s,  440.65/s)  LR: 7.421e-05  Data: 0.025 (1.731)
Train: 102 [ 750/1171 ( 64%)]  Loss:  3.023602 (2.8605)  Time: 0.588s, 1742.91/s  (2.306s,  444.15/s)  LR: 7.421e-05  Data: 0.021 (1.713)
Train: 102 [ 800/1171 ( 68%)]  Loss:  2.944709 (2.8654)  Time: 0.589s, 1738.46/s  (2.329s,  439.73/s)  LR: 7.421e-05  Data: 0.022 (1.736)
Train: 102 [ 850/1171 ( 73%)]  Loss:  2.368151 (2.8378)  Time: 0.590s, 1736.49/s  (2.323s,  440.85/s)  LR: 7.421e-05  Data: 0.023 (1.731)
Train: 102 [ 900/1171 ( 77%)]  Loss:  2.731016 (2.8322)  Time: 0.590s, 1736.04/s  (2.330s,  439.40/s)  LR: 7.421e-05  Data: 0.021 (1.738)
Train: 102 [ 950/1171 ( 81%)]  Loss:  3.301304 (2.8556)  Time: 0.585s, 1749.36/s  (2.328s,  439.89/s)  LR: 7.421e-05  Data: 0.020 (1.735)
Train: 102 [1000/1171 ( 85%)]  Loss:  2.850518 (2.8554)  Time: 0.591s, 1732.12/s  (2.328s,  439.83/s)  LR: 7.421e-05  Data: 0.024 (1.736)
Train: 102 [1050/1171 ( 90%)]  Loss:  2.971132 (2.8606)  Time: 0.587s, 1744.54/s  (2.319s,  441.59/s)  LR: 7.421e-05  Data: 0.022 (1.727)
Train: 102 [1100/1171 ( 94%)]  Loss:  3.224753 (2.8765)  Time: 0.702s, 1457.73/s  (2.316s,  442.20/s)  LR: 7.421e-05  Data: 0.072 (1.723)
Train: 102 [1150/1171 ( 98%)]  Loss:  2.948389 (2.8795)  Time: 0.591s, 1733.43/s  (2.305s,  444.24/s)  LR: 7.421e-05  Data: 0.023 (1.712)
Train: 102 [1170/1171 (100%)]  Loss:  3.125873 (2.8893)  Time: 0.568s, 1802.81/s  (2.315s,  442.37/s)  LR: 7.421e-05  Data: 0.000 (1.722)
Test: [   0/97]  Time: 15.093 (15.093)  Loss:  0.3303 (0.3303)  Acc@1: 96.2891 (96.2891)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.200 (3.221)  Loss:  0.5270 (0.4202)  Acc@1: 90.7227 (94.5964)  Acc@5: 98.3398 (98.8492)
Test: [  97/97]  Time: 0.120 (3.113)  Loss:  0.3918 (0.4280)  Acc@1: 93.4524 (94.1150)  Acc@5: 99.2560 (98.6580)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-102.pth.tar', 94.11500002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-101.pth.tar', 94.10000004638673)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-100.pth.tar', 93.9380000341797)

Train: 103 [   0/1171 (  0%)]  Loss:  3.226470 (3.2265)  Time: 10.701s,   95.69/s  (10.701s,   95.69/s)  LR: 6.807e-05  Data: 9.963 (9.963)
Train: 103 [  50/1171 (  4%)]  Loss:  3.096478 (3.1615)  Time: 0.585s, 1751.34/s  (2.410s,  424.82/s)  LR: 6.807e-05  Data: 0.018 (1.801)
Train: 103 [ 100/1171 (  9%)]  Loss:  2.761922 (3.0283)  Time: 2.308s,  443.58/s  (2.331s,  439.34/s)  LR: 6.807e-05  Data: 0.994 (1.707)
Train: 103 [ 150/1171 ( 13%)]  Loss:  3.006505 (3.0228)  Time: 0.585s, 1749.68/s  (2.260s,  453.09/s)  LR: 6.807e-05  Data: 0.019 (1.638)
Train: 103 [ 200/1171 ( 17%)]  Loss:  2.524846 (2.9232)  Time: 0.590s, 1736.61/s  (2.222s,  460.83/s)  LR: 6.807e-05  Data: 0.020 (1.605)
Train: 103 [ 250/1171 ( 21%)]  Loss:  2.807132 (2.9039)  Time: 0.586s, 1746.72/s  (2.186s,  468.39/s)  LR: 6.807e-05  Data: 0.021 (1.574)
Train: 103 [ 300/1171 ( 26%)]  Loss:  3.181474 (2.9435)  Time: 3.746s,  273.34/s  (2.259s,  453.29/s)  LR: 6.807e-05  Data: 3.058 (1.647)
Train: 103 [ 350/1171 ( 30%)]  Loss:  3.246431 (2.9814)  Time: 0.589s, 1738.90/s  (2.270s,  451.19/s)  LR: 6.807e-05  Data: 0.023 (1.660)
Train: 103 [ 400/1171 ( 34%)]  Loss:  2.909493 (2.9734)  Time: 7.543s,  135.75/s  (2.280s,  449.04/s)  LR: 6.807e-05  Data: 6.923 (1.673)
Train: 103 [ 450/1171 ( 38%)]  Loss:  3.083542 (2.9844)  Time: 0.588s, 1740.58/s  (2.280s,  449.03/s)  LR: 6.807e-05  Data: 0.021 (1.674)
Train: 103 [ 500/1171 ( 43%)]  Loss:  2.821846 (2.9696)  Time: 7.291s,  140.45/s  (2.294s,  446.38/s)  LR: 6.807e-05  Data: 6.725 (1.688)
Train: 103 [ 550/1171 ( 47%)]  Loss:  2.628120 (2.9412)  Time: 0.587s, 1743.66/s  (2.290s,  447.13/s)  LR: 6.807e-05  Data: 0.020 (1.686)
Train: 103 [ 600/1171 ( 51%)]  Loss:  2.472026 (2.9051)  Time: 4.427s,  231.33/s  (2.293s,  446.58/s)  LR: 6.807e-05  Data: 3.861 (1.690)
Train: 103 [ 650/1171 ( 56%)]  Loss:  3.042642 (2.9149)  Time: 0.590s, 1737.06/s  (2.313s,  442.69/s)  LR: 6.807e-05  Data: 0.019 (1.710)
Train: 103 [ 700/1171 ( 60%)]  Loss:  2.808126 (2.9078)  Time: 0.698s, 1467.75/s  (2.309s,  443.54/s)  LR: 6.807e-05  Data: 0.017 (1.707)
Train: 103 [ 750/1171 ( 64%)]  Loss:  3.052113 (2.9168)  Time: 0.596s, 1718.78/s  (2.329s,  439.76/s)  LR: 6.807e-05  Data: 0.018 (1.727)
Train: 103 [ 800/1171 ( 68%)]  Loss:  3.148485 (2.9305)  Time: 0.640s, 1599.65/s  (2.329s,  439.63/s)  LR: 6.807e-05  Data: 0.039 (1.729)
Train: 103 [ 850/1171 ( 73%)]  Loss:  3.258345 (2.9487)  Time: 0.586s, 1747.69/s  (2.330s,  439.51/s)  LR: 6.807e-05  Data: 0.020 (1.730)
Train: 103 [ 900/1171 ( 77%)]  Loss:  3.057622 (2.9544)  Time: 0.587s, 1744.83/s  (2.320s,  441.39/s)  LR: 6.807e-05  Data: 0.020 (1.721)
Train: 103 [ 950/1171 ( 81%)]  Loss:  2.762841 (2.9448)  Time: 0.587s, 1743.23/s  (2.316s,  442.13/s)  LR: 6.807e-05  Data: 0.022 (1.718)
Train: 103 [1000/1171 ( 85%)]  Loss:  2.762981 (2.9362)  Time: 0.586s, 1747.43/s  (2.304s,  444.39/s)  LR: 6.807e-05  Data: 0.020 (1.707)
Train: 103 [1050/1171 ( 90%)]  Loss:  3.124636 (2.9447)  Time: 0.588s, 1741.72/s  (2.320s,  441.45/s)  LR: 6.807e-05  Data: 0.024 (1.722)
Train: 103 [1100/1171 ( 94%)]  Loss:  3.010336 (2.9476)  Time: 0.586s, 1746.94/s  (2.315s,  442.35/s)  LR: 6.807e-05  Data: 0.019 (1.718)
Train: 103 [1150/1171 ( 98%)]  Loss:  3.173282 (2.9570)  Time: 0.590s, 1735.18/s  (2.319s,  441.50/s)  LR: 6.807e-05  Data: 0.024 (1.723)
Train: 103 [1170/1171 (100%)]  Loss:  3.196127 (2.9666)  Time: 0.566s, 1809.32/s  (2.322s,  441.08/s)  LR: 6.807e-05  Data: 0.000 (1.725)
Test: [   0/97]  Time: 13.776 (13.776)  Loss:  0.2988 (0.2988)  Acc@1: 96.0938 (96.0938)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (3.015)  Loss:  0.4581 (0.3703)  Acc@1: 91.7969 (94.6729)  Acc@5: 98.2422 (98.8568)
Test: [  97/97]  Time: 0.119 (2.920)  Loss:  0.3203 (0.3840)  Acc@1: 94.9405 (94.1530)  Acc@5: 99.2560 (98.6720)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-103.pth.tar', 94.15300004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-102.pth.tar', 94.11500002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-101.pth.tar', 94.10000004638673)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-100.pth.tar', 93.9380000341797)

Train: 104 [   0/1171 (  0%)]  Loss:  2.846299 (2.8463)  Time: 11.327s,   90.41/s  (11.327s,   90.41/s)  LR: 6.223e-05  Data: 10.062 (10.062)
Train: 104 [  50/1171 (  4%)]  Loss:  2.884727 (2.8655)  Time: 0.588s, 1740.12/s  (2.250s,  455.02/s)  LR: 6.223e-05  Data: 0.023 (1.654)
Train: 104 [ 100/1171 (  9%)]  Loss:  2.770683 (2.8339)  Time: 1.892s,  541.15/s  (2.232s,  458.83/s)  LR: 6.223e-05  Data: 1.325 (1.635)
Train: 104 [ 150/1171 ( 13%)]  Loss:  2.852753 (2.8386)  Time: 0.592s, 1728.92/s  (2.315s,  442.27/s)  LR: 6.223e-05  Data: 0.024 (1.715)
Train: 104 [ 200/1171 ( 17%)]  Loss:  3.372662 (2.9454)  Time: 0.997s, 1026.73/s  (2.405s,  425.81/s)  LR: 6.223e-05  Data: 0.432 (1.802)
Train: 104 [ 250/1171 ( 21%)]  Loss:  2.873254 (2.9334)  Time: 0.586s, 1747.64/s  (2.415s,  423.96/s)  LR: 6.223e-05  Data: 0.020 (1.816)
Train: 104 [ 300/1171 ( 26%)]  Loss:  3.124673 (2.9607)  Time: 4.460s,  229.59/s  (2.415s,  423.99/s)  LR: 6.223e-05  Data: 3.800 (1.816)
Train: 104 [ 350/1171 ( 30%)]  Loss:  2.579868 (2.9131)  Time: 0.584s, 1752.32/s  (2.397s,  427.16/s)  LR: 6.223e-05  Data: 0.019 (1.797)
Train: 104 [ 400/1171 ( 34%)]  Loss:  3.176685 (2.9424)  Time: 3.921s,  261.14/s  (2.383s,  429.77/s)  LR: 6.223e-05  Data: 3.236 (1.779)
Train: 104 [ 450/1171 ( 38%)]  Loss:  2.882559 (2.9364)  Time: 0.585s, 1751.07/s  (2.362s,  433.47/s)  LR: 6.223e-05  Data: 0.020 (1.759)
Train: 104 [ 500/1171 ( 43%)]  Loss:  2.944350 (2.9371)  Time: 3.506s,  292.11/s  (2.392s,  428.10/s)  LR: 6.223e-05  Data: 2.928 (1.790)
Train: 104 [ 550/1171 ( 47%)]  Loss:  3.136943 (2.9538)  Time: 0.587s, 1745.37/s  (2.400s,  426.70/s)  LR: 6.223e-05  Data: 0.020 (1.798)
Train: 104 [ 600/1171 ( 51%)]  Loss:  3.225754 (2.9747)  Time: 3.456s,  296.29/s  (2.419s,  423.33/s)  LR: 6.223e-05  Data: 2.883 (1.817)
Train: 104 [ 650/1171 ( 56%)]  Loss:  3.285164 (2.9969)  Time: 0.590s, 1736.77/s  (2.416s,  423.89/s)  LR: 6.223e-05  Data: 0.019 (1.814)
Train: 104 [ 700/1171 ( 60%)]  Loss:  3.060890 (3.0012)  Time: 0.812s, 1261.48/s  (2.411s,  424.68/s)  LR: 6.223e-05  Data: 0.132 (1.810)
Train: 104 [ 750/1171 ( 64%)]  Loss:  2.689636 (2.9817)  Time: 0.587s, 1745.18/s  (2.398s,  427.00/s)  LR: 6.223e-05  Data: 0.021 (1.797)
Train: 104 [ 800/1171 ( 68%)]  Loss:  3.150854 (2.9916)  Time: 0.588s, 1740.77/s  (2.392s,  428.01/s)  LR: 6.223e-05  Data: 0.019 (1.792)
Train: 104 [ 850/1171 ( 73%)]  Loss:  2.831393 (2.9827)  Time: 0.593s, 1725.56/s  (2.382s,  429.94/s)  LR: 6.223e-05  Data: 0.020 (1.781)
Train: 104 [ 900/1171 ( 77%)]  Loss:  2.963433 (2.9817)  Time: 0.589s, 1737.46/s  (2.401s,  426.54/s)  LR: 6.223e-05  Data: 0.023 (1.800)
Train: 104 [ 950/1171 ( 81%)]  Loss:  3.246203 (2.9949)  Time: 0.591s, 1733.94/s  (2.400s,  426.68/s)  LR: 6.223e-05  Data: 0.025 (1.800)
Train: 104 [1000/1171 ( 85%)]  Loss:  2.633821 (2.9777)  Time: 0.586s, 1746.31/s  (2.404s,  426.01/s)  LR: 6.223e-05  Data: 0.022 (1.804)
Train: 104 [1050/1171 ( 90%)]  Loss:  2.301242 (2.9470)  Time: 0.591s, 1733.17/s  (2.397s,  427.19/s)  LR: 6.223e-05  Data: 0.025 (1.797)
Train: 104 [1100/1171 ( 94%)]  Loss:  3.140907 (2.9554)  Time: 0.589s, 1739.50/s  (2.395s,  427.52/s)  LR: 6.223e-05  Data: 0.023 (1.795)
Train: 104 [1150/1171 ( 98%)]  Loss:  3.189874 (2.9652)  Time: 0.587s, 1745.36/s  (2.384s,  429.45/s)  LR: 6.223e-05  Data: 0.020 (1.785)
Train: 104 [1170/1171 (100%)]  Loss:  2.770400 (2.9574)  Time: 0.568s, 1803.71/s  (2.380s,  430.33/s)  LR: 6.223e-05  Data: 0.000 (1.781)
Test: [   0/97]  Time: 11.768 (11.768)  Loss:  0.3036 (0.3036)  Acc@1: 96.1914 (96.1914)  Acc@5: 99.6094 (99.6094)
Test: [  50/97]  Time: 0.202 (2.853)  Loss:  0.4729 (0.3913)  Acc@1: 92.0898 (94.6461)  Acc@5: 98.3398 (98.8626)
Test: [  97/97]  Time: 0.120 (2.990)  Loss:  0.3529 (0.4009)  Acc@1: 94.3452 (94.1220)  Acc@5: 99.1071 (98.6730)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-103.pth.tar', 94.15300004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-104.pth.tar', 94.12199999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-102.pth.tar', 94.11500002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-101.pth.tar', 94.10000004638673)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-100.pth.tar', 93.9380000341797)

Train: 105 [   0/1171 (  0%)]  Loss:  2.742277 (2.7423)  Time: 11.902s,   86.03/s  (11.902s,   86.03/s)  LR: 5.668e-05  Data: 10.551 (10.551)
Train: 105 [  50/1171 (  4%)]  Loss:  2.476026 (2.6092)  Time: 0.590s, 1734.40/s  (2.410s,  424.91/s)  LR: 5.668e-05  Data: 0.021 (1.795)
Train: 105 [ 100/1171 (  9%)]  Loss:  2.950781 (2.7230)  Time: 1.478s,  692.85/s  (2.409s,  425.02/s)  LR: 5.668e-05  Data: 0.913 (1.800)
Train: 105 [ 150/1171 ( 13%)]  Loss:  3.133868 (2.8257)  Time: 0.595s, 1719.84/s  (2.368s,  432.51/s)  LR: 5.668e-05  Data: 0.027 (1.762)
Train: 105 [ 200/1171 ( 17%)]  Loss:  2.509490 (2.7625)  Time: 4.448s,  230.20/s  (2.349s,  435.96/s)  LR: 5.668e-05  Data: 3.883 (1.738)
Train: 105 [ 250/1171 ( 21%)]  Loss:  2.819275 (2.7720)  Time: 0.592s, 1731.05/s  (2.355s,  434.89/s)  LR: 5.668e-05  Data: 0.021 (1.746)
Train: 105 [ 300/1171 ( 26%)]  Loss:  3.316421 (2.8497)  Time: 3.856s,  265.53/s  (2.372s,  431.78/s)  LR: 5.668e-05  Data: 3.292 (1.759)
Train: 105 [ 350/1171 ( 30%)]  Loss:  3.451414 (2.9249)  Time: 0.587s, 1745.66/s  (2.414s,  424.23/s)  LR: 5.668e-05  Data: 0.020 (1.804)
Train: 105 [ 400/1171 ( 34%)]  Loss:  3.032943 (2.9369)  Time: 5.476s,  186.98/s  (2.459s,  416.44/s)  LR: 5.668e-05  Data: 4.799 (1.850)
Train: 105 [ 450/1171 ( 38%)]  Loss:  3.253967 (2.9686)  Time: 0.587s, 1745.07/s  (2.465s,  415.48/s)  LR: 5.668e-05  Data: 0.020 (1.853)
Train: 105 [ 500/1171 ( 43%)]  Loss:  2.713857 (2.9455)  Time: 7.716s,  132.72/s  (2.495s,  410.44/s)  LR: 5.668e-05  Data: 7.090 (1.884)
Train: 105 [ 550/1171 ( 47%)]  Loss:  2.856495 (2.9381)  Time: 0.586s, 1748.40/s  (2.502s,  409.35/s)  LR: 5.668e-05  Data: 0.020 (1.892)
Train: 105 [ 600/1171 ( 51%)]  Loss:  3.031583 (2.9453)  Time: 6.677s,  153.36/s  (2.487s,  411.77/s)  LR: 5.668e-05  Data: 6.048 (1.879)
Train: 105 [ 650/1171 ( 56%)]  Loss:  2.964738 (2.9467)  Time: 0.588s, 1740.80/s  (2.460s,  416.25/s)  LR: 5.668e-05  Data: 0.021 (1.854)
Train: 105 [ 700/1171 ( 60%)]  Loss:  2.813173 (2.9378)  Time: 9.597s,  106.70/s  (2.485s,  412.05/s)  LR: 5.668e-05  Data: 8.921 (1.880)
Train: 105 [ 750/1171 ( 64%)]  Loss:  2.768121 (2.9272)  Time: 0.586s, 1748.88/s  (2.496s,  410.22/s)  LR: 5.668e-05  Data: 0.019 (1.883)
Train: 105 [ 800/1171 ( 68%)]  Loss:  3.141478 (2.9398)  Time: 8.791s,  116.48/s  (2.505s,  408.72/s)  LR: 5.668e-05  Data: 8.078 (1.893)
Train: 105 [ 850/1171 ( 73%)]  Loss:  3.205445 (2.9545)  Time: 0.587s, 1745.09/s  (2.513s,  407.52/s)  LR: 5.668e-05  Data: 0.020 (1.901)
Train: 105 [ 900/1171 ( 77%)]  Loss:  2.743089 (2.9434)  Time: 7.936s,  129.04/s  (2.525s,  405.54/s)  LR: 5.668e-05  Data: 7.019 (1.913)
Train: 105 [ 950/1171 ( 81%)]  Loss:  3.177463 (2.9551)  Time: 0.587s, 1745.02/s  (2.523s,  405.79/s)  LR: 5.668e-05  Data: 0.021 (1.902)
Train: 105 [1000/1171 ( 85%)]  Loss:  3.233232 (2.9683)  Time: 4.233s,  241.89/s  (2.511s,  407.79/s)  LR: 5.668e-05  Data: 3.586 (1.890)
Train: 105 [1050/1171 ( 90%)]  Loss:  3.235439 (2.9805)  Time: 1.775s,  576.98/s  (2.512s,  407.65/s)  LR: 5.668e-05  Data: 1.209 (1.891)
Train: 105 [1100/1171 ( 94%)]  Loss:  3.102791 (2.9858)  Time: 2.673s,  383.07/s  (2.522s,  406.06/s)  LR: 5.668e-05  Data: 2.009 (1.900)
Train: 105 [1150/1171 ( 98%)]  Loss:  3.179664 (2.9939)  Time: 0.922s, 1110.97/s  (2.514s,  407.25/s)  LR: 5.668e-05  Data: 0.317 (1.893)
Train: 105 [1170/1171 (100%)]  Loss:  3.420948 (3.0110)  Time: 0.565s, 1811.35/s  (2.510s,  407.91/s)  LR: 5.668e-05  Data: 0.000 (1.890)
Test: [   0/97]  Time: 13.568 (13.568)  Loss:  0.3128 (0.3128)  Acc@1: 96.3867 (96.3867)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (3.111)  Loss:  0.4729 (0.3809)  Acc@1: 92.0898 (94.7534)  Acc@5: 98.1445 (98.8703)
Test: [  97/97]  Time: 0.120 (2.973)  Loss:  0.3327 (0.3908)  Acc@1: 95.3869 (94.3220)  Acc@5: 99.1071 (98.7140)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-105.pth.tar', 94.32200003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-103.pth.tar', 94.15300004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-104.pth.tar', 94.12199999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-102.pth.tar', 94.11500002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-101.pth.tar', 94.10000004638673)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-100.pth.tar', 93.9380000341797)

Train: 106 [   0/1171 (  0%)]  Loss:  3.438718 (3.4387)  Time: 9.620s,  106.44/s  (9.620s,  106.44/s)  LR: 5.142e-05  Data: 8.857 (8.857)
Train: 106 [  50/1171 (  4%)]  Loss:  2.830673 (3.1347)  Time: 0.587s, 1744.13/s  (2.126s,  481.55/s)  LR: 5.142e-05  Data: 0.021 (1.540)
Train: 106 [ 100/1171 (  9%)]  Loss:  3.169678 (3.1464)  Time: 0.588s, 1740.27/s  (2.084s,  491.42/s)  LR: 5.142e-05  Data: 0.023 (1.483)
Train: 106 [ 150/1171 ( 13%)]  Loss:  3.306968 (3.1865)  Time: 0.590s, 1735.38/s  (2.158s,  474.55/s)  LR: 5.142e-05  Data: 0.022 (1.557)
Train: 106 [ 200/1171 ( 17%)]  Loss:  2.966848 (3.1426)  Time: 1.541s,  664.67/s  (2.218s,  461.76/s)  LR: 5.142e-05  Data: 0.959 (1.611)
Train: 106 [ 250/1171 ( 21%)]  Loss:  3.036371 (3.1249)  Time: 0.589s, 1739.26/s  (2.206s,  464.15/s)  LR: 5.142e-05  Data: 0.023 (1.599)
Train: 106 [ 300/1171 ( 26%)]  Loss:  2.355119 (3.0149)  Time: 0.703s, 1457.09/s  (2.209s,  463.48/s)  LR: 5.142e-05  Data: 0.112 (1.603)
Train: 106 [ 350/1171 ( 30%)]  Loss:  2.455528 (2.9450)  Time: 0.592s, 1729.05/s  (2.189s,  467.86/s)  LR: 5.142e-05  Data: 0.021 (1.582)
Train: 106 [ 400/1171 ( 34%)]  Loss:  2.965529 (2.9473)  Time: 2.259s,  453.33/s  (2.188s,  468.07/s)  LR: 5.142e-05  Data: 1.648 (1.580)
Train: 106 [ 450/1171 ( 38%)]  Loss:  3.136598 (2.9662)  Time: 0.588s, 1740.08/s  (2.175s,  470.77/s)  LR: 5.142e-05  Data: 0.020 (1.567)
Train: 106 [ 500/1171 ( 43%)]  Loss:  3.043635 (2.9732)  Time: 1.183s,  865.73/s  (2.180s,  469.70/s)  LR: 5.142e-05  Data: 0.523 (1.572)
Train: 106 [ 550/1171 ( 47%)]  Loss:  2.919518 (2.9688)  Time: 0.589s, 1737.25/s  (2.219s,  461.44/s)  LR: 5.142e-05  Data: 0.020 (1.613)
Train: 106 [ 600/1171 ( 51%)]  Loss:  3.165244 (2.9839)  Time: 0.587s, 1744.35/s  (2.239s,  457.32/s)  LR: 5.142e-05  Data: 0.019 (1.634)
Train: 106 [ 650/1171 ( 56%)]  Loss:  2.887711 (2.9770)  Time: 0.588s, 1740.71/s  (2.241s,  456.86/s)  LR: 5.142e-05  Data: 0.023 (1.638)
Train: 106 [ 700/1171 ( 60%)]  Loss:  2.803172 (2.9654)  Time: 0.761s, 1345.18/s  (2.278s,  449.49/s)  LR: 5.142e-05  Data: 0.196 (1.673)
Train: 106 [ 750/1171 ( 64%)]  Loss:  3.031794 (2.9696)  Time: 1.158s,  884.36/s  (2.273s,  450.47/s)  LR: 5.142e-05  Data: 0.490 (1.669)
Train: 106 [ 800/1171 ( 68%)]  Loss:  3.019118 (2.9725)  Time: 0.591s, 1731.93/s  (2.275s,  450.07/s)  LR: 5.142e-05  Data: 0.023 (1.672)
Train: 106 [ 850/1171 ( 73%)]  Loss:  2.817908 (2.9639)  Time: 0.588s, 1742.89/s  (2.263s,  452.51/s)  LR: 5.142e-05  Data: 0.020 (1.660)
Train: 106 [ 900/1171 ( 77%)]  Loss:  3.163391 (2.9744)  Time: 0.588s, 1741.64/s  (2.259s,  453.25/s)  LR: 5.142e-05  Data: 0.022 (1.657)
Train: 106 [ 950/1171 ( 81%)]  Loss:  3.240606 (2.9877)  Time: 0.585s, 1750.66/s  (2.275s,  450.08/s)  LR: 5.142e-05  Data: 0.020 (1.674)
Train: 106 [1000/1171 ( 85%)]  Loss:  2.500232 (2.9645)  Time: 0.590s, 1735.73/s  (2.287s,  447.73/s)  LR: 5.142e-05  Data: 0.023 (1.686)
Train: 106 [1050/1171 ( 90%)]  Loss:  3.079603 (2.9697)  Time: 0.586s, 1746.71/s  (2.286s,  447.95/s)  LR: 5.142e-05  Data: 0.021 (1.685)
Train: 106 [1100/1171 ( 94%)]  Loss:  2.850425 (2.9645)  Time: 0.588s, 1741.93/s  (2.286s,  447.99/s)  LR: 5.142e-05  Data: 0.022 (1.685)
Train: 106 [1150/1171 ( 98%)]  Loss:  2.893903 (2.9616)  Time: 0.589s, 1737.14/s  (2.280s,  449.17/s)  LR: 5.142e-05  Data: 0.023 (1.679)
Train: 106 [1170/1171 (100%)]  Loss:  3.260463 (2.9736)  Time: 0.566s, 1808.52/s  (2.278s,  449.47/s)  LR: 5.142e-05  Data: 0.000 (1.678)
Test: [   0/97]  Time: 12.808 (12.808)  Loss:  0.3305 (0.3305)  Acc@1: 96.4844 (96.4844)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.200 (2.915)  Loss:  0.5029 (0.4007)  Acc@1: 91.2109 (94.7629)  Acc@5: 98.0469 (98.8396)
Test: [  97/97]  Time: 0.121 (2.911)  Loss:  0.3562 (0.4125)  Acc@1: 94.4940 (94.2120)  Acc@5: 99.4048 (98.6670)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-105.pth.tar', 94.32200003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-106.pth.tar', 94.21200000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-103.pth.tar', 94.15300004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-104.pth.tar', 94.12199999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-102.pth.tar', 94.11500002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-101.pth.tar', 94.10000004638673)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-100.pth.tar', 93.9380000341797)

Train: 107 [   0/1171 (  0%)]  Loss:  3.238635 (3.2386)  Time: 18.250s,   56.11/s  (18.250s,   56.11/s)  LR: 4.647e-05  Data: 16.888 (16.888)
Train: 107 [  50/1171 (  4%)]  Loss:  2.865667 (3.0522)  Time: 0.590s, 1736.33/s  (2.551s,  401.48/s)  LR: 4.647e-05  Data: 0.024 (1.941)
Train: 107 [ 100/1171 (  9%)]  Loss:  2.768126 (2.9575)  Time: 0.595s, 1722.02/s  (2.483s,  412.39/s)  LR: 4.647e-05  Data: 0.027 (1.878)
Train: 107 [ 150/1171 ( 13%)]  Loss:  2.467699 (2.8350)  Time: 0.585s, 1751.36/s  (2.408s,  425.28/s)  LR: 4.647e-05  Data: 0.019 (1.805)
Train: 107 [ 200/1171 ( 17%)]  Loss:  3.334130 (2.9349)  Time: 3.248s,  315.25/s  (2.399s,  426.81/s)  LR: 4.647e-05  Data: 2.665 (1.799)
Train: 107 [ 250/1171 ( 21%)]  Loss:  2.864521 (2.9231)  Time: 0.589s, 1738.38/s  (2.342s,  437.21/s)  LR: 4.647e-05  Data: 0.020 (1.740)
Train: 107 [ 300/1171 ( 26%)]  Loss:  3.390960 (2.9900)  Time: 4.410s,  232.18/s  (2.314s,  442.55/s)  LR: 4.647e-05  Data: 3.840 (1.712)
Train: 107 [ 350/1171 ( 30%)]  Loss:  3.520364 (3.0563)  Time: 0.586s, 1746.64/s  (2.318s,  441.82/s)  LR: 4.647e-05  Data: 0.019 (1.716)
Train: 107 [ 400/1171 ( 34%)]  Loss:  2.676339 (3.0140)  Time: 6.038s,  169.58/s  (2.332s,  439.05/s)  LR: 4.647e-05  Data: 5.391 (1.727)
Train: 107 [ 450/1171 ( 38%)]  Loss:  3.086603 (3.0213)  Time: 0.585s, 1749.40/s  (2.318s,  441.76/s)  LR: 4.647e-05  Data: 0.020 (1.714)
Train: 107 [ 500/1171 ( 43%)]  Loss:  3.132799 (3.0314)  Time: 5.217s,  196.28/s  (2.341s,  437.38/s)  LR: 4.647e-05  Data: 4.643 (1.738)
Train: 107 [ 550/1171 ( 47%)]  Loss:  2.896520 (3.0202)  Time: 0.588s, 1741.76/s  (2.346s,  436.40/s)  LR: 4.647e-05  Data: 0.019 (1.742)
Train: 107 [ 600/1171 ( 51%)]  Loss:  3.075329 (3.0244)  Time: 8.035s,  127.45/s  (2.349s,  435.84/s)  LR: 4.647e-05  Data: 7.152 (1.745)
Train: 107 [ 650/1171 ( 56%)]  Loss:  3.119558 (3.0312)  Time: 0.588s, 1740.46/s  (2.335s,  438.61/s)  LR: 4.647e-05  Data: 0.020 (1.730)
Train: 107 [ 700/1171 ( 60%)]  Loss:  2.856345 (3.0196)  Time: 6.643s,  154.14/s  (2.324s,  440.65/s)  LR: 4.647e-05  Data: 6.031 (1.720)
Train: 107 [ 750/1171 ( 64%)]  Loss:  2.890336 (3.0115)  Time: 0.585s, 1749.33/s  (2.301s,  444.99/s)  LR: 4.647e-05  Data: 0.020 (1.699)
Train: 107 [ 800/1171 ( 68%)]  Loss:  3.093982 (3.0163)  Time: 7.683s,  133.29/s  (2.319s,  441.55/s)  LR: 4.647e-05  Data: 7.118 (1.719)
Train: 107 [ 850/1171 ( 73%)]  Loss:  3.058246 (3.0187)  Time: 0.586s, 1747.80/s  (2.309s,  443.47/s)  LR: 4.647e-05  Data: 0.021 (1.710)
Train: 107 [ 900/1171 ( 77%)]  Loss:  2.847121 (3.0096)  Time: 6.516s,  157.16/s  (2.312s,  442.93/s)  LR: 4.647e-05  Data: 5.808 (1.713)
Train: 107 [ 950/1171 ( 81%)]  Loss:  3.091813 (3.0138)  Time: 0.587s, 1745.26/s  (2.305s,  444.23/s)  LR: 4.647e-05  Data: 0.021 (1.706)
Train: 107 [1000/1171 ( 85%)]  Loss:  3.089244 (3.0173)  Time: 5.604s,  182.73/s  (2.299s,  445.45/s)  LR: 4.647e-05  Data: 4.999 (1.699)
Train: 107 [1050/1171 ( 90%)]  Loss:  2.890425 (3.0116)  Time: 0.588s, 1741.18/s  (2.286s,  447.97/s)  LR: 4.647e-05  Data: 0.023 (1.687)
Train: 107 [1100/1171 ( 94%)]  Loss:  3.191473 (3.0194)  Time: 6.227s,  164.45/s  (2.276s,  449.82/s)  LR: 4.647e-05  Data: 5.580 (1.678)
Train: 107 [1150/1171 ( 98%)]  Loss:  2.697362 (3.0060)  Time: 0.587s, 1745.12/s  (2.262s,  452.76/s)  LR: 4.647e-05  Data: 0.021 (1.663)
Train: 107 [1170/1171 (100%)]  Loss:  3.134042 (3.0111)  Time: 0.565s, 1810.93/s  (2.257s,  453.75/s)  LR: 4.647e-05  Data: 0.000 (1.658)
Test: [   0/97]  Time: 10.909 (10.909)  Loss:  0.3147 (0.3147)  Acc@1: 96.4844 (96.4844)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (2.938)  Loss:  0.5044 (0.3881)  Acc@1: 91.2109 (94.8836)  Acc@5: 98.2422 (98.8703)
Test: [  97/97]  Time: 0.120 (3.054)  Loss:  0.3587 (0.4016)  Acc@1: 94.9405 (94.3250)  Acc@5: 99.1071 (98.6940)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-107.pth.tar', 94.32500004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-105.pth.tar', 94.32200003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-106.pth.tar', 94.21200000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-103.pth.tar', 94.15300004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-104.pth.tar', 94.12199999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-102.pth.tar', 94.11500002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-101.pth.tar', 94.10000004638673)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-100.pth.tar', 93.9380000341797)

Train: 108 [   0/1171 (  0%)]  Loss:  2.653190 (2.6532)  Time: 9.433s,  108.56/s  (9.433s,  108.56/s)  LR: 4.182e-05  Data: 8.694 (8.694)
Train: 108 [  50/1171 (  4%)]  Loss:  2.833177 (2.7432)  Time: 0.587s, 1743.43/s  (2.170s,  471.89/s)  LR: 4.182e-05  Data: 0.022 (1.584)
Train: 108 [ 100/1171 (  9%)]  Loss:  2.764054 (2.7501)  Time: 0.592s, 1728.69/s  (2.155s,  475.10/s)  LR: 4.182e-05  Data: 0.023 (1.566)
Train: 108 [ 150/1171 ( 13%)]  Loss:  2.921560 (2.7930)  Time: 0.589s, 1737.08/s  (2.108s,  485.80/s)  LR: 4.182e-05  Data: 0.024 (1.515)
Train: 108 [ 200/1171 ( 17%)]  Loss:  2.990382 (2.8325)  Time: 2.278s,  449.42/s  (2.148s,  476.69/s)  LR: 4.182e-05  Data: 1.599 (1.546)
Train: 108 [ 250/1171 ( 21%)]  Loss:  3.098202 (2.8768)  Time: 0.800s, 1279.42/s  (2.146s,  477.11/s)  LR: 4.182e-05  Data: 0.120 (1.541)
Train: 108 [ 300/1171 ( 26%)]  Loss:  2.732586 (2.8562)  Time: 7.292s,  140.42/s  (2.167s,  472.62/s)  LR: 4.182e-05  Data: 6.714 (1.561)
Train: 108 [ 350/1171 ( 30%)]  Loss:  2.747090 (2.8425)  Time: 0.588s, 1741.43/s  (2.205s,  464.43/s)  LR: 4.182e-05  Data: 0.021 (1.602)
Train: 108 [ 400/1171 ( 34%)]  Loss:  2.433151 (2.7970)  Time: 6.304s,  162.44/s  (2.248s,  455.56/s)  LR: 4.182e-05  Data: 5.703 (1.645)
Train: 108 [ 450/1171 ( 38%)]  Loss:  2.804531 (2.7978)  Time: 0.589s, 1737.62/s  (2.258s,  453.54/s)  LR: 4.182e-05  Data: 0.023 (1.655)
Train: 108 [ 500/1171 ( 43%)]  Loss:  3.157416 (2.8305)  Time: 7.026s,  145.75/s  (2.279s,  449.35/s)  LR: 4.182e-05  Data: 6.393 (1.677)
Train: 108 [ 550/1171 ( 47%)]  Loss:  3.034136 (2.8475)  Time: 0.591s, 1731.42/s  (2.286s,  448.00/s)  LR: 4.182e-05  Data: 0.020 (1.685)
Train: 108 [ 600/1171 ( 51%)]  Loss:  2.903040 (2.8517)  Time: 6.023s,  170.00/s  (2.304s,  444.45/s)  LR: 4.182e-05  Data: 5.421 (1.704)
Train: 108 [ 650/1171 ( 56%)]  Loss:  3.199837 (2.8766)  Time: 0.590s, 1734.18/s  (2.299s,  445.44/s)  LR: 4.182e-05  Data: 0.025 (1.699)
Train: 108 [ 700/1171 ( 60%)]  Loss:  3.010623 (2.8855)  Time: 5.734s,  178.59/s  (2.331s,  439.29/s)  LR: 4.182e-05  Data: 5.075 (1.732)
Train: 108 [ 750/1171 ( 64%)]  Loss:  2.943456 (2.8892)  Time: 0.590s, 1735.59/s  (2.338s,  437.90/s)  LR: 4.182e-05  Data: 0.019 (1.739)
Train: 108 [ 800/1171 ( 68%)]  Loss:  2.981429 (2.8946)  Time: 7.617s,  134.44/s  (2.353s,  435.22/s)  LR: 4.182e-05  Data: 6.961 (1.752)
Train: 108 [ 850/1171 ( 73%)]  Loss:  3.233435 (2.9134)  Time: 0.586s, 1746.43/s  (2.344s,  436.89/s)  LR: 4.182e-05  Data: 0.022 (1.744)
Train: 108 [ 900/1171 ( 77%)]  Loss:  2.859133 (2.9105)  Time: 7.294s,  140.39/s  (2.366s,  432.82/s)  LR: 4.182e-05  Data: 6.718 (1.765)
Train: 108 [ 950/1171 ( 81%)]  Loss:  3.142275 (2.9221)  Time: 0.587s, 1743.68/s  (2.358s,  434.24/s)  LR: 4.182e-05  Data: 0.019 (1.758)
Train: 108 [1000/1171 ( 85%)]  Loss:  3.302519 (2.9402)  Time: 6.845s,  149.59/s  (2.353s,  435.14/s)  LR: 4.182e-05  Data: 6.144 (1.754)
Train: 108 [1050/1171 ( 90%)]  Loss:  2.525792 (2.9214)  Time: 0.588s, 1741.26/s  (2.360s,  433.81/s)  LR: 4.182e-05  Data: 0.021 (1.762)
Train: 108 [1100/1171 ( 94%)]  Loss:  3.048892 (2.9270)  Time: 9.930s,  103.13/s  (2.368s,  432.48/s)  LR: 4.182e-05  Data: 9.138 (1.769)
Train: 108 [1150/1171 ( 98%)]  Loss:  3.145869 (2.9361)  Time: 0.588s, 1741.48/s  (2.376s,  430.92/s)  LR: 4.182e-05  Data: 0.021 (1.779)
Train: 108 [1170/1171 (100%)]  Loss:  2.790532 (2.9303)  Time: 0.568s, 1802.50/s  (2.377s,  430.79/s)  LR: 4.182e-05  Data: 0.000 (1.780)
Test: [   0/97]  Time: 13.019 (13.019)  Loss:  0.2930 (0.2930)  Acc@1: 96.3867 (96.3867)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.200 (3.073)  Loss:  0.4428 (0.3622)  Acc@1: 92.6758 (94.9851)  Acc@5: 98.2422 (98.8990)
Test: [  97/97]  Time: 0.120 (2.997)  Loss:  0.3196 (0.3782)  Acc@1: 94.1964 (94.3590)  Acc@5: 99.5536 (98.7060)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-108.pth.tar', 94.35899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-107.pth.tar', 94.32500004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-105.pth.tar', 94.32200003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-106.pth.tar', 94.21200000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-103.pth.tar', 94.15300004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-104.pth.tar', 94.12199999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-102.pth.tar', 94.11500002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-101.pth.tar', 94.10000004638673)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-100.pth.tar', 93.9380000341797)

Train: 109 [   0/1171 (  0%)]  Loss:  2.730953 (2.7310)  Time: 10.609s,   96.52/s  (10.609s,   96.52/s)  LR: 3.748e-05  Data: 9.728 (9.728)
Train: 109 [  50/1171 (  4%)]  Loss:  2.726634 (2.7288)  Time: 0.585s, 1750.00/s  (2.340s,  437.60/s)  LR: 3.748e-05  Data: 0.020 (1.743)
Train: 109 [ 100/1171 (  9%)]  Loss:  3.138337 (2.8653)  Time: 0.589s, 1739.39/s  (2.272s,  450.65/s)  LR: 3.748e-05  Data: 0.020 (1.679)
Train: 109 [ 150/1171 ( 13%)]  Loss:  2.898002 (2.8735)  Time: 0.587s, 1745.73/s  (2.374s,  431.41/s)  LR: 3.748e-05  Data: 0.020 (1.781)
Train: 109 [ 200/1171 ( 17%)]  Loss:  2.802827 (2.8594)  Time: 0.589s, 1738.51/s  (2.405s,  425.71/s)  LR: 3.748e-05  Data: 0.021 (1.810)
Train: 109 [ 250/1171 ( 21%)]  Loss:  2.727346 (2.8373)  Time: 0.586s, 1747.20/s  (2.393s,  427.97/s)  LR: 3.748e-05  Data: 0.020 (1.796)
Train: 109 [ 300/1171 ( 26%)]  Loss:  2.756904 (2.8259)  Time: 3.364s,  304.44/s  (2.385s,  429.30/s)  LR: 3.748e-05  Data: 2.679 (1.786)
Train: 109 [ 350/1171 ( 30%)]  Loss:  2.893549 (2.8343)  Time: 0.589s, 1737.75/s  (2.367s,  432.55/s)  LR: 3.748e-05  Data: 0.024 (1.769)
Train: 109 [ 400/1171 ( 34%)]  Loss:  2.857632 (2.8369)  Time: 3.913s,  261.70/s  (2.362s,  433.58/s)  LR: 3.748e-05  Data: 3.240 (1.763)
Train: 109 [ 450/1171 ( 38%)]  Loss:  2.964121 (2.8496)  Time: 0.594s, 1723.73/s  (2.339s,  437.87/s)  LR: 3.748e-05  Data: 0.027 (1.740)
Train: 109 [ 500/1171 ( 43%)]  Loss:  2.675559 (2.8338)  Time: 5.495s,  186.36/s  (2.411s,  424.79/s)  LR: 3.748e-05  Data: 4.849 (1.810)
Train: 109 [ 550/1171 ( 47%)]  Loss:  3.075193 (2.8539)  Time: 0.590s, 1736.14/s  (2.418s,  423.53/s)  LR: 3.748e-05  Data: 0.019 (1.817)
Train: 109 [ 600/1171 ( 51%)]  Loss:  3.245965 (2.8841)  Time: 3.650s,  280.51/s  (2.443s,  419.24/s)  LR: 3.748e-05  Data: 2.806 (1.840)
Train: 109 [ 650/1171 ( 56%)]  Loss:  3.067347 (2.8972)  Time: 0.594s, 1724.73/s  (2.437s,  420.25/s)  LR: 3.748e-05  Data: 0.025 (1.833)
Train: 109 [ 700/1171 ( 60%)]  Loss:  3.007548 (2.9045)  Time: 0.587s, 1743.93/s  (2.433s,  420.84/s)  LR: 3.748e-05  Data: 0.023 (1.829)
Train: 109 [ 750/1171 ( 64%)]  Loss:  3.525011 (2.9433)  Time: 0.593s, 1727.76/s  (2.421s,  423.05/s)  LR: 3.748e-05  Data: 0.020 (1.817)
Train: 109 [ 800/1171 ( 68%)]  Loss:  2.955318 (2.9440)  Time: 0.982s, 1042.91/s  (2.419s,  423.38/s)  LR: 3.748e-05  Data: 0.018 (1.816)
Train: 109 [ 850/1171 ( 73%)]  Loss:  2.865010 (2.9396)  Time: 0.589s, 1737.35/s  (2.405s,  425.80/s)  LR: 3.748e-05  Data: 0.023 (1.804)
Train: 109 [ 900/1171 ( 77%)]  Loss:  3.084813 (2.9473)  Time: 0.586s, 1748.32/s  (2.425s,  422.19/s)  LR: 3.748e-05  Data: 0.021 (1.825)
Train: 109 [ 950/1171 ( 81%)]  Loss:  3.140944 (2.9570)  Time: 0.590s, 1736.25/s  (2.422s,  422.86/s)  LR: 3.748e-05  Data: 0.024 (1.822)
Train: 109 [1000/1171 ( 85%)]  Loss:  3.263390 (2.9715)  Time: 0.586s, 1747.89/s  (2.422s,  422.81/s)  LR: 3.748e-05  Data: 0.020 (1.822)
Train: 109 [1050/1171 ( 90%)]  Loss:  2.751793 (2.9616)  Time: 0.587s, 1743.27/s  (2.416s,  423.92/s)  LR: 3.748e-05  Data: 0.022 (1.817)
Train: 109 [1100/1171 ( 94%)]  Loss:  3.368202 (2.9792)  Time: 0.588s, 1740.78/s  (2.416s,  423.93/s)  LR: 3.748e-05  Data: 0.021 (1.817)
Train: 109 [1150/1171 ( 98%)]  Loss:  3.116732 (2.9850)  Time: 0.588s, 1740.69/s  (2.407s,  425.51/s)  LR: 3.748e-05  Data: 0.023 (1.809)
Train: 109 [1170/1171 (100%)]  Loss:  2.783723 (2.9769)  Time: 0.566s, 1810.19/s  (2.404s,  425.88/s)  LR: 3.748e-05  Data: 0.000 (1.807)
Test: [   0/97]  Time: 12.892 (12.892)  Loss:  0.2992 (0.2992)  Acc@1: 96.5820 (96.5820)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.197 (3.017)  Loss:  0.4750 (0.3789)  Acc@1: 92.0898 (94.9602)  Acc@5: 98.5352 (98.9028)
Test: [  97/97]  Time: 0.120 (3.139)  Loss:  0.3186 (0.3910)  Acc@1: 95.5357 (94.4380)  Acc@5: 99.2560 (98.7200)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-109.pth.tar', 94.43800004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-108.pth.tar', 94.35899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-107.pth.tar', 94.32500004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-105.pth.tar', 94.32200003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-106.pth.tar', 94.21200000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-103.pth.tar', 94.15300004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-104.pth.tar', 94.12199999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-102.pth.tar', 94.11500002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-101.pth.tar', 94.10000004638673)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-100.pth.tar', 93.9380000341797)

Train: 110 [   0/1171 (  0%)]  Loss:  2.660443 (2.6604)  Time: 11.971s,   85.54/s  (11.971s,   85.54/s)  LR: 3.345e-05  Data: 11.294 (11.294)
Train: 110 [  50/1171 (  4%)]  Loss:  2.355510 (2.5080)  Time: 0.591s, 1731.69/s  (2.502s,  409.20/s)  LR: 3.345e-05  Data: 0.023 (1.905)
Train: 110 [ 100/1171 (  9%)]  Loss:  2.411816 (2.4759)  Time: 1.790s,  571.93/s  (2.651s,  386.20/s)  LR: 3.345e-05  Data: 1.226 (2.046)
Train: 110 [ 150/1171 ( 13%)]  Loss:  2.815088 (2.5607)  Time: 0.586s, 1748.82/s  (2.534s,  404.13/s)  LR: 3.345e-05  Data: 0.020 (1.927)
Train: 110 [ 200/1171 ( 17%)]  Loss:  3.038178 (2.6562)  Time: 6.066s,  168.80/s  (2.488s,  411.63/s)  LR: 3.345e-05  Data: 5.415 (1.881)
Train: 110 [ 250/1171 ( 21%)]  Loss:  3.141295 (2.7371)  Time: 0.737s, 1388.96/s  (2.430s,  421.44/s)  LR: 3.345e-05  Data: 0.058 (1.826)
Train: 110 [ 300/1171 ( 26%)]  Loss:  2.668183 (2.7272)  Time: 8.087s,  126.62/s  (2.421s,  422.92/s)  LR: 3.345e-05  Data: 7.523 (1.819)
Train: 110 [ 350/1171 ( 30%)]  Loss:  3.074546 (2.7706)  Time: 0.587s, 1744.86/s  (2.442s,  419.27/s)  LR: 3.345e-05  Data: 0.019 (1.840)
Train: 110 [ 400/1171 ( 34%)]  Loss:  3.074369 (2.8044)  Time: 6.037s,  169.62/s  (2.461s,  416.10/s)  LR: 3.345e-05  Data: 5.465 (1.860)
Train: 110 [ 450/1171 ( 38%)]  Loss:  2.681612 (2.7921)  Time: 0.585s, 1749.43/s  (2.444s,  418.98/s)  LR: 3.345e-05  Data: 0.020 (1.843)
Train: 110 [ 500/1171 ( 43%)]  Loss:  2.826513 (2.7952)  Time: 8.299s,  123.40/s  (2.454s,  417.33/s)  LR: 3.345e-05  Data: 7.640 (1.855)
Train: 110 [ 550/1171 ( 47%)]  Loss:  2.577433 (2.7771)  Time: 0.586s, 1747.31/s  (2.449s,  418.09/s)  LR: 3.345e-05  Data: 0.021 (1.850)
Train: 110 [ 600/1171 ( 51%)]  Loss:  2.909592 (2.7873)  Time: 5.219s,  196.19/s  (2.454s,  417.31/s)  LR: 3.345e-05  Data: 4.654 (1.854)
Train: 110 [ 650/1171 ( 56%)]  Loss:  2.793813 (2.7877)  Time: 0.589s, 1739.62/s  (2.438s,  420.06/s)  LR: 3.345e-05  Data: 0.019 (1.838)
Train: 110 [ 700/1171 ( 60%)]  Loss:  2.921991 (2.7967)  Time: 2.118s,  483.54/s  (2.459s,  416.42/s)  LR: 3.345e-05  Data: 1.416 (1.858)
Train: 110 [ 750/1171 ( 64%)]  Loss:  2.871024 (2.8013)  Time: 2.352s,  435.42/s  (2.450s,  418.04/s)  LR: 3.345e-05  Data: 1.769 (1.845)
Train: 110 [ 800/1171 ( 68%)]  Loss:  2.952629 (2.8102)  Time: 0.587s, 1744.53/s  (2.448s,  418.23/s)  LR: 3.345e-05  Data: 0.021 (1.843)
Train: 110 [ 850/1171 ( 73%)]  Loss:  2.923000 (2.8165)  Time: 0.589s, 1739.72/s  (2.447s,  418.54/s)  LR: 3.345e-05  Data: 0.021 (1.841)
Train: 110 [ 900/1171 ( 77%)]  Loss:  2.994699 (2.8259)  Time: 3.799s,  269.57/s  (2.445s,  418.78/s)  LR: 3.345e-05  Data: 3.234 (1.840)
Train: 110 [ 950/1171 ( 81%)]  Loss:  2.440338 (2.8066)  Time: 0.588s, 1740.41/s  (2.448s,  418.24/s)  LR: 3.345e-05  Data: 0.019 (1.843)
Train: 110 [1000/1171 ( 85%)]  Loss:  3.103630 (2.8207)  Time: 1.747s,  586.12/s  (2.440s,  419.75/s)  LR: 3.345e-05  Data: 1.165 (1.835)
Train: 110 [1050/1171 ( 90%)]  Loss:  2.982852 (2.8281)  Time: 3.520s,  290.94/s  (2.440s,  419.75/s)  LR: 3.345e-05  Data: 2.954 (1.834)
Train: 110 [1100/1171 ( 94%)]  Loss:  3.459846 (2.8556)  Time: 0.585s, 1751.37/s  (2.440s,  419.68/s)  LR: 3.345e-05  Data: 0.019 (1.834)
Train: 110 [1150/1171 ( 98%)]  Loss:  2.482604 (2.8400)  Time: 0.591s, 1733.24/s  (2.438s,  420.07/s)  LR: 3.345e-05  Data: 0.025 (1.833)
Train: 110 [1170/1171 (100%)]  Loss:  2.740353 (2.8361)  Time: 0.568s, 1803.08/s  (2.434s,  420.72/s)  LR: 3.345e-05  Data: 0.000 (1.829)
Test: [   0/97]  Time: 13.369 (13.369)  Loss:  0.3046 (0.3046)  Acc@1: 96.5820 (96.5820)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.198 (3.097)  Loss:  0.4588 (0.3793)  Acc@1: 92.7734 (94.9946)  Acc@5: 98.5352 (98.9105)
Test: [  97/97]  Time: 0.120 (3.019)  Loss:  0.3370 (0.3910)  Acc@1: 94.4940 (94.4270)  Acc@5: 99.4048 (98.7390)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-109.pth.tar', 94.43800004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-110.pth.tar', 94.42700000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-108.pth.tar', 94.35899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-107.pth.tar', 94.32500004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-105.pth.tar', 94.32200003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-106.pth.tar', 94.21200000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-103.pth.tar', 94.15300004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-104.pth.tar', 94.12199999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-102.pth.tar', 94.11500002685547)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-101.pth.tar', 94.10000004638673)

Train: 111 [   0/1171 (  0%)]  Loss:  2.863174 (2.8632)  Time: 9.934s,  103.08/s  (9.934s,  103.08/s)  LR: 2.973e-05  Data: 9.344 (9.344)
Train: 111 [  50/1171 (  4%)]  Loss:  2.158481 (2.5108)  Time: 0.588s, 1740.70/s  (2.292s,  446.83/s)  LR: 2.973e-05  Data: 0.019 (1.709)
Train: 111 [ 100/1171 (  9%)]  Loss:  2.921706 (2.6478)  Time: 1.856s,  551.67/s  (2.232s,  458.75/s)  LR: 2.973e-05  Data: 1.286 (1.643)
Train: 111 [ 150/1171 ( 13%)]  Loss:  2.786993 (2.6826)  Time: 1.051s,  974.40/s  (2.324s,  440.55/s)  LR: 2.973e-05  Data: 0.469 (1.725)
Train: 111 [ 200/1171 ( 17%)]  Loss:  2.840636 (2.7142)  Time: 0.810s, 1263.89/s  (2.373s,  431.45/s)  LR: 2.973e-05  Data: 0.245 (1.770)
Train: 111 [ 250/1171 ( 21%)]  Loss:  2.422601 (2.6656)  Time: 0.587s, 1743.28/s  (2.377s,  430.77/s)  LR: 2.973e-05  Data: 0.019 (1.770)
Train: 111 [ 300/1171 ( 26%)]  Loss:  3.015650 (2.7156)  Time: 2.530s,  404.81/s  (2.393s,  427.91/s)  LR: 2.973e-05  Data: 1.890 (1.789)
Train: 111 [ 350/1171 ( 30%)]  Loss:  2.951636 (2.7451)  Time: 0.587s, 1743.92/s  (2.372s,  431.69/s)  LR: 2.973e-05  Data: 0.019 (1.764)
Train: 111 [ 400/1171 ( 34%)]  Loss:  2.295769 (2.6952)  Time: 2.809s,  364.48/s  (2.366s,  432.76/s)  LR: 2.973e-05  Data: 2.138 (1.758)
Train: 111 [ 450/1171 ( 38%)]  Loss:  3.286724 (2.7543)  Time: 0.586s, 1748.92/s  (2.343s,  436.96/s)  LR: 2.973e-05  Data: 0.020 (1.736)
Train: 111 [ 500/1171 ( 43%)]  Loss:  3.541615 (2.8259)  Time: 0.973s, 1052.27/s  (2.347s,  436.23/s)  LR: 2.973e-05  Data: 0.408 (1.741)
Train: 111 [ 550/1171 ( 47%)]  Loss:  2.440863 (2.7938)  Time: 0.590s, 1736.09/s  (2.428s,  421.69/s)  LR: 2.973e-05  Data: 0.024 (1.821)
Train: 111 [ 600/1171 ( 51%)]  Loss:  3.373081 (2.8384)  Time: 0.588s, 1742.72/s  (2.447s,  418.50/s)  LR: 2.973e-05  Data: 0.021 (1.841)
Train: 111 [ 650/1171 ( 56%)]  Loss:  2.708645 (2.8291)  Time: 0.763s, 1342.67/s  (2.445s,  418.80/s)  LR: 2.973e-05  Data: 0.125 (1.839)
Train: 111 [ 700/1171 ( 60%)]  Loss:  3.325476 (2.8622)  Time: 1.444s,  709.34/s  (2.448s,  418.35/s)  LR: 2.973e-05  Data: 0.751 (1.842)
Train: 111 [ 750/1171 ( 64%)]  Loss:  2.893773 (2.8642)  Time: 0.587s, 1745.50/s  (2.433s,  420.82/s)  LR: 2.973e-05  Data: 0.020 (1.827)
Train: 111 [ 800/1171 ( 68%)]  Loss:  2.420104 (2.8381)  Time: 2.211s,  463.07/s  (2.428s,  421.71/s)  LR: 2.973e-05  Data: 1.646 (1.823)
Train: 111 [ 850/1171 ( 73%)]  Loss:  2.718249 (2.8314)  Time: 1.257s,  814.33/s  (2.416s,  423.93/s)  LR: 2.973e-05  Data: 0.692 (1.809)
Train: 111 [ 900/1171 ( 77%)]  Loss:  2.858114 (2.8328)  Time: 1.076s,  952.10/s  (2.434s,  420.64/s)  LR: 2.973e-05  Data: 0.424 (1.828)
Train: 111 [ 950/1171 ( 81%)]  Loss:  3.105120 (2.8464)  Time: 0.684s, 1496.08/s  (2.435s,  420.47/s)  LR: 2.973e-05  Data: 0.025 (1.830)
Train: 111 [1000/1171 ( 85%)]  Loss:  3.105459 (2.8588)  Time: 1.219s,  839.69/s  (2.437s,  420.24/s)  LR: 2.973e-05  Data: 0.491 (1.830)
Train: 111 [1050/1171 ( 90%)]  Loss:  2.710015 (2.8520)  Time: 0.900s, 1138.08/s  (2.429s,  421.64/s)  LR: 2.973e-05  Data: 0.336 (1.822)
Train: 111 [1100/1171 ( 94%)]  Loss:  3.258922 (2.8697)  Time: 0.587s, 1745.35/s  (2.429s,  421.51/s)  LR: 2.973e-05  Data: 0.020 (1.824)
Train: 111 [1150/1171 ( 98%)]  Loss:  3.272795 (2.8865)  Time: 4.883s,  209.71/s  (2.423s,  422.59/s)  LR: 2.973e-05  Data: 4.302 (1.818)
Train: 111 [1170/1171 (100%)]  Loss:  2.895173 (2.8868)  Time: 0.566s, 1810.73/s  (2.418s,  423.40/s)  LR: 2.973e-05  Data: 0.000 (1.813)
Test: [   0/97]  Time: 13.050 (13.050)  Loss:  0.3113 (0.3113)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (2.983)  Loss:  0.4977 (0.3852)  Acc@1: 91.8945 (94.9449)  Acc@5: 98.1445 (98.8856)
Test: [  97/97]  Time: 0.120 (3.094)  Loss:  0.3559 (0.3962)  Acc@1: 94.4940 (94.4160)  Acc@5: 99.1071 (98.7130)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-109.pth.tar', 94.43800004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-110.pth.tar', 94.42700000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-111.pth.tar', 94.41600000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-108.pth.tar', 94.35899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-107.pth.tar', 94.32500004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-105.pth.tar', 94.32200003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-106.pth.tar', 94.21200000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-103.pth.tar', 94.15300004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-104.pth.tar', 94.12199999755859)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-102.pth.tar', 94.11500002685547)

Train: 112 [   0/1171 (  0%)]  Loss:  2.432471 (2.4325)  Time: 11.789s,   86.86/s  (11.789s,   86.86/s)  LR: 2.632e-05  Data: 11.159 (11.159)
Train: 112 [  50/1171 (  4%)]  Loss:  3.180108 (2.8063)  Time: 0.590s, 1736.48/s  (2.473s,  414.03/s)  LR: 2.632e-05  Data: 0.023 (1.881)
Train: 112 [ 100/1171 (  9%)]  Loss:  3.245370 (2.9526)  Time: 2.035s,  503.13/s  (2.641s,  387.77/s)  LR: 2.632e-05  Data: 1.449 (2.043)
Train: 112 [ 150/1171 ( 13%)]  Loss:  2.584988 (2.8607)  Time: 0.772s, 1327.20/s  (2.528s,  405.09/s)  LR: 2.632e-05  Data: 0.170 (1.926)
Train: 112 [ 200/1171 ( 17%)]  Loss:  3.052811 (2.8991)  Time: 1.305s,  784.95/s  (2.490s,  411.27/s)  LR: 2.632e-05  Data: 0.677 (1.893)
Train: 112 [ 250/1171 ( 21%)]  Loss:  3.180720 (2.9461)  Time: 0.588s, 1742.71/s  (2.416s,  423.82/s)  LR: 2.632e-05  Data: 0.021 (1.821)
Train: 112 [ 300/1171 ( 26%)]  Loss:  3.151516 (2.9754)  Time: 0.586s, 1747.17/s  (2.402s,  426.24/s)  LR: 2.632e-05  Data: 0.020 (1.808)
Train: 112 [ 350/1171 ( 30%)]  Loss:  2.797448 (2.9532)  Time: 1.144s,  895.10/s  (2.436s,  420.35/s)  LR: 2.632e-05  Data: 0.576 (1.841)
Train: 112 [ 400/1171 ( 34%)]  Loss:  2.953763 (2.9532)  Time: 0.584s, 1753.00/s  (2.449s,  418.11/s)  LR: 2.632e-05  Data: 0.018 (1.854)
Train: 112 [ 450/1171 ( 38%)]  Loss:  2.873563 (2.9453)  Time: 0.586s, 1748.34/s  (2.439s,  419.93/s)  LR: 2.632e-05  Data: 0.021 (1.844)
Train: 112 [ 500/1171 ( 43%)]  Loss:  3.241338 (2.9722)  Time: 0.585s, 1751.27/s  (2.441s,  419.47/s)  LR: 2.632e-05  Data: 0.019 (1.847)
Train: 112 [ 550/1171 ( 47%)]  Loss:  3.019016 (2.9761)  Time: 0.587s, 1744.56/s  (2.444s,  418.92/s)  LR: 2.632e-05  Data: 0.021 (1.850)
Train: 112 [ 600/1171 ( 51%)]  Loss:  2.758027 (2.9593)  Time: 0.589s, 1739.62/s  (2.448s,  418.28/s)  LR: 2.632e-05  Data: 0.021 (1.855)
Train: 112 [ 650/1171 ( 56%)]  Loss:  2.868941 (2.9529)  Time: 0.596s, 1718.51/s  (2.433s,  420.91/s)  LR: 2.632e-05  Data: 0.020 (1.839)
Train: 112 [ 700/1171 ( 60%)]  Loss:  2.958494 (2.9532)  Time: 0.586s, 1748.71/s  (2.453s,  417.53/s)  LR: 2.632e-05  Data: 0.019 (1.860)
Train: 112 [ 750/1171 ( 64%)]  Loss:  3.075477 (2.9609)  Time: 0.588s, 1740.14/s  (2.437s,  420.11/s)  LR: 2.632e-05  Data: 0.019 (1.845)
Train: 112 [ 800/1171 ( 68%)]  Loss:  2.696019 (2.9453)  Time: 0.585s, 1750.36/s  (2.440s,  419.67/s)  LR: 2.632e-05  Data: 0.018 (1.848)
Train: 112 [ 850/1171 ( 73%)]  Loss:  3.135907 (2.9559)  Time: 0.590s, 1736.37/s  (2.437s,  420.11/s)  LR: 2.632e-05  Data: 0.020 (1.845)
Train: 112 [ 900/1171 ( 77%)]  Loss:  3.316088 (2.9748)  Time: 0.586s, 1748.44/s  (2.438s,  420.07/s)  LR: 2.632e-05  Data: 0.020 (1.846)
Train: 112 [ 950/1171 ( 81%)]  Loss:  3.311731 (2.9917)  Time: 0.590s, 1735.99/s  (2.426s,  422.02/s)  LR: 2.632e-05  Data: 0.024 (1.835)
Train: 112 [1000/1171 ( 85%)]  Loss:  2.844287 (2.9847)  Time: 1.205s,  850.00/s  (2.438s,  420.07/s)  LR: 2.632e-05  Data: 0.530 (1.846)
Train: 112 [1050/1171 ( 90%)]  Loss:  2.741419 (2.9736)  Time: 0.591s, 1732.67/s  (2.421s,  422.90/s)  LR: 2.632e-05  Data: 0.025 (1.830)
Train: 112 [1100/1171 ( 94%)]  Loss:  2.607651 (2.9577)  Time: 1.818s,  563.20/s  (2.434s,  420.75/s)  LR: 2.632e-05  Data: 1.233 (1.841)
Train: 112 [1150/1171 ( 98%)]  Loss:  2.942323 (2.9571)  Time: 2.748s,  372.66/s  (2.433s,  420.92/s)  LR: 2.632e-05  Data: 2.183 (1.839)
Train: 112 [1170/1171 (100%)]  Loss:  2.354401 (2.9330)  Time: 0.565s, 1811.46/s  (2.428s,  421.78/s)  LR: 2.632e-05  Data: 0.000 (1.834)
Test: [   0/97]  Time: 11.864 (11.864)  Loss:  0.2965 (0.2965)  Acc@1: 96.4844 (96.4844)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (3.048)  Loss:  0.4672 (0.3679)  Acc@1: 92.3828 (94.9889)  Acc@5: 98.4375 (98.9105)
Test: [  97/97]  Time: 0.120 (2.993)  Loss:  0.3398 (0.3795)  Acc@1: 94.3452 (94.4800)  Acc@5: 99.5536 (98.7440)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-112.pth.tar', 94.4799999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-109.pth.tar', 94.43800004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-110.pth.tar', 94.42700000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-111.pth.tar', 94.41600000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-108.pth.tar', 94.35899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-107.pth.tar', 94.32500004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-105.pth.tar', 94.32200003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-106.pth.tar', 94.21200000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-103.pth.tar', 94.15300004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-104.pth.tar', 94.12199999755859)

Train: 113 [   0/1171 (  0%)]  Loss:  2.353306 (2.3533)  Time: 10.548s,   97.08/s  (10.548s,   97.08/s)  LR: 2.323e-05  Data: 9.462 (9.462)
Train: 113 [  50/1171 (  4%)]  Loss:  3.053312 (2.7033)  Time: 0.588s, 1741.37/s  (2.313s,  442.69/s)  LR: 2.323e-05  Data: 0.023 (1.709)
Train: 113 [ 100/1171 (  9%)]  Loss:  2.853823 (2.7535)  Time: 4.729s,  216.56/s  (2.272s,  450.79/s)  LR: 2.323e-05  Data: 4.161 (1.668)
Train: 113 [ 150/1171 ( 13%)]  Loss:  2.341295 (2.6504)  Time: 0.587s, 1743.93/s  (2.308s,  443.69/s)  LR: 2.323e-05  Data: 0.022 (1.704)
Train: 113 [ 200/1171 ( 17%)]  Loss:  2.622992 (2.6449)  Time: 6.304s,  162.44/s  (2.329s,  439.68/s)  LR: 2.323e-05  Data: 5.590 (1.724)
Train: 113 [ 250/1171 ( 21%)]  Loss:  2.728208 (2.6588)  Time: 0.587s, 1745.29/s  (2.329s,  439.71/s)  LR: 2.323e-05  Data: 0.021 (1.724)
Train: 113 [ 300/1171 ( 26%)]  Loss:  3.048997 (2.7146)  Time: 7.718s,  132.67/s  (2.354s,  435.08/s)  LR: 2.323e-05  Data: 7.131 (1.750)
Train: 113 [ 350/1171 ( 30%)]  Loss:  3.200404 (2.7753)  Time: 0.588s, 1742.81/s  (2.336s,  438.36/s)  LR: 2.323e-05  Data: 0.021 (1.735)
Train: 113 [ 400/1171 ( 34%)]  Loss:  3.237489 (2.8266)  Time: 7.115s,  143.92/s  (2.348s,  436.07/s)  LR: 2.323e-05  Data: 6.398 (1.746)
Train: 113 [ 450/1171 ( 38%)]  Loss:  2.996846 (2.8437)  Time: 0.585s, 1750.11/s  (2.341s,  437.38/s)  LR: 2.323e-05  Data: 0.019 (1.741)
Train: 113 [ 500/1171 ( 43%)]  Loss:  2.498008 (2.8122)  Time: 8.104s,  126.36/s  (2.349s,  435.84/s)  LR: 2.323e-05  Data: 7.518 (1.752)
Train: 113 [ 550/1171 ( 47%)]  Loss:  3.025045 (2.8300)  Time: 0.586s, 1746.65/s  (2.383s,  429.76/s)  LR: 2.323e-05  Data: 0.020 (1.787)
Train: 113 [ 600/1171 ( 51%)]  Loss:  2.128195 (2.7760)  Time: 8.337s,  122.82/s  (2.445s,  418.82/s)  LR: 2.323e-05  Data: 7.770 (1.848)
Train: 113 [ 650/1171 ( 56%)]  Loss:  2.656642 (2.7675)  Time: 0.587s, 1745.86/s  (2.445s,  418.88/s)  LR: 2.323e-05  Data: 0.020 (1.849)
Train: 113 [ 700/1171 ( 60%)]  Loss:  2.425773 (2.7447)  Time: 7.450s,  137.45/s  (2.449s,  418.12/s)  LR: 2.323e-05  Data: 6.844 (1.854)
Train: 113 [ 750/1171 ( 64%)]  Loss:  2.526903 (2.7311)  Time: 0.586s, 1748.67/s  (2.435s,  420.45/s)  LR: 2.323e-05  Data: 0.019 (1.842)
Train: 113 [ 800/1171 ( 68%)]  Loss:  3.036211 (2.7490)  Time: 6.936s,  147.64/s  (2.428s,  421.78/s)  LR: 2.323e-05  Data: 6.298 (1.834)
Train: 113 [ 850/1171 ( 73%)]  Loss:  2.723103 (2.7476)  Time: 0.587s, 1744.66/s  (2.418s,  423.57/s)  LR: 2.323e-05  Data: 0.020 (1.825)
Train: 113 [ 900/1171 ( 77%)]  Loss:  3.436465 (2.7838)  Time: 7.525s,  136.07/s  (2.435s,  420.51/s)  LR: 2.323e-05  Data: 6.929 (1.843)
Train: 113 [ 950/1171 ( 81%)]  Loss:  3.161406 (2.8027)  Time: 0.588s, 1740.90/s  (2.431s,  421.15/s)  LR: 2.323e-05  Data: 0.024 (1.839)
Train: 113 [1000/1171 ( 85%)]  Loss:  2.806347 (2.8029)  Time: 6.757s,  151.54/s  (2.431s,  421.28/s)  LR: 2.323e-05  Data: 6.068 (1.838)
Train: 113 [1050/1171 ( 90%)]  Loss:  2.926232 (2.8085)  Time: 0.590s, 1736.01/s  (2.423s,  422.54/s)  LR: 2.323e-05  Data: 0.024 (1.831)
Train: 113 [1100/1171 ( 94%)]  Loss:  2.937637 (2.8141)  Time: 8.081s,  126.71/s  (2.426s,  422.17/s)  LR: 2.323e-05  Data: 7.234 (1.833)
Train: 113 [1150/1171 ( 98%)]  Loss:  2.717522 (2.8101)  Time: 0.585s, 1749.65/s  (2.417s,  423.62/s)  LR: 2.323e-05  Data: 0.019 (1.825)
Train: 113 [1170/1171 (100%)]  Loss:  3.177767 (2.8248)  Time: 0.565s, 1811.37/s  (2.414s,  424.15/s)  LR: 2.323e-05  Data: 0.000 (1.823)
Test: [   0/97]  Time: 12.058 (12.058)  Loss:  0.3054 (0.3054)  Acc@1: 96.3867 (96.3867)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (2.978)  Loss:  0.4590 (0.3740)  Acc@1: 92.3828 (95.0042)  Acc@5: 98.4375 (98.9124)
Test: [  97/97]  Time: 0.120 (3.091)  Loss:  0.3517 (0.3863)  Acc@1: 95.0893 (94.5170)  Acc@5: 99.1071 (98.7540)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-113.pth.tar', 94.51700000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-112.pth.tar', 94.4799999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-109.pth.tar', 94.43800004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-110.pth.tar', 94.42700000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-111.pth.tar', 94.41600000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-108.pth.tar', 94.35899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-107.pth.tar', 94.32500004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-105.pth.tar', 94.32200003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-106.pth.tar', 94.21200000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-103.pth.tar', 94.15300004638672)

Train: 114 [   0/1171 (  0%)]  Loss:  3.232951 (3.2330)  Time: 10.956s,   93.46/s  (10.956s,   93.46/s)  LR: 2.047e-05  Data: 10.381 (10.381)
Train: 114 [  50/1171 (  4%)]  Loss:  2.417324 (2.8251)  Time: 1.765s,  580.11/s  (2.524s,  405.70/s)  LR: 2.047e-05  Data: 1.145 (1.933)
Train: 114 [ 100/1171 (  9%)]  Loss:  2.621618 (2.7573)  Time: 2.327s,  440.10/s  (2.449s,  418.07/s)  LR: 2.047e-05  Data: 1.762 (1.849)
Train: 114 [ 150/1171 ( 13%)]  Loss:  2.813798 (2.7714)  Time: 1.900s,  539.05/s  (2.504s,  408.91/s)  LR: 2.047e-05  Data: 1.208 (1.891)
Train: 114 [ 200/1171 ( 17%)]  Loss:  2.614245 (2.7400)  Time: 0.590s, 1736.97/s  (2.451s,  417.78/s)  LR: 2.047e-05  Data: 0.022 (1.835)
Train: 114 [ 250/1171 ( 21%)]  Loss:  2.558186 (2.7097)  Time: 0.960s, 1067.05/s  (2.408s,  425.25/s)  LR: 2.047e-05  Data: 0.376 (1.797)
Train: 114 [ 300/1171 ( 26%)]  Loss:  2.729704 (2.7125)  Time: 0.589s, 1739.95/s  (2.388s,  428.73/s)  LR: 2.047e-05  Data: 0.023 (1.779)
Train: 114 [ 350/1171 ( 30%)]  Loss:  2.667454 (2.7069)  Time: 0.589s, 1739.05/s  (2.405s,  425.76/s)  LR: 2.047e-05  Data: 0.018 (1.798)
Train: 114 [ 400/1171 ( 34%)]  Loss:  2.502549 (2.6842)  Time: 0.593s, 1726.38/s  (2.410s,  424.90/s)  LR: 2.047e-05  Data: 0.021 (1.806)
Train: 114 [ 450/1171 ( 38%)]  Loss:  2.896883 (2.7055)  Time: 2.144s,  477.66/s  (2.396s,  427.40/s)  LR: 2.047e-05  Data: 1.480 (1.793)
Train: 114 [ 500/1171 ( 43%)]  Loss:  2.734912 (2.7081)  Time: 0.587s, 1743.97/s  (2.409s,  425.04/s)  LR: 2.047e-05  Data: 0.020 (1.807)
Train: 114 [ 550/1171 ( 47%)]  Loss:  3.083934 (2.7395)  Time: 2.988s,  342.67/s  (2.417s,  423.72/s)  LR: 2.047e-05  Data: 2.363 (1.814)
Train: 114 [ 600/1171 ( 51%)]  Loss:  2.655417 (2.7330)  Time: 0.588s, 1742.46/s  (2.417s,  423.61/s)  LR: 2.047e-05  Data: 0.021 (1.815)
Train: 114 [ 650/1171 ( 56%)]  Loss:  2.741857 (2.7336)  Time: 3.269s,  313.29/s  (2.408s,  425.19/s)  LR: 2.047e-05  Data: 2.609 (1.807)
Train: 114 [ 700/1171 ( 60%)]  Loss:  3.104734 (2.7584)  Time: 0.588s, 1740.66/s  (2.395s,  427.49/s)  LR: 2.047e-05  Data: 0.021 (1.793)
Train: 114 [ 750/1171 ( 64%)]  Loss:  2.522004 (2.7436)  Time: 3.571s,  286.73/s  (2.416s,  423.89/s)  LR: 2.047e-05  Data: 2.742 (1.812)
Train: 114 [ 800/1171 ( 68%)]  Loss:  2.585092 (2.7343)  Time: 0.588s, 1741.78/s  (2.417s,  423.63/s)  LR: 2.047e-05  Data: 0.022 (1.812)
Train: 114 [ 850/1171 ( 73%)]  Loss:  2.689037 (2.7318)  Time: 7.796s,  131.34/s  (2.417s,  423.61/s)  LR: 2.047e-05  Data: 7.086 (1.811)
Train: 114 [ 900/1171 ( 77%)]  Loss:  2.584896 (2.7240)  Time: 0.588s, 1741.79/s  (2.412s,  424.52/s)  LR: 2.047e-05  Data: 0.020 (1.806)
Train: 114 [ 950/1171 ( 81%)]  Loss:  2.612566 (2.7185)  Time: 4.779s,  214.28/s  (2.407s,  425.41/s)  LR: 2.047e-05  Data: 4.199 (1.801)
Train: 114 [1000/1171 ( 85%)]  Loss:  2.415266 (2.7040)  Time: 0.587s, 1743.99/s  (2.399s,  426.84/s)  LR: 2.047e-05  Data: 0.021 (1.793)
Train: 114 [1050/1171 ( 90%)]  Loss:  2.732036 (2.7053)  Time: 2.938s,  348.52/s  (2.403s,  426.09/s)  LR: 2.047e-05  Data: 2.373 (1.797)
Train: 114 [1100/1171 ( 94%)]  Loss:  3.162176 (2.7252)  Time: 0.587s, 1745.88/s  (2.420s,  423.21/s)  LR: 2.047e-05  Data: 0.020 (1.814)
Train: 114 [1150/1171 ( 98%)]  Loss:  2.713603 (2.7247)  Time: 7.939s,  128.98/s  (2.418s,  423.42/s)  LR: 2.047e-05  Data: 7.257 (1.813)
Train: 114 [1170/1171 (100%)]  Loss:  2.707339 (2.7240)  Time: 0.565s, 1811.69/s  (2.414s,  424.27/s)  LR: 2.047e-05  Data: 0.000 (1.809)
Test: [   0/97]  Time: 13.003 (13.003)  Loss:  0.3169 (0.3169)  Acc@1: 96.1914 (96.1914)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (3.137)  Loss:  0.4609 (0.3857)  Acc@1: 92.4805 (95.0636)  Acc@5: 98.4375 (98.8932)
Test: [  97/97]  Time: 0.120 (3.034)  Loss:  0.3408 (0.3974)  Acc@1: 95.5357 (94.4860)  Acc@5: 99.2560 (98.7220)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-113.pth.tar', 94.51700000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-114.pth.tar', 94.48600004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-112.pth.tar', 94.4799999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-109.pth.tar', 94.43800004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-110.pth.tar', 94.42700000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-111.pth.tar', 94.41600000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-108.pth.tar', 94.35899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-107.pth.tar', 94.32500004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-105.pth.tar', 94.32200003173828)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-106.pth.tar', 94.21200000976563)

Train: 115 [   0/1171 (  0%)]  Loss:  2.673394 (2.6734)  Time: 10.745s,   95.30/s  (10.745s,   95.30/s)  LR: 1.802e-05  Data: 9.492 (9.492)
Train: 115 [  50/1171 (  4%)]  Loss:  3.124390 (2.8989)  Time: 0.586s, 1747.24/s  (2.323s,  440.78/s)  LR: 1.802e-05  Data: 0.020 (1.713)
Train: 115 [ 100/1171 (  9%)]  Loss:  2.822139 (2.8733)  Time: 0.586s, 1747.49/s  (2.295s,  446.20/s)  LR: 1.802e-05  Data: 0.020 (1.693)
Train: 115 [ 150/1171 ( 13%)]  Loss:  2.864640 (2.8711)  Time: 0.590s, 1736.08/s  (2.219s,  461.47/s)  LR: 1.802e-05  Data: 0.022 (1.619)
Train: 115 [ 200/1171 ( 17%)]  Loss:  2.937155 (2.8843)  Time: 3.248s,  315.28/s  (2.342s,  437.28/s)  LR: 1.802e-05  Data: 2.583 (1.742)
Train: 115 [ 250/1171 ( 21%)]  Loss:  3.183569 (2.9342)  Time: 0.622s, 1645.36/s  (2.348s,  436.10/s)  LR: 1.802e-05  Data: 0.023 (1.743)
Train: 115 [ 300/1171 ( 26%)]  Loss:  2.849116 (2.9221)  Time: 0.589s, 1739.06/s  (2.365s,  433.02/s)  LR: 1.802e-05  Data: 0.024 (1.761)
Train: 115 [ 350/1171 ( 30%)]  Loss:  3.178094 (2.9541)  Time: 0.588s, 1741.58/s  (2.350s,  435.75/s)  LR: 1.802e-05  Data: 0.022 (1.749)
Train: 115 [ 400/1171 ( 34%)]  Loss:  3.252173 (2.9872)  Time: 0.862s, 1188.31/s  (2.363s,  433.32/s)  LR: 1.802e-05  Data: 0.297 (1.765)
Train: 115 [ 450/1171 ( 38%)]  Loss:  3.116970 (3.0002)  Time: 0.586s, 1747.21/s  (2.350s,  435.71/s)  LR: 1.802e-05  Data: 0.020 (1.751)
Train: 115 [ 500/1171 ( 43%)]  Loss:  3.304881 (3.0279)  Time: 0.589s, 1738.38/s  (2.359s,  434.05/s)  LR: 1.802e-05  Data: 0.023 (1.761)
Train: 115 [ 550/1171 ( 47%)]  Loss:  2.781263 (3.0073)  Time: 0.590s, 1736.55/s  (2.388s,  428.77/s)  LR: 1.802e-05  Data: 0.021 (1.791)
Train: 115 [ 600/1171 ( 51%)]  Loss:  3.049764 (3.0106)  Time: 0.846s, 1210.96/s  (2.414s,  424.18/s)  LR: 1.802e-05  Data: 0.270 (1.818)
Train: 115 [ 650/1171 ( 56%)]  Loss:  3.019871 (3.0112)  Time: 0.591s, 1731.98/s  (2.445s,  418.85/s)  LR: 1.802e-05  Data: 0.023 (1.849)
Train: 115 [ 700/1171 ( 60%)]  Loss:  2.738962 (2.9931)  Time: 1.331s,  769.52/s  (2.445s,  418.77/s)  LR: 1.802e-05  Data: 0.767 (1.850)
Train: 115 [ 750/1171 ( 64%)]  Loss:  3.014747 (2.9944)  Time: 0.588s, 1742.75/s  (2.433s,  420.87/s)  LR: 1.802e-05  Data: 0.020 (1.838)
Train: 115 [ 800/1171 ( 68%)]  Loss:  2.550861 (2.9684)  Time: 0.589s, 1739.57/s  (2.431s,  421.23/s)  LR: 1.802e-05  Data: 0.021 (1.836)
Train: 115 [ 850/1171 ( 73%)]  Loss:  2.555302 (2.9454)  Time: 0.586s, 1748.83/s  (2.415s,  423.97/s)  LR: 1.802e-05  Data: 0.020 (1.820)
Train: 115 [ 900/1171 ( 77%)]  Loss:  3.255448 (2.9617)  Time: 0.593s, 1728.20/s  (2.421s,  422.94/s)  LR: 1.802e-05  Data: 0.018 (1.827)
Train: 115 [ 950/1171 ( 81%)]  Loss:  2.800992 (2.9537)  Time: 0.589s, 1739.47/s  (2.419s,  423.29/s)  LR: 1.802e-05  Data: 0.017 (1.825)
Train: 115 [1000/1171 ( 85%)]  Loss:  2.975295 (2.9547)  Time: 1.744s,  587.04/s  (2.430s,  421.43/s)  LR: 1.802e-05  Data: 1.036 (1.835)
Train: 115 [1050/1171 ( 90%)]  Loss:  3.110320 (2.9618)  Time: 0.589s, 1738.34/s  (2.426s,  422.12/s)  LR: 1.802e-05  Data: 0.019 (1.831)
Train: 115 [1100/1171 ( 94%)]  Loss:  3.175076 (2.9711)  Time: 0.591s, 1731.75/s  (2.429s,  421.55/s)  LR: 1.802e-05  Data: 0.020 (1.835)
Train: 115 [1150/1171 ( 98%)]  Loss:  2.814388 (2.9645)  Time: 0.588s, 1741.78/s  (2.420s,  423.16/s)  LR: 1.802e-05  Data: 0.019 (1.825)
Train: 115 [1170/1171 (100%)]  Loss:  3.245502 (2.9758)  Time: 0.565s, 1812.12/s  (2.417s,  423.69/s)  LR: 1.802e-05  Data: 0.000 (1.823)
Test: [   0/97]  Time: 13.376 (13.376)  Loss:  0.2881 (0.2881)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.201 (3.027)  Loss:  0.4538 (0.3587)  Acc@1: 92.4805 (95.1804)  Acc@5: 98.4375 (98.9239)
Test: [  97/97]  Time: 0.120 (3.064)  Loss:  0.3315 (0.3705)  Acc@1: 94.4940 (94.5760)  Acc@5: 99.2560 (98.7560)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-115.pth.tar', 94.57600000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-113.pth.tar', 94.51700000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-114.pth.tar', 94.48600004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-112.pth.tar', 94.4799999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-109.pth.tar', 94.43800004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-110.pth.tar', 94.42700000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-111.pth.tar', 94.41600000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-108.pth.tar', 94.35899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-107.pth.tar', 94.32500004638672)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-105.pth.tar', 94.32200003173828)

Train: 116 [   0/1171 (  0%)]  Loss:  3.032081 (3.0321)  Time: 11.150s,   91.84/s  (11.150s,   91.84/s)  LR: 1.590e-05  Data: 9.964 (9.964)
Train: 116 [  50/1171 (  4%)]  Loss:  3.121336 (3.0767)  Time: 3.055s,  335.15/s  (2.530s,  404.75/s)  LR: 1.590e-05  Data: 2.383 (1.895)
Train: 116 [ 100/1171 (  9%)]  Loss:  2.752890 (2.9688)  Time: 1.039s,  985.94/s  (2.432s,  421.00/s)  LR: 1.590e-05  Data: 0.418 (1.803)
Train: 116 [ 150/1171 ( 13%)]  Loss:  2.667663 (2.8935)  Time: 2.663s,  384.59/s  (2.391s,  428.27/s)  LR: 1.590e-05  Data: 2.008 (1.765)
Train: 116 [ 200/1171 ( 17%)]  Loss:  2.676105 (2.8500)  Time: 6.280s,  163.06/s  (2.445s,  418.75/s)  LR: 1.590e-05  Data: 5.715 (1.823)
Train: 116 [ 250/1171 ( 21%)]  Loss:  2.831563 (2.8469)  Time: 0.588s, 1740.50/s  (2.419s,  423.25/s)  LR: 1.590e-05  Data: 0.018 (1.799)
Train: 116 [ 300/1171 ( 26%)]  Loss:  2.937829 (2.8599)  Time: 5.379s,  190.38/s  (2.393s,  427.93/s)  LR: 1.590e-05  Data: 4.654 (1.776)
Train: 116 [ 350/1171 ( 30%)]  Loss:  2.871206 (2.8613)  Time: 0.586s, 1746.22/s  (2.365s,  432.98/s)  LR: 1.590e-05  Data: 0.020 (1.752)
Train: 116 [ 400/1171 ( 34%)]  Loss:  3.071990 (2.8847)  Time: 7.086s,  144.51/s  (2.414s,  424.24/s)  LR: 1.590e-05  Data: 6.357 (1.801)
Train: 116 [ 450/1171 ( 38%)]  Loss:  3.289410 (2.9252)  Time: 0.588s, 1740.29/s  (2.420s,  423.20/s)  LR: 1.590e-05  Data: 0.022 (1.807)
Train: 116 [ 500/1171 ( 43%)]  Loss:  2.902752 (2.9232)  Time: 2.769s,  369.77/s  (2.428s,  421.82/s)  LR: 1.590e-05  Data: 2.204 (1.813)
Train: 116 [ 550/1171 ( 47%)]  Loss:  2.712332 (2.9056)  Time: 0.589s, 1739.12/s  (2.443s,  419.10/s)  LR: 1.590e-05  Data: 0.020 (1.830)
Train: 116 [ 600/1171 ( 51%)]  Loss:  2.865328 (2.9025)  Time: 4.329s,  236.53/s  (2.447s,  418.49/s)  LR: 1.590e-05  Data: 3.681 (1.835)
Train: 116 [ 650/1171 ( 56%)]  Loss:  2.850370 (2.8988)  Time: 0.590s, 1735.45/s  (2.440s,  419.72/s)  LR: 1.590e-05  Data: 0.019 (1.828)
Train: 116 [ 700/1171 ( 60%)]  Loss:  2.975388 (2.9039)  Time: 3.346s,  306.04/s  (2.423s,  422.69/s)  LR: 1.590e-05  Data: 2.652 (1.812)
Train: 116 [ 750/1171 ( 64%)]  Loss:  2.938178 (2.9060)  Time: 0.589s, 1739.94/s  (2.446s,  418.68/s)  LR: 1.590e-05  Data: 0.020 (1.834)
Train: 116 [ 800/1171 ( 68%)]  Loss:  3.140158 (2.9198)  Time: 4.800s,  213.35/s  (2.432s,  420.98/s)  LR: 1.590e-05  Data: 4.156 (1.821)
Train: 116 [ 850/1171 ( 73%)]  Loss:  3.391941 (2.9460)  Time: 0.587s, 1743.41/s  (2.429s,  421.53/s)  LR: 1.590e-05  Data: 0.021 (1.817)
Train: 116 [ 900/1171 ( 77%)]  Loss:  2.972654 (2.9474)  Time: 4.693s,  218.19/s  (2.424s,  422.50/s)  LR: 1.590e-05  Data: 4.030 (1.812)
Train: 116 [ 950/1171 ( 81%)]  Loss:  2.654767 (2.9328)  Time: 0.588s, 1741.08/s  (2.414s,  424.18/s)  LR: 1.590e-05  Data: 0.021 (1.802)
Train: 116 [1000/1171 ( 85%)]  Loss:  3.081890 (2.9399)  Time: 5.734s,  178.58/s  (2.407s,  425.43/s)  LR: 1.590e-05  Data: 5.031 (1.796)
Train: 116 [1050/1171 ( 90%)]  Loss:  2.759293 (2.9317)  Time: 0.586s, 1747.17/s  (2.395s,  427.54/s)  LR: 1.590e-05  Data: 0.019 (1.784)
Train: 116 [1100/1171 ( 94%)]  Loss:  2.659414 (2.9198)  Time: 8.263s,  123.92/s  (2.414s,  424.26/s)  LR: 1.590e-05  Data: 7.680 (1.803)
Train: 116 [1150/1171 ( 98%)]  Loss:  3.236104 (2.9330)  Time: 0.586s, 1747.53/s  (2.414s,  424.24/s)  LR: 1.590e-05  Data: 0.019 (1.802)
Train: 116 [1170/1171 (100%)]  Loss:  2.931418 (2.9330)  Time: 0.566s, 1808.94/s  (2.408s,  425.18/s)  LR: 1.590e-05  Data: 0.000 (1.797)
Test: [   0/97]  Time: 11.331 (11.331)  Loss:  0.2872 (0.2872)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.197 (3.112)  Loss:  0.4370 (0.3536)  Acc@1: 92.9688 (95.1363)  Acc@5: 98.4375 (98.9354)
Test: [  97/97]  Time: 0.121 (3.053)  Loss:  0.3246 (0.3669)  Acc@1: 94.4940 (94.5560)  Acc@5: 99.2560 (98.7710)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-115.pth.tar', 94.57600000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-116.pth.tar', 94.55600000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-113.pth.tar', 94.51700000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-114.pth.tar', 94.48600004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-112.pth.tar', 94.4799999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-109.pth.tar', 94.43800004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-110.pth.tar', 94.42700000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-111.pth.tar', 94.41600000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-108.pth.tar', 94.35899998535156)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-107.pth.tar', 94.32500004638672)

Train: 117 [   0/1171 (  0%)]  Loss:  2.910988 (2.9110)  Time: 10.936s,   93.63/s  (10.936s,   93.63/s)  LR: 1.410e-05  Data: 10.222 (10.222)
Train: 117 [  50/1171 (  4%)]  Loss:  2.970531 (2.9408)  Time: 0.589s, 1739.33/s  (2.358s,  434.30/s)  LR: 1.410e-05  Data: 0.022 (1.763)
Train: 117 [ 100/1171 (  9%)]  Loss:  3.141684 (3.0077)  Time: 0.592s, 1730.91/s  (2.264s,  452.28/s)  LR: 1.410e-05  Data: 0.020 (1.661)
Train: 117 [ 150/1171 ( 13%)]  Loss:  2.926764 (2.9875)  Time: 0.590s, 1735.67/s  (2.201s,  465.21/s)  LR: 1.410e-05  Data: 0.024 (1.593)
Train: 117 [ 200/1171 ( 17%)]  Loss:  2.447613 (2.8795)  Time: 0.585s, 1749.35/s  (2.299s,  445.43/s)  LR: 1.410e-05  Data: 0.018 (1.693)
Train: 117 [ 250/1171 ( 21%)]  Loss:  3.118705 (2.9194)  Time: 0.589s, 1738.58/s  (2.294s,  446.38/s)  LR: 1.410e-05  Data: 0.022 (1.689)
Train: 117 [ 300/1171 ( 26%)]  Loss:  2.580104 (2.8709)  Time: 0.590s, 1736.74/s  (2.310s,  443.32/s)  LR: 1.410e-05  Data: 0.020 (1.706)
Train: 117 [ 350/1171 ( 30%)]  Loss:  2.596774 (2.8366)  Time: 0.591s, 1733.72/s  (2.290s,  447.12/s)  LR: 1.410e-05  Data: 0.022 (1.690)
Train: 117 [ 400/1171 ( 34%)]  Loss:  3.166997 (2.8734)  Time: 0.583s, 1756.70/s  (2.298s,  445.54/s)  LR: 1.410e-05  Data: 0.019 (1.698)
Train: 117 [ 450/1171 ( 38%)]  Loss:  3.184781 (2.9045)  Time: 0.595s, 1719.74/s  (2.296s,  446.00/s)  LR: 1.410e-05  Data: 0.024 (1.695)
Train: 117 [ 500/1171 ( 43%)]  Loss:  2.444629 (2.8627)  Time: 0.588s, 1741.33/s  (2.306s,  444.07/s)  LR: 1.410e-05  Data: 0.021 (1.707)
Train: 117 [ 550/1171 ( 47%)]  Loss:  2.672932 (2.8469)  Time: 0.585s, 1750.77/s  (2.299s,  445.32/s)  LR: 1.410e-05  Data: 0.019 (1.702)
Train: 117 [ 600/1171 ( 51%)]  Loss:  3.041821 (2.8619)  Time: 0.585s, 1749.60/s  (2.351s,  435.64/s)  LR: 1.410e-05  Data: 0.019 (1.754)
Train: 117 [ 650/1171 ( 56%)]  Loss:  2.885326 (2.8635)  Time: 0.587s, 1743.99/s  (2.341s,  437.50/s)  LR: 1.410e-05  Data: 0.022 (1.744)
Train: 117 [ 700/1171 ( 60%)]  Loss:  3.302848 (2.8928)  Time: 0.587s, 1744.99/s  (2.387s,  429.05/s)  LR: 1.410e-05  Data: 0.022 (1.791)
Train: 117 [ 750/1171 ( 64%)]  Loss:  2.841714 (2.8896)  Time: 0.585s, 1751.09/s  (2.373s,  431.48/s)  LR: 1.410e-05  Data: 0.019 (1.778)
Train: 117 [ 800/1171 ( 68%)]  Loss:  2.658117 (2.8760)  Time: 0.586s, 1747.33/s  (2.373s,  431.60/s)  LR: 1.410e-05  Data: 0.019 (1.776)
Train: 117 [ 850/1171 ( 73%)]  Loss:  2.756436 (2.8694)  Time: 0.592s, 1728.54/s  (2.364s,  433.15/s)  LR: 1.410e-05  Data: 0.020 (1.768)
Train: 117 [ 900/1171 ( 77%)]  Loss:  3.208169 (2.8872)  Time: 0.589s, 1738.31/s  (2.359s,  434.15/s)  LR: 1.410e-05  Data: 0.023 (1.763)
Train: 117 [ 950/1171 ( 81%)]  Loss:  2.664975 (2.8761)  Time: 0.587s, 1743.13/s  (2.368s,  432.43/s)  LR: 1.410e-05  Data: 0.022 (1.773)
Train: 117 [1000/1171 ( 85%)]  Loss:  2.764895 (2.8708)  Time: 0.588s, 1740.38/s  (2.370s,  432.08/s)  LR: 1.410e-05  Data: 0.022 (1.775)
Train: 117 [1050/1171 ( 90%)]  Loss:  2.536176 (2.8556)  Time: 0.585s, 1749.24/s  (2.367s,  432.59/s)  LR: 1.410e-05  Data: 0.020 (1.773)
Train: 117 [1100/1171 ( 94%)]  Loss:  2.860749 (2.8558)  Time: 0.588s, 1742.48/s  (2.366s,  432.79/s)  LR: 1.410e-05  Data: 0.021 (1.772)
Train: 117 [1150/1171 ( 98%)]  Loss:  2.741225 (2.8510)  Time: 0.591s, 1732.28/s  (2.361s,  433.67/s)  LR: 1.410e-05  Data: 0.022 (1.767)
Train: 117 [1170/1171 (100%)]  Loss:  2.718898 (2.8458)  Time: 0.566s, 1808.54/s  (2.361s,  433.72/s)  LR: 1.410e-05  Data: 0.000 (1.767)
Test: [   0/97]  Time: 12.278 (12.278)  Loss:  0.2968 (0.2968)  Acc@1: 96.4844 (96.4844)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (2.953)  Loss:  0.4562 (0.3621)  Acc@1: 92.4805 (95.1287)  Acc@5: 98.3398 (98.9220)
Test: [  97/97]  Time: 0.120 (2.896)  Loss:  0.3366 (0.3749)  Acc@1: 94.1964 (94.5690)  Acc@5: 99.1071 (98.7500)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-115.pth.tar', 94.57600000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-117.pth.tar', 94.56900003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-116.pth.tar', 94.55600000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-113.pth.tar', 94.51700000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-114.pth.tar', 94.48600004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-112.pth.tar', 94.4799999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-109.pth.tar', 94.43800004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-110.pth.tar', 94.42700000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-111.pth.tar', 94.41600000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-108.pth.tar', 94.35899998535156)

Train: 118 [   0/1171 (  0%)]  Loss:  2.708230 (2.7082)  Time: 9.874s,  103.70/s  (9.874s,  103.70/s)  LR: 1.262e-05  Data: 9.101 (9.101)
Train: 118 [  50/1171 (  4%)]  Loss:  3.184416 (2.9463)  Time: 0.586s, 1746.84/s  (2.755s,  371.75/s)  LR: 1.262e-05  Data: 0.020 (2.169)
Train: 118 [ 100/1171 (  9%)]  Loss:  2.979226 (2.9573)  Time: 0.588s, 1742.02/s  (2.487s,  411.67/s)  LR: 1.262e-05  Data: 0.018 (1.897)
Train: 118 [ 150/1171 ( 13%)]  Loss:  2.516021 (2.8470)  Time: 0.585s, 1750.76/s  (2.455s,  417.19/s)  LR: 1.262e-05  Data: 0.019 (1.864)
Train: 118 [ 200/1171 ( 17%)]  Loss:  2.552993 (2.7882)  Time: 0.587s, 1743.63/s  (2.413s,  424.42/s)  LR: 1.262e-05  Data: 0.022 (1.823)
Train: 118 [ 250/1171 ( 21%)]  Loss:  2.654358 (2.7659)  Time: 0.588s, 1741.62/s  (2.381s,  430.15/s)  LR: 1.262e-05  Data: 0.022 (1.792)
Train: 118 [ 300/1171 ( 26%)]  Loss:  2.673419 (2.7527)  Time: 0.587s, 1744.82/s  (2.438s,  420.05/s)  LR: 1.262e-05  Data: 0.021 (1.850)
Train: 118 [ 350/1171 ( 30%)]  Loss:  3.276753 (2.8182)  Time: 0.589s, 1738.10/s  (2.395s,  427.48/s)  LR: 1.262e-05  Data: 0.019 (1.808)
Train: 118 [ 400/1171 ( 34%)]  Loss:  2.703907 (2.8055)  Time: 1.472s,  695.44/s  (2.368s,  432.45/s)  LR: 1.262e-05  Data: 0.885 (1.780)
Train: 118 [ 450/1171 ( 38%)]  Loss:  3.022943 (2.8272)  Time: 5.242s,  195.34/s  (2.410s,  424.97/s)  LR: 1.262e-05  Data: 4.541 (1.819)
Train: 118 [ 500/1171 ( 43%)]  Loss:  2.854869 (2.8297)  Time: 0.589s, 1739.59/s  (2.419s,  423.35/s)  LR: 1.262e-05  Data: 0.018 (1.828)
Train: 118 [ 550/1171 ( 47%)]  Loss:  2.967596 (2.8412)  Time: 7.189s,  142.43/s  (2.422s,  422.87/s)  LR: 1.262e-05  Data: 6.607 (1.830)
Train: 118 [ 600/1171 ( 51%)]  Loss:  3.319067 (2.8780)  Time: 0.585s, 1749.93/s  (2.423s,  422.55/s)  LR: 1.262e-05  Data: 0.020 (1.832)
Train: 118 [ 650/1171 ( 56%)]  Loss:  3.149507 (2.8974)  Time: 7.850s,  130.44/s  (2.419s,  423.24/s)  LR: 1.262e-05  Data: 7.267 (1.827)
Train: 118 [ 700/1171 ( 60%)]  Loss:  2.958299 (2.9014)  Time: 0.587s, 1745.67/s  (2.404s,  425.95/s)  LR: 1.262e-05  Data: 0.019 (1.812)
Train: 118 [ 750/1171 ( 64%)]  Loss:  3.110341 (2.9145)  Time: 5.327s,  192.23/s  (2.396s,  427.46/s)  LR: 1.262e-05  Data: 4.758 (1.803)
Train: 118 [ 800/1171 ( 68%)]  Loss:  2.574173 (2.8945)  Time: 0.585s, 1750.91/s  (2.411s,  424.74/s)  LR: 1.262e-05  Data: 0.019 (1.817)
Train: 118 [ 850/1171 ( 73%)]  Loss:  2.318748 (2.8625)  Time: 4.822s,  212.36/s  (2.404s,  425.94/s)  LR: 1.262e-05  Data: 4.215 (1.809)
Train: 118 [ 900/1171 ( 77%)]  Loss:  2.660803 (2.8519)  Time: 0.586s, 1746.36/s  (2.403s,  426.14/s)  LR: 1.262e-05  Data: 0.020 (1.808)
Train: 118 [ 950/1171 ( 81%)]  Loss:  3.296873 (2.8741)  Time: 3.391s,  301.97/s  (2.398s,  427.03/s)  LR: 1.262e-05  Data: 2.736 (1.801)
Train: 118 [1000/1171 ( 85%)]  Loss:  2.782221 (2.8698)  Time: 0.590s, 1736.83/s  (2.393s,  427.96/s)  LR: 1.262e-05  Data: 0.024 (1.796)
Train: 118 [1050/1171 ( 90%)]  Loss:  2.467022 (2.8514)  Time: 5.304s,  193.07/s  (2.389s,  428.64/s)  LR: 1.262e-05  Data: 4.645 (1.790)
Train: 118 [1100/1171 ( 94%)]  Loss:  3.001115 (2.8580)  Time: 0.592s, 1729.12/s  (2.379s,  430.42/s)  LR: 1.262e-05  Data: 0.021 (1.780)
Train: 118 [1150/1171 ( 98%)]  Loss:  2.798294 (2.8555)  Time: 6.148s,  166.57/s  (2.375s,  431.19/s)  LR: 1.262e-05  Data: 5.464 (1.775)
Train: 118 [1170/1171 (100%)]  Loss:  3.305871 (2.8735)  Time: 0.565s, 1812.80/s  (2.390s,  428.50/s)  LR: 1.262e-05  Data: 0.000 (1.790)
Test: [   0/97]  Time: 17.283 (17.283)  Loss:  0.2888 (0.2888)  Acc@1: 96.6797 (96.6797)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.200 (3.155)  Loss:  0.4562 (0.3581)  Acc@1: 92.0898 (95.1727)  Acc@5: 98.3398 (98.9239)
Test: [  97/97]  Time: 0.120 (3.065)  Loss:  0.3233 (0.3712)  Acc@1: 94.7917 (94.6160)  Acc@5: 99.2560 (98.7530)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-118.pth.tar', 94.61600003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-115.pth.tar', 94.57600000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-117.pth.tar', 94.56900003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-116.pth.tar', 94.55600000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-113.pth.tar', 94.51700000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-114.pth.tar', 94.48600004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-112.pth.tar', 94.4799999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-109.pth.tar', 94.43800004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-110.pth.tar', 94.42700000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-111.pth.tar', 94.41600000976563)

Train: 119 [   0/1171 (  0%)]  Loss:  3.220915 (3.2209)  Time: 10.948s,   93.54/s  (10.948s,   93.54/s)  LR: 1.148e-05  Data: 10.266 (10.266)
Train: 119 [  50/1171 (  4%)]  Loss:  2.160746 (2.6908)  Time: 0.588s, 1741.44/s  (2.380s,  430.34/s)  LR: 1.148e-05  Data: 0.022 (1.779)
Train: 119 [ 100/1171 (  9%)]  Loss:  2.565997 (2.6492)  Time: 2.850s,  359.31/s  (2.313s,  442.77/s)  LR: 1.148e-05  Data: 2.249 (1.719)
Train: 119 [ 150/1171 ( 13%)]  Loss:  2.924883 (2.7181)  Time: 2.068s,  495.06/s  (2.262s,  452.78/s)  LR: 1.148e-05  Data: 1.503 (1.657)
Train: 119 [ 200/1171 ( 17%)]  Loss:  3.109196 (2.7963)  Time: 1.417s,  722.64/s  (2.245s,  456.21/s)  LR: 1.148e-05  Data: 0.852 (1.639)
Train: 119 [ 250/1171 ( 21%)]  Loss:  2.953268 (2.8225)  Time: 0.590s, 1736.80/s  (2.223s,  460.74/s)  LR: 1.148e-05  Data: 0.024 (1.613)
Train: 119 [ 300/1171 ( 26%)]  Loss:  2.796103 (2.8187)  Time: 5.173s,  197.95/s  (2.318s,  441.80/s)  LR: 1.148e-05  Data: 4.561 (1.708)
Train: 119 [ 350/1171 ( 30%)]  Loss:  3.033971 (2.8456)  Time: 0.590s, 1735.35/s  (2.296s,  445.97/s)  LR: 1.148e-05  Data: 0.023 (1.689)
Train: 119 [ 400/1171 ( 34%)]  Loss:  2.630031 (2.8217)  Time: 7.133s,  143.56/s  (2.321s,  441.12/s)  LR: 1.148e-05  Data: 6.534 (1.715)
Train: 119 [ 450/1171 ( 38%)]  Loss:  2.851314 (2.8246)  Time: 0.589s, 1738.16/s  (2.319s,  441.57/s)  LR: 1.148e-05  Data: 0.020 (1.715)
Train: 119 [ 500/1171 ( 43%)]  Loss:  2.531923 (2.7980)  Time: 7.532s,  135.95/s  (2.333s,  439.00/s)  LR: 1.148e-05  Data: 6.860 (1.730)
Train: 119 [ 550/1171 ( 47%)]  Loss:  3.210227 (2.8324)  Time: 0.591s, 1732.62/s  (2.333s,  439.01/s)  LR: 1.148e-05  Data: 0.025 (1.732)
Train: 119 [ 600/1171 ( 51%)]  Loss:  3.046726 (2.8489)  Time: 6.225s,  164.51/s  (2.337s,  438.16/s)  LR: 1.148e-05  Data: 5.656 (1.736)
Train: 119 [ 650/1171 ( 56%)]  Loss:  3.126180 (2.8687)  Time: 0.591s, 1732.50/s  (2.360s,  433.96/s)  LR: 1.148e-05  Data: 0.025 (1.760)
Train: 119 [ 700/1171 ( 60%)]  Loss:  2.772311 (2.8623)  Time: 6.267s,  163.39/s  (2.355s,  434.76/s)  LR: 1.148e-05  Data: 5.634 (1.756)
Train: 119 [ 750/1171 ( 64%)]  Loss:  2.273378 (2.8254)  Time: 0.592s, 1730.97/s  (2.355s,  434.87/s)  LR: 1.148e-05  Data: 0.024 (1.757)
Train: 119 [ 800/1171 ( 68%)]  Loss:  3.257336 (2.8509)  Time: 6.223s,  164.54/s  (2.389s,  428.64/s)  LR: 1.148e-05  Data: 5.658 (1.789)
Train: 119 [ 850/1171 ( 73%)]  Loss:  2.817855 (2.8490)  Time: 0.590s, 1735.44/s  (2.373s,  431.55/s)  LR: 1.148e-05  Data: 0.022 (1.772)
Train: 119 [ 900/1171 ( 77%)]  Loss:  3.041610 (2.8592)  Time: 4.905s,  208.78/s  (2.375s,  431.16/s)  LR: 1.148e-05  Data: 4.340 (1.775)
Train: 119 [ 950/1171 ( 81%)]  Loss:  3.076398 (2.8700)  Time: 0.589s, 1737.40/s  (2.364s,  433.15/s)  LR: 1.148e-05  Data: 0.018 (1.764)
Train: 119 [1000/1171 ( 85%)]  Loss:  3.127167 (2.8823)  Time: 9.786s,  104.64/s  (2.366s,  432.85/s)  LR: 1.148e-05  Data: 9.214 (1.766)
Train: 119 [1050/1171 ( 90%)]  Loss:  2.630394 (2.8708)  Time: 0.587s, 1745.66/s  (2.376s,  430.93/s)  LR: 1.148e-05  Data: 0.021 (1.776)
Train: 119 [1100/1171 ( 94%)]  Loss:  3.111948 (2.8813)  Time: 6.735s,  152.04/s  (2.369s,  432.18/s)  LR: 1.148e-05  Data: 6.161 (1.770)
Train: 119 [1150/1171 ( 98%)]  Loss:  3.014025 (2.8868)  Time: 0.588s, 1741.89/s  (2.371s,  431.97/s)  LR: 1.148e-05  Data: 0.020 (1.771)
Train: 119 [1170/1171 (100%)]  Loss:  3.128269 (2.8965)  Time: 0.566s, 1808.26/s  (2.367s,  432.67/s)  LR: 1.148e-05  Data: 0.000 (1.767)
Test: [   0/97]  Time: 11.883 (11.883)  Loss:  0.2966 (0.2966)  Acc@1: 96.5820 (96.5820)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.201 (2.988)  Loss:  0.4545 (0.3628)  Acc@1: 92.6758 (95.1268)  Acc@5: 98.3398 (98.9373)
Test: [  97/97]  Time: 0.120 (2.913)  Loss:  0.3307 (0.3753)  Acc@1: 94.7917 (94.5910)  Acc@5: 99.2560 (98.7670)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-118.pth.tar', 94.61600003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-119.pth.tar', 94.59100003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-115.pth.tar', 94.57600000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-117.pth.tar', 94.56900003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-116.pth.tar', 94.55600000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-113.pth.tar', 94.51700000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-114.pth.tar', 94.48600004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-112.pth.tar', 94.4799999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-109.pth.tar', 94.43800004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-110.pth.tar', 94.42700000976562)

Train: 120 [   0/1171 (  0%)]  Loss:  2.883981 (2.8840)  Time: 10.319s,   99.23/s  (10.319s,   99.23/s)  LR: 1.066e-05  Data: 9.739 (9.739)
Train: 120 [  50/1171 (  4%)]  Loss:  2.866042 (2.8750)  Time: 0.590s, 1736.43/s  (2.241s,  456.89/s)  LR: 1.066e-05  Data: 0.023 (1.653)
Train: 120 [ 100/1171 (  9%)]  Loss:  2.975475 (2.9085)  Time: 0.589s, 1738.70/s  (2.355s,  434.87/s)  LR: 1.066e-05  Data: 0.018 (1.759)
Train: 120 [ 150/1171 ( 13%)]  Loss:  2.736581 (2.8655)  Time: 0.588s, 1742.15/s  (2.345s,  436.63/s)  LR: 1.066e-05  Data: 0.022 (1.752)
Train: 120 [ 200/1171 ( 17%)]  Loss:  2.358652 (2.7641)  Time: 0.590s, 1734.67/s  (2.344s,  436.87/s)  LR: 1.066e-05  Data: 0.018 (1.752)
Train: 120 [ 250/1171 ( 21%)]  Loss:  3.122369 (2.8239)  Time: 0.648s, 1579.84/s  (2.325s,  440.50/s)  LR: 1.066e-05  Data: 0.017 (1.735)
Train: 120 [ 300/1171 ( 26%)]  Loss:  3.281651 (2.8893)  Time: 0.715s, 1432.32/s  (2.306s,  443.98/s)  LR: 1.066e-05  Data: 0.149 (1.718)
Train: 120 [ 350/1171 ( 30%)]  Loss:  3.222487 (2.9309)  Time: 1.560s,  656.43/s  (2.292s,  446.84/s)  LR: 1.066e-05  Data: 0.995 (1.702)
Train: 120 [ 400/1171 ( 34%)]  Loss:  2.934615 (2.9313)  Time: 0.908s, 1128.15/s  (2.324s,  440.57/s)  LR: 1.066e-05  Data: 0.333 (1.731)
Train: 120 [ 450/1171 ( 38%)]  Loss:  2.852219 (2.9234)  Time: 2.871s,  356.72/s  (2.306s,  443.98/s)  LR: 1.066e-05  Data: 2.203 (1.713)
Train: 120 [ 500/1171 ( 43%)]  Loss:  3.143335 (2.9434)  Time: 0.592s, 1728.54/s  (2.349s,  435.91/s)  LR: 1.066e-05  Data: 0.022 (1.753)
Train: 120 [ 550/1171 ( 47%)]  Loss:  2.553030 (2.9109)  Time: 1.800s,  568.79/s  (2.348s,  436.03/s)  LR: 1.066e-05  Data: 1.172 (1.753)
Train: 120 [ 600/1171 ( 51%)]  Loss:  2.535184 (2.8820)  Time: 0.610s, 1679.64/s  (2.369s,  432.25/s)  LR: 1.066e-05  Data: 0.020 (1.773)
Train: 120 [ 650/1171 ( 56%)]  Loss:  2.198890 (2.8332)  Time: 0.774s, 1323.22/s  (2.360s,  433.84/s)  LR: 1.066e-05  Data: 0.203 (1.765)
Train: 120 [ 700/1171 ( 60%)]  Loss:  2.975425 (2.8427)  Time: 0.585s, 1749.86/s  (2.359s,  434.00/s)  LR: 1.066e-05  Data: 0.018 (1.762)
Train: 120 [ 750/1171 ( 64%)]  Loss:  3.190671 (2.8644)  Time: 0.587s, 1745.40/s  (2.346s,  436.55/s)  LR: 1.066e-05  Data: 0.023 (1.748)
Train: 120 [ 800/1171 ( 68%)]  Loss:  3.393585 (2.8955)  Time: 0.586s, 1748.20/s  (2.341s,  437.51/s)  LR: 1.066e-05  Data: 0.019 (1.744)
Train: 120 [ 850/1171 ( 73%)]  Loss:  2.442356 (2.8704)  Time: 0.590s, 1735.42/s  (2.329s,  439.60/s)  LR: 1.066e-05  Data: 0.024 (1.733)
Train: 120 [ 900/1171 ( 77%)]  Loss:  2.578271 (2.8550)  Time: 0.586s, 1747.97/s  (2.352s,  435.47/s)  LR: 1.066e-05  Data: 0.020 (1.755)
Train: 120 [ 950/1171 ( 81%)]  Loss:  3.283741 (2.8764)  Time: 1.340s,  764.29/s  (2.335s,  438.59/s)  LR: 1.066e-05  Data: 0.769 (1.739)
Train: 120 [1000/1171 ( 85%)]  Loss:  3.112811 (2.8877)  Time: 0.590s, 1734.31/s  (2.345s,  436.66/s)  LR: 1.066e-05  Data: 0.024 (1.749)
Train: 120 [1050/1171 ( 90%)]  Loss:  3.309751 (2.9069)  Time: 0.588s, 1742.63/s  (2.337s,  438.17/s)  LR: 1.066e-05  Data: 0.022 (1.741)
Train: 120 [1100/1171 ( 94%)]  Loss:  2.533960 (2.8907)  Time: 0.585s, 1749.96/s  (2.338s,  437.98/s)  LR: 1.066e-05  Data: 0.020 (1.742)
Train: 120 [1150/1171 ( 98%)]  Loss:  3.196804 (2.9034)  Time: 0.589s, 1739.33/s  (2.329s,  439.70/s)  LR: 1.066e-05  Data: 0.020 (1.734)
Train: 120 [1170/1171 (100%)]  Loss:  2.980457 (2.9065)  Time: 0.569s, 1799.61/s  (2.326s,  440.19/s)  LR: 1.066e-05  Data: 0.000 (1.731)
Test: [   0/97]  Time: 12.460 (12.460)  Loss:  0.2983 (0.2983)  Acc@1: 96.4844 (96.4844)  Acc@5: 99.5117 (99.5117)
Test: [  50/97]  Time: 0.199 (2.888)  Loss:  0.4703 (0.3663)  Acc@1: 92.2852 (95.1555)  Acc@5: 98.2422 (98.9220)
Test: [  97/97]  Time: 0.120 (3.185)  Loss:  0.3390 (0.3788)  Acc@1: 94.3452 (94.6090)  Acc@5: 99.2560 (98.7620)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-118.pth.tar', 94.61600003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-120.pth.tar', 94.6089999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-119.pth.tar', 94.59100003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-115.pth.tar', 94.57600000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-117.pth.tar', 94.56900003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-116.pth.tar', 94.55600000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-113.pth.tar', 94.51700000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-114.pth.tar', 94.48600004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-112.pth.tar', 94.4799999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-109.pth.tar', 94.43800004394531)

Train: 121 [   0/1171 (  0%)]  Loss:  2.988004 (2.9880)  Time: 15.001s,   68.26/s  (15.001s,   68.26/s)  LR: 1.016e-05  Data: 13.776 (13.776)
Train: 121 [  50/1171 (  4%)]  Loss:  2.607184 (2.7976)  Time: 0.586s, 1747.73/s  (2.617s,  391.26/s)  LR: 1.016e-05  Data: 0.020 (1.991)
Train: 121 [ 100/1171 (  9%)]  Loss:  2.859703 (2.8183)  Time: 1.643s,  623.11/s  (2.411s,  424.71/s)  LR: 1.016e-05  Data: 0.980 (1.799)
Train: 121 [ 150/1171 ( 13%)]  Loss:  3.275757 (2.9327)  Time: 0.591s, 1731.19/s  (2.356s,  434.58/s)  LR: 1.016e-05  Data: 0.018 (1.749)
Train: 121 [ 200/1171 ( 17%)]  Loss:  2.630858 (2.8723)  Time: 2.195s,  466.58/s  (2.321s,  441.16/s)  LR: 1.016e-05  Data: 1.586 (1.718)
Train: 121 [ 250/1171 ( 21%)]  Loss:  2.940312 (2.8836)  Time: 0.584s, 1753.07/s  (2.286s,  447.97/s)  LR: 1.016e-05  Data: 0.019 (1.680)
Train: 121 [ 300/1171 ( 26%)]  Loss:  2.680456 (2.8546)  Time: 0.591s, 1733.23/s  (2.273s,  450.54/s)  LR: 1.016e-05  Data: 0.025 (1.668)
Train: 121 [ 350/1171 ( 30%)]  Loss:  2.480785 (2.8079)  Time: 0.587s, 1743.10/s  (2.293s,  446.55/s)  LR: 1.016e-05  Data: 0.023 (1.689)
Train: 121 [ 400/1171 ( 34%)]  Loss:  2.743855 (2.8008)  Time: 1.246s,  822.01/s  (2.307s,  443.88/s)  LR: 1.016e-05  Data: 0.681 (1.705)
Train: 121 [ 450/1171 ( 38%)]  Loss:  2.970289 (2.8177)  Time: 0.588s, 1741.78/s  (2.300s,  445.13/s)  LR: 1.016e-05  Data: 0.020 (1.700)
Train: 121 [ 500/1171 ( 43%)]  Loss:  3.054462 (2.8392)  Time: 2.608s,  392.71/s  (2.322s,  440.98/s)  LR: 1.016e-05  Data: 1.960 (1.722)
Train: 121 [ 550/1171 ( 47%)]  Loss:  2.585747 (2.8181)  Time: 0.587s, 1745.50/s  (2.332s,  439.20/s)  LR: 1.016e-05  Data: 0.021 (1.733)
Train: 121 [ 600/1171 ( 51%)]  Loss:  3.070303 (2.8375)  Time: 0.751s, 1362.98/s  (2.341s,  437.51/s)  LR: 1.016e-05  Data: 0.068 (1.742)
Train: 121 [ 650/1171 ( 56%)]  Loss:  2.973763 (2.8472)  Time: 0.587s, 1743.11/s  (2.334s,  438.80/s)  LR: 1.016e-05  Data: 0.022 (1.736)
Train: 121 [ 700/1171 ( 60%)]  Loss:  2.822999 (2.8456)  Time: 0.586s, 1747.24/s  (2.332s,  439.19/s)  LR: 1.016e-05  Data: 0.021 (1.734)
Train: 121 [ 750/1171 ( 64%)]  Loss:  2.893414 (2.8486)  Time: 0.589s, 1738.69/s  (2.354s,  435.06/s)  LR: 1.016e-05  Data: 0.022 (1.756)
Train: 121 [ 800/1171 ( 68%)]  Loss:  3.305970 (2.8755)  Time: 0.584s, 1752.10/s  (2.345s,  436.60/s)  LR: 1.016e-05  Data: 0.019 (1.749)
Train: 121 [ 850/1171 ( 73%)]  Loss:  2.895163 (2.8766)  Time: 0.586s, 1746.92/s  (2.352s,  435.34/s)  LR: 1.016e-05  Data: 0.019 (1.756)
Train: 121 [ 900/1171 ( 77%)]  Loss:  2.896579 (2.8777)  Time: 0.589s, 1737.68/s  (2.369s,  432.18/s)  LR: 1.016e-05  Data: 0.023 (1.774)
Train: 121 [ 950/1171 ( 81%)]  Loss:  2.658414 (2.8667)  Time: 0.590s, 1735.35/s  (2.363s,  433.33/s)  LR: 1.016e-05  Data: 0.020 (1.768)
Train: 121 [1000/1171 ( 85%)]  Loss:  3.005247 (2.8733)  Time: 0.587s, 1745.63/s  (2.355s,  434.73/s)  LR: 1.016e-05  Data: 0.020 (1.761)
Train: 121 [1050/1171 ( 90%)]  Loss:  3.042958 (2.8810)  Time: 0.588s, 1741.77/s  (2.347s,  436.31/s)  LR: 1.016e-05  Data: 0.023 (1.752)
Train: 121 [1100/1171 ( 94%)]  Loss:  2.893498 (2.8816)  Time: 0.590s, 1735.04/s  (2.353s,  435.14/s)  LR: 1.016e-05  Data: 0.024 (1.759)
Train: 121 [1150/1171 ( 98%)]  Loss:  2.696871 (2.8739)  Time: 0.586s, 1748.07/s  (2.351s,  435.54/s)  LR: 1.016e-05  Data: 0.019 (1.757)
Train: 121 [1170/1171 (100%)]  Loss:  3.218832 (2.8877)  Time: 0.566s, 1810.55/s  (2.347s,  436.39/s)  LR: 1.016e-05  Data: 0.000 (1.752)
Test: [   0/97]  Time: 11.734 (11.734)  Loss:  0.2976 (0.2976)  Acc@1: 96.7773 (96.7773)  Acc@5: 99.4141 (99.4141)
Test: [  50/97]  Time: 0.199 (3.049)  Loss:  0.4569 (0.3641)  Acc@1: 92.3828 (95.1517)  Acc@5: 98.3398 (98.9028)
Test: [  97/97]  Time: 0.120 (2.975)  Loss:  0.3312 (0.3757)  Acc@1: 94.6429 (94.5910)  Acc@5: 99.2560 (98.7500)
Current checkpoints:
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-118.pth.tar', 94.61600003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-120.pth.tar', 94.6089999975586)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-119.pth.tar', 94.59100003417969)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-121.pth.tar', 94.59100002197266)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-115.pth.tar', 94.57600000976562)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-117.pth.tar', 94.56900003662109)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-116.pth.tar', 94.55600000976563)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-113.pth.tar', 94.51700000732421)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-114.pth.tar', 94.48600004394531)
 ('train_result/PreTraining_vit_deit_tiny_patch16_224_fake_1k/checkpoint-112.pth.tar', 94.4799999975586)

*** Best metric: 94.61600003417969 (epoch 118)

wandb: Waiting for W&B process to finish, PID 32435
wandb: Program ended successfully.
wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)wandb: \ 0.00MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: \ 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: | 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: / 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb: - 0.01MB of 0.01MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Find user logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210531_140256-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1/logs/debug.log
wandb: Find internal logs for this run at: /gs/hs0/tga-i/sugiyama.y.al/TIMM/pytorch-image-models/wandb/run-20210531_140256-PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1/logs/debug-internal.log
wandb: Run summary:
wandb:        epoch 121
wandb:     _runtime 395177
wandb:    eval_loss 0.37573
wandb:    eval_top1 94.591
wandb:    eval_top5 98.75
wandb:   _timestamp 1622505181
wandb:   train_loss 2.88766
wandb:        _step 121
wandb: Run history:
wandb:        epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   train_loss ‚ñÖ‚ñá‚ñÖ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÅ‚ñá‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb:    eval_loss ‚ñÉ‚ñÇ‚ñà‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ
wandb:    eval_top1 ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:    eval_top5 ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÖ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá
wandb:     _runtime ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:   _timestamp ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb:        _step ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà
wandb: 
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: 
wandb: Synced PreTraining_vit_deit_tiny_patch16_224_fake_1k: https://wandb.ai/gyama_x/pytorch-image-models/runs/PreTraining_vit_deit_tiny_patch16_224_fake_1k_v1
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
--End--
Tue Jun 1 08:53:11 JST 2021
